[
  {
    "link": "watch?v=kCc8FmEb1nY",
    "title": "Let's build GPT: from scratch, in code, spelled out.",
    "tags": "deep learning, neural network, language model, pytorch, gpt, chatgpt, openai, generatively, pretrained, transformer, attention is all you need, self",
    "scraped_at": 1684583413.0888941,
    "genre": "Science",
    "views": "2432627",
    "desc": "We build a Generatively Pretrained Transformer (GPT), following the paper \\\\\"",
    "lengthSeconds": "6980",
    "uploadDate": "2023-01-17",
    "thumbnail_url": "https://i.ytimg.com/vi/kCc8FmEb1nY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=t3YJ5hKiMQ0",
    "title": "Building makemore Part 5: Building a WaveNet",
    "tags": "deep learning, neural network, language model, tensors, pytorch, convolution",
    "scraped_at": 1684583412.6439202,
    "genre": "Science",
    "views": "87841",
    "desc": "We take the 2-layer MLP from previous video and make it deeper with a tree-like structure, arriving at a convolutional neural network architecture similar to the WaveNet (2016) from DeepMind. In the WaveNet paper, the same hierarchical architecture is implemented more efficiently using causal dilated convolutions (not yet covered). Along the way we get a better sense of torch.nn and what it is and how it works under the hood, and what a typical deep learning development process looks like (a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...).\\\\n\\\\nLinks:\\\\n- makemore on github: https://github.com/karpathy/makemore\\\\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb\\\\n- collab notebook: https://colab.research.google.com/drive/1CXVEmCO_7r7WYZGb5qnjfyxTvQa13g5X?usp=sharing\\\\n- my website: https://karpathy.ai\\\\n- my twitter: https://twitter.com/karpathy\\\\n- our Discord channel: https://discord.gg/3zy8kqD9Cp\\\\n\\\\nSupplementary links:\\\\n- WaveNet 2016 from DeepMind https://arxiv.org/abs/1609.03499\\\\n- Bengio et al. 2003 MLP LM https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf \\\\n\\\\nChapters:\\\\nintro\\\\n00:00:00 intro\\\\n00:01:40 starter code walkthrough\\\\n00:06:56 let\\xe2\\x80\\x99s fix the learning rate plot\\\\n00:09:16 pytorchifying our code: layers, containers, torch.nn, fun bugs\\\\nimplementing wavenet\\\\n00:17:11 overview: WaveNet\\\\n00:19:33 dataset bump the context size to 8\\\\n00:19:55 re-running baseline code on block_size 8\\\\n00:21:36 implementing WaveNet\\\\n00:37:41 training the WaveNet: first pass\\\\n00:38:50 fixing batchnorm1d bug\\\\n00:45:21 re-training WaveNet with bug fix\\\\n00:46:07 scaling up our WaveNet\\\\nconclusions\\\\n00:46:58 experimental harness\\\\n00:47:44 WaveNet but with \\xe2\\x80\\x9cdilated causal convolutions\\xe2\\x80\\x9d\\\\n00:51:34 torch.nn\\\\n00:52:28 the development process of building deep neural nets\\\\n00:54:17 going forward\\\\n00:55:26 improve on my loss! how far can we improve a WaveNet on this data?\"",
    "lengthSeconds": "3381",
    "uploadDate": "2022-11-20",
    "thumbnail_url": "https://i.ytimg.com/vi/t3YJ5hKiMQ0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=q8SA3rM6ckI",
    "title": "Building makemore Part 4: Becoming a Backprop Ninja",
    "tags": "deep learning, backpropagation, neural network, language model, chain rule, tensors",
    "scraped_at": 1684583412.4948945,
    "genre": "Science",
    "views": "77387",
    "desc": "We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd\\'s loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.\\\\n\\\\n!!!!!!!!!!!!\\\\nI recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is here:\\\\nhttps://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\\\\n!!!!!!!!!!!!\\\\n\\\\nLinks:\\\\n- makemore on github: https://github.com/karpathy/makemore\\\\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb\\\\n- collab notebook: https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing\\\\n- my website: https://karpathy.ai\\\\n- my twitter: https://twitter.com/karpathy\\\\n- our Discord channel: https://discord.gg/3zy8kqD9Cp\\\\n\\\\nSupplementary links:\\\\n- Yes you should understand backprop: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\\\\n- BatchNorm paper: https://arxiv.org/abs/1502.03167\\\\n- Bessel\\xe2\\x80\\x99s Correction: http://math.oxford.emory.edu/site/math117/besselCorrection/\\\\n- Bengio et al. 2003 MLP LM https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf \\\\n\\\\nChapters:\\\\n00:00:00 intro: why you should care \\\\u0026 fun history\\\\n00:07:26 starter code\\\\n00:13:01 exercise 1: backproping the atomic compute graph\\\\n01:05:17 brief digression: bessel\\xe2\\x80\\x99s correction in batchnorm\\\\n01:26:31 exercise 2: cross entropy loss backward pass\\\\n01:36:37 exercise 3: batch norm layer backward pass\\\\n01:50:02 exercise 4: putting it all together\\\\n01:54:24 outro\"",
    "lengthSeconds": "6924",
    "uploadDate": "2022-10-11",
    "thumbnail_url": "https://i.ytimg.com/vi/q8SA3rM6ckI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=P6sfmUTpUmc",
    "title": "Building makemore Part 3: Activations & Gradients, BatchNorm",
    "tags": "neural network, deep learning, makemore, batchnorm, batch normalization, pytorch",
    "scraped_at": 1684583412.853895,
    "genre": "Science",
    "views": "112348",
    "desc": "We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you\\'d want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.\\\\n\\\\nLinks:\\\\n- makemore on github: https://github.com/karpathy/makemore\\\\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb\\\\n- collab notebook: https://colab.research.google.com/drive/1H5CSy-OnisagUgDUXhHwo1ng2pjKHYSN?usp=sharing\\\\n- my website: https://karpathy.ai\\\\n- my twitter: https://twitter.com/karpathy\\\\n- Discord channel: https://discord.gg/3zy8kqD9Cp\\\\n\\\\nUseful links:\\\\n- \\\\\"",
    "lengthSeconds": "6957",
    "uploadDate": "2022-10-04",
    "thumbnail_url": "https://i.ytimg.com/vi/P6sfmUTpUmc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=TCH_1BHY58I",
    "title": "Building makemore Part 2: MLP",
    "tags": "deep learning, neural network, multilayer perceptron, nlp, language model",
    "scraped_at": 1684583413.166895,
    "genre": "Science",
    "views": "131671",
    "desc": "We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).\\\\n\\\\nLinks:\\\\n- makemore on github: https://github.com/karpathy/makemore\\\\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb\\\\n- collab notebook (new)!!!: https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing\\\\n- Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\\\\n- my website: https://karpathy.ai\\\\n- my twitter: https://twitter.com/karpathy\\\\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/3zy8kqD9Cp , for people who\\'d like to chat more and go beyond youtube comments\\\\n\\\\nUseful links:\\\\n- PyTorch internals ref http://blog.ezyang.com/2019/05/pytorch-internals/\\\\n\\\\nExercises:\\\\n- E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2\\\\n- E02: I was not careful with the intialization of the network in this video. (1) What is the loss you\\'d get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?\\\\n- E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?\\\\n\\\\nChapters:\\\\n00:00:00 intro\\\\n00:01:48 Bengio et al. 2003 (MLP language model) paper walkthrough\\\\n00:09:03 (re-)building our training dataset\\\\n00:12:19 implementing the embedding lookup table\\\\n00:18:35 implementing the hidden layer + internals of torch.Tensor: storage, views\\\\n00:29:15 implementing the output layer\\\\n00:29:53 implementing the negative log likelihood loss\\\\n00:32:17 summary of the full network\\\\n00:32:49 introducing F.cross_entropy and why\\\\n00:37:56 implementing the training loop, overfitting one batch\\\\n00:41:25 training on the full dataset, minibatches\\\\n00:45:40 finding a good initial learning rate\\\\n00:53:20 splitting up the dataset into train/val/test splits and why\\\\n01:00:49 experiment: larger hidden layer\\\\n01:05:27 visualizing the character embeddings\\\\n01:07:16 experiment: larger embedding size\\\\n01:11:46 summary of our final code, conclusion\\\\n01:13:24 sampling from the model\\\\n01:14:55 google collab (new!!) notebook advertisement\"",
    "lengthSeconds": "4539",
    "uploadDate": "2022-09-12",
    "thumbnail_url": "https://i.ytimg.com/vi/TCH_1BHY58I/maxresdefault.jpg"
  },
  {
    "link": "watch?v=PaCmpygFfXo",
    "title": "The spelled-out intro to language modeling: building makemore",
    "tags": "deep learning, language model, gpt, bigram, neural network, pytorch, torch, tensor",
    "scraped_at": 1684583413.0008943,
    "genre": "Science",
    "views": "250694",
    "desc": "We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).\\\\n\\\\nLinks:\\\\n- makemore on github: https://github.com/karpathy/makemore\\\\n- jupyter notebook I built in this video: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb\\\\n- my website: https://karpathy.ai\\\\n- my twitter: https://twitter.com/karpathy\\\\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/3zy8kqD9Cp , for people who\\'d like to chat more and go beyond youtube comments\\\\n\\\\nUseful links for practice:\\\\n- Python + Numpy tutorial from CS231n https://cs231n.github.io/python-numpy-tutorial/ . We use torch.tensor instead of numpy.array in this video. Their design (e.g. broadcasting, data types, etc.) is so similar that practicing one is basically practicing the other, just be careful with some of the APIs - how various functions are named, what arguments they take, etc. - these details can vary.\\\\n- PyTorch tutorial on Tensor https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\\\\n- Another PyTorch intro to Tensor https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html\\\\n\\\\nExercises:\\\\nE01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\\\\nE02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\\\\nE03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\\\\nE04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\\\\nE05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we\\'d prefer to use F.cross_entropy instead?\\\\nE06: meta-exercise! Think of a fun/interesting exercise and complete it.\\\\n\\\\nChapters:\\\\n00:00:00 intro\\\\n00:03:03 reading and exploring the dataset\\\\n00:06:24 exploring the bigrams in the dataset\\\\n00:09:24 counting bigrams in a python dictionary\\\\n00:12:45 counting bigrams in a 2D torch tensor (\\\\\"",
    "lengthSeconds": "7065",
    "uploadDate": "2022-09-07",
    "thumbnail_url": "https://i.ytimg.com/vi/PaCmpygFfXo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kVpDARqZdrQ",
    "title": "Stable diffusion dreams of psychedelic faces",
    "tags": "film, udost",
    "scraped_at": 1684583412.9179225,
    "genre": "People",
    "views": "21363",
    "desc": "Prompt: \\\\\"",
    "lengthSeconds": "241",
    "uploadDate": "2022-08-19",
    "thumbnail_url": "https://i.ytimg.com/vi/kVpDARqZdrQ/hqdefault.jpg"
  },
  {
    "link": "watch?v=2oKjtvYslMY",
    "title": "Stable diffusion dreams of steampunk brains",
    "tags": "film, udost",
    "scraped_at": 1684583412.5619214,
    "genre": "People",
    "views": "16860",
    "desc": "Prompt: \\\\\"",
    "lengthSeconds": "1166",
    "uploadDate": "2022-08-17",
    "thumbnail_url": "https://i.ytimg.com/vi/2oKjtvYslMY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=sM9bozW295Q",
    "title": "Stable diffusion dreams of tattoos",
    "tags": "film, udost",
    "scraped_at": 1684583412.774895,
    "genre": "People",
    "views": "42822",
    "desc": "Dreams of tattoos. (There are a few discrete jumps in the video because I had to erase portions that got just a little \\xf0\\x9f\\x8c\\xb6\\xef\\xb8\\x8f, believe I got most of it)\\\\n\\\\nLinks\\\\n- Stable diffusion: https://stability.ai/blog\\\\n- Code used to make this video: https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355\\\\n- My twitter: https://twitter.com/karpathy\"",
    "lengthSeconds": "106",
    "uploadDate": "2022-08-16",
    "thumbnail_url": "https://i.ytimg.com/vi/sM9bozW295Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=VMj-3S1tku0",
    "title": "The spelled-out intro to neural networks and backpropagation: building micrograd",
    "tags": "neural, network, backpropagation, lecture",
    "scraped_at": 1684583413.282921,
    "genre": "Science",
    "views": "760625",
    "desc": "This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.\\\\n\\\\nLinks:\\\\n- micrograd on github: https://github.com/karpathy/micrograd\\\\n- jupyter notebooks I built in this video: https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd\\\\n- my website: https://karpathy.ai\\\\n- my twitter: https://twitter.com/karpathy\\\\n- \\\\\"",
    "lengthSeconds": "8752",
    "uploadDate": "2022-08-16",
    "thumbnail_url": "https://i.ytimg.com/vi/VMj"
  },
  {
    "link": "watch?v=vEnetcj_728",
    "title": "Stable diffusion dreams of \"blueberry spaghetti\" for one night",
    "tags": "film, udost",
    "scraped_at": 1684583412.7018964,
    "genre": "People",
    "views": "28602",
    "desc": "Prompt: \\\\\"",
    "lengthSeconds": "302",
    "uploadDate": "2022-08-16",
    "thumbnail_url": "https://i.ytimg.com/vi/vEnetcj_728/maxresdefault.jpg"
  }
]