[
  {
    "link": "watch?v=WCxYHGc4kd0",
    "title": "Can AI Already Do Your Job? | GPT-4 | Auto-GPT",
    "tags": "film, udost",
    "scraped_at": 1684585910.4520278,
    "genre": "Science",
    "views": "2433",
    "desc": "In this video, we\\'ll explore the question of whether AI can already do your or my job. Sounds a bit scary, doesn\\'t it? To do so, we\\'ll use the Auto-GPT library, which is an extension of the GPT-4 model and basically builds a fully autonomous agent (that you can control though, don\\'t worry), with the GPT-4 model being the brain of the agent. Since Microsoft researchers have found in an in-depth study that GPT-4 can be considered an early version of an artificial general intelligence (AGI), the question of whether an autonomous agent using the GPT-4 model can do your or my job is no longer so utopian. If this has caught your attention, make sure to watch the whole video. :-)\\\\n\\\\n\\\\nMy Medium Article for This Video:\\\\nhttps://medium.com/p/1a33ca0fc6d1\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:01:48 Motivation\\\\n00:03:05 GPT-4 Limitations\\\\n00:05:13 Auto-GPT Features\\\\n00:06:09 Vector Databases\\\\n00:10:06 Auto-GPT Agent Summary\\\\n00:10:45 Tasks To Ask The Agent\\\\n00:11:46 Results\\\\n00:22:31 Outro\\\\n\\\\n\\\\nReferences\\\\n\\\\nAuto-GPT: https://github.com/Significant-Gravitas/Auto-GPT\\\\nbabyagi: https://github.com/yoheinakajima/babyagi\\\\nChatGPT: https://chat.openai.com/\\\\nGPT-4 Information: https://openai.com/research/gpt-4\\\\nVector Database: https://www.pinecone.io/learn/vector-database/\\\\nEmbeddings: https://www.pinecone.io/learn/dense-vector-embeddings-nlp/\\\\nAuto-GPT Tweet: https://twitter.com/SullyOmarr/status/1645205292756418562\\\\nSparks of AGI Paper: https://arxiv.org/pdf/2303.12712.pdf\\\\nCode Highlighter: https://carbon.now.sh/\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\n\\\\nOf course, financial support is completely voluntary, but I was asked for it:\\\\nhttps://patreon.com/MartinThissen\\\\nhttps://ko-fi.com/martinthissen\"",
    "lengthSeconds": "1376",
    "uploadDate": "2023-04-22",
    "thumbnail_url": "https://i.ytimg.com/vi/WCxYHGc4kd0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8i4T5v1Fl_M",
    "title": "5x Faster Voice Cloning | Tortoise-TTS-Fast |\u00a0Tutorial",
    "tags": "film, udost",
    "scraped_at": 1684585911.4090571,
    "genre": "Science",
    "views": "15432",
    "desc": "In this video I will show you how to generate language 5x faster using the Tortoise TTS model. For this we will use the tortoise-tts-fast library. Compared to the original repository, this repository contains some really cool improvements. As I mentioned before, more than five times faster voice cloning, a web UI that makes speech generation super easy and also an option for fine tuning of the Tortoise-TTS model. In this video, I first explain in more detail how these improvements were possible, and then show you how to clone a voice and use it to generate speech. At the end, I\\'ll show you one of my examples. Let me know what you think of it in the comments. :-)\\\\n\\\\n\\\\nMy Medium Article for This Video:\\\\nhttps://medium.com/p/5b8c1d4de975\\\\n\\\\n\\\\nColab notebook for This Video:\\\\nhttps://colab.research.google.com/drive/11FG_ZRdAZ09Euoqc40RiRZyLaRXqjM7b?usp=sharing\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:01:15 KV Cache\\\\n00:03:50 Half Precision\\\\n00:04:45 DPM-Solver\\\\n00:06:04 Low VRAM\\\\n00:07:19 Other Highlights \\\\n00:07:19 Setting Up Tortoise-TTS-Fast\\\\n00:13:16 Working With The Web UI\\\\n00:18:41 Outro\\\\n\\\\n\\\\nReferences\\\\n\\\\nTortoise-TTS: https://github.com/neonbjb/tortoise-tts\\\\nTortoise-TTS-Fast: https://github.com/152334H/tortoise-tts-fast\\\\nIllustration Diffusion Model: https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/\\\\nIllustration Mel Spectrogram: https://www.riffusion.com/about\\\\nDPM-Solver Paper: https://arxiv.org/pdf/2206.00927.pdf\\\\nDPM-Solver++ Paper: https://arxiv.org/pdf/2211.01095.pdf\\\\nKV Cache: https://kipp.ly/blog/transformer-inference-arithmetic/\\\\nNgrok: https://ngrok.com/\\\\nLocalTunnel Repository: https://github.com/localtunnel/localtunnel\\\\n\\\\n\\\\nVideos Used for the Outro\\\\n\\\\nVideo by Al d\\'Vilas: https://www.pexels.com/video/timelapse-video-of-the-sunrise-5799104/\\\\nVideo by Shah Jahan: https://www.pexels.com/video/lion-standing-on-the-field-5446310/\\\\nVideo by Mikhail Nilov: https://www.pexels.com/video/a-man-looking-at-computer-monitor-6962343/\\\\nVideo by Ivan Samkov: https://www.pexels.com/video/man-watching-online-yoga-lessons-6447692/\\\\nVideo by PNW Production: https://www.pexels.com/video/a-woman-holding-a-camera-9218155/\\\\nVideo by Kampus Production: https://www.pexels.com/video/a-woman-sitting-at-the-table-7514225/\\\\n\\\\nThe script for the outro was generated by ChatGPT.\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\n\\\\nOf course, financial support is completely voluntary, but I was asked for it:\\\\nhttps://patreon.com/MartinThissen\\\\nhttps://ko-fi.com/martinthissen\"",
    "lengthSeconds": "1193",
    "uploadDate": "2023-04-20",
    "thumbnail_url": "https://i.ytimg.com/vi/8i4T5v1Fl_M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=F_pFH-AngoE",
    "title": "Run Vicuna-13B On Your Local Computer \ud83e\udd2f | Tutorial\u00a0(GPU)",
    "tags": "film, udost",
    "scraped_at": 1684585910.6980286,
    "genre": "Science",
    "views": "30300",
    "desc": "In this video, I\\'ll show you how to install and interact with the Vicuna-13B model, which is the best free chat bot according to GPT-4. But that\\'s not all - I\\'ve created a fork that allows you to use a quantized version of the model, which has yielded amazing results (and solved issues encountered in my last video). I\\'ll show you how you can install the forked repository and start using the Vicuna-13B on your local computer. Also, I will show you how you can interact with the Vicuna-13B model using either a CLI or Web UI. This will allow you to explore the capabilities of the Vicuna-13B model, which in my opinion is way better than the Alpaca model. But let me know in the comments what you think. :-)\\\\n\\\\n\\\\nMy Medium Article for This Video:\\\\nhttps://medium.com/p/ec6eb513a717\\\\n\\\\n\\\\nColab notebook for This Video:\\\\nhttps://colab.research.google.com/drive/1KlgMCJvZKD8Gnmll1tugF6xAD-Qv51ww?usp=sharing\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:00:39 Vicuna Fork\\\\n00:01:43 How To Install\\\\n00:04:14 Usage of the Vicuna Model\\\\n00:09:36 Time to Get Creative\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\n\\\\nOf course, financial support is completely voluntary, but I was asked for it:\\\\nhttps://patreon.com/MartinThissen\\\\nhttps://ko-fi.com/martinthissen\"",
    "lengthSeconds": "620",
    "uploadDate": "2023-04-05",
    "thumbnail_url": "https://i.ytimg.com/vi/F_pFH"
  },
  {
    "link": "watch?v=jb4r1CL2tcc",
    "title": "Run Vicuna on Your CPU & GPU | Best Free Chatbot According to GPT-4",
    "tags": "film, udost",
    "scraped_at": 1684585910.5450292,
    "genre": "Science",
    "views": "17707",
    "desc": "In this video, we will take a deep dive into the Vicuna model. In the start I will give you an overview of the model and how it got trained and evaluated. Next, I will show you how you can install a quantised GPU version of the Vicuna model that only requires less than 12GB VRAM, compared to the 28GB VRAM required by the full precision version. I will guide you through the installation process and highlight some of the benefits of using the quantised GPU version. Finally, I will show you how to install and run the Vicuna model on your local computer using only the CPU and requiring around 10GB RAM. I will provide step-by-step instructions to help you get started, and highlight some of the challenges you may encounter during the installation process. Enjoy! :-)\\\\n\\\\n\\\\nMy Medium Article for This Video (WIP):\\\\nhttps://medium.com/p/c24b322a193a\\\\n\\\\nI also created a Colab notebook for the quantised GPU version, feel free to check it out:\\\\nhttps://colab.research.google.com/drive/17EAqbCMDDDjAbMVFBiKohzyL4XfcCqYL?usp=sharing\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:00:10 Vicuna Model\\\\n00:04:59 Model Weights\\\\n00:08:18 GPU Installation\\\\n00:14:32 CPU Installation\\\\n00:17:12 Outro\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\n\\\\nOf course, financial support is completely voluntary, but I was asked for it:\\\\nhttps://patreon.com/MartinThissen\\\\nhttps://ko-fi.com/martinthissen\"",
    "lengthSeconds": "1083",
    "uploadDate": "2023-04-04",
    "thumbnail_url": "https://i.ytimg.com/vi/jb4r1CL2tcc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yTROqe8T_eA",
    "title": "How To Fine-Tune the Alpaca Model For Any Language | ChatGPT Alternative",
    "tags": "film, udost",
    "scraped_at": 1684585910.3710308,
    "genre": "Science",
    "views": "24482",
    "desc": "In this video I will show you how to fine-tune the Alpaca model for any language. And it only costs $3! How did I figure this out? Watch the whole video to understand. I\\'ll show you how to translate the cleaned Alpaca dataset. We will then use the translated dataset to fine-tune the Alpaca model (not the LLaMA model) for our desired language. For this, we will either use DeepL or ChatGPT. Also, I will show you how to evaluate the quality of your fine-tuned model. Last but not least you will learn how you can interact with your fine-tuned model in a UI. As always, if you have any questions, don\\'t hesitate to reach out. Enjoy! :-)\\\\n\\\\n\\\\nMy Medium Article for This Video:\\\\nhttps://medium.com/p/370f63753f94\\\\n\\\\nMedium Article Showing the Evaluation Results:\\\\nhttps://medium.com/p/8e363a0a99ca\\\\n\\\\nGitHub Repository for This Video:\\\\nhttps://github.com/thisserand/alpaca-lora-finetune-language\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:01:52 Calculating Estimated Costs\\\\n00:09:03 Decision Making\\\\n00:10:03 Creating A Subset Dataset\\\\n00:12:00 Dataset Translation\\\\n00:17:36 Fine-Tuning the Alpaca Model\\\\n00:26:15 Model Inference\\\\n00:27:18 How Much Training Data Do We Need?\\\\n00:31:24 Evaluation\\\\n00:37:32 Outro\\\\n\\\\n\\\\nReferences:\\\\nSelf-Instruct Repository: https://github.com/yizhongw/self-instruct\\\\nAlpaca Blog Post: https://crfm.stanford.edu/2023/03/13/...\\\\nAlpaca Repository: https://github.com/tatsu-lab/stanford_alpaca\\\\nAlpaca-LoRA: https://github.com/tloen/alpaca-lora\\\\nAlpacaDataCleaned Repository: https://github.com/gururise/AlpacaDataCleaned\\\\nGPT-3.5 Documentation: https://platform.openai.com/docs/models/gpt-3-5\\\\nOpenAI API Introduction: https://platform.openai.com/docs/guides/chat/introduction\\\\nDeepL Pricing: https://www.deepl.com/pro-api\\\\nLLaMA Paper: https://arxiv.org/pdf/2302.13971.pdf\\\\nIs ChatGPT A Good Translator? Paper: https://arxiv.org/pdf/2301.08745v2.pdf\\\\nOpenAI Pricing: https://openai.com/pricing\\\\nvast.ai: https://vast.ai/\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\n\\\\nOf course, financial support is completely voluntary, but I was asked for it:\\\\nhttps://patreon.com/MartinThissen\\\\nhttps://ko-fi.com/martinthissen\"",
    "lengthSeconds": "2275",
    "uploadDate": "2023-03-27",
    "thumbnail_url": "https://i.ytimg.com/vi/yTROqe8T_eA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_PO5NyOve6M",
    "title": "LLaMA & Alpaca: Fine-Tuning, Code Generation, RAM Requirements and More | Answers to Your Questions",
    "tags": "film, udost",
    "scraped_at": 1684585910.928028,
    "genre": "Science",
    "views": "34764",
    "desc": "In this video I will answer your questions regarding my previous video where I showed you how to run the LLaMA and Alpaca model on your local computer. To do this, I will answer what the input/output token size of the models is and how to fine-tune the models. I will also show you in more detail how the Standford researchers generated the training data and how they fine-tuned the LLaMA model. I will also show you the Alpaca-LoRA repository, which is particularly useful if you want to train the model on a single GPU or fine-tune the model for multiple tasks. I\\'ll also show you that the Alpaca model could be much better than you thought. I\\'ll also give you an example of how to generate code using the Alpaca model and how to run the Alpaca LoRA repository code on a GPU using Google Colab. In addition, I\\'ll also show you the RAM requirements for each LLaMa model variant and explain why the models work best in English. Enjoy! :-)\\\\n\\\\nMedium article where I answered even more questions:\\\\nhttps://medium.com/p/b7e20b28e9e1\\\\n\\\\nColab notebook used for running the Alpaca-LoRA model:\\\\nhttps://colab.research.google.com/drive/1ZfMTIjjSCUKIci_zglqX94JEKuoRTTdI?usp=sharing\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:00:45 How can the LLaMA and Alpaca models be fine-tuned (also input/output token limit)?\\\\n00:07:25 Can the models generate code (WATCH THIS if you are disappointed with the results of the Alpaca model)?\\\\n00:11:24 Do these models use the GPU?\\\\n00:11:49 What are the RAM requirements to run larger LLaMA variants?\\\\n00:12:07 Do the models work for languages other than English?\\\\n00:12:52 Can we use this to query a database with natural language?\\\\n00:13:29 More questions answered in my Medium article\\\\n00:14:03 Outro\\\\n\\\\nReferences:\\\\nDalai Repository: https://github.com/cocktailpeanut/dalai\\\\nStanford Alpaca Repository: https://github.com/tatsu-lab/stanford_alpaca\\\\nAlpaca-LoRA Repository: https://github.com/tloen/alpaca-lora\\\\nAlpaca-LoRA-13B Model Weights: https://huggingface.co/Draff/llama-alpaca-stuff/tree/main/Alpaca-Loras/13b-e3\\\\nLoRA Paper: https://arxiv.org/pdf/2106.09685v2.pdf\\\\nLLaMA Paper: https://arxiv.org/pdf/2302.13971v1.pdf\\\\nLlama.cpp Repository: https://github.com/ggerganov/llama.cpp\\\\nhttps://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022\\\\nhttps://github.com/tloen/alpaca-lora/pull/32\\\\nhttps://github.com/facebookresearch/llama/issues/148\\\\nhttps://crfm.stanford.edu/2023/03/13/alpaca.html\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\"",
    "lengthSeconds": "864",
    "uploadDate": "2023-03-20",
    "thumbnail_url": "https://i.ytimg.com/vi/_PO5NyOve6M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kT_-qUxrlOU",
    "title": "LLaMA & Alpaca: \u201cChatGPT\u201d On Your Local Computer \ud83e\udd2f |  Tutorial",
    "tags": "film, udost",
    "scraped_at": 1684585910.6200275,
    "genre": "Science",
    "views": "162154",
    "desc": "In this video I will show you how you can run state-of-the-art large language models on your local computer. Yes, you\\xe2\\x80\\x99ve heard right. For this we will use the Dalai library which allows us to run the foundational language model LLaMA as well as the instruction-following Alpaca model. While the LLaMA model is a foundational (or broad) language model that is able to predict the next token (word) based on a given input sequence (sentence), the Alpaca model is a fine-tuned version of the LLaMA model capable of following instructions (which you can think of as ChatGPT behaviour). What\\xe2\\x80\\x99s even more impressive, both these models achieve comparable results or even outperform their GPT counterparts while still being small enough to run on your local computer. In this video I will show you that it only takes a few steps (thanks to the Dalai library) to run \\xe2\\x80\\x9cChatGPT\\xe2\\x80\\x9d on your local computer.\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:00:30 LLaMA Model \\\\n00:03:54 Dalai Library Prerequisites\\\\n00:06:04 Setup Dalai \\\\u0026 LLaMA\\\\n00:08:39 Alpaca Model\\\\n00:10:57 Setup Dalai \\\\u0026 Alpaca\\\\n00:12:50 Outro\\\\n\\\\nMy Medium Article for This Video:\\\\nhttps://medium.com/p/17adda704c23\\\\n\\\\nDalai GitHub Repository:\\\\nhttps://github.com/cocktailpeanut/dalai\\\\n\\\\nLLaMA Weird Character Generation Issue:\\\\nhttps://github.com/cocktailpeanut/dalai/issues/65\\\\n\\\\nHelpful LLaMA prompts: https://github.com/facebookresearch/llama/blob/main/FAQ.md#2\\\\n\\\\n\\\\nResources:\\\\nAI Explained YT Video: https://www.youtube.com/watch?v=xslW5sQOkC8\\\\u0026ab_channel=AIExplained\\\\nSelf-Instruct: https://arxiv.org/pdf/2212.10560.pdf\\\\nArk Forecast: https://research.ark-invest.com/hubfs/1_Download_Files_ARK-Invest/Big_Ideas/ARK%20Invest_013123_Presentation_Big%20Ideas%202023_Final.pdf\\\\nAlpaca: https://crfm.stanford.edu/2023/03/13/alpaca.html\\\\nDalai: https://github.com/cocktailpeanut/dalai\\\\nIntroducing LLaMA: https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\\\\nLLaMA Paper: https://arxiv.org/pdf/2302.13971.pdf\\\\nhttps://twitter.com/miolini/status/1634982361757790209\\\\nhttps://twitter.com/summerlinARK/status/1599196885675544576?lang=en\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\"",
    "lengthSeconds": "787",
    "uploadDate": "2023-03-17",
    "thumbnail_url": "https://i.ytimg.com/vi/kT_"
  },
  {
    "link": "watch?v=mq9lBd8XMY4",
    "title": "Run Tortoise-TTS On Your Local Computer \ud83d\udd0a | Tutorial | Voice Cloning",
    "tags": "film, udost",
    "scraped_at": 1684585911.3140588,
    "genre": "Science",
    "views": "21672",
    "desc": "All information about how to set up and run the Tortoise-TTS model on your local computer is summarized in this guide (including links to Miniconda):\\\\nhttps://github.com/thisserand/tortoise_local\\\\n\\\\n\\\\nIn this video I will show you how to set up and run the Tortoise-TTS model on your local computer. Many of you have asked me for this. Since I don\\'t have a GPU myself, I had to use a Lambda Labs cloud instance to test the setup on a local computer. I hope this video helps you to set up the Tortoise-TTS model on your PC, so you can start cloning voices and generating speech. Enjoy! :-)\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:00:18 Install Conda\\\\n00:02:15 Setup Tortoise-TTS\\\\n00:07:58 Initialize Tortoise-TTS\\\\n00:11:07 Generate Speech\\\\n00:14:32 Outro \\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\"",
    "lengthSeconds": "908",
    "uploadDate": "2023-03-16",
    "thumbnail_url": "https://i.ytimg.com/vi/mq9lBd8XMY4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hxLypJ-sAy4",
    "title": "ControlNet: A Beginner's Guide to Getting Started",
    "tags": "film, udost",
    "scraped_at": 1684585910.8500285,
    "genre": "Science",
    "views": "1408",
    "desc": "In this video I want to show you how to get started quickly with the ControlNet without needing your own GPU. Once the user interface is up and running, it\\'s super easy to create images with the ControlNet (or Control Net) and the results are simply WOW. This video is not meant to be an in-depth tutorial on what settings to use for what conditional inputs, but rather can be viewed as a starting point for someone who has no experience with the ControlNet (basically a beginner\\'s guide to getting started).\\\\n\\\\n\\\\nUseful Links\\\\n\\\\nColab Notebook Used in This Video:\\\\nhttps://colab.research.google.com/github/camenduru/controlnet-colab/blob/main/stable_diffusion_1_5_controlnet_colab.ipynb\\\\n\\\\nControlNet Paper:\\\\nhttps://arxiv.org/pdf/2302.05543.pdf\\\\n\\\\nControlNet GitHub Repository:\\\\nhttps://github.com/lllyasviel/ControlNet\\\\n\\\\nControlNet Model Weights:\\\\nhttps://huggingface.co/lllyasviel/ControlNet\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\\\\n\\\\n\\\\nI hope this video helped you to achieve awesome results :) If so, feel free to share your success story in the comment section, but also if you have questions or ideas. Any like or subscription is very much appreciated \\xf0\\x9f\\x99\\x8f\\xf0\\x9f\\x8f\\xbb\"",
    "lengthSeconds": "326",
    "uploadDate": "2023-03-03",
    "thumbnail_url": "https://i.ytimg.com/vi/hxLypJ"
  },
  {
    "link": "watch?v=vIvWwrvcTkc",
    "title": "How To Remove People From Pictures Using AI \ud83e\udd29 | Stable Diffusion | Lama Cleaner",
    "tags": "film, udost",
    "scraped_at": 1684585911.1405401,
    "genre": "Science",
    "views": "2777",
    "desc": "In this video I will show you how to remove any object or person from your pictures using state-of-the-art AI models. All models are free, and it only takes a few minutes to edit your images using the LaMa Cleaner web app. I think that the image inpainting results obtained with the LaMa, MAT or Stable Diffusion models are really impressive, especially because they are so easy to use thanks to LaMa Cleaner. I hope this video will also help you to edit your images the way you want. :-)\\\\n\\\\n\\\\nColab Notebook Used In The Video:\\\\nhttps://colab.research.google.com/drive/1X8iyr0RnUi5mB5qqoeLJXEQ-NQGvpf-o?usp=sharing\\\\n\\\\n\\\\nMy Medium Article for This Video:\\\\nhttps://medium.com/@martin-thissen/remove-any-object-from-an-image-using-ai-stable-diffusion-lama-cleaner-d52d5f3542f1\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:01:38 Part 1: Theoretical Part\\\\n00:04:19 LaMa Model\\\\n00:06:59 Summary - LaMa Model\\\\n00:08:31 MAT Model\\\\n00:16:47 Summary - MAT Model\\\\n00:17:43 Stable Diffusion Model\\\\n00:21:56 Summary - Stable Diffusion Model\\\\n00:23:11 Part 2: Hands-On Inpainting\\\\n00:31:47 Results Comparison\\\\n00:37:16 Thumbnail Creation\\\\n00:43:46 Outro\\\\n\\\\n\\\\nLinks Used in This Video\\\\n\\\\nLama Cleaner:\\\\nhttps://github.com/Sanster/lama-cleaner\\\\n\\\\nNgrok:\\\\nhttps://ngrok.com/\\\\n\\\\nLaMa Model:\\\\nhttps://github.com/saic-mdal/lama (GitHub)\\\\nhttps://arxiv.org/abs/2109.07161 (Paper)\\\\n\\\\nMAT Model:\\\\nhttps://github.com/fenglinglwb/MAT (GitHub)\\\\nhttps://arxiv.org/abs/2203.15270 (Paper)\\\\n\\\\nStable Diffusion Model:\\\\nhttps://github.com/CompVis/stable-diffusion (GitHub v1)\\\\nhttps://github.com/Stability-AI/stablediffusion (GitHub v2)\\\\nhttps://arxiv.org/abs/2112.10752 (Paper)\\\\n\\\\nSwin (Shifting Windows) Transformer Paper:\\\\nhttps://arxiv.org/abs/2103.14030\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\"",
    "lengthSeconds": "2663",
    "uploadDate": "2023-03-01",
    "thumbnail_url": "https://i.ytimg.com/vi/vIvWwrvcTkc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-tE0UqE1R8E",
    "title": "Multi-Language Speech Generation \ud83d\udd0a | Coqui-TTS | Tutorial",
    "tags": "film, udost",
    "scraped_at": 1684585911.2210603,
    "genre": "Science",
    "views": "7180",
    "desc": "Many of you have asked me if it would be possible to generate speech using the Tortoise-TTS model for languages other than English. Unfortunately the Tortoise model was only trained on English audio samples and hence can only generate English speech. Since voice cloning is a difficult task, to my knowledge there are no equivalent voice cloning models to the Tortoise-TTS model for languages other than English. To still enable you to generate speech in a language other than English, we will have a look at the Coqui-TTS library in this video.  The Coqui-TTS library allows you to generate speech in 29 different languages \\xf0\\x9f\\xa4\\xaf. In this video I will show you how to generate speech using the Coqui-TTS library. It\\xe2\\x80\\x99s super easy and faster than the Tortoise-TTS model, let me know what you think about your results in the comments. :-)\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:00:21 Multi-Language Speech Generation\\\\n00:23:05 Outro\\\\n\\\\n\\\\nLinks Used in This Video\\\\n\\\\nColab Notebook Used in This Video:\\\\nhttps://colab.research.google.com/drive/1F_gatrMCSDwtlXuQHCGSA_p096q1YiNI?usp=sharing\\\\n\\\\nCoqui-TTS Repository:\\\\nhttps://github.com/coqui-ai/TTS\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\\\\n\\\\nI hope this video helped you to generate awesome speech in your language :-) If so, feel free to share your success story in the comment section. Any like or subscription is very much appreciated and helps me creating even more videos. \\xf0\\x9f\\x99\\x8f\\xf0\\x9f\\x8f\\xbb\"",
    "lengthSeconds": "707",
    "uploadDate": "2023-02-01",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=FN3yxL0Rr0c",
    "title": "Longer Speech With Tortoise-TTS \ud83d\udd0a | Tutorial | Voice Cloning",
    "tags": "film, udost",
    "scraped_at": 1684585911.0460308,
    "genre": "Science",
    "views": "14079",
    "desc": "Many of you have asked me for this and now it\\'s here. A video about how to generate longer speech with the Tortoise-TTS model. Since I didn\\'t have a nice text at hand, I created one using ChatGPT. What do you think about ChatGPT? After generating the text, I will show you how to generate longer speech using the Tortoise-TTS model. I hope this will be useful for all of you to get even better or at least longer voice cloning results. :)\\\\n\\\\n00:00:00 Intro\\\\n00:00:44 Text Generation\\\\n00:03:39 Generating Longer Speech\\\\n00:15:27 Outro\\\\n\\\\n\\\\nLinks Used in This Video\\\\n\\\\nColab Notebook:\\\\nhttps://colab.research.google.com/drive/1g_CssJK34kwRi7VRtFd73WvTLq9UbnZT?usp=sharing\\\\n\\\\nChatGPT:\\\\nhttps://chat.openai.com/\\\\n\\\\nTortoise Repository:\\\\nhttps://github.com/neonbjb/tortoise-tts\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\\\\n\\\\nI hope this video helped you generating longer speech using the Tortoise-TTS model :) If so, feel free to share your success story in the comment section, but also if you have questions or ideas. Any like or subscription is very much appreciated \\xf0\\x9f\\x99\\x8f\\xf0\\x9f\\x8f\\xbb\"",
    "lengthSeconds": "975",
    "uploadDate": "2023-01-19",
    "thumbnail_url": "https://i.ytimg.com/vi/FN3yxL0Rr0c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JBKuRPSaou0",
    "title": "Custom Object Detection Using YOLOv7 & Python \ud83d\udcaf | Coding Tutorial",
    "tags": "film, udost",
    "scraped_at": 1684585911.502057,
    "genre": "Science",
    "views": "1034",
    "desc": "Object detection has been studied for a long time with increasingly better prediction results. Usually, large datasets are required to get good results. In this video, we will use the pre-trained YOLOv7 model and fine-tune it on our custom dataset to recognize our custom object. As I will show in the video, really good results can be achieved with as few as 16 training images.\\\\n\\\\n00:00:00 Intro\\\\n00:00:21 Getting Started\\\\n00:01:23 Data Collection\\\\n00:03:28 Labelling\\\\n00:07:37 Dataset Formatting\\\\n00:12:09 Training YOLOv7 Model\\\\n00:17:40 Detecting Your Own Class\\\\n00:23:05 Outro\\\\n\\\\n\\\\nLinks Used in This Video\\\\n\\\\nColab Notebook:\\\\nhttps://colab.research.google.com/drive/1h74P5OCr76KTw8GsRR2L4JFf7MfMwBXZ?usp=sharing\\\\n\\\\nPixabay:\\\\nhttps://pixabay.com/\\\\n\\\\nLabel Studio:\\\\nhttps://labelstud.io/\\\\n\\\\nYOLOv7 Repository:\\\\nhttps://github.com/WongKinYiu/yolov7\\\\n\\\\nYOLOv7 Open Issue:\\\\nhttps://github.com/WongKinYiu/yolov7/issues/1101\\\\n\\\\n\\\\nStay in Touch\\\\n\\\\nMedium\\\\nhttps://medium.com/@martin-thissen\\\\n\\\\nLinkedIn\\\\nhttps://linkedin.com/in/mthissen135/\\\\n\\\\nYouTube\\\\nOf course, feel free to subscribe to my channel! :-)\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\\\\n\\\\nI hope this video helped you to achieve awesome object detection results :) If so, feel free to share your success story in the comment section, but also if you have questions or ideas. Any like or subscription is very much appreciated \\xf0\\x9f\\x99\\x8f\\xf0\\x9f\\x8f\\xbb\"",
    "lengthSeconds": "1407",
    "uploadDate": "2022-12-21",
    "thumbnail_url": "https://i.ytimg.com/vi/JBKuRPSaou0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3RvKEundVps",
    "title": "Attention | The Magic Behind Transformers | Understanding & Coding Tutorial",
    "tags": "film, udost",
    "scraped_at": 1684585910.7830281,
    "genre": "Science",
    "views": "1840",
    "desc": "Transformers have taken over in many areas of AI, including NLP or computer vision. But why do Transformers work so well? In this video, I\\'ll introduce you to the mechanism that makes Transformers so powerful: the attention mechanism. We\\'ll cover everything you need to understand and use the attention mechanism yourself. Starting with a historical background, we will continue to work our way through to the mathematical foundations. After developing a deep understanding of attention, we will finally code it ourselves using PyTorch.\\\\n\\\\n\\\\n00:00:00 Intro\\\\n00:00:53 Intuition\\\\n00:02:14 Machine Translation\\\\n00:05:23 The Maths Behind\\\\n00:12:50 Hands on Coding\\\\n00:18:45 Outro\\\\n\\\\nMy Medium Article About Attention:\\\\nhttps://medium.com/p/fe707a85cc3f\\\\n\\\\n\\\\nRelated Papers:\\\\n\\\\nAttention Is All You Need, Vaswani et al.\\\\nhttps://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\\\\n\\\\nNeural Machine Translation By Jointly Learning To Align And Translate, Bandanau et al.\\\\nhttps://arxiv.org/pdf/1409.0473.pdf\\\\n\\\\n\\\\nAttention Implementation by Stefania Cristina:\\\\nhttps://machinelearningmastery.com/the-attention-mechanism-from-scratch/\\\\n\\\\n\\\\nColab Notebook Used in This Video:\\\\nhttps://colab.research.google.com/drive/1uDY8LZiEiF4syaWIHpkwNn9mQWfrJMf9?usp=sharing\\\\n\\\\n\\\\nhttps://patreon.com/MartinThissen (of course, financial support is completely voluntary, but I was asked for this)\\\\n\\\\nAny feedback is appreciated very much, so please don\\xe2\\x80\\x99t hesitate to share your thoughts / ideas / questions with me in the comment section. :-)\"",
    "lengthSeconds": "1156",
    "uploadDate": "2022-12-12",
    "thumbnail_url": "https://i.ytimg.com/vi/3RvKEundVps/maxresdefault.jpg"
  }
]