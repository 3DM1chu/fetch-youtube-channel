[
  {
    "link": "watch?v=cjs7QKJNVYM",
    "title": "[ML News] Geoff Hinton leaves Google | Google has NO MOAT | OpenAI down half a billion",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, anthropic, google, google io, llm, gpt",
    "scraped_at": 1684582635.4415386,
    "genre": "Science",
    "views": "36135",
    "desc": "#google #openai #mlnews \\\\n\\\\nUpdates from the world of Machine Learning and AI\\\\nGreat AI memes here: https://twitter.com/untitled01ipynb\\\\n\\\\nOUTLINE:\\\\n0:00 - Google I/O 2023: Generative AI in everything\\\\n0:20 - Anthropic announces 100k tokens context\\\\n0:35 - Intro\\\\n1:20 - Geoff Hinton leaves Google\\\\n7:00 - Google memo leaked: we have no moat\\\\n11:30 - OpenAI loses 540M\\\\n12:30 - Google AI: Product first\\\\n15:50 - Ilya Sutskever on safety vs competition\\\\n18:00 - AI works cannot be copyrighted\\\\n19:40 - OpenAI tries to trademark GPT\\\\n20:30 - StarCoder: accessible code model\\\\n21:40 - RedPyjama \\\\u0026 OpenLlama\\\\n22:55 - Mosaic 7B model\\\\n23:50 - YoloNAS\\\\n24:10 - Mojo programming language\\\\n25:30 - Random helpful things\\\\n37:40 - DeepMind soccer robots\\\\n\\\\nReferences:\\\\nhttps://twitter.com/weirddalle/status/1649908805788893185\\\\nhttps://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\\\\nhttps://www.technologyreview.com/2023/05/01/1072478/deep-learning-pioneer-geoffrey-hinton-quits-google/\\\\nhttps://archive.ph/TrPoH\\\\nhttps://twitter.com/DanHendrycks/status/1654560913939374080\\\\nhttps://twitter.com/ylecun/status/1654930029569101824\\\\nhttps://twitter.com/home\\\\nhttps://twitter.com/ylecun/status/1654931495419621376\\\\nhttps://twitter.com/pkedrosky/status/1653955254181068801\\\\nhttps://www.semianalysis.com/p/google-we-have-no-moat-and-neither\\\\nhttps://twitter.com/untitled01ipynb/media\\\\nhttps://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt\\\\nhttps://archive.ph/bKsdM\\\\nhttps://www.washingtonpost.com/technology/2023/05/04/google-ai-stop-sharing-research/\\\\nhttps://twitter.com/giffmana/status/1654962145707130880\\\\nhttps://twitter.com/Ken_Goldberg/status/1651309843804987393\\\\nhttps://tsdr.uspto.gov/documentviewer?caseId=sn97733259\\\\u0026docId=PTD20230418160641\\\\u0026s=09#docIndex=1\\\\u0026page=1\\\\nhttps://twitter.com/osanseviero/status/1654230764513370112\\\\nhttps://huggingface.co/bigcode/starcoder\\\\nhttps://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\\\\nhttps://twitter.com/hardmaru/status/1654649036333514753\\\\nhttps://www.together.xyz/blog/redpajama-models-v1\\\\nhttps://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\\\\nhttps://github.com/openlm-research/open_llama\\\\nhttps://www.mosaicml.com/blog/mpt-7b\\\\nhttps://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md\\\\nhttps://www.modular.com/mojo\\\\nhttps://www.aicrowd.com/challenges/hackaprompt-2023\\\\nhttps://learnprompting.org/\\\\nhttps://developer.nvidia.com/blog/nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems/?ncid=prsy-552511\\\\nhttps://blogs.nvidia.com/blog/2023/04/25/ai-chatbot-guardrails-nemo/\\\\nhttps://lmql.ai/#distribution\\\\nhttps://github.com/gventuri/pandas-ai?utm_source=pocket_reader\\\\nhttps://lamini.ai/blog/introducing-lamini\\\\nhttps://github.com/deep-floyd/IF\\\\nhttps://huggingface.co/spaces/DeepFloyd/IF\\\\nhttps://twitter.com/FaramaFound/status/1650952295901720576\\\\nhttps://txt.cohere.com/embedding-archives-wikipedia/?hsa_acc=509563538\\\\u0026hsa_ad=242008083\\\\u0026hsa_cam=626636963\\\\u0026hsa_grp=205646033\\\\u0026hsa_net=linkedin\\\\u0026hsa_ver=3\\\\u0026hss_channel=lcp-24024765\\\\nhttps://arxiv.org/abs/2304.12210\\\\nhttps://github.com/h2oai/h2ogpt\\\\nhttps://huggingface.co/h2oai/h2ogpt-oasst1-512-20b\\\\nhttps://github.com/h2oai/h2o-llmstudio\\\\nhttps://ai.facebook.com/blog/ai-dataset-animating-kids-drawings/\\\\nhttps://www.camel-ai.org/\\\\nhttps://github.com/lightaime/camel?utm_source=pocket_reader\\\\nhttps://huggingface.co/Writer/camel-5b-hf\\\\nhttps://laion.ai/blog/paella/\\\\nhttps://magazine.sebastianraschka.com/p/finetuning-large-language-models\\\\nhttps://pickapic.io/\\\\nhttps://github.com/yuvalkirstain/heroku_app\\\\nhttps://huggingface.co/datasets/yuvalkirstain/PickaPic\\\\nhttps://future.snorkel.ai/poster-contest/\\\\nhttps://twitter.com/d_feldman/status/1649466422018318338/photo/4\\\\nhttps://twitter.com/DeepMind/status/1651897358894919680\\\\nhttps://arxiv.org/abs/2304.13653\\\\nhttps://twitter.com/SmokeAwayyy/status/1652712832738422784\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2347",
    "uploadDate": "2023-05-12",
    "thumbnail_url": "https://i.ytimg.com/vi/cjs7QKJNVYM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4Cclp6yPDuw",
    "title": "Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, transformers, longformer, transformer long documents, paper explained, recurrent transformer, transformer xl, what is deep learning, mit deep learning, deep learning basics",
    "scraped_at": 1684582627.8516672,
    "genre": "Science",
    "views": "46186",
    "desc": "#ai #transformer #gpt4 \\\\n\\\\nThis paper promises to scale transformers to 1 million tokens and beyond. We take a look at the technique behind it: The Recurrent Memory Transformer, and what its strenghts and weaknesses are.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n2:15 - Transformers on long sequences\\\\n4:30 - Tasks considered\\\\n8:00 - Recurrent Memory Transformer\\\\n19:40 - Experiments on scaling and attention maps\\\\n24:00 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2304.11062\\\\n\\\\nAbstract:\\\\nThis technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model\\'s effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.\\\\n\\\\nAuthors: Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1474",
    "uploadDate": "2023-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/4Cclp6yPDuw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ddG2fM9i4Kk",
    "title": "OpenAssistant RELEASED! The world's best open-source Chat AI!",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, what is deep learning, open source ai, open source chatgpt, laion, laion chatgpt, yannic chatgpt, open source intelligence, open source gpt, open source gpt 4, dolly, alpaca, llama, pythia, vincuna, huggingface, streaming",
    "scraped_at": 1684582628.1106422,
    "genre": "Science",
    "views": "128713",
    "desc": "#openassistant #chatgpt #mlnews \\\\n\\\\nTry the chat: https://open-assistant.io/chat\\\\nHomepage: https://open-assistant.io \\\\nDataset: https://huggingface.co/datasets/OpenAssistant/oasst1\\\\nCode: https://github.com/LAION-AI/Open-Assistant\\\\nPaper (temporary): https://ykilcher.com/oa-paper\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1265",
    "uploadDate": "2023-04-15",
    "thumbnail_url": "https://i.ytimg.com/vi/ddG2fM9i4Kk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Hi6cbeBY2oQ",
    "title": "OpenAssistant First Models are here! (Open-Source ChatGPT)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, chatgpt, open source chatgpt, open source chatbot alternatives, chatgpt open source, llama, open assistant, open assistent, openassistant, open",
    "scraped_at": 1684582630.7513473,
    "genre": "Science",
    "views": "70516",
    "desc": "#openassistant #chatgpt #gpt4\\\\n\\\\nhttps://open-assistant.io/chat\\\\nhttps://huggingface.co/OpenAssistant\\\\nhttps://github.com/LAION-AI/Open-Assistant\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1013",
    "uploadDate": "2023-04-06",
    "thumbnail_url": "https://i.ytimg.com/vi/Hi6cbeBY2oQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YqPYDWPYXFs",
    "title": "The biggest week in AI (GPT-4, Office Copilot, Google PaLM, Anthropic Claude & more)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582635.5445433,
    "genre": "Science",
    "views": "89590",
    "desc": "#mlnews #gpt4 #copilot\\\\n\\\\nYour weekly news all around the AI world\\\\n\\\\nCheck out W\\\\u0026B courses (free): https://wandb.courses/\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - GPT-4 announced!\\\\n4:30 - GigaGAN: The comeback of Generative Adversarial Networks\\\\n7:55 - ChoppedAI: AI Recipes\\\\n8:45 - Samsung accused of faking space zoom effect\\\\n14:00 - Weights \\\\u0026 Biases courses are free\\\\n16:55 - Data Portraits\\\\n18:50 - Data2Vec 2.0\\\\n19:50 - Gated Models on Hugging Face \\\\u0026 huggingface.js\\\\n22:05 - Visual ChatGPT\\\\n23:35 - Bing crosses 100 million daily active users\\\\n24:50 - Casual Conversations Dataset\\\\n25:50 - Anthropic AI Safety Research\\\\n27:30 - Magnushammer \\\\u0026 more advances in AI-assisted math\\\\n30:30 - LLaMA license change PR\\\\n32:00 - Self-Instruct dataset\\\\n33:35 - PaLM-E: Multimodal Pathways\\\\n35:45 - USM: Universal Speech Model\\\\n37:25 - GILGEN: Grounded Text-to-Image\\\\n39:55 - Fruit Fly Connectome released\\\\n\\\\nReferences:\\\\nhttps://www.heise.de/news/GPT-4-kommt-naechste-Woche-und-es-wird-multimodal-Vorankuendigung-von-Microsoft-7540383.html\\\\nhttps://mingukkang.github.io/GigaGAN/\\\\nhttps://www.choppedai.com/\\\\nhttps://www.reddit.com/r/Android/comments/11nzrb0/samsung_space_zoom_moon_shots_are_fake_and_here/\\\\nhttps://imgur.com/ULVX933\\\\nhttps://imgur.com/9XMgt06\\\\nhttps://imgur.com/9kichAp\\\\nhttps://imgur.com/RSHAz1l\\\\nhttps://imgur.com/PIAjVKp\\\\nhttps://imgur.com/xEyLajW\\\\nhttps://imgur.com/3STX9mZ\\\\nhttps://imgur.com/ifIHr3S\\\\nhttps://imgur.com/bXJOZgI\\\\nhttps://dataportraits.org/\\\\nhttps://arxiv.org/abs/2303.03919\\\\nhttps://arxiv.org/pdf/2303.03919.pdf\\\\nhttps://ai.facebook.com/blog/ai-self-supervised-learning-data2vec/\\\\nhttps://github.com/facebookresearch/fairseq/tree/main/examples/data2vec\\\\nhttps://huggingface.co/docs/hub/models-gated\\\\nhttps://huggingface.co/about\\\\nhttps://github.com/huggingface/huggingface.js?utm_source=pocket_reader\\\\nhttps://github.com/microsoft/visual-chatgpt\\\\nhttps://arxiv.org/abs/2303.04671\\\\nhttps://github.com/microsoft/visual-chatgpt/blob/main/visual_chatgpt.py\\\\nhttps://huggingface.co/spaces/RamAnanth1/visual-chatGPT\\\\nhttps://www.engadget.com/microsoft-bing-crossed-100-million-daily-active-users-080138371.html\\\\nhttps://ai.facebook.com/blog/casual-conversations-v2-dataset-measure-fairness/\\\\nhttps://ai.facebook.com/datasets/casual-conversations-v2-dataset/\\\\nhttps://www.anthropic.com/index/core-views-on-ai-safety\\\\nhttps://arxiv.org/abs/2303.04488\\\\nhttps://arxiv.org/pdf/2303.04488.pdf\\\\nhttps://arxiv.org/abs/2303.04910\\\\nhttps://arxiv.org/pdf/2303.04910.pdf\\\\nhttps://twitter.com/astro_wassim/status/1633645134934949888\\\\nhttps://ai.papers.bar/paper/ede58b1ebca911ed8f9c3d8021bca7c8\\\\nhttps://arxiv.org/pdf/2303.03192.pdf\\\\nhttps://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse\\\\nhttps://knightcolumbia.org/blog/the-llama-is-out-of-the-bag-should-we-expect-a-tidal-wave-of-disinformation\\\\nhttps://github.com/facebookresearch/llama/pull/184\\\\nhttps://huggingface.co/datasets/yizhongw/self_instruct\\\\nhttps://openai.com/policies/terms-of-use\\\\nhttps://palm-e.github.io/\\\\nhttps://pickapic.io/\\\\nhttps://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html\\\\nhttps://arxiv.org/abs/2303.01037\\\\nhttps://github.com/BlinkDL/RWKV-LM?utm_source=pocket_reader\\\\nhttps://gligen.github.io/\\\\nhttps://github.com/microsoft/GLIP\\\\nhttps://arxiv.org/abs/2301.07093\\\\nhttps://huggingface.co/spaces/gligen/demo\\\\nhttps://www.sciencealert.com/the-first-ever-complete-map-of-an-insect-brain-is-truly-mesmerizing\\\\nhttps://en.wikipedia.org/wiki/Tidal_locking\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2462",
    "uploadDate": "2023-03-18",
    "thumbnail_url": "https://i.ytimg.com/vi/YqPYDWPYXFs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2zW33LfffPc",
    "title": "GPT-4 is here! What we know so far (Full Analysis)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, openai, gpt 4, gpt",
    "scraped_at": 1684582630.6663797,
    "genre": "Science",
    "views": "104710",
    "desc": "#gpt4 #chatgpt #openai \\\\n\\\\nReferences:\\\\nhttps://openai.com/product/gpt-4\\\\nhttps://openai.com/research/gpt-4\\\\nhttps://cdn.openai.com/papers/gpt-4.pdf\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2050",
    "uploadDate": "2023-03-15",
    "thumbnail_url": "https://i.ytimg.com/vi/2zW33LfffPc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yR4hNBNS6yc",
    "title": "This ChatGPT Skill will earn you $10B (also, AI reads your mind!) | ML News",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, ml news, chatgpt, chat gpt, chatgpt api, llama, llama meta, llama weights, llama ai, open assistant, deep learning news, ai news, what is deep learning, deep learning tutorial",
    "scraped_at": 1684582632.2093487,
    "genre": "Science",
    "views": "71399",
    "desc": "#mlnews #chatgpt #llama\\\\n\\\\nChatGPT goes around the world and is finally available via API. Stunning mind-reading performed using fMRI and Stable Diffusion. LLaMA weights leak and hilarity ensues. GTC23 is around the corner!\\\\n\\\\nERRATA: It\\'s a 4090, not a 4090 ti \\xf0\\x9f\\x99\\x83\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n0:20 - GTC 23 on March 20\\\\n1:55 - ChatGPT API is out!\\\\n4:50 - OpenAI becomes more business-friendly\\\\n7:15 - OpenAI plans for AGI\\\\n10:00 - ChatGPT influencers\\\\n12:15 - Open-Source Prompting Course\\\\n12:35 - Flan UL2 20B\\\\n13:30 - LLaMA weights leaked\\\\n15:50 - Mind-Reading from fMRI\\\\n20:10 - Random News / Helpful Things\\\\n25:30 - Interview with Bryan Catanzaro\\\\n\\\\nParticipate in the GTC Raffle: https://ykilcher.com/gtc\\\\n\\\\nReferences:\\\\nGTC 23 on March 20\\\\nhttps://www.nvidia.com/gtc/\\\\nhttps://ykilcher.com/gtc\\\\n\\\\nChatGPT API is out!\\\\nhttps://twitter.com/gdb/status/1630991925984755714\\\\nhttps://openai.com/blog/introducing-chatgpt-and-whisper-apis\\\\nhttps://twitter.com/greggyb/status/1631121912679002112\\\\nhttps://www.haihai.ai/chatgpt-api/\\\\n\\\\nOpenAI becomes more business-friendly\\\\nhttps://twitter.com/sama/status/1631002519311888385\\\\nhttps://techcrunch.com/2023/02/21/openai-foundry-will-let-customers-buy-dedicated-capacity-to-run-its-ai-models/?guccounter=1\\\\u0026guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8\\\\u0026guce_referrer_sig=AQAAAFL1O8s22qBsEtytYZWR7O2VlTa9nAGhdZPFfeQfZCDWjkNBIac7WlDikRNLEH1tqSszUN02ouqRyyCsShDa1kQyUbiApD1IUPfgmHXZxgIMFxr8bwr8BuBa7sK55dYqMRFFbE7YILuBn_rmj7aJI1tp7GAXubODfCUaKvOkoOYj\\\\nhttps://www.bain.com/vector-digital/partnerships-alliance-ecosystem/openai-alliance/\\\\n\\\\nOpenAI plans for AGI\\\\nhttps://openai.com/blog/planning-for-agi-and-beyond\\\\n\\\\nChatGPT influencers\\\\nhttps://www.youtube.com/watch?v=4kp7oVTu9Ck\\\\nhttps://www.youtube.com/watch?v=k13v8jp8H5o\\\\nhttps://www.linkedin.com/posts/eniascailliau_create-an-online-course-100-ai-ugcPost-7036969935796891648-H_uj/\\\\nhttps://www.linkedin.com/posts/linasbeliunas_must-know-ai-tools-ugcPost-7035700089947836416-Qri4/\\\\nhttps://twitter.com/LinusEkenstam/status/1629879567514238976\\\\nhttps://www.linkedin.com/posts/imarpit_50-awesome-chatgpt-prompts-ugcPost-7036905788631646209-2CU-/\\\\n\\\\nOpen-Source Prompting Course\\\\nhttps://learnprompting.org/\\\\n\\\\nFlan UL2 20B\\\\nhttps://www.yitay.net/blog/flan-ul2-20b\\\\nhttps://huggingface.co/google/flan-ul2\\\\n\\\\nLLaMA weights leaked\\\\nhttps://github.com/facebookresearch/llama/pull/73\\\\nhttps://github.com/facebookresearch/llama/pull/73/files#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5\\\\nhttps://github.com/ChristopherKing42\\\\nhttps://open-assistant.io/dashboard\\\\n\\\\nMind-Reading from fMRI\\\\nhttps://sites.google.com/view/stablediffusion-with-brain/?s=09\\\\nhttps://www.nature.com/articles/s41562-022-01516-2?utm_content=animation\\\\n\\\\nRandom News\\\\nhttps://www.wired.com/story/alphabet-layoffs-hit-trash-sorting-robots/\\\\nhttps://huggingface.co/blog/fast-mac-diffusers\\\\nhttps://pyribs.org/\\\\nhttps://twitter.com/rowancheung/status/1630569844654460928\\\\nhttps://pimeyes.com/en\\\\nhttps://cacti-framework.github.io/\\\\nhttps://twitter.com/bhutanisanyam1/status/1630980866775330819\\\\nhttps://www.linkedin.com/in/bryancatanzaro/\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2607",
    "uploadDate": "2023-03-11",
    "thumbnail_url": "https://i.ytimg.com/vi/yR4hNBNS6yc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=E5OnoYF2oAk",
    "title": "LLaMA: Open and Efficient Foundation Language Models (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, meta ai, meta llama, llama llm, gpt",
    "scraped_at": 1684582627.742641,
    "genre": "Science",
    "views": "71514",
    "desc": "#ai #meta #languagemodel \\\\n\\\\nLLaMA is a series of large language models from 7B to 65B parameters, trained by Meta AI. They train for longer on more data and show that something like gpt-3 can be outperformed by significantly smaller models when trained like this. Meta also releases the trained models to the research community.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction \\\\u0026 Paper Overview\\\\n4:30 - Rant on Open-Sourcing\\\\n8:05 - Training Data\\\\n12:40 - Training Hyperparameters\\\\n14:50 - Architecture Modifications\\\\n17:10 - Optimizer\\\\n19:40 - Efficient Implementation\\\\n26:15 - Main Results\\\\n38:00 - Some more completions\\\\n40:00 - Conclusion\\\\n\\\\n\\\\nPaper: https://arxiv.org/abs/2302.13971\\\\nWebsite: https://ai.facebook.com/blog/large-language-model-llama-meta-ai/\\\\n\\\\nAbstract:\\\\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\\\\n\\\\nAuthors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\\xc3\\xa9e Lacroix, Baptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2467",
    "uploadDate": "2023-03-02",
    "thumbnail_url": "https://i.ytimg.com/vi/E5OnoYF2oAk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6OozhhI6U4g",
    "title": "Open Assistant Inference Backend Development (Hands-On Coding)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, open assistant, openassistant, open",
    "scraped_at": 1684582628.7503068,
    "genre": "Science",
    "views": "31512",
    "desc": "#ai #huggingface #coding \\\\n\\\\nJoin me as I build streaming inference into the Hugging Face text generation server, going through cuda, python, rust, grpc, websockets, server-sent events, and more...\\\\n\\\\nOriginal repo is here: https://github.com/huggingface/text-generation-inference\\\\n\\\\nOpenAssistant repo is here: https://github.com/LAION-AI/Open-Assistant (see inference/)\\\\n\\\\nCheck out https://www.wandb.courses/ for free MLOps courses!\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "4883",
    "uploadDate": "2023-02-24",
    "thumbnail_url": "https://i.ytimg.com/vi/6OozhhI6U4g/maxresdefault.jpg"
  },
  {
    "link": "watch?v=64Izfm24FKA",
    "title": "OpenAssistant - ChatGPT's Open Alternative (We need your help!)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, chatgpt, chat gpt, openai chat gpt, chat gpt alternative, chat gpt alternative open source, chatgpt alternative, open source chatgpt, open source language model, chatgpt github, openassistant, open assistant, open",
    "scraped_at": 1684582634.2725124,
    "genre": "Science",
    "views": "122479",
    "desc": "#openassistant #chatgpt #ai \\\\n\\\\nHelp us collect data for OpenAssistant, the largest and most open alternative to ChatGPT.\\\\nhttps://open-assistant.io\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - The Project\\\\n2:05 - Getting to Minimum Viable Prototype\\\\n5:30 - First Tasks\\\\n10:00 - Leaderboard\\\\n11:45 - Playing the Assistant\\\\n14:40 - Tricky Facts\\\\n16:25 - What if humans had wings?\\\\n17:05 - Can foxes be tamed?\\\\n23:45 - Can zebras be tamed?\\\\n26:15 - Yo (spam)\\\\n27:00 - More tasks\\\\n29:10 - Entitled Emails\\\\n34:35 - Final Words\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2147",
    "uploadDate": "2023-02-04",
    "thumbnail_url": "https://i.ytimg.com/vi/64Izfm24FKA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=0A8ljAkdFtg",
    "title": "ChatGPT: This AI has a JAILBREAK?! (Unbelievable AI Progress)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, chatgpt, chat gpt, openai chat gpt, openai chatbot gpt, openai chatbot, gpt",
    "scraped_at": 1684582635.6455133,
    "genre": "Science",
    "views": "432861",
    "desc": "#chatgpt #ai #openai \\\\n\\\\nChatGPT, OpenAI\\'s newest model is a GPT-3 variant that has been fine-tuned using Reinforcement Learning from Human Feedback, and it is taking the world by storm!\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:40 - Sponsor: Weights \\\\u0026 Biases\\\\n3:20 - ChatGPT: How does it work?\\\\n5:20 - Reinforcement Learning from Human Feedback\\\\n7:10 - ChatGPT Origins: The GPT-3.5 Series\\\\n8:20 - OpenAI\\'s strategy: Iterative Refinement\\\\n9:10 - ChatGPT\\'s amazing capabilities\\\\n14:10 - Internals: What we know so far\\\\n16:10 - Building a virtual machine in ChatGPT\\'s imagination (insane)\\\\n20:15 - Jailbreaks: Circumventing the safety mechanisms\\\\n29:25 - How OpenAI sees the future\\\\n\\\\nReferences:\\\\nhttps://openai.com/blog/chatgpt/\\\\nhttps://openai.com/blog/language-model-safety-and-misuse/\\\\nhttps://beta.openai.com/docs/model-index-for-researchers\\\\nhttps://scale.com/blog/gpt-3-davinci-003-comparison#Conclusion\\\\nhttps://twitter.com/johnvmcdonnell/status/1598470129121374209\\\\nhttps://twitter.com/blennon_/status/1597374826305318912\\\\nhttps://twitter.com/TimKietzmann/status/1598230759118376960/photo/1\\\\nhttps://twitter.com/_lewtun/status/1598056075672027137/photo/2\\\\nhttps://twitter.com/raphaelmilliere/status/1598469100535259136\\\\nhttps://twitter.com/CynthiaSavard/status/1598498138658070530/photo/1\\\\nhttps://twitter.com/tylerangert/status/1598389755997290507/photo/1\\\\nhttps://twitter.com/amasad/status/1598042665375105024/photo/1\\\\nhttps://twitter.com/goodside/status/1598129631609380864/photo/1\\\\nhttps://twitter.com/moyix/status/1598081204846489600/photo/2\\\\nhttps://twitter.com/JusticeRage/status/1598959136531546112\\\\nhttps://twitter.com/yoavgo/status/1598594145605636097\\\\nhttps://twitter.com/EladRichardson/status/1598333315764871174\\\\nhttps://twitter.com/charles_irl/status/1598319027327307785/photo/4\\\\nhttps://twitter.com/jasondebolt/status/1598243854343606273\\\\nhttps://twitter.com/mattshumer_/status/1598185710166896641/photo/1\\\\nhttps://twitter.com/i/web/status/1598246145171804161\\\\nhttps://twitter.com/bleedingedgeai/status/1598378564373471232\\\\nhttps://twitter.com/MasterScrat/status/1598830356115124224\\\\nhttps://twitter.com/Sentdex/status/1598803009844256769\\\\nhttps://twitter.com/harrison_ritz/status/1598828017446371329\\\\nhttps://twitter.com/parafactual/status/1598212029479026689\\\\nhttps://www.engraved.blog/building-a-virtual-machine-inside/\\\\nhttps://twitter.com/317070\\\\nhttps://twitter.com/zehavoc/status/1599193444043268096\\\\nhttps://twitter.com/yoavgo/status/1598360581496459265\\\\nhttps://twitter.com/yoavgo/status/1599037412411596800\\\\nhttps://twitter.com/yoavgo/status/1599045344863879168\\\\nhttps://twitter.com/natfriedman/status/1598477452661383168\\\\nhttps://twitter.com/conradev/status/1598487973351362561/photo/1\\\\nhttps://twitter.com/zswitten/status/1598100186605441024\\\\nhttps://twitter.com/CatEmbedded/status/1599141379879600128/photo/2\\\\nhttps://twitter.com/mattshumer_/status/1599175127148949505\\\\nhttps://twitter.com/vaibhavk97/status/1598930958769860608/photo/1\\\\nhttps://twitter.com/dan_abramov/status/1598800508160024588/photo/1\\\\nhttps://twitter.com/MinqiJiang/status/1598832656422432768/photo/2\\\\nhttps://twitter.com/zswitten/status/1598088280066920453\\\\nhttps://twitter.com/m1guelpf/status/1598203861294252033/photo/1\\\\nhttps://twitter.com/SilasAlberti/status/1598257908567117825/photo/1\\\\nhttps://twitter.com/gf_256/status/1598962842861899776/photo/1\\\\nhttps://twitter.com/zswitten/status/1598088267789787136\\\\nhttps://twitter.com/gf_256/status/1598178469955112961/photo/1\\\\nhttps://twitter.com/samczsun/status/1598564871653789696/photo/1\\\\nhttps://twitter.com/haus_cole/status/1598541468058390534/photo/3\\\\nhttps://twitter.com/tailcalled/status/1599181030065246208/photo/1\\\\nhttps://twitter.com/pensharpiero/status/1598731292278865920\\\\nhttps://twitter.com/sleepdensity/status/1598233414683197441\\\\nhttps://twitter.com/goodside/status/1598253337400717313\\\\nhttps://twitter.com/Carnage4Life/status/1598332648723976193/photo/2\\\\nhttps://github.com/sw-yx/ai-notes/blob/main/TEXT.md#jailbreaks\\\\nhttps://twitter.com/dannypostmaa/status/1599352584963170309/photo/4\\\\nhttps://twitter.com/sama/status/1599112749833125888\\\\nhttps://twitter.com/sama/status/1599114807474810884\\\\nhttps://twitter.com/sama/status/1599461195005587456\\\\nhttps://twitter.com/deliprao/status/1599451192215887872\\\\nhttps://twitter.com/michlbrmly/status/1599168681711656961\\\\nhttps://twitter.com/zoink/status/1599281052115034113\\\\n\\\\n\\\\nLinks:\\\\nhttps://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\"",
    "lengthSeconds": "1915",
    "uploadDate": "2022-12-07",
    "thumbnail_url": "https://i.ytimg.com/vi/0A8ljAkdFtg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=r8wiBA3ZaQE",
    "title": "[ML News] GPT-4 Rumors | AI Mind Reading | Neuron Interaction Solved | AI Theorem Proving",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, introduction to deep learning, what is deep learning, ml news, mlnews, kilcher news, ai news, gpt4, gpt 4, gpt 4 rumors, gpt",
    "scraped_at": 1684582635.3365326,
    "genre": "Science",
    "views": "105796",
    "desc": "#ai #mlnews #gpt4\\\\n\\\\nYour weekly news from the AI \\\\u0026 Machine Learning world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n0:25 - AI reads brain signals to predict what you\\'re thinking\\\\n3:00 - Closed-form solution for neuron interactions\\\\n4:15 - GPT-4 rumors\\\\n6:50 - Cerebras supercomputer\\\\n7:45 - Meta releases metagenomics atlas\\\\n9:15 - AI advances in theorem proving\\\\n10:40 - Better diffusion models with expert denoisers\\\\n12:00 - BLOOMZ \\\\u0026 mT0\\\\n13:05 - ICLR reviewers going mad\\\\n21:40 - Scaling Transformer inference\\\\n22:10 - Infinite nature flythrough generation\\\\n23:55 - Blazing fast denoising\\\\n24:45 - Large-scale AI training with MultiRay\\\\n25:30 - arXiv to include Hugging Face spaces\\\\n26:10 - Multilingual Diffusion\\\\n26:30 - Music source separation\\\\n26:50 - Multilingual CLIP\\\\n27:20 - Drug response prediction\\\\n27:50 - Helpful Things\\\\n\\\\nERRATA:\\\\nHF did not acquire spaces, they launched spaces themselves and supported Gradio from the start. They later acquired Gradio.\\\\n\\\\nReferences:\\\\nAI reads brain signals to predict what you\\'re thinking\\\\nhttps://mind-vis.github.io/?s=09\\\\u0026utm_source=pocket_saves\\\\nhttps://neurosciencenews.com/bmi-internal-speech-21837/\\\\n\\\\nClosed-form solution for neuron interactions\\\\nhttps://twitter.com/ramin_m_h/status/1592585672606769153/photo/1\\\\nhttps://github.com/raminmh/CfC\\\\nhttps://github.com/raminmh/CfC/blob/main/torch_cfc.py\\\\n\\\\nGPT-4 rumors\\\\nhttps://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley?utm_source=pocket_reader\\\\n\\\\nCerebras supercomputer\\\\nhttps://www.cerebras.net/andromeda/\\\\n\\\\nMeta releases metagenomics atlas\\\\nhttps://ai.facebook.com/blog/protein-folding-esmfold-metagenomics/\\\\nhttps://www.genome.gov/genetics-glossary/Metagenomics\\\\n\\\\nAI advances in theorem proving\\\\nhttps://ai.facebook.com/blog/ai-math-theorem-proving/\\\\nhttps://marketplace.visualstudio.com/items?itemName=jroesch.lean\\\\n\\\\nBetter diffusion models with expert denoisers\\\\nhttps://deepimagination.cc/eDiffi/\\\\n\\\\nBLOOMZ \\\\u0026 mT0\\\\nhttps://arxiv.org/abs/2211.01786?utm_source=pocket_reader\\\\nhttps://huggingface.co/bigscience/bloomz?text=Suggest+at+least+five+related+search+terms+to+%22M%E1%BA%A1ng+neural+nh%C3%A2n+t%E1%BA%A1o%22.\\\\n\\\\nICLR reviewers going mad\\\\nhttps://twitter.com/XiangruTang/status/1589703605098975237?utm_source=pocket_reader\\\\nhttps://twitter.com/BlancheMinerva/status/1588164585961422849?utm_source=pocket_reader\\\\nhttps://openreview.net/forum?id=pfuqQQCB34\\\\nhttps://twitter.com/peter_richtarik/status/1591408710366408706?utm_source=pocket_reader\\\\n\\\\nScaling Transformer inference\\\\nhttps://arxiv.org/abs/2211.05102\\\\n\\\\nInfinite nature flythrough generation\\\\nhttps://ai.googleblog.com/2022/11/infinite-nature-generating-3d.html?utm_source=pocket_reader\\\\n\\\\nBlazing fast denoising\\\\nhttps://github.com/dome272/Paella\\\\nhttps://arxiv.org/abs/2211.07292\\\\n\\\\nLarge-scale AI training with MultiRay\\\\nhttps://ai.facebook.com/blog/multiray-large-scale-AI-models/\\\\n\\\\narXiv to include Hugging Face spaces\\\\nhttps://blog.arxiv.org/2022/11/17/discover-state-of-the-art-machine-learning-demos-on-arxiv/\\\\n\\\\nMultilingual Diffusion\\\\nhttps://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltDiffusion\\\\n\\\\nMusic source separation\\\\nhttps://github.com/facebookresearch/demucs\\\\nhttps://arxiv.org/abs/2211.08553\\\\n\\\\nMultilingual CLIP\\\\nhttps://twitter.com/rom1504/status/1593719037808320513\\\\n\\\\nDrug response prediction\\\\nhttps://phys.org/news/2022-10-ai-accurately-human-response-drug.html\\\\n\\\\nhttps://huggingface.co/Onodofthenorth/SD_PixelArt_SpriteSheet_Generator\\\\nhttps://huggingface.co/spaces/ronvolutional/sd-spritesheets\\\\nhttps://github.com/daspartho/prompt-extend\\\\nhttps://huggingface.co/blog/fine-tune-whisper\\\\nhttps://twitter.com/CarsonKatri/status/1585412662724272128\\\\nhttps://github.com/carson-katri/dream-textures/\\\\nhttps://www.youtube.com/playlist?list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr\\\\nhttps://github.com/xl0/lovely-tensors\\\\nhttps://github.com/jerryjliu/gpt_index\\\\nhttps://colab.research.google.com/drive/1o1qYJcFeywzCIdkfKJy7cTpgZTCM2EI4\\\\nhttps://dagshub.com/blog/launching-data-streaming-and-upload/\\\\nhttps://dagshub.com/blog/build-an-end-2-end-active-learning-pipeline-part-1/\\\\nhttps://github.com/run-ai/genv\\\\nhttps://arxiv.org/abs/2210.14868\\\\nhttps://github.com/timeseriesAI/tsai\\\\nhttps://medium.com/@yangyou_berkeley/diffusion-pretraining-and-hardware-fine-tuning-can-be-almost-7x-cheaper-85e970fe207b\\\\nhttps://medium.com/@hpcaitech/accelerating-structure-prediction-of-protein-monomers-and-multimer-by-11-times-769715dcb5b5\\\\nhttps://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion\\\\nhttps://arxiv.org/abs/2211.03726\\\\nhttps://github.com/Deci-AI/super-gradients\\\\nhttps://github.com/facebookresearch/shumai\\\\nhttps://github.com/huggingface/safetensors\\\\nhttps://github.com/google/learned_optimization/tree/main/learned_optimization/research/general_lopt\\\\nhttps://github.com/NVIDIA-Merlin/dataloader\\\\nhttps://loda-lang.org/\\\\nhttps://loda-lang.org/edit/\\\\nhttps://github.com/EelcoHoogendoorn/numga\\\\nhttps://arxiv.org/abs/2210.07316v1\\\\nhttps://huggingface.co/spaces/mteb/leaderboard\\\\nhttps://twitter.com/natfriedman/status/1575631194032549888\\\\nhttps://github.com/nat/natbot\"",
    "lengthSeconds": "2515",
    "uploadDate": "2022-11-27",
    "thumbnail_url": "https://i.ytimg.com/vi/r8wiBA3ZaQE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ciNMc0Czmfc",
    "title": "CICERO: An AI agent that negotiates, persuades, and cooperates with people",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, introduction to deep learning, deep learning tutorial, meta, meta ai, meta cicero, cicero ai, meta cicero ai, diplomacy ai, web diplomacy, facebook ai, fair ai, language model, politics ai, geopolitics ai, ai online game",
    "scraped_at": 1684582627.9386733,
    "genre": "Science",
    "views": "36213",
    "desc": "#ai #cicero #diplomacy \\\\n\\\\nA team from Meta AI has developed Cicero, an agent that can play the game Diplomacy, in which players have to communicate via chat messages to coordinate and plan into the future.\\\\n\\\\nPaper Title: Human-level play in the game of Diplomacy by combining language models with strategic reasoning\\\\n\\\\nCommented game by human expert: https://www.youtube.com/watch?v=u5192bvUS7k\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n9:50 - AI in cooperation games\\\\n13:50 - Cicero agent overview\\\\n25:00 - A controllable dialogue model\\\\n36:50 - Dialogue-conditional strategic planning\\\\n49:00 - Message filtering\\\\n53:45 - Cicero\\'s play against humans\\\\n55:15 - More examples \\\\u0026 discussion\\\\n\\\\nHomepage: https://ai.facebook.com/research/cicero/\\\\nCode: https://github.com/facebookresearch/diplomacy_cicero\\\\nBlog: https://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/\\\\nPaper: https://www.science.org/doi/10.1126/science.ade9097\\\\n\\\\nAbstract:\\\\nDespite much progress in training AI systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players\\' beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.\\\\n\\\\nAuthors: Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3663",
    "uploadDate": "2022-11-25",
    "thumbnail_url": "https://i.ytimg.com/vi/ciNMc0Czmfc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZTs_mXwMCs8",
    "title": "Galactica: A Large Language Model for Science (Drama & Paper Review)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, galactica, meta, meta ai, facebook ai, ai science, galactica ai, galactica model, yann lecun, research, fair, deep learning tutorial, what is deep learning, introduction to deep learning",
    "scraped_at": 1684582628.1986675,
    "genre": "Science",
    "views": "41625",
    "desc": "#ai #galactica #meta\\\\n\\\\nGalactica is a language model trained on a curated corpus of scientific documents, such as papers, knowledge bases, reviews, and other articles. The model can be used in a generative fasion to assist scientific writing, do reference prediction, and much more, including a new approach to do step-by-step reasoning using a clever encoding of intermediate steps. This video explains the paper, but also dives into the drama that ensued once Meta released a public demo of the model.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n1:30 - Drama around the public demo\\\\n16:00 - Start of paper review\\\\n20:30 - Dataset construction and encoding\\\\n23:30 - Encoding step-by-step reasoning using a scratchpad\\\\n33:00 - Modelling scientific references \\\\u0026 citations\\\\n35:05 - Prompt Pre-Training\\\\n37:10 - Architecture details\\\\n38:30 - Experimental results\\\\n49:20 - Conclusion\\\\n\\\\nPaper: https://galactica.org/static/paper.pdf\\\\nWebsite: https://galactica.org/explore/\\\\n\\\\nAbstract:\\\\nInformation overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.\\\\n\\\\nAuthors: Ross Taylor Marcin Kardas Guillem Cucurull Thomas Scialom Anthony Hartshorn Elvis Saravia Andrew Poulton Viktor Kerkez Robert Stojnic\\\\n\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3092",
    "uploadDate": "2022-11-19",
    "thumbnail_url": "https://i.ytimg.com/vi/ZTs_mXwMCs8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=TOo-HnjjuhU",
    "title": "[ML News] Multiplayer Stable Diffusion | OpenAI needs more funding | Text-to-Video models incoming",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, ml news, kilcher news, ml news yannic, phenaki, imagen, imagen video, phenaki ai, phenaki google, google ai, make a video, ai video, text to video, ai video generator, huggingface, hugging face, what is deep learning, deep learning tutorial, introduction to deep learning, mlinpl, ml in pl",
    "scraped_at": 1684582630.4973485,
    "genre": "Science",
    "views": "22362",
    "desc": "#mlnews #ai #mlinpl\\\\n\\\\nYour news from the world of Machine Learning!\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n1:25 - Stable Diffusion Multiplayer\\\\n2:15 - Huggingface: DOI for Models \\\\u0026 Datasets\\\\n3:10 - OpenAI asks for more funding\\\\n4:25 - The Stack: Source Code Dataset\\\\n6:30 - Google Vizier Open-Sourced\\\\n7:10 - New Models\\\\n11:50 - Helpful Things\\\\n20:30 - Prompt Databases\\\\n22:15 - Lexicap by Karpathy\\\\n\\\\nReferences:\\\\nStable Diffusion Multiplayer\\\\nhttps://huggingface.co/spaces/huggingface-projects/stable-diffusion-multiplayer?roomid=room-0\\\\n\\\\nHuggingface: DOI for Models \\\\u0026 Datasets\\\\nhttps://huggingface.co/blog/introducing-doi\\\\n\\\\nOpenAI asks for more funding\\\\nhttps://www.theinformation.com/articles/openai-valued-at-nearly-20-billion-in-advanced-talks-with-microsoft-for-more-funding\\\\nhttps://www.wsj.com/articles/microsoft-in-advanced-talks-to-increase-investment-in-openai-11666299548\\\\n\\\\nThe Stack: Source Code Dataset\\\\nhttps://huggingface.co/datasets/bigcode/the-stack?utm_source=pocket_mylist\\\\n\\\\nGoogle Vizier Open-Sourced\\\\nhttps://github.com/google/vizier\\\\n\\\\nNew Models\\\\nhttps://imagen.research.google/video/\\\\nhttps://phenaki.github.io/\\\\nhttps://makeavideo.studio/?utm_source=pocket_mylist\\\\nhttps://dreamfusion3d.github.io/\\\\nhttps://arxiv.org/pdf/2210.15257.pdf\\\\nhttps://huggingface.co/spaces/PaddlePaddle/ERNIE-ViLG\\\\nhttps://github.com/PaddlePaddle/PaddleHub\\\\n\\\\nHelpful Things\\\\nhttps://thecharlieblake.co.uk/visualising-ml-number-formats\\\\nhttps://griddly.ai/\\\\nhttps://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/?utm_source=twitter\\\\u0026utm_medium=organic_social\\\\u0026utm_campaign=eng2022h2\\\\nhttps://twitter.com/psuraj28/status/1580640841583902720?utm_source=pocket_mylist\\\\nhttps://huggingface.co/blog/stable_diffusion_jax\\\\nhttps://github.com/Lightning-AI/stable-diffusion-deploy\\\\nhttps://lightning.ai/docs/stable/\\\\nhttps://github.com/CarperAI/trlx\\\\nhttps://github.com/DLR-RM/rl-baselines3-zoo\\\\nhttps://github.com/Sea-Snell/JAXSeq\\\\nhttps://www.reddit.com/r/MachineLearning/comments/xoitw9/p_albumentations_13_is_released_a_python_library/?utm_source=pocket_mylist\\\\nhttps://twitter.com/Warvito/status/1570691960792580096?utm_source=pocket_mylist\\\\nhttps://arxiv.org/abs/2209.07162\\\\nhttps://academictorrents.com/details/63aeb864bbe2115ded0aa0d7d36334c026f0660b\\\\nhttps://huggingface.co/spaces/THUDM/CodeGeeX\\\\nhttps://ai.facebook.com/blog/gpu-inference-engine-nvidia-amd-open-source/?utm_source=twitter\\\\u0026utm_medium=organic_social\\\\u0026utm_campaign=blog\\\\nhttps://github.com/nerfstudio-project/nerfstudio\\\\nhttps://www.nerfacc.com/en/latest/\\\\nhttps://github.com/dstackai/dstack\\\\nhttps://www.reddit.com/r/MachineLearning/comments/yeyxlo/p_openai_whisper_3x_cpu_inference_speedup/?utm_source=pocket_mylist\\\\nhttps://github.com/MiscellaneousStuff/openai-whisper-cpu/issues/1\\\\n\\\\nPrompt Databases\\\\nhttps://huggingface.co/datasets/poloclub/diffusiondb\\\\nhttps://publicprompts.art/\\\\nhttps://visualise.ai/\\\\nhttps://twitter.com/SamuelAlbanie/status/1574111928431026179/photo/1\\\\n\\\\nLexicap by Karpathy\\\\nhttps://karpathy.ai/lexicap/0139-large.html\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1372",
    "uploadDate": "2022-11-13",
    "thumbnail_url": "https://i.ytimg.com/vi/TOo"
  },
  {
    "link": "watch?v=W5M-dvzpzSQ",
    "title": "The New AI Model Licenses have a Legal Loophole (OpenRAIL-M of BLOOM, Stable Diffusion, etc.)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openrail, openarail m, ai license, ai model license, ai model copyright, stable diffusion copyright, bloom copyright, stable diffusion license, open source ai, machine learning open source, ai art license, ai art copyright",
    "scraped_at": 1684582637.1190643,
    "genre": "Science",
    "views": "17452",
    "desc": "#ai #stablediffusion #license \\\\n\\\\nSo-called responsible AI licenses are stupid, counterproductive, and have a dangerous legal loophole in them.\\\\n\\\\nOpenRAIL++ License here: https://www.ykilcher.com/license\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n0:40 - Responsible AI Licenses (RAIL) of BLOOM and Stable Diffusion\\\\n3:35 - Open source software\\'s dilemma of bad usage and restrictions\\\\n8:45 - Good applications, bad applications\\\\n12:45 - A dangerous legal loophole\\\\n15:50 - OpenRAIL++ License\\\\n16:50 - This has nothing to do with copyright\\\\n26:00 - Final thoughts\\\\n\\\\nReferences:\\\\nhttps://huggingface.co/CompVis/stable-diffusion/tree/main\\\\nhttps://huggingface.co/spaces/CompVis/stable-diffusion-license\\\\nhttps://huggingface.co/bigscience/bloom?text=34%2B10%3D44+%0A54%2B20%3D\\\\nhttps://huggingface.co/spaces/bigscience/license\\\\nhttps://huggingface.co/runwayml/stable-diffusion-v1-5\\\\nhttps://huggingface.co/spaces/CompVis/stable-diffusion-license/raw/main/license.txt\\\\nhttps://www.gnu.org/philosophy/programs-must-not-limit-freedom-to-run.en.html\\\\nhttps://www.gnu.org/philosophy/free-sw.html#four-freedoms\\\\nhttps://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license\\\\nhttps://bigscience.huggingface.co/blog/bigscience-ethical-charter\\\\nhttps://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses\\\\nhttps://en.wikipedia.org/wiki/Copyright#Eligible_works\\\\nhttps://en.wikipedia.org/wiki/Creative_work\\\\nhttps://www.pearlcohen.com/copyright-office-reiterates-that-works-created-by-ai-cannot-be-copyrighted/\\\\nhttps://jipel.law.nyu.edu/vol-8-no-2-1-hedrick/#II\\\\nhttps://www.ykilcher.com/license\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1671",
    "uploadDate": "2022-11-09",
    "thumbnail_url": "https://i.ytimg.com/vi/W5M"
  },
  {
    "link": "watch?v=_NMQyOu2HTo",
    "title": "ROME: Locating and Editing Factual Associations in GPT (Paper Explained & Author Interview)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582631.3893785,
    "genre": "Science",
    "views": "30847",
    "desc": "#ai #language #knowledge \\\\n\\\\nLarge Language Models have the ability to store vast amounts of facts about the world. But little is known, how these models actually do this. This paper aims at discovering the mechanism and location of storage and recall of factual associations in GPT models, and then proposes a mechanism for the targeted editing of such facts, in form of a simple rank-one update to a single MLP layer. This has wide implications both for how we understand such models\\' inner workings, and for our ability to gain greater control over such models in the future.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n1:40 - What are the main questions in this subfield?\\\\n6:55 - How causal tracing reveals where facts are stored\\\\n18:40 - Clever experiments show the importance of MLPs\\\\n24:30 - How do MLPs store information?\\\\n29:10 - How to edit language model knowledge with precision?\\\\n36:45 - What does it mean to know something?\\\\n39:00 - Experimental Evaluation \\\\u0026 the CounterFact benchmark\\\\n45:40 - How to obtain the required latent representations?\\\\n51:15 - Where is the best location in the model to perform edits?\\\\n58:00 - What do these models understand about language?\\\\n1:02:00 - Questions for the community\\\\n\\\\nPaper: https://arxiv.org/abs/2202.05262\\\\nFollow-up paper on Mass-Editing Memory in a Transformer: https://arxiv.org/abs/2210.07229\\\\n\\\\nAbstract:\\\\nWe analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model\\'s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL\\\\n\\\\nAuthors: Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3899",
    "uploadDate": "2022-11-04",
    "thumbnail_url": "https://i.ytimg.com/vi/_NMQyOu2HTo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=igS2Wy8ur5U",
    "title": "Is Stability turning into OpenAI?",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, stable diffusion, stability ai, stable diffusion subreddit, stable diffusion discord, runwayml, runway ml, stable",
    "scraped_at": 1684582628.028641,
    "genre": "Science",
    "views": "29171",
    "desc": "#stablediffusion #aiart #openai \\\\n\\\\nStability AI has stepped into some drama recently. They are accused of a hostile takeover of the community-led sub-reddits and Discord servers, of going after an alternative web UI, and of falsely dealing out IP takedown notices.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n2:40 - Stability takes over community Discord \\\\u0026 Reddit\\\\n14:50 - AUTOMATIC1111 web UI, stolen or not ?\\\\n24:50 - Stable Diffusion 1.5 takedown request\\\\n31:20 - Scary: Stability CIO statement on safety \\\\u0026 openness\\\\n\\\\nReferences:\\\\nhttps://finance.yahoo.com/news/stability-ai-startup-behind-stable-170151950.html?guccounter=1\\\\nhttps://analyticsindiamag.com/when-stability-ai-went-rogue-on-reddit-rampage%ef%bf%bc/\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y12jo3/comment/irvsek2/?utm_source=share\\\\u0026utm_medium=web2x\\\\u0026context=3\\\\nhttps://imgur.com/a/JjpRpmP\\\\nhttps://imgur.com/a/JjpRpmP\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y19kdh/mod_here_my_side_of_the_story/\\\\nhttps://imgur.com/a/TpTMr0S\\\\nhttps://imgur.com/a/zTae3hz\\\\nhttps://imgur.com/a/QDNA6cG\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y17xn1/emad_in_discord_right_now/\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y156op/new_mods_hijacked_this_sub_2_weeks_ago/\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y1nc7t/rstablediffusion_should_be_independent_and_run_by/\\\\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui\\\\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui-feature-showcase\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y34h2a/comment/isiymmj/?context=3\\\\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/2509\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y1uuvj/automatic1111_did_nothing_wrong_some_people_are/is298ix/?context=3\\\\nhttps://www.reddit.com/r/OutOfTheLoop/comments/y22zg6/comment/is1h02a/\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y1uuvj/automatic1111_did_nothing_wrong_some_people_are/\\\\nhttps://imgur.com/a/Z2QsOEw\\\\nhttps://www.reddit.com/r/StableDiffusion/comments/y0uvps/automatic1111_removed_from_pinned_guide/\\\\nhttps://huggingface.co/runwayml/stable-diffusion-v1-5/discussions/1#6351a36ca9a9ae18220726c7\\\\nhttps://danieljeffries.substack.com/p/why-the-future-of-open-source-ai\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2344",
    "uploadDate": "2022-11-01",
    "thumbnail_url": "https://i.ytimg.com/vi/igS2Wy8ur5U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_okxGdHM5b8",
    "title": "Neural Networks are Decision Trees (w/ Alexander Mattick)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582632.400347,
    "genre": "Science",
    "views": "52478",
    "desc": "#neuralnetworks #machinelearning #ai \\\\n\\\\nAlexander Mattick joins me to discuss the paper \\\\\"",
    "lengthSeconds": "1910",
    "uploadDate": "2022-10-21",
    "thumbnail_url": "https://i.ytimg.com/vi/_okxGdHM5b8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3N3Bl5AA5QU",
    "title": "This is a game changer! (AlphaTensor by DeepMind explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deepmind, deep mind, deepmind alphatensor, alpha tensor, deepmind math, google deep mind, google deepmind, matrix multiplication, ai matrix multiplication, matrix multiplication reinforcement learning, alphazero, alpha zero, alphazero math, deep learning tutorial, introduction to deep learning, what is deep learning, alphatensor explained, alpha tensor explained",
    "scraped_at": 1684582628.292668,
    "genre": "Science",
    "views": "173483",
    "desc": "#alphatensor #deepmind #ai \\\\n\\\\nMatrix multiplication is the most used mathematical operation in all of science and engineering. Speeding this up has massive consequences. Thus, over the years, this operation has become more and more optimized. A fascinating discovery was made when it was shown that one actually needs less than N^3 multiplication operations to multiply to NxN matrices. DeepMind goes a step further and creates AlphaTensor, a Deep Reinforcement Learning algorithm that plays a single-player game, TensorGame, in order to find even more optimized algorithms for matrix multiplication. And it turns out, there exists a plethora of undiscovered matrix multiplication algorithms, which not only will make everything from computers to smart toasters faster, but also bring new insights into fundamental math and complexity theory.\\\\n\\\\nSponsor: Assembly AI\\\\nLink: https://www.assemblyai.com/?utm_source=youtube\\\\u0026utm_medium=social\\\\u0026utm_campaign=yannic_sentiment\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:50 - Sponsor: Assembly AI (link in description)\\\\n3:25 - What even is Matrix Multiplication?\\\\n6:10 - A very astounding fact\\\\n8:45 - Trading multiplications for additions\\\\n12:35 - Matrix Multiplication as a Tensor\\\\n17:30 - Tensor Decompositions\\\\n20:30 - A formal way of finding multiplication algorithms\\\\n31:00 - How to formulate this as a game?\\\\n39:30 - A brief primer on AlphaZero / MCTS\\\\n45:40 - The Results\\\\n48:15 - Optimizing for different hardware\\\\n52:40 - Expanding fundamental math\\\\n53:45 - Summary \\\\u0026 Final Comments\\\\n\\\\nPaper: https://www.nature.com/articles/s41586-022-05172-4\\\\nTitle: Discovering faster matrix multiplication algorithms with reinforcement learning\\\\n\\\\nAbstract:\\\\nImproving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\\xe2\\x80\\x94from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\\xe2\\x80\\x89\\xc3\\x97\\xe2\\x80\\x894 matrices in a finite field, where AlphaTensor\\xe2\\x80\\x99s algorithm improves on Strassen\\xe2\\x80\\x99s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\\xe2\\x80\\x99s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.\\\\n\\\\nAuthors: Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis \\\\u0026 Pushmeet Kohli\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3307",
    "uploadDate": "2022-10-07",
    "thumbnail_url": "https://i.ytimg.com/vi/3N3Bl5AA5QU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=S-7r0-oysaU",
    "title": "[ML News] OpenAI's Whisper | Meta Reads Brain Waves | AI Wins Art Fair, Annoys Humans",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, ml news, machine learning news, ml news yannic, sql injection, neural network attack, openai, open ai, openai whisper, open ai whisper, openai whisper speech recognition system",
    "scraped_at": 1684582632.312377,
    "genre": "Science",
    "views": "51168",
    "desc": "#mlnews #openai #ai \\\\n\\\\nEverything important going on in the ML world right here!\\\\n\\\\nSponsor: Paperspace\\\\nhttps://www.paperspace.com/?src=yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n0:20 - Whisper: Open-Source Speech Transcription\\\\n6:30 - Sponsor: Paperspace\\\\n9:30 - Meta: How the brain hears audio\\\\n11:25 - PyTorch moves to Linux Foundation\\\\n12:15 - French Government uses AI to find unlicensed swimming pools\\\\n13:35 - AlphaFold extends database\\\\n14:10 - John Carmack raises 20M to build AGI0729970510422016\\\\n16:10 - Cerebras achieves model size record\\\\n17:40 - Andrej Karpathy on YouTube\\\\n18:35 - ColabPro changes pricing\\\\n19:15 - Huggingface runs evaluation on the hub\\\\n20:35 - AI wins art fair\\\\n22:50 - PaLI: Multilingual Language-Image Learning\\\\n23:40 - Operationalizing Machine Learning: An Interview Study\\\\n24:35 - LAION OpenCLIP: New Models\\\\n25:10 - BlenderBot 3 175B Released\\\\n25:45 - OWL-ViT on the Hub\\\\n26:10 - GLM-130B\\\\n26:35 - Ernie-ViLG\\\\n27:10 - Digitizing Smell using Molecular Maps\\\\n28:00 - AlexaTM 20B\\\\n29:00 - Audio-LM\\\\n29:45 - Useful Things\\\\n37:20 - Raycasting in JAX\\\\n38:00 - GPT-3 Prompt Injection\\\\n39:20 - GPT-3 plus Python\\\\n40:45 - Game Emulation via DNN\\\\n\\\\nReferences here (external bc too long for YT):\\\\nhttps://early-hair-c20.notion.site/ML-News-Whisper-References-17e51ca488ef4eb6b8be12749c10870c\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2551",
    "uploadDate": "2022-10-02",
    "thumbnail_url": "https://i.ytimg.com/vi/S"
  },
  {
    "link": "watch?v=xbxe-x6wvRw",
    "title": "[ML News] Stable Diffusion Takes Over! (Open Source AI Art)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, stablediffusion, stable diffusion, ml news, mlnews, ml news yannic, yannick ml news, what is deep learning, introduction to deep learning, deep learning tutorial",
    "scraped_at": 1684582633.5953472,
    "genre": "Science",
    "views": "81508",
    "desc": "#stablediffusion #aiart #mlnews \\\\n\\\\nStable Diffusion has been released and is riding a wave of creativity and collaboration. But not everyone is happy about this...\\\\n\\\\nSponsor: NVIDIA\\\\nGPU Raffle: https://ykilcher.com/gtc\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n0:30 - What is Stable Diffusion?\\\\n2:25 - Open-Source Contributions and Creations\\\\n7:55 - Textual Inversion\\\\n9:30 - OpenAI vs Open AI\\\\n14:20 - Journalists be outraged\\\\n16:20 - AI Ethics be even more outraged\\\\n19:45 - Do we need a new social contract?\\\\n21:30 - More applications\\\\n22:55 - Helpful Things\\\\n23:45 - Sponsor: NVIDIA (\\\\u0026 how to enter the GPU raffle)\\\\n\\\\nReferences: https://early-hair-c20.notion.site/Stable-Diffusion-Takes-Over-Referenes-7a2f45b8f7e04ae0ba19dbfcd2b7f7c0\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1647",
    "uploadDate": "2022-09-18",
    "thumbnail_url": "https://i.ytimg.com/vi/xbxe"
  },
  {
    "link": "watch?v=0PAiQ1jTN5k",
    "title": "How to make your CPU as fast as a GPU - Advances in Sparsity w/ Nir Shavit",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, neuralmagic, neural magic, deepsparse, deep sparse, what is deep learning, deep learning tutorial, introduction to deep learning, cpu vs gpu, deep learning on cpu, deep learning cpu vs gpu",
    "scraped_at": 1684582633.8955123,
    "genre": "Science",
    "views": "47299",
    "desc": "#ai #sparsity #gpu \\\\n\\\\nSparsity is awesome, but only recently has it become possible to properly handle sparse models at good performance. Neural Magic does exactly this, using a plain CPU. No specialized hardware needed, just clever algorithms for pruning and forward-propagation of neural networks. Nir Shavit and I talk about how this is possible, what it means in terms of applications, and why sparsity should play a much larger role in the Deep Learning community.\\\\n\\\\nSponsor: AssemblyAI\\\\nLink: https://www.assemblyai.com/?utm_source=youtube\\\\u0026utm_medium=social\\\\u0026utm_campaign=yannic_autochapters\\\\n\\\\nCheck out Neural Magic: https://neuralmagic.com/\\\\nand DeepSparse: https://github.com/neuralmagic/deepsparse\\\\n\\\\nOUTLINE:\\\\n0:00 Introduction\\\\n1:08 Sponsor: AssemblyAI\\\\n2:50 Start of Interview\\\\n4:15 How the NIR company was founded? \\\\n5:10 What is Sparsity about? \\\\n9:30 Link between the human brain and sparsity\\\\n12:10 Where should the extra resource that the human brain doesn\\'t have go?\\\\n14:40 Analogy for Sparse Architecture\\\\n16:48 Possible future for Sparse Architecture as standard architure for Neural Networks\\\\n20:08 Pruning \\\\u0026 Sparsification\\\\n22:57 What keeps us from building sparse models?\\\\n25:34 Why are GPUs so unsuited for sparse models?\\\\n28:47 CPU and GPU in connection with memory\\\\n30:14 What Neural Magic does?\\\\n32:54 How do you deal with overlaps in tensor columns?\\\\n33:41 The best type of sparsity to execute tons of CPU\\\\n37:24 What kind of architecture would make the best use out of a combined system of CPUs and GPUs?\\\\n41:04 Graph Neural Networks in connection to sparsity\\\\n43:04 Intrinsic connection between the Sparsification of Neural Networks, Non Layer-Wise Computation, Blockchain Technology, Smart Contracts and Distributed Computing\\\\n45:23 Neural Magic\\'s target audience\\\\n48:16 Is there a type of model where it works particularly well and the type where it doesn\\'t?\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3020",
    "uploadDate": "2022-09-17",
    "thumbnail_url": "https://i.ytimg.com/vi/0PAiQ1jTN5k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=K-cXYoqHxBc",
    "title": "More Is Different for AI - Scaling Up, Emergence, and Paperclip Maximizers (w/ Jacob Steinhardt)",
    "tags": "film, udost",
    "scraped_at": 1684582631.5713468,
    "genre": "Science",
    "views": "19243",
    "desc": "#ai #interview #research \\\\n\\\\nJacob Steinhardt believes that future AI systems will be qualitatively different than the ones we know currently. We talk about how emergence happens when scaling up, what implications that has on AI Safety, and why thought experiments like the Paperclip Maximizer might be more useful than most people think.\\\\n\\\\nOUTLINE:\\\\n0:00 Introduction\\\\n1:10 Start of Interview\\\\n2:10 Blog posts series\\\\n3:56 More Is Different for AI (Blog Post)\\\\n7:40 Do you think this emergence is mainly a property from the interaction of things?\\\\n9:17 How does phase transition or scaling-up play into AI and Machine Learning?\\\\n12:10 GPT-3 as an example of qualitative difference in scaling up\\\\n14:08 GPT-3 as an emergent phenomenon in context learning\\\\n15:58 Brief introduction of different viewpoints on the future of AI and its alignment\\\\n18:51 How does the phenomenon of emergence play into this game between the Engineering and the Philosophy viewpoint? \\\\n22:41 Paperclip Maximizer on AI safety and alignment\\\\n31:37 Thought Experiments\\\\n37:34 Imitative Deception\\\\n39:30 TruthfulQA: Measuring How Models Mimic Human Falsehoods (Paper)\\\\n42:24 ML Systems Will Have Weird Failure Models (Blog Post)\\\\n51:10 Is there any work to get a system to be deceptive?\\\\n54:37 Empirical Findings Generalize Surprisingly Far (Blog Post)\\\\n1:00:18 What would you recommend to guarantee better AI alignment or safety?\\\\n1:05:13 Remarks\\\\n\\\\nReferences:\\\\nhttps://bounded-regret.ghost.io/more-is-different-for-ai/\\\\nhttps://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit#heading=h.n1wk9bxo847o\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3996",
    "uploadDate": "2022-09-13",
    "thumbnail_url": "https://i.ytimg.com/vi/K"
  },
  {
    "link": "watch?v=2ethDz9KnLk",
    "title": "The hidden dangers of loading open-source AI models (ARBITRARY CODE EXPLOIT!)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, wandb, huggingface, hugging face, is hugging face dangerous, is ai dangerous, ai exploit, pickle exploit, pytorch exploit, is hugging face safe, reduce, python pickle, python pickletools, python pickle exploit, pytorch pickle exploit, ai model backdoor, arbitrary code execution, pickle code injection, pytorch danger, pytorch load danger, is pytorch safe, is pytorch dangerous",
    "scraped_at": 1684582629.5673053,
    "genre": "Science",
    "views": "48543",
    "desc": "#huggingface #pickle #exploit \\\\n\\\\nDid you know that something as simple as loading a model can execute arbitrary code on your machine?\\\\n\\\\nTry the model: https://huggingface.co/ykilcher/totally-harmless-model\\\\nGet the code: https://github.com/yk/patch-torch-save\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nGo here: https://wandb.me/yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n1:10 - Sponsor: Weights \\\\u0026 Biases\\\\n3:20 - How Hugging Face models are loaded\\\\n5:30 - From PyTorch to pickle\\\\n7:10 - Understanding how pickle saves data\\\\n13:00 - Executing arbitrary code\\\\n15:05 - The final code\\\\n17:25 - How can you protect yourself?\\\\n\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1183",
    "uploadDate": "2022-09-02",
    "thumbnail_url": "https://i.ytimg.com/vi/2ethDz9KnLk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_7xpGve9QEE",
    "title": "The Future of AI is Self-Organizing and Self-Assembling (w/ Prof. Sebastian Risi)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, sebastian risi, copenhagen, minecraft ai, self",
    "scraped_at": 1684582630.8463485,
    "genre": "Science",
    "views": "38324",
    "desc": "#ai #selforganization #emergence\\\\n\\\\nRead Sebastian\\'s article here: https://sebastianrisi.com/self_assembling_ai/\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n2:25 - Start of Interview\\\\n4:00 - The intelligence of swarms\\\\n9:15 - The game of life \\\\u0026 neural cellular automata\\\\n14:10 - What\\'s missing from neural CAs?\\\\n17:20 - How does local computation compare to centralized computation?\\\\n25:40 - Applications beyond games and graphics\\\\n33:00 - Can we do away with goals?\\\\n35:30 - Where do these methods shine?\\\\n43:30 - The paradox of scales \\\\u0026 brains\\\\n49:45 - Connections to graphical systems \\\\u0026 GNNs\\\\n51:30 - Could this solve ARC?\\\\n57:45 - Where can people get started?\\\\n\\\\nReferences:\\\\nhttps://sebastianrisi.com/\\\\nhttps://modl.ai/\\\\nhttps://sebastianrisi.com/self_assembling_ai/\\\\nhttps://twitter.com/risi1979/status/1519053654921293827?cxt=HHwWhsC9hYfQ4ZQqAAAA\\\\nhttps://distill.pub/2020/growing-ca/\\\\nhttps://arxiv.org/abs/2201.12360?source=techstories.org\\\\nhttps://distill.pub/2020/selforg/mnist/\\\\nhttps://arxiv.org/pdf/2204.11674.pdf\\\\nhttps://github.com/fchollet/ARC\\\\nhttps://github.com/volotat/ARC-Game\\\\nhttp://animalaiolympics.com/AAI/\\\\nhttps://www.deepmind.com/publications/alchemy-a-structured-task-distribution-for-meta-reinforcement-learning-f\\\\nhttps://melaniemitchell.me/BooksContent/CAGTReviews.html \\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3708",
    "uploadDate": "2022-08-26",
    "thumbnail_url": "https://i.ytimg.com/vi/_7xpGve9QEE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YQ2QtKcK2dA",
    "title": "The Man behind Stable Diffusion",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, stabilityai, stabiliity ai, stablediffusion, stable diffusion, eleuther ai, laion, laion 5b, open source, ai art, diffusion models, open source ai art",
    "scraped_at": 1684582631.9313471,
    "genre": "Science",
    "views": "139495",
    "desc": "#stablediffusion #ai #stabilityai\\\\n\\\\nAn interview with Emad Mostaque, founder of Stability AI.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:30 - What is Stability AI?\\\\n3:45 - Where does the money come from?\\\\n5:20 - Is this the CERN of AI?\\\\n6:15 - Who gets access to the resources?\\\\n8:00 - What is Stable Diffusion?\\\\n11:40 - What if your model produces bad outputs?\\\\n14:20 - Do you employ people?\\\\n16:35 - Can you prevent the corruption of profit?\\\\n19:50 - How can people find you?\\\\n22:45 - Final thoughts, let\\'s destroy PowerPoint\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1542",
    "uploadDate": "2022-08-13",
    "thumbnail_url": "https://i.ytimg.com/vi/YQ2QtKcK2dA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_9aN1-0T8hg",
    "title": "[ML News] AI models that write code (Copilot, CodeWhisperer, Pangu-Coder, etc.)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, ml news, copilot, codewhisperer, copilot legal, copilot github, google code, ai code, ai coding, ai code assistant, what is deep learning",
    "scraped_at": 1684582631.2993472,
    "genre": "Science",
    "views": "51842",
    "desc": "#mlnews #ai #copilot \\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Copilot Now Generally Available\\\\n3:20 - FOSS Org leaves GitHub\\\\n6:45 - Google\\'s Internal ML Code Completion\\\\n9:10 - AI Trains Itself to Code Better\\\\n14:30 - Amazon CodeWhisperer in Preview\\\\n15:15 - Pangu-Coder: A New Coding Model\\\\n17:10 - Useful Things\\\\n\\\\nReferences:\\\\nCopilot Now Generally Available\\\\nhttps://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/\\\\n\\\\nFOSS Org leaves GitHub\\\\nhttps://www.theregister.com/2022/06/30/software_freedom_conservancy_quits_github/\\\\nhttps://sfconservancy.org/blog/2022/jun/30/give-up-github-launch/\\\\nhttps://sfconservancy.org/GiveUpGitHub/\\\\nhttps://sfconservancy.org/docs/SupportGiveUpGitHub-README-snippet.md\\\\n\\\\nGoogle\\'s Internal ML Code Completion\\\\nhttps://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html\\\\n\\\\nAI Trains Itself to Code Better\\\\nhttps://arxiv.org/abs/2207.14502\\\\nhttps://arxiv.org/pdf/2207.14502.pdf\\\\n\\\\nAmazon CodeWhisperer in Preview\\\\nhttps://aws.amazon.com/blogs/aws/now-in-preview-amazon-codewhisperer-ml-powered-coding-companion/\\\\nhttps://aws.amazon.com/codewhisperer/\\\\nhttps://aws.amazon.com/codewhisperer/features/\\\\n\\\\nPangu-Coder: A New Coding Model\\\\nhttps://arxiv.org/abs/2207.11280\\\\nhttps://arxiv.org/pdf/2207.11280.pdf\\\\n\\\\nUseful Things\\\\nhttps://github.com/qdrant/quaterion\\\\nhttps://github.com/facebookresearch/torchdim\\\\nhttps://www.mosaicml.com/blog/farewell-oom\\\\nhttps://github.com/hristo-vrigazov/mmap.ninja#when-do-i-use-it\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1277",
    "uploadDate": "2022-08-10",
    "thumbnail_url": "https://i.ytimg.com/vi/_9aN1"
  },
  {
    "link": "watch?v=af6WPqvzjjk",
    "title": "[ML News] Text-to-Image models are taking over! (Imagen, DALL-E 2, Midjourney, CogView 2 & more)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, imagen, dalle, dalle 2, dall e, dall e 2, midjourney, midjourney diffusion, generative models, ai art, aiart, mlnews, ml news, kilcher news, ml news yannic, google imagen, cogview, cog view, cog view 2, dalle mini, dalle",
    "scraped_at": 1684582628.8443375,
    "genre": "Science",
    "views": "27661",
    "desc": "#mlnews #dalle #imagen \\\\n\\\\nAll things text-to-image models like DALL-E and Imagen!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - Imagen: Google\\'s Text-to-Image Diffusion Model\\\\n7:15 - Unified I/O by AllenAI\\\\n9:40 - CogView2 is Open-Source\\\\n11:05 - Google bans DeepFakes from Colab\\\\n13:05 - DALL-E generates real Cosmopolitan cover\\\\n15:45 - DALL-E tips \\\\u0026 tricks\\\\n17:00 - Midjourney moves to Open Beta\\\\n17:50 - DALLE-mini is not Crayon\\\\n19:00 - Deep Learning Resources\\\\n\\\\nAMENDMENTS:\\\\nThe Unified-IO paper is here: https://arxiv.org/abs/2206.08916\\\\n\\\\nReferences:\\\\nImagen: Google\\'s Text-to-Image Diffusion Model\\\\nhttps://imagen.research.google/?utm_source=pocket_mylist\\\\nhttps://arxiv.org/pdf/2205.11487.pdf\\\\n\\\\nUnified I/O by AllenAI\\\\nhttps://unified-io.allenai.org/\\\\nhttps://blog.allenai.org/introducing-ai2s-unified-io-9c0ec7fe1e43\\\\n\\\\nCogView2 is Open-Source\\\\nhttps://github.com/THUDM/CogView2\\\\nfile:///Users/yk/Downloads/big.1.pdf\\\\nhttps://huggingface.co/spaces/THUDM/CogView2\\\\nhttps://arxiv.org/pdf/2204.14217.pdf\\\\n\\\\nGoogle bans DeepFakes from Colab\\\\nhttps://www-vice-com.cdn.ampproject.org/c/s/www.vice.com/amp/en/article/v7v4gx/google-bans-deepfakes-from-its-machine-learning-platform?utm_source=pocket_mylist\\\\n\\\\nDALL-E generates real Cosmopolitan cover\\\\nhttps://www.cosmopolitan.com/lifestyle/a40314356/dall-e-2-artificial-intelligence-cover/\\\\nhttps://www.instagram.com/p/CfEwohiJdXW/?hl=en\\\\n\\\\nDALL-E tips \\\\u0026 tricks\\\\nhttps://twitter.com/GuyP/status/1544710725708513280?s=09\\\\u0026t=c3NpErPx80INQVeaWkIqIg\\\\u0026utm_source=pocket_mylist\\\\nhttps://twitter.com/GuyP/status/1552681939806691329?s=09\\\\u0026t=LV2ChcukUziXfvfNK-sY0A\\\\u0026utm_source=pocket_mylist\\\\nhttps://twitter.com/GuyP/status/1547234780001042432\\\\nhttps://dallery.gallery/the-dalle-2-prompt-book/\\\\n\\\\nMidjourney moves to Open Beta\\\\nhttps://twitter.com/midjourney?lang=en\\\\nhttps://twitter.com/search?q=%23midjourney\\\\u0026f=image\\\\n\\\\nDALLE-mini is not Crayon\\\\nhttps://www.craiyon.com/\\\\n\\\\nDeep Learning Resources\\\\nhttps://github.com/jacobhilton/deep_learning_curriculum\\\\nhttps://arxiv.org/abs/2206.13446\\\\nhttps://arxiv.org/pdf/2206.13446.pdf\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1248",
    "uploadDate": "2022-08-07",
    "thumbnail_url": "https://i.ytimg.com/vi/af6WPqvzjjk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=xnChXNUNS2A",
    "title": "[ML News] This AI completes Wikipedia! Meta AI Sphere | Google Minerva | GPT-3 writes a paper",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, meta, meta ai, wikipedia, wikipedia wrong, wikipedia editors, minerva, ai math, math ai, google minerva, ai solves math, minerva latex, schmidhuber, schmidhuber lecun, schmidhuber gan, schmidhuber reinforcement learning, gpt 3, gpt",
    "scraped_at": 1684582636.095449,
    "genre": "Science",
    "views": "33673",
    "desc": "#mlnews #ai #minerva \\\\n\\\\nThis episode is all about models that reason.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:35 - Meta AI learns Wikipedia citations\\\\n5:25 - Google\\'s Minerva solves math problems by reading papers\\\\n9:10 - GPT-3 writes a paper on itself\\\\n13:35 - J\\xc3\\xbcrgen Schmidhuber prompts LeCun for missing citations\\\\n\\\\nReferences:\\\\nMeta AI learns Wikipedia citations\\\\nhttps://tech.fb.com/artificial-intelligence/2022/07/how-ai-could-help-make-wikipedia-entries-more-accurate/\\\\nhttps://ai.facebook.com/blog/introducing-sphere-meta-ais-web-scale-corpus-for-better-knowledge-intensive-nlp/?d=%7B%22u%22%3A100051861999022%2C%22f%22%3A207799259245384%2C%22t%22%3A1658664021%2C%22ed%22%3A[]%7D\\\\u0026s=AWVELTip1y4HowJprXc\\\\nhttps://github.com/facebookresearch/sphere\\\\nhttps://github.com/facebookresearch/side\\\\nhttps://verifier.sideeditor.com/main\\\\nhttps://openreview.net/forum?id=qfTqRtkDbWZ\\\\n\\\\nGoogle\\'s Minerva solves math problems by reading papers\\\\nhttps://minerva-demo.github.io/#category=Precalculus\\\\u0026index=9\\\\nhttps://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\\\\n\\\\nGPT-3 writes a paper on itself\\\\nhttps://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-then-we-tried-to-get-it-published/\\\\nhttps://hal.archives-ouvertes.fr/hal-03701250v1\\\\nhttps://hal.archives-ouvertes.fr/hal-03701250/document\\\\n\\\\nJ\\xc3\\xbcrgen Schmidhuber prompts LeCun for missing citations\\\\nhttps://people.idsia.ch/~juergen/lecun-rehash-1990-2022.html\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1037",
    "uploadDate": "2022-07-31",
    "thumbnail_url": "https://i.ytimg.com/vi/xnChXNUNS2A/maxresdefault.jpg"
  },
  {
    "link": "watch?v=W3mrgqtm5R4",
    "title": "[ML News] BLOOM: 176B Open-Source | Chinese Brain-Scale Computer | Meta AI: No Language Left Behind",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, bloom, nlp, gpt3, gpt 3, gpt",
    "scraped_at": 1684582629.0213363,
    "genre": "Science",
    "views": "46909",
    "desc": "#mlnews #bloom #ai \\\\n\\\\nToday we look at all the recent giant language models in the AI world!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:55 - BLOOM: Open-Source 176B Language Model\\\\n5:25 - YALM 100B\\\\n5:40 - Chinese Brain-Scale Supercomputer\\\\n7:25 - Meta AI Translates over 200 Languages\\\\n10:05 - Reproducibility Crisis Workshop\\\\n10:55 - AI21 Raises $64M\\\\n11:50 - Ian Goodfellow leaves Apple\\\\n12:20 - Andrej Karpathy leaves Tesla\\\\n12:55 - Wordalle\\\\n\\\\n\\\\nReferences:\\\\nBLOOM: Open-Source 176B Language Model\\\\nhttps://bigscience.huggingface.co/blog/bloom\\\\nhttps://huggingface.co/spaces/bigscience/license\\\\nhttps://huggingface.co/bigscience/bloom?text=34%2B10%3D44+%0A54%2B20%3D\\\\n\\\\nYALM 100B\\\\nhttps://github.com/yandex/YaLM-100B\\\\n\\\\nChinese Brain-Scale Supercomputer\\\\nhttps://www.scmp.com/news/china/science/article/3182498/china-supercomputer-achieves-global-first-brain-scale-ai-model?utm_source=pocket_mylist\\\\nhttps://archive.ph/YaoA6#selection-1237.156-1237.246\\\\n\\\\nMeta AI Translates over 200 Languages\\\\nhttps://ai.facebook.com/research/no-language-left-behind/\\\\n\\\\nReproducibility Crisis Workshop\\\\nhttps://reproducible.cs.princeton.edu/\\\\n\\\\nAI21 Raises $64M\\\\nhttps://techcrunch.com/2022/07/12/openai-rival-ai21-labs-raises-64m-to-ramp-up-its-ai-powered-language-services/?guccounter=1\\\\n\\\\nIan Goodfellow leaves Apple\\\\nhttps://twitter.com/goodfellow_ian/status/1544638709039091717\\\\n\\\\nAndrey Karpathy leaves Tesla\\\\nhttps://mobile.twitter.com/karpathy/status/1547332300186066944\\\\nhttps://www.businessinsider.com/report-tesla-laid-off-about-200-people-in-autopilot-unit-2022-6?r=US\\\\u0026IR=T\\\\n\\\\nWordalle\\\\nhttps://huggingface.co/spaces/huggingface-projects/wordalle?utm_source=pocket_mylist\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "842",
    "uploadDate": "2022-07-27",
    "thumbnail_url": "https://i.ytimg.com/vi/W3mrgqtm5R4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jSdHmImyUjk",
    "title": "JEPA - A Path Towards Autonomous Machine Intelligence (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, jepa, h",
    "scraped_at": 1684582629.2923257,
    "genre": "Science",
    "views": "46051",
    "desc": "#jepa #ai #machinelearning \\\\n\\\\nYann LeCun\\'s position paper on a path towards machine intelligence combines Self-Supervised Learning, Energy-Based Models, and hierarchical predictive embedding models to arrive at a system that can teach itself to learn useful abstractions at multiple levels and use that as a world model to plan ahead in time.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n2:00 - Main Contributions\\\\n5:45 - Mode 1 and Mode 2 actors\\\\n15:40 - Self-Supervised Learning and Energy-Based Models\\\\n20:15 - Introducing latent variables\\\\n25:00 - The problem of collapse\\\\n29:50 - Contrastive vs regularized methods\\\\n36:00 - The JEPA architecture\\\\n47:00 - Hierarchical JEPA (H-JEPA)\\\\n53:00 - Broader relevance\\\\n56:00 - Summary \\\\u0026 Comments\\\\n\\\\nPaper: https://openreview.net/forum?id=BZ5a1r-kVsf\\\\n\\\\nAbstract: How could machines learn as efficiently as humans and animals?  How could machines learn to reason and plan?  How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons?  This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.\\\\n\\\\nAuthor: Yann LeCun\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3577",
    "uploadDate": "2022-07-06",
    "thumbnail_url": "https://i.ytimg.com/vi/jSdHmImyUjk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=oz5yZc9ULAc",
    "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, minerl, minecraft ai, diamond pickaxe, ai diamond pickaxe, openai minecraft, deep learning projects, what is deep learning, deep learning tutorial, introduction to deep learning, gpt 3, gpt",
    "scraped_at": 1684582632.4883485,
    "genre": "Science",
    "views": "38361",
    "desc": "#openai #vpt #minecraft \\\\n\\\\nMinecraft is one of the harder challenges any RL agent could face. Episodes are long, and the world is procedurally generated, complex, and huge. Further, the action space is a keyboard and a mouse, which has to be operated only given the game\\'s video input. OpenAI tackles this challenge using Video PreTraining, leveraging a small set of contractor data in order to pseudo-label a giant corpus of scraped footage of gameplay. The pre-trained model is highly capable in basic game mechanics and can be fine-tuned much better than a blank slate model. This is the first Minecraft agent that achieves the elusive goal of crafting a diamond pickaxe all by itself.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n3:50 - How to spend money most effectively?\\\\n8:20 - Getting a large dataset with labels\\\\n14:40 - Model architecture\\\\n19:20 - Experimental results and fine-tuning\\\\n25:40 - Reinforcement Learning to the Diamond Pickaxe\\\\n30:00 - Final comments and hardware\\\\n\\\\nBlog: https://openai.com/blog/vpt/\\\\nPaper: https://arxiv.org/abs/2206.11795\\\\nCode \\\\u0026 Model weights: https://github.com/openai/Video-Pre-Training\\\\n\\\\nAbstract:\\\\nPretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.\\\\n\\\\nAuthors: Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1953",
    "uploadDate": "2022-06-26",
    "thumbnail_url": "https://i.ytimg.com/vi/oz5yZc9ULAc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qS-iYnp00uc",
    "title": "Parti - Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, diffusion models, what is deep learning, deep learning tutorial, introduction to deep learning, generative models, parti, google parti, google party, google pathways, google imagen, image, dalle, dalle2, dalle 2, dall e 2, dall e 2 vs graphic designer, anubis",
    "scraped_at": 1684582631.657347,
    "genre": "Science",
    "views": "23522",
    "desc": "#parti #ai #aiart \\\\n\\\\nParti is a new autoregressive text-to-image model that shows just how much scale can achieve. This model\\'s outputs are crips, accurate, realistic, and can combine arbitrary styles, concepts, and fulfil even challenging requests.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n2:40 - Example Outputs\\\\n6:00 - Model Architecture\\\\n17:15 - Datasets (incl. PartiPrompts)\\\\n21:45 - Experimental Results\\\\n27:00 - Picking a cherry tree\\\\n29:30 - Failure cases\\\\n33:20 - Final comments\\\\n\\\\nWebsite: https://parti.research.google/\\\\nPaper: https://arxiv.org/abs/2206.10789\\\\nGithub: https://github.com/google-research/parti\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2098",
    "uploadDate": "2022-06-23",
    "thumbnail_url": "https://i.ytimg.com/vi/qS"
  },
  {
    "link": "watch?v=mIZLGBD99iU",
    "title": "Did Google's LaMDA chatbot just become sentient?",
    "tags": "film, udost",
    "scraped_at": 1684582629.8343043,
    "genre": "Science",
    "views": "884064",
    "desc": "#lamda #google #ai \\\\n\\\\nGoogle engineer Blake Lemoine was put on leave after releasing proprietary information: An interview with the chatbot LaMDA that he believes demonstrates that this AI is, in fact, sentient. We analyze the claims and the interview in detail and trace how a statistical machine managed to convince at least one human that it is more than just an algorithm.\\\\n\\\\nOUTLINE:\\\\n0:00 - Whistleblower put on leave\\\\n4:30 - What is a language model?\\\\n6:40 - The prompt is the key\\\\n10:40 - Who are we talking to exactly?\\\\n12:50 - LaMDA analyzes stories\\\\n15:20 - Fear, pain, and consent\\\\n20:25 - How would we recognize sentience? When is a machine conscious?\\\\n\\\\nReferences:\\\\nhttps://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917\\\\nhttps://cajundiscordian.medium.com/what-is-lamda-and-what-does-it-want-688632134489\\\\nhttps://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/\\\\nhttps://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine\\\\nhttps://www.businessinsider.com/transcript-of-sentient-google-ai-chatbot-was-edited-for-readability-2022-6?inline-endstory-related-recommendations=\\\\u0026r=US\\\\u0026IR=T\\\\n\\\\nLinks:\\\\nHomepage: https://ykilcher.com\\\\nMerch: https://ykilcher.com/merch\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1343",
    "uploadDate": "2022-06-15",
    "thumbnail_url": "https://i.ytimg.com/vi/mIZLGBD99iU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=efPrtcLdcdM",
    "title": "GPT-4chan: This is the worst AI ever",
    "tags": "gpt",
    "scraped_at": 1684582631.840346,
    "genre": "Science",
    "views": "941924",
    "desc": "#gpt4chan #4chan #ai \\\\n\\\\nGPT-4chan was trained on over 3 years of posts from 4chan\\'s \\\\\"",
    "lengthSeconds": "1159",
    "uploadDate": "2022-06-03",
    "thumbnail_url": "https://i.ytimg.com/vi/efPrtcLdcdM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=smUHQndcmOY",
    "title": "[ML News] DeepMind's Flamingo Image-Text model | Locked-Image Tuning | Jurassic X & MRKL",
    "tags": "film, udost",
    "scraped_at": 1684582629.1123302,
    "genre": "Science",
    "views": "26214",
    "desc": "#flamingo #mlnews #tech\\\\n\\\\nYour updates directly from the state of the art in Machine Learning!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - DeepMind\\'s Flamingo: Unified Vision-Language Model\\\\n8:25 - LiT: Locked Image Tuning\\\\n10:20 - Jurassic X \\\\u0026 MRKL Systems\\\\n15:05 - Helpful Things\\\\n22:40 - This AI does not exist\\\\n\\\\nReferences:\\\\nDeepMind\\'s Flamingo: Unified Vision-Language Model\\\\nhttps://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model\\\\nhttps://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf\\\\nhttps://twitter.com/Inoryy/status/1522621712382234624\\\\n\\\\nLiT: Locked Image Tuning\\\\nhttps://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html\\\\nhttps://google-research.github.io/vision_transformer/lit/\\\\n\\\\nJurassic X \\\\u0026 MRKL Systems\\\\nhttps://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system#reading\\\\nhttps://arxiv.org/pdf/2205.00445.pdf\\\\nhttps://arxiv.org/pdf/2204.10019.pdf\\\\nhttps://studio.ai21.com/jurassic-x\\\\n\\\\nStyleGAN Human\\\\nhttps://stylegan-human.github.io/\\\\nhttps://github.com/stylegan-human/StyleGAN-Human?utm_source=pocket_mylist\\\\nhttps://huggingface.co/spaces/hysts/StyleGAN-Human\\\\n\\\\nHelpful Things\\\\nhttps://github.com/rish-16/grafog\\\\nhttps://huggingface.co/bertin-project/bertin-gpt-j-6B\\\\nhttps://github.com/pytorch/torchdistx\\\\nhttps://pytorch.org/torchdistx/latest/fake_tensor.html\\\\nhttps://github.com/Netflix/vectorflow?utm_source=pocket_mylist\\\\nhttps://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\\\\nhttps://twitter.com/DeepMind/status/1517146462571794433\\\\nhttps://github.com/ai-forever/mgpt\\\\nhttps://github.com/cleanlab/cleanlab\\\\nhttps://efficientdlbook.com/?utm_source=pocket_mylist\\\\nhttps://minihack-editor.github.io/\\\\nhttps://mugen-org.github.io/\\\\nhttps://www.amazon.science/blog/amazon-releases-51-language-dataset-for-language-understanding\\\\nhttps://github.com/phuselab/openFACS?utm_source=pocket_mylist\\\\nhttps://medium.com/pytorch/avalanche-and-end-to-end-library-for-continual-learning-based-on-pytorch-a99cf5661a0d\\\\n\\\\nThis AI does not exist\\\\nhttps://thisaidoesnotexist.com/\\\\n\\\\nLinks:\\\\nMerch: https://ykilcher.com/merch\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1458",
    "uploadDate": "2022-05-13",
    "thumbnail_url": "https://i.ytimg.com/vi/smUHQndcmOY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=pwSnC8jlh50",
    "title": "[ML News] Meta's OPT 175B language model | DALL-E Mega is training | TorToiSe TTS fakes my voice",
    "tags": "film, udost",
    "scraped_at": 1684582633.2173767,
    "genre": "Science",
    "views": "43396",
    "desc": "#mlnews #dalle #gpt3\\\\n\\\\nAn inside look of what\\'s happening in the ML world!\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n1:40 - Meta AI releases OPT-175B\\\\n4:55 - CoCa: New CLIP-Competitor\\\\n8:15 - DALL-E Mega is training\\\\n10:05 - TorToiSe TTS is amazing!\\\\n11:50 - Investigating Vision Transformers\\\\n12:50 - Hugging Face Deep RL class launched\\\\n13:40 - Helpful Things\\\\n17:00 - John Deere\\'s driverless tractors\\\\n\\\\nReferences:\\\\nMeta AI releases OPT-175B\\\\nhttps://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/\\\\nhttps://arxiv.org/abs/2205.01068\\\\nhttps://arxiv.org/pdf/2205.01068.pdf\\\\nhttps://github.com/facebookresearch/metaseq/tree/main/projects/OPT\\\\nhttps://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf\\\\nhttps://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles\\\\nhttps://twitter.com/yoavgo/status/1522150063815987201\\\\n\\\\nCoCa: New CLIP-Competitor\\\\nhttps://arxiv.org/abs/2205.01917\\\\nhttps://arxiv.org/pdf/2205.01917.pdf\\\\n\\\\nDALL-E Mega is training\\\\nhttps://twitter.com/borisdayma\\\\nhttps://twitter.com/borisdayma/status/1521891895001112577\\\\nhttps://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega--VmlldzoxODMxMDI2\\\\n\\\\nTorToiSe TTS is amazing!\\\\nhttps://github.com/neonbjb/tortoise-tts\\\\nhttps://nonint.com/static/tortoise_v2_examples.html\\\\nhttps://colab.research.google.com/drive/1wVVqUPqwiDBUVeWWOUNglpGhU3hg_cbR\\\\nhttps://github.com/neonbjb\\\\n\\\\nInvestigating Vision Transformers\\\\nhttps://github.com/sayakpaul/probing-vits/?utm_source=pocket_mylist\\\\nhttps://twitter.com/RisingSayak/status/1515918406171914240?utm_source=pocket_mylist\\\\nhttps://keras.io/examples/vision/probing_vits/\\\\nhttps://github.com/sayakpaul/probing-vits/tree/main/notebooks?utm_source=pocket_mylist\\\\n\\\\nHugging Face Deep RL class launched\\\\nhttps://github.com/huggingface/deep-rl-class\\\\n\\\\nHelpful Things\\\\nhttps://merantix-momentum.com/technology/squirrel/?utm_source=pocket_mylist\\\\nhttps://github.com/merantix-momentum/squirrel-core?utm_source=pocket_mylist\\\\nhttps://pyscript.net/?utm_source=pocket_mylist\\\\nhttps://github.com/google-research/big_vision\\\\nhttps://deepsportradar.github.io/challenge.html\\\\nhttps://github.com/DeepSportRadar/camera-calibration-challenge\\\\nhttps://twitter.com/alekseykorshuk/status/1515989357961920514?utm_source=pocket_mylist\\\\nhttps://github.com/AlekseyKorshuk/huggingnft\\\\n\\\\nJohn Deere\\'s driverless tractors\\\\nhttps://thenextweb.com/news/john-deere-slowly-becoming-one-worlds-most-important-ai-companies\\\\nhttps://tractorhacking.github.io/\\\\n\\\\nLinks:\\\\nMerch: https://ykilcher.com/merch\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1164",
    "uploadDate": "2022-05-10",
    "thumbnail_url": "https://i.ytimg.com/vi/pwSnC8jlh50/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Pm93D8CVlY8",
    "title": "This A.I. creates infinite NFTs",
    "tags": "film, udost",
    "scraped_at": 1684582631.208346,
    "genre": "Science",
    "views": "43651",
    "desc": "#nft #gan #ai\\\\n\\\\nToday we build our own AI that can create as many bored apes as we want! Fungibility for everyone!\\\\n\\\\nTry the model here: https://huggingface.co/spaces/ykilcher/apes\\\\nor here: https://ykilcher.com/apes\\\\nFiles \\\\u0026 Models here: https://huggingface.co/ykilcher/apes/tree/main\\\\nCode here: https://github.com/yk/apes-public (for the \\\\\"",
    "lengthSeconds": "1127",
    "uploadDate": "2022-05-05",
    "thumbnail_url": "https://i.ytimg.com/vi/Pm93D8CVlY8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=X4S8F3bwuuw",
    "title": "Author Interview: SayCan - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
    "tags": "film, udost",
    "scraped_at": 1684582628.5763042,
    "genre": "Science",
    "views": "5740",
    "desc": "#saycan #robots #ai\\\\n\\\\nThis is an interview with the authors Brian Ichter, Karol Hausman, and Fei Xia.\\\\nOriginal Paper Review Video: https://youtu.be/Ru23eWAQ6_E\\\\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy\\'s ability to progress towards the goal, given by the language model, and the policy\\'s probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction \\\\u0026 Setup\\\\n3:40 - Acquiring atomic low-level skills\\\\n7:45 - How does the language model come in?\\\\n11:45 - Why are you scoring instead of generating?\\\\n15:20 - How do you deal with ambiguity in language?\\\\n20:00 - The whole system is modular\\\\n22:15 - Going over the full algorithm\\\\n23:20 - What if an action fails?\\\\n24:30 - Debunking a marketing video :)\\\\n27:25 - Experimental Results\\\\n32:50 - The insane scale of data collection\\\\n40:15 - How do you go about large-scale projects?\\\\n43:20 - Where did things go wrong?\\\\n45:15 - Where do we go from here?\\\\n52:00 - What is the largest unsolved problem in this?\\\\n53:35 - Thoughts on the Tesla Bot\\\\n55:00 - Final thoughts\\\\n\\\\nPaper: https://arxiv.org/abs/2204.01691\\\\nWebsite: https://say-can.github.io/\\\\n\\\\nAbstract:\\\\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model\\'s \\\\\"",
    "lengthSeconds": "3511",
    "uploadDate": "2022-05-02",
    "thumbnail_url": "https://i.ytimg.com/vi/X4S8F3bwuuw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Ru23eWAQ6_E",
    "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan - Paper Explained)",
    "tags": "film, udost",
    "scraped_at": 1684582634.1775124,
    "genre": "Science",
    "views": "11855",
    "desc": "#saycan #robots #ai\\\\n\\\\nLarge Language Models are excellent at generating plausible plans in response to real-world problems, but without interacting with the environment, they have no abilities to estimate which of these plans are feasible or appropriate. SayCan combines the semantic capabilities of language models with a bank of low-level skills, which are available to the agent as individual policies to execute. SayCan automatically finds the best policy to execute by considering a trade-off between the policy\\'s ability to progress towards the goal, given by the language model, and the policy\\'s probability of executing successfully, given by the respective value function. The result is a system that can generate and execute long-horizon action sequences in the real world to fulfil complex tasks.\\\\n\\\\nSponsor: Zeta Alpha\\\\nhttps://zeta-alpha.com\\\\nUse code YANNIC for 20% off!\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction \\\\u0026 Overview\\\\n3:20 - Sponsor: Zeta Alpha\\\\n5:00 - Using language models for action planning\\\\n8:00 - Combining LLMs with learned atomic skills\\\\n16:50 - The full SayCan system\\\\n20:30 - Experimental setup and data collection\\\\n21:25 - Some weaknesses \\\\u0026 strengths of the system\\\\n27:00 - Experimental results\\\\n\\\\nPaper: https://arxiv.org/abs/2204.01691\\\\nWebsite: https://say-can.github.io/\\\\n\\\\nAbstract:\\\\nLarge language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model\\'s \\\\\"",
    "lengthSeconds": "1726",
    "uploadDate": "2022-04-30",
    "thumbnail_url": "https://i.ytimg.com/vi/Ru23eWAQ6_E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=16BsJI5I-Yw",
    "title": "Author Interview - ACCEL: Evolving Curricula with Regret-Based Environment Design",
    "tags": "film, udost",
    "scraped_at": 1684582631.0313797,
    "genre": "Science",
    "views": "3922",
    "desc": "#ai #accel #evolution\\\\n\\\\nThis is an interview with the authors Jack Parker-Holder and Minqi Jiang.\\\\nOriginal Paper Review Video: https://www.youtube.com/watch?v=povBDxUn1VQ\\\\n\\\\nAutomatic curriculum generation is one of the most promising avenues for Reinforcement Learning today. Multiple approaches have been proposed, each with their own set of advantages and drawbacks. This paper presents ACCEL, which takes the next step into the direction of constructing curricula for multi-capable agents. ACCEL combines the adversarial adaptiveness of regret-based sampling methods with the capabilities of level-editing, usually found in Evolutionary Methods.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:00 - Start of interview\\\\n4:45 - How did you get into this field?\\\\n8:10 - What is minimax regret?\\\\n11:45 - What levels does the regret objective select?\\\\n14:20 - Positive value loss (correcting my mistakes)\\\\n21:05 - Why is the teacher not learned?\\\\n24:45 - How much domain-specific knowledge is needed?\\\\n29:30 - What problems is this applicable to?\\\\n33:15 - Single agent vs population of agents\\\\n37:25 - Measuring and balancing level difficulty\\\\n40:35 - How does generalization emerge?\\\\n42:50 - Diving deeper into the experimental results\\\\n47:00 - What are the unsolved challenges in the field?\\\\n50:00 - Where do we go from here?\\\\n\\\\nWebsite: https://accelagent.github.io\\\\nPaper: https://arxiv.org/abs/2203.01302\\\\nICLR Workshop: https://sites.google.com/view/aloe2022\\\\nBook on topic: https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/\\\\n\\\\nAbstract:\\\\nIt remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent\\'s capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent\\'s capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at this http URL.\\\\n\\\\nAuthors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rockt\\xc3\\xa4schel\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3465",
    "uploadDate": "2022-04-26",
    "thumbnail_url": "https://i.ytimg.com/vi/16BsJI5I"
  },
  {
    "link": "watch?v=povBDxUn1VQ",
    "title": "ACCEL: Evolving Curricula with Regret-Based Environment Design (Paper Review)",
    "tags": "film, udost",
    "scraped_at": 1684582632.0203462,
    "genre": "Science",
    "views": "10053",
    "desc": "#ai #accel #evolution\\\\n\\\\nAutomatic curriculum generation is one of the most promising avenues for Reinforcement Learning today. Multiple approaches have been proposed, each with their own set of advantages and drawbacks. This paper presents ACCEL, which takes the next step into the direction of constructing curricula for multi-capable agents. ACCEL combines the adversarial adaptiveness of regret-based sampling methods with the capabilities of level-editing, usually found in Evolutionary Methods.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Demonstration\\\\n3:50 - Paper overview\\\\n5:20 - The ACCEL algorithm\\\\n15:25 - Looking at the pseudocode\\\\n23:10 - Approximating regret\\\\n33:45 - Experimental results\\\\n40:00 - Discussion \\\\u0026 Comments\\\\n\\\\nWebsite: https://accelagent.github.io\\\\nPaper: https://arxiv.org/abs/2203.01302\\\\n\\\\nAbstract:\\\\nIt remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent\\'s capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent\\'s capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at this http URL.\\\\n\\\\nAuthors: Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rockt\\xc3\\xa4schel\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2646",
    "uploadDate": "2022-04-25",
    "thumbnail_url": "https://i.ytimg.com/vi/povBDxUn1VQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=AIOE1l1W0Tw",
    "title": "LAION-5B: 5 billion image-text-pairs dataset (with the authors)",
    "tags": "film, udost",
    "scraped_at": 1684582629.6593306,
    "genre": "Science",
    "views": "17383",
    "desc": "#laion #clip #dalle\\\\n\\\\nLAION-5B is an open, free dataset consisting of over 5 billion image-text-pairs. Today\\'s video is an interview with three of its creators. We dive into the mechanics and challenges of operating at such large scale, how to keep cost low, what new possibilities are enabled with open datasets like this, and how to best handle safety and legal concerns.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:30 - Start of Interview\\\\n2:30 - What is LAION?\\\\n11:10 - What are the effects of CLIP filtering?\\\\n16:40 - How big is this dataset?\\\\n19:05 - Does the text always come from the alt-property?\\\\n22:45 - What does it take to work at scale?\\\\n25:50 -When will we replicate DALL-E?\\\\n31:30 - The surprisingly efficient pipeline\\\\n35:20 - How do you cover the S3 costs?\\\\n40:30 - Addressing safety \\\\u0026 legal concerns\\\\n55:15 - Where can people get started?\\\\n\\\\nReferences:\\\\nLAION website: https://laion.ai/\\\\nLAION Discord: https://discord.com/invite/mVcgxMPD7e\\\\nLAION-5B: https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/\\\\nimg2dataset tool: https://github.com/rom1504/img2dataset\\\\nLAION-400M: https://paperswithcode.com/dataset/laion-400m\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3481",
    "uploadDate": "2022-04-22",
    "thumbnail_url": "https://i.ytimg.com/vi/AIOE1l1W0Tw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ccBMRryxGog",
    "title": "Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)",
    "tags": "film, udost",
    "scraped_at": 1684582630.0233378,
    "genre": "Science",
    "views": "11276",
    "desc": "#nlp #sparsity #transformers\\\\n\\\\nThis video is an interview with Barret Zoph and William Fedus of Google Brain about Sparse Expert Models.\\\\nSparse Expert models have been hugely successful at distributing parts of models, mostly Transformers, across large array of machines and use a routing function to effectively route signals between them. This means that even though these models have a huge number of parameters, the computational load for a given signal does not increase because the model is only sparsely activated. Sparse expert models, such as Switch Transformers and GLAM can scale up to trillions of parameters and bring a number of desirable properties. We discuss everything from the fundamentals, history, strengths and weaknesses, up to the current state of the art of these models.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - What are sparse expert models?\\\\n4:25 - Start of Interview\\\\n5:55 - What do you mean by sparse experts?\\\\n8:10 - How does routing work in these models?\\\\n12:10 - What is the history of sparse experts?\\\\n14:45 - What does an individual expert learn?\\\\n19:25 - When are these models appropriate?\\\\n22:30 - How comparable are sparse to dense models?\\\\n26:30 - How does the pathways system connect to this?\\\\n28:45 - What improvements did GLAM make?\\\\n31:30 - The \\\\\"",
    "lengthSeconds": "3502",
    "uploadDate": "2022-04-21",
    "thumbnail_url": "https://i.ytimg.com/vi/ccBMRryxGog/maxresdefault.jpg"
  },
  {
    "link": "watch?v=C7mUYocWdG0",
    "title": "Author Interview - Transformer Memory as a Differentiable Search Index",
    "tags": "film, udost",
    "scraped_at": 1684582630.9373477,
    "genre": "Science",
    "views": "6952",
    "desc": "#neuralsearch #interview #google\\\\n\\\\nThis is an interview with the authors Yi Tay and Don Metzler.\\\\nPaper Review Video: https://youtu.be/qlB0TPBQ7YY\\\\n\\\\nSearch engines work by building an index and then looking up things in it. Usually, that index is a separate data structure. In keyword search, we build and store reverse indices. In neural search, we build nearest-neighbor indices. This paper does something different: It directly trains a Transformer to return the ID of the most relevant document. No similarity search over embeddings or anything like this is performed, and no external data structure is needed, as the entire index is essentially captured by the model\\'s weights. The paper experiments with various ways of representing documents and training the system, which works surprisingly well!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:50 - Start of Interview\\\\n1:30 - How did this idea start?\\\\n4:30 - How does memorization play into this?\\\\n5:50 - Why did you not compare to cross-encoders?\\\\n7:50 - Instead of the ID, could one reproduce the document itself?\\\\n10:50 - Passages vs documents\\\\n12:00 - Where can this model be applied?\\\\n14:25 - Can we make this work on large collections?\\\\n19:20 - What\\'s up with the NQ100K dataset?\\\\n23:55 - What is going on inside these models?\\\\n28:30 - What\\'s the smallest scale to obtain meaningful results?\\\\n30:15 - Investigating the document identifiers\\\\n34:45 - What\\'s the end goal?\\\\n38:40 - What are the hardest problems currently?\\\\n40:40 - Final comments \\\\u0026 how to get started\\\\n\\\\nPaper: https://arxiv.org/abs/2202.06991\\\\n\\\\nAbstract:\\\\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\\\\n\\\\nAuthors: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2584",
    "uploadDate": "2022-04-17",
    "thumbnail_url": "https://i.ytimg.com/vi/C7mUYocWdG0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qlB0TPBQ7YY",
    "title": "Transformer Memory as a Differentiable Search Index (Machine Learning Research Paper Explained)",
    "tags": "film, udost",
    "scraped_at": 1684582631.4793472,
    "genre": "Science",
    "views": "23054",
    "desc": "#dsi #search #google\\\\n\\\\nSearch engines work by building an index and then looking up things in it. Usually, that index is a separate data structure. In keyword search, we build and store reverse indices. In neural search, we build nearest-neighbor indices. This paper does something different: It directly trains a Transformer to return the ID of the most relevant document. No similarity search over embeddings or anything like this is performed, and no external data structure is needed, as the entire index is essentially captured by the model\\'s weights. The paper experiments with various ways of representing documents and training the system, which works surprisingly well!\\\\n\\\\nSponsor: Diffgram\\\\nhttps://diffgram.com?ref=yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:45 - Sponsor: Diffgram\\\\n1:35 - Paper overview\\\\n3:15 - The search problem, classic and neural\\\\n8:15 - Seq2seq for directly predicting document IDs\\\\n11:05 - Differentiable search index architecture\\\\n18:05 - Indexing\\\\n25:15 - Retrieval and document representation\\\\n33:25 - Training DSI\\\\n39:15 - Experimental results\\\\n49:25 - Comments \\\\u0026 Conclusions\\\\n\\\\nPaper: https://arxiv.org/abs/2202.06991\\\\n\\\\nAbstract:\\\\nIn this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\\\\n\\\\nAuthors: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3111",
    "uploadDate": "2022-04-16",
    "thumbnail_url": "https://i.ytimg.com/vi/qlB0TPBQ7YY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=RJwPN4qNi_Y",
    "title": "[ML News] Google's 540B PaLM Language Model & OpenAI's DALL-E 2 Text-to-Image Revolution",
    "tags": "film, udost",
    "scraped_at": 1684582631.74838,
    "genre": "Science",
    "views": "50927",
    "desc": "#mlnews #palm #dalle2\\\\n\\\\nGoogle releases PaLM and OpenAI releases DALL-E 2 (and more news).\\\\n\\\\nSponsor: Weights \\\\u0026 BIases\\\\nStart here: https://wandb.me/yannic\\\\n\\\\nThumbnail credit: DALL-E 2 via Sam Altman\\\\n\\\\nOUTLINE\\\\n0:00 - Street interview w/ random stranger\\\\n2:25 - Intro\\\\n2:50 - PaLM - Google\\'s 540B Pathways Language Model\\\\n7:50 - Sponsor: Weights \\\\u0026 Biases\\\\n9:10 - OpenAI releases DALL-E 2\\\\n12:05 - Open Source Datasets and Models\\\\n13:20 - Salesforce releases CodeGen\\\\n\\\\nMy Live Reaction to DALL-E 2: https://youtu.be/gGPv_SYVDC8\\\\nMy Video on GLIDE: https://youtu.be/gwI6g1pBD84\\\\nMy Video on the Pathways System: https://youtu.be/vGFaiLeoLWw\\\\n\\\\nReferences:\\\\nPaLM - Google\\'s 540B Pathways Language Model\\\\nhttps://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\\\\nhttps://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf\\\\n\\\\nOpenAI releases DALL-E 2\\\\nhttps://openai.com/dall-e-2/\\\\nhttps://cdn.openai.com/papers/dall-e-2.pdf\\\\nhttps://www.instagram.com/openaidalle/\\\\nhttps://twitter.com/sama/status/1511724264629678084?s=09\\\\u0026t=58fWOJMHUDnOla5nD_ygjg\\\\u0026utm_source=pocket_mylist\\\\nhttps://twitter.com/sama/media\\\\nhttps://twitter.com/BorisMPower/status/1511738735175610371\\\\nhttps://twitter.com/ariskonstant/status/1511744708875218945\\\\n\\\\nOpen Source Datasets and Models\\\\nhttps://twitter.com/multimodalart/status/1510999907498442756\\\\nhttps://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/\\\\nhttps://github.com/mlfoundations/open_clip\\\\n\\\\nSalesforce releases CodeGen\\\\nhttps://github.com/salesforce/CodeGen\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "860",
    "uploadDate": "2022-04-10",
    "thumbnail_url": "https://i.ytimg.com/vi/RJwPN4qNi_Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DdkenV-ZdJU",
    "title": "The Weird and Wonderful World of AI Art (w/ Author Jack Morris)",
    "tags": "film, udost",
    "scraped_at": 1684582629.2033315,
    "genre": "Science",
    "views": "5653",
    "desc": "#aiart #deeplearning #clip\\\\n\\\\nSince the release of CLIP, the world of AI art has seen an unprecedented level of acceleration in what\\'s possible to do. Whereas image generation had previously been mostly in the domain of scientists, now a community of professional artists, researchers, and amateurs are sending around colab notebooks and sharing their creations via social media. How did this happen? What is going on? And where do we go from here? Jack Morris and I attempt to answer some of these questions, following his blog post \\\\\"",
    "lengthSeconds": "3569",
    "uploadDate": "2022-04-04",
    "thumbnail_url": "https://i.ytimg.com/vi/DdkenV"
  },
  {
    "link": "watch?v=z4lAlVRwbrc",
    "title": "Author Interview - Improving Intrinsic Exploration with Language Abstractions",
    "tags": "film, udost",
    "scraped_at": 1684582629.9283357,
    "genre": "Science",
    "views": "4087",
    "desc": "#reinforcementlearning #ai #explained\\\\n\\\\nThis is an interview with Jesse Mu, first author of the paper.\\\\nOriginal Paper Review: https://youtu.be/NeGJAUSQEJI\\\\n\\\\nExploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:55 - Paper Overview\\\\n4:30 - Aren\\'t you just adding extra data?\\\\n9:35 - Why are you splitting up the AMIGo teacher?\\\\n13:10 - How do you train the grounding network?\\\\n16:05 - What about causally structured environments?\\\\n17:30 - Highlights of the experimental results\\\\n20:40 - Why is there so much variance?\\\\n22:55 - How much does it matter that we are testing in a video game?\\\\n27:00 - How does novelty interface with the goal specification?\\\\n30:20 - The fundamental problems of exploration\\\\n32:15 - Are these algorithms subject to catastrophic forgetting?\\\\n34:45 - What current models could bring language to other environments?\\\\n40:30 - What does it take in terms of hardware?\\\\n43:00 - What problems did you encounter during the project?\\\\n46:40 - Where do we go from here?\\\\n\\\\nPaper: https://arxiv.org/abs/2202.08938\\\\n\\\\nAbstract:\\\\nReinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.\\\\n\\\\nAuthors: Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rockt\\xc3\\xa4schel, Edward Grefenstette\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2965",
    "uploadDate": "2022-04-02",
    "thumbnail_url": "https://i.ytimg.com/vi/z4lAlVRwbrc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=NeGJAUSQEJI",
    "title": "Improving Intrinsic Exploration with Language Abstractions (Machine Learning Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, machine learning news, ml paper, machine learning paper, language, nlp, natural language processing, stanford, reinforcement learning, data science, deep learning tutorial, deep learning paper, language in reinforcement learning, rl nlp, nlp rl, nlp reinforcement learning, exploration exploitation, rl exploration",
    "scraped_at": 1684582629.7463057,
    "genre": "Science",
    "views": "9062",
    "desc": "#reinforcementlearning #ai #explained\\\\n\\\\nExploration is one of the oldest challenges for Reinforcement Learning algorithms, with no clear solution to date. Especially in environments with sparse rewards, agents face significant challenges in deciding which parts of the environment to explore further. Providing intrinsic motivation in form of a pseudo-reward is sometimes used to overcome this challenge, but often relies on hand-crafted heuristics, and can lead to deceptive dead-ends. This paper proposes to use language descriptions of encountered states as a method of assessing novelty. In two procedurally generated environments, they demonstrate the usefulness of language, which is in itself highly concise and abstractive, which lends itself well for this task.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:10 - Paper Overview: Language for exploration\\\\n5:40 - The MiniGrid \\\\u0026 MiniHack environments\\\\n7:00 - Annotating states with language\\\\n9:05 - Baseline algorithm: AMIGo\\\\n12:20 - Adding language to AMIGo\\\\n22:55 - Baseline algorithm: NovelD and Random Network Distillation\\\\n29:45 - Adding language to NovelD\\\\n31:50 - Aren\\'t we just using extra data?\\\\n34:55 - Investigating the experimental results\\\\n40:45 - Final comments\\\\n\\\\nPaper: https://arxiv.org/abs/2202.08938\\\\n\\\\nAbstract:\\\\nReinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.\\\\n\\\\nAuthors: Jesse Mu, Victor Zhong, Roberta Raileanu, Minqi Jiang, Noah Goodman, Tim Rockt\\xc3\\xa4schel, Edward Grefenstette\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2545",
    "uploadDate": "2022-04-01",
    "thumbnail_url": "https://i.ytimg.com/vi/NeGJAUSQEJI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vGFaiLeoLWw",
    "title": "[ML News] GPT-3 learns to edit | Google Pathways | Make-A-Scene | CLIP meets GamePhysics | DouBlind",
    "tags": "film, udost",
    "scraped_at": 1684582628.670305,
    "genre": "Science",
    "views": "14688",
    "desc": "#mlnews #gpt3 #pathways\\\\n\\\\nYour updates on the latest and greatest from the depths of Machine Learning!\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Weights \\\\u0026 Biases Report about Reports\\\\n2:45 - GPT-3 learns to edit\\\\n6:30 - Make-A-Scene: Text-to-Image with Human Priors\\\\n8:00 - Pathways: Google\\'s new High-Performance ML scheduler\\\\n10:45 - DouBlind: Open Peer-Review\\\\n12:45 - CLIP meets GamePhysics\\\\n14:40 - Residual Quantization pushes Image Generation SOTA\\\\n16:15 - Helpful Things\\\\n\\\\nReferences:\\\\nWeights \\\\u0026 Biases Report about Reports\\\\nhttps://wandb.ai/wandb/wandb_example/reports/How-many-discoveries-were-lost-because-they-weren-t-written-down---VmlldzoxMjY3MDk5\\\\n\\\\nGPT-3 learns to edit\\\\nhttps://openai.com/blog/gpt-3-edit-insert/?utm_source=pocket_mylist\\\\nhttps://beta.openai.com/playground?model=code-davinci-002\\\\n\\\\nMake-A-Scene: Text-to-Image with Human Priors\\\\nhttps://arxiv.org/pdf/2203.13131.pdf\\\\nhttps://www.youtube.com/watch?v=QLTyqoJJKTo\\\\n\\\\nPathways: Google\\'s new High-Performance ML scheduler\\\\nhttps://arxiv.org/pdf/2203.12533.pdf\\\\n\\\\nDouBlind: Open Peer-Review\\\\nhttps://doublind.com/#web-intro\\\\nhttps://doublind.com/search?query=kilcher\\\\n\\\\nCLIP meets GamePhysics\\\\nhttps://arxiv.org/pdf/2203.11096.pdf\\\\nhttps://www.reddit.com/r/GamePhysics/comments/9rqabp/red_dead_redemption_2_things_you_find_in_rdr2/\\\\nhttps://asgaardlab.github.io/CLIPxGamePhysics/\\\\n\\\\nResidual Quantization pushes Image Generation SOTA\\\\nhttps://arxiv.org/pdf/2203.01941.pdf\\\\nhttps://github.com/kakaobrain/rq-vae-transformer\\\\n\\\\nHelpful Things\\\\nhttps://github.com/TDAmeritrade/stumpy\\\\nhttps://github.com/linkedin/fasttreeshap\\\\nhttps://github.com/vopani/jaxton\\\\nhttps://twitter.com/mark_riedl/status/1507351959422087173?utm_source=pocket_mylist\\\\nhttps://github.com/eilab-gt/NovGrid\\\\nhttps://developer.nvidia.com/isaac-gym\\\\nhttps://github.com/NVIDIA-Omniverse/IsaacGymEnvs\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1083",
    "uploadDate": "2022-03-30",
    "thumbnail_url": "https://i.ytimg.com/vi/vGFaiLeoLWw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3ks2gpqAKY8",
    "title": "Author Interview - Memory-assisted prompt editing to improve GPT-3 after deployment",
    "tags": "film, udost",
    "scraped_at": 1684582629.3843064,
    "genre": "Science",
    "views": "2885",
    "desc": "#nlp #gpt3 #prompt\\\\n\\\\nThis is an interview with the authors of this work, Aman Madaan and Niket Tandon.\\\\nLarge language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:45 - Paper Overview\\\\n2:00 - What was your original motivation?\\\\n4:20 - There is an updated version of the paper!\\\\n9:00 - Have you studied this on real-world users?\\\\n12:10 - How does model size play into providing feedback?\\\\n14:10 - Can this be used for personalization?\\\\n16:30 - Discussing experimental results\\\\n17:45 - Can this be paired with recommender systems?\\\\n20:00 - What are obvious next steps to make the system more powerful?\\\\n23:15 - Clarifying the baseline methods\\\\n26:30 - Exploring cross-lingual customization\\\\n31:00 - Where did the idea for the clarification prompt come from?\\\\n33:05 - What did not work out during this project?\\\\n34:45 - What did you learn about interacting with large models?\\\\n37:30 - Final thoughts\\\\n\\\\nPaper: https://arxiv.org/abs/2201.06009\\\\nCode \\\\u0026 Data: https://github.com/madaan/memprompt\\\\n\\\\nAbstract:\\\\nLarge LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret \\\\\"",
    "lengthSeconds": "2437",
    "uploadDate": "2022-03-29",
    "thumbnail_url": "https://i.ytimg.com/vi/3ks2gpqAKY8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gYxJEd3EUKs",
    "title": "Memory-assisted prompt editing to improve GPT-3 after deployment (Machine Learning Paper Explained)",
    "tags": "film, udost",
    "scraped_at": 1684582632.6693478,
    "genre": "Science",
    "views": "9411",
    "desc": "#nlp #gpt3 #prompt\\\\n\\\\nLarge language models such as GPT-3 have enabled many breakthroughs and new applications recently, but they come with an important downside: Training them is very expensive, and even fine-tuning is often difficult. This paper presents an adaptive method to improve performance of such models after deployment, without ever changing the model itself. This is done by maintaining a memory of interactions and then dynamically adapting new prompts by augmenting them with memory content. This has many applications, from non-intrusive fine-tuning to personalization.\\\\n\\\\nSponsor: Introduction to Graph Neural Networks Course\\\\nhttps://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES\\\\u0026affcode=999036_lzknae-d\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:40 - Sponsor: Introduction to GNNs Course (link in description)\\\\n1:30 - Paper Overview: Improve GPT-3 after deployment via user feedback\\\\n5:30 - Proposed memory-based architecture\\\\n13:00 - A detailed look at the components\\\\n15:00 - Example tasks\\\\n24:30 - My concerns with the example setup\\\\n26:20 - Baselines used for comparison\\\\n29:50 - Experimental Results\\\\n34:20 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2201.06009\\\\nCode \\\\u0026 Data: https://github.com/madaan/memprompt\\\\n\\\\nAbstract:\\\\nLarge LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret \\\\\"",
    "lengthSeconds": "2201",
    "uploadDate": "2022-03-28",
    "thumbnail_url": "https://i.ytimg.com/vi/gYxJEd3EUKs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=AvHLJqtmQkE",
    "title": "Author Interview - Typical Decoding for Natural Language Generation",
    "tags": "film, udost",
    "scraped_at": 1684582628.3836412,
    "genre": "Science",
    "views": "8677",
    "desc": "#deeplearning #nlp #sampling\\\\n\\\\nThis is an interview with first author Clara Meister.\\\\nPaper review video here\\xc3\\xa9 https://youtu.be/_EDr3ryrT_Y\\\\n\\\\nModern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.\\\\n\\\\nSponsor: Introduction to Graph Neural Networks Course\\\\nhttps://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES\\\\u0026affcode=999036_lzknae-d\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:35 - Sponsor: Introduction to GNNs Course (link in description)\\\\n1:30 - Why does sampling matter?\\\\n5:40 - What is a \\\\\"",
    "lengthSeconds": "2936",
    "uploadDate": "2022-03-26",
    "thumbnail_url": "https://i.ytimg.com/vi/AvHLJqtmQkE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_EDr3ryrT_Y",
    "title": "Typical Decoding for Natural Language Generation (Get more human-like outputs from language models!)",
    "tags": "film, udost",
    "scraped_at": 1684582636.373031,
    "genre": "Science",
    "views": "17581",
    "desc": "#deeplearning #nlp #sampling\\\\n\\\\nModern language models like T5 or GPT-3 achieve remarkably low perplexities on both training and validation data, yet when sampling from their output distributions, the generated text often seems dull and uninteresting. Various workarounds have been proposed, such as top-k sampling and nucleus sampling, but while these manage to somewhat improve the generated samples, they are hacky and unfounded. This paper introduces typical sampling, a new decoding method that is principled, effective, and can be implemented efficiently. Typical sampling turns away from sampling purely based on likelihood and explicitly finds a trade-off between generating high-probability samples and generating high-information samples. The paper connects typical sampling to psycholinguistic theories on human speech generation, and shows experimentally that typical sampling achieves much more diverse and interesting results than any of the current methods.\\\\n\\\\nSponsor: Fully Connected by Weights \\\\u0026 Biases\\\\nhttps://wandb.ai/fully-connected\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:50 - Sponsor: Fully Connected by Weights \\\\u0026 Biases\\\\n4:10 - Paper Overview\\\\n7:40 - What\\'s the problem with sampling?\\\\n11:45 - Beam Search: The good and the bad\\\\n14:10 - Top-k and Nucleus Sampling\\\\n16:20 - Why the most likely things might not be the best\\\\n21:30 - The expected information content of the next word\\\\n25:00 - How to trade off information and likelihood\\\\n31:25 - Connections to information theory and psycholinguistics\\\\n36:40 - Introducing Typical Sampling\\\\n43:00 - Experimental Evaluation\\\\n44:40 - My thoughts on this paper\\\\n\\\\nPaper: https://arxiv.org/abs/2202.00666\\\\nCode: https://github.com/cimeister/typical-sampling/blob/3e676cfd88fa2e6a24f2bdc6f9f07fddb87827c2/src/transformers/generation_logits_process.py#L242-L272\\\\n\\\\nAbstract:\\\\nDespite achieving incredibly low perplexities on myriad natural language corpora, today\\'s language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel (\\xc3\\xa0 la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions.\\\\n\\\\nAuthors: Clara Meister, Tiago Pimentel, Gian Wiher, Ryan Cotterell\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2936",
    "uploadDate": "2022-03-25",
    "thumbnail_url": "https://i.ytimg.com/vi/_EDr3ryrT_Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Z3knUzwuIgo",
    "title": "One Model For All The Tasks - BLIP (Author Interview)",
    "tags": "film, udost",
    "scraped_at": 1684582635.8195117,
    "genre": "Science",
    "views": "4385",
    "desc": "#blip #interview #salesforce\\\\n\\\\nPaper Review Video: https://youtu.be/X2k7n4FuI7c\\\\nSponsor: Assembly AI\\\\nhttps://www.assemblyai.com/?utm_source=youtube\\\\u0026utm_medium=social\\\\u0026utm_campaign=yannic2\\\\n\\\\nThis is an interview with Junnan Li and Dongxu Li, authors of BLIP and members of Salesforce research.\\\\nCross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:40 - Sponsor: Assembly AI\\\\n1:30 - Start of Interview\\\\n2:30 - What\\'s the pitch?\\\\n4:40 - How did data bootstrapping come into the project?\\\\n7:10 - How big of a problem is data quality?\\\\n11:10 - Are the captioning \\\\u0026 filtering models biased towards COCO data?\\\\n14:40 - Could the data bootstrapping be done multiple times?\\\\n16:20 - What was the evolution of the BLIP architecture?\\\\n21:15 - Are there additional benefits to adding language modelling?\\\\n23:50 - Can we imagine a modular future for pre-training?\\\\n29:45 - Diving into the experimental results\\\\n42:40 - What did and did not work out during the research?\\\\n45:00 - How is research life at Salesforce?\\\\n46:45 - Where do we go from here?\\\\n\\\\nPaper: https://arxiv.org/abs/2201.12086\\\\nCode: https://github.com/salesforce/BLIP\\\\nDemo: https://huggingface.co/spaces/Salesforce/BLIP\\\\n\\\\nAbstract:\\\\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.\\\\n\\\\nAuthors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://ykilcher.com/discord\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2914",
    "uploadDate": "2022-03-24",
    "thumbnail_url": "https://i.ytimg.com/vi/Z3knUzwuIgo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=X2k7n4FuI7c",
    "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding&Generation",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, zeta alpha, blip, language vision pre training, language vision pre",
    "scraped_at": 1684582632.1113482,
    "genre": "Science",
    "views": "16530",
    "desc": "#blip #review #ai\\\\n\\\\nCross-modal pre-training has been all the rage lately in deep learning, especially training vision and language models together. However, there are a number of issues, such as low quality datasets that limit the performance of any model trained on it, and also the fact that pure contrastive pre-training cannot be easily fine-tuned for most downstream tasks. BLIP unifies different tasks and objectives in a single pre-training run and achieves a much more versatile model, which the paper immediately uses to create, filter, clean and thus bootstrap its own dataset to improve performance even more!\\\\n\\\\nSponsor: Zeta Alpha\\\\nhttps://zeta-alpha.com\\\\nUse code YANNIC for 20% off!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:50 - Sponsor: Zeta Alpha\\\\n3:40 - Paper Overview\\\\n6:40 - Vision-Language Pre-Training\\\\n11:15 - Contributions of the paper\\\\n14:30 - Model architecture: many parts for many tasks\\\\n19:50 - How data flows in the model\\\\n26:50 - Parameter sharing between the modules\\\\n29:45 - Captioning \\\\u0026 Filtering bootstrapping\\\\n41:10 - Fine-tuning the model for downstream tasks\\\\n\\\\nPaper: https://arxiv.org/abs/2201.12086\\\\nCode: https://github.com/salesforce/BLIP\\\\nDemo: https://huggingface.co/spaces/Salesforce/BLIP\\\\n\\\\nAbstract:\\\\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at this https URL.\\\\n\\\\nAuthors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2801",
    "uploadDate": "2022-03-23",
    "thumbnail_url": "https://i.ytimg.com/vi/X2k7n4FuI7c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=RXwZKzczkF8",
    "title": "[ML News] AI Threatens Biological Arms Race",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gtc, gtc22, nvidia, jensen huang, 3090, rtx 3090, ithaca, deepmind, deep mind, deepmind greek text, deepmind ithaca, ml news, mlnews, ai news, kilcher news, drug discovery, ai drug discovery, ai drug development, yoshua bengio, joshua bengio, yosha bengio, bengio knight, gary marcus, deep learning wall, gary marcus deep learning, pig grunts, ai animal communication, meta ai",
    "scraped_at": 1684582633.7033749,
    "genre": "Science",
    "views": "14001",
    "desc": "#mlnews #gtc22 #ithaca\\\\n\\\\nGTC Registration Link: https://ykilcher.com/gtc\\\\nYour regular updates on what\\'s going on in the ML world!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Register to Nvidia GTC and win a 3090!\\\\n4:15 - DeepMind\\'s Ithaca deciphers Lost Ancient Texts\\\\n6:45 - Drug discovery model turns toxic\\\\n10:00 - Gary Marcus: Deep Learning is hitting a wall\\\\n19:40 - GopherCite: Backing up answers with citations\\\\n22:40 - Yoshua Bengio appointed knight of the legion of honour\\\\n23:00 - Meta AI tags parody account of Yoshua Bengio\\\\n23:40 - Building games using just natural language\\\\n24:55 - YOU.com adds writing assistant\\\\n25:45 - Horace He: How to brrr\\\\n26:35 - Karpathy: Reproducing Yann LeCun\\'s 1989 paper\\\\n27:50 - Pig grunt emotion classifier\\\\n28:20 - AI annotates protein domain functions\\\\n29:40 - Atwood \\\\u0026 Carmack: 10k self-driving car bet\\\\n30:50 - Helpful Things\\\\n\\\\nReferences:\\\\nRegister to GTC and win a 3090!\\\\nhttps://twitter.com/NVIDIAEU/status/1501881813651836930\\\\nhttps://www.nvidia.com/gtc/keynote/?ncid=so-twit-533413\\\\u0026=\\\\u0026linkId=100000114410590\\\\nhttps://www.nvidia.com/gtc/?ncid=ref-inpa-330612\\\\nhttps://www.nvidia.com/gtc/keynote/\\\\nhttps://www.nvidia.com/gtc/training/\\\\nhttps://developer.nvidia.com/nvidia-omniverse-platform\\\\n\\\\nDeepMind deciphers Lost Ancient Texts\\\\nhttps://deepmind.com/blog/article/Predicting-the-past-with-Ithaca\\\\nhttps://www.nature.com/articles/s41586-022-04448-z\\\\nhttps://github.com/deepmind/ithaca\\\\nhttps://ithaca.deepmind.com/?job=eyJyZXF1ZXN0SUQiOiI1N2I4MWFjNTIxNGM3NDBiMjc3YzA1YzFiOTYwYzI0NCIsImF0dHJpYnV0aW9uIjp0cnVlLCJyZXN0b3JhdGlvbiI6dHJ1ZX0%3D\\\\n\\\\nDrug discovery model turns toxic\\\\nhttps://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx\\\\nhttps://www.nature.com/articles/s42256-022-00465-9.pdf?utm_source=pocket_mylist\\\\n\\\\nGary Marcus: Deep Learning is hitting a wall\\\\nhttps://nautil.us/deep-learning-is-hitting-a-wall-14467/\\\\nhttps://www.youtube.com/watch?v=fVkXE330Bh0\\\\u0026t=4437s\\\\n\\\\nGopherCite: Backing up answers with citations\\\\nhttps://deepmind.com/research/publications/2022/GopherCite-Teaching-Language-Models-To-Support-Answers-With-Verified-Quotes\\\\n\\\\nYoshua Bengio appointed knight of the legion of honour\\\\nhttps://mila.quebec/en/professor-yoshua-bengio-appointed-knight-of-the-legion-of-honour-by-france/\\\\n\\\\nMeta AI tags parody account\\\\nhttps://twitter.com/MetaAI/status/1504575140532613125\\\\n\\\\nBuilding games using just natural language\\\\nhttps://andrewmayneblog.wordpress.com/2022/03/17/building-games-and-apps-entirely-through-natural-language-using-openais-davinci-code-model/\\\\n\\\\nYOU.com adds writing assistant\\\\nhttps://you.com/search?q=how%20to%20write%20well\\\\n\\\\nHorace He: How to brrr\\\\nhttps://horace.io/brrr_intro.html\\\\n\\\\nKarpathy: Reproducing Yann LeCun\\'s 1989 paper\\\\nhttps://karpathy.github.io/2022/03/14/lecun1989/\\\\n\\\\nPig grunt emotion classifier\\\\nhttps://science.ku.dk/english/press/news/2022/pig-grunts-reveal-their-emotions/?utm_source=pocket_mylist\\\\n\\\\nAI annotates protein domain functions\\\\nhttps://ai.googleblog.com/2022/03/using-deep-learning-to-annotate-protein.html?utm_source=pocket_mylist\\\\nhttps://google-research.github.io/proteinfer/\\\\n\\\\nAtwood \\\\u0026 Carmack: 10k self-driving car bet\\\\nhttps://blog.codinghorror.com/the-2030-self-driving-car-bet/?utm_source=pocket_mylist\\\\n\\\\nHelpful Things\\\\nhttps://github.com/recognai/rubrix\\\\nhttps://twitter.com/taiyasaki/status/1501288630697877504\\\\nhttps://github.com/mosaicml/composer?src=twitter\\\\nhttps://mujoco.org/\\\\nhttps://mujoco.readthedocs.io/en/latest/changelog.html\\\\nhttps://github.com/deepmind/mctx?utm_source=pocket_mylist\\\\nhttps://padl.ai/\\\\nhttps://github.com/LaihoE/did-it-spill\\\\nhttps://pytorch.org/blog/pytorch-1.11-released/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2000",
    "uploadDate": "2022-03-21",
    "thumbnail_url": "https://i.ytimg.com/vi/RXwZKzczkF8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=smxwT82o40Y",
    "title": "Active Dendrites avoid catastrophic forgetting - Interview with the Authors",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, active dendrites, neurons dendrites, biological deep learning, deep learning biology, numenta, numenta research, numenta deep learning, dendrites deep learning, deep learning tutorial, hierarchical temporal memory, computational neuroscience, reinforcement learning, robotics, multi task learning, continuous learning, continual learning, permuted mnist",
    "scraped_at": 1684582628.481641,
    "genre": "Science",
    "views": "6889",
    "desc": "#multitasklearning #biology #neuralnetworks\\\\n\\\\nThis is an interview with the paper\\'s authors: Abhiram Iyer, Karan Grewal, and Akash Velu!\\\\nPaper Review Video: https://youtu.be/O_dJ31T01i8\\\\n\\\\nCheck out Zak\\'s course on Graph Neural Networks (discount with this link): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES\\\\u0026affcode=999036_lzknae-d\\\\n\\\\nCatastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:55 - Sponsor: GNN Course\\\\n2:30 - How did the idea come to be?\\\\n7:05 - What roles do the different parts of the method play?\\\\n8:50 - What was missing in the paper review?\\\\n10:35 - Are biological concepts viable if we still have backprop?\\\\n11:50 - How many dendrites are necessary?\\\\n14:10 - Why is there a plateau in the sparsity plot?\\\\n20:50 - How does task difficulty play into the algorithm?\\\\n24:10 - Why are there different setups in the experiments?\\\\n30:00 - Is there a place for unsupervised pre-training?\\\\n32:50 - How can we apply the online prototyping to more difficult tasks?\\\\n37:00 - What did not work out during the project?\\\\n41:30 - How do you debug a project like this?\\\\n47:10 - How is this related to other architectures?\\\\n51:10 - What other things from neuroscience are to be included?\\\\n55:50 - Don\\'t miss the awesome ending :)\\\\n\\\\nPaper: https://arxiv.org/abs/2201.00042\\\\nBlog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting\\\\n\\\\nLink to the GNN course (with discount): https://www.graphneuralnets.com/p/introduction-to-gnns?coupon_code=SUNGLASSES\\\\u0026affcode=999036_lzknae-d\\\\n\\\\nAbstract:\\\\nA key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model\\'s prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.\\\\n\\\\nAuthors: Abhiram Iyer, Karan Grewal, Akash Velu, Lucas Oliveira Souza, Jeremy Forest, Subutai Ahmad\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3392",
    "uploadDate": "2022-03-20",
    "thumbnail_url": "https://i.ytimg.com/vi/smxwT82o40Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=O_dJ31T01i8",
    "title": "Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments (Review)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, active dendrites, neurons dendrites, biological deep learning, deep learning biology, numenta, numenta research, numenta deep learning, dendrites deep learning, deep learning tutorial, hierarchical temporal memory, computational neuroscience, reinforcement learning, robotics, multi task learning, continuous learning, continual learning, permuted mnist",
    "scraped_at": 1684582635.9125116,
    "genre": "Science",
    "views": "17285",
    "desc": "#multitasklearning #biology #neuralnetworks\\\\n\\\\nCatastrophic forgetting is a big problem in mutli-task and continual learning. Gradients of different objectives tend to conflict, and new tasks tend to override past knowledge. In biological neural networks, each neuron carries a complex network of dendrites that mitigate such forgetting by recognizing the context of an input signal. This paper introduces Active Dendrites, which carries over the principle of context-sensitive gating by dendrites into the deep learning world. Various experiments show the benefit in combatting catastrophic forgetting, while preserving sparsity and limited parameter counts.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n1:20 - Paper Overview\\\\n3:15 - Catastrophic forgetting in continuous and multi-task learning\\\\n9:30 - Dendrites in biological neurons\\\\n16:55 - Sparse representations in biology\\\\n18:35 - Active dendrites in deep learning\\\\n34:15 - Experiments on multi-task learning\\\\n39:00 - Experiments in continual learning and adaptive prototyping\\\\n49:20 - Analyzing the inner workings of the algorithm\\\\n53:30 - Is this the same as just training a larger network?\\\\n59:15 - How does this relate to attention mechanisms?\\\\n1:02:55 - Final thoughts and comments\\\\n\\\\nPaper: https://arxiv.org/abs/2201.00042\\\\nBlog: https://numenta.com/blog/2021/11/08/can-active-dendrites-mitigate-catastrophic-forgetting\\\\n\\\\nERRATA:\\\\n- I was made aware of this by https://twitter.com/ChainlessCoder: \\\\\"",
    "lengthSeconds": "3920",
    "uploadDate": "2022-03-18",
    "thumbnail_url": "https://i.ytimg.com/vi/O_dJ31T01i8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=MgJ3JsE3Tqo",
    "title": "Author Interview - VOS: Learning What You Don't Know by Virtual Outlier Synthesis",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, paper explained, virtual outliers, how to detect outliers, deep learning outliers, deep learning outlier detection, vos, deep learning energy, latent space outliers, density estimation, classification boundaries, generative models",
    "scraped_at": 1684582634.576537,
    "genre": "Science",
    "views": "6444",
    "desc": "#deeplearning #objectdetection #outliers\\\\n\\\\nAn interview with the authors of \\\\\"",
    "lengthSeconds": "2159",
    "uploadDate": "2022-03-14",
    "thumbnail_url": "https://i.ytimg.com/vi/MgJ3JsE3Tqo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=i-J4T3uLC9M",
    "title": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, paper explained, virtual outliers, how to detect outliers, deep learning outliers, deep learning outlier detection, vos, deep learning energy, latent space outliers, density estimation, classification boundaries, generative models",
    "scraped_at": 1684582631.1183457,
    "genre": "Science",
    "views": "13211",
    "desc": "#vos #outliers #deeplearning\\\\nSponsor: Assembly AI\\\\nCheck them out here: https://www.assemblyai.com/?utm_source=youtube\\\\u0026utm_medium=social\\\\u0026utm_campaign=yannic1\\\\n\\\\nOutliers are data points that are highly unlikely to be seen in the training distribution, and therefore deep neural networks have troubles when dealing with them. Many approaches to detecting outliers at inference time have been proposed, but most of them show limited success. This paper presents Virtual Outlier Synthesis, which is a method that pairs synthetic outliers, forged in the latent space, with an energy-based regularization of the network at training time. The result is a deep network that can reliably detect outlier datapoints during inference with minimal overhead.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n2:00 - Sponsor: Assembly AI (Link below)\\\\n4:05 - Paper Overview\\\\n6:45 - Where do traditional classifiers fail?\\\\n11:00 - How object detectors work\\\\n17:00 - What are virtual outliers and how are they created?\\\\n24:00 - Is this really an appropriate model for outliers?\\\\n26:30 - How virtual outliers are used during training\\\\n34:00 - Plugging it all together to detect outliers\\\\n\\\\nPaper: https://arxiv.org/abs/2202.01197\\\\nCode: https://github.com/deeplearning-wisc/vos\\\\n\\\\nAbstract:\\\\nOut-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model\\'s decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at this https URL.\\\\n\\\\nAuthors: Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2157",
    "uploadDate": "2022-03-13",
    "thumbnail_url": "https://i.ytimg.com/vi/i"
  },
  {
    "link": "watch?v=6dvcYx9hcbE",
    "title": "Spurious normativity enhances learning of compliance and enforcement behavior in artificial agents",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deepmind, deep mind, ml and society, ai and society, sociology and machine learning, machine learning for sociology, machine learning for economics, ai microeconomics, reinforcement learning economics, society simulations, silly rules, social norms, social norms enforcement, why do social norms exist, why do silly rules exist, deep mind society",
    "scraped_at": 1684582633.9925122,
    "genre": "Science",
    "views": "6927",
    "desc": "#deepmind #rl #society\\\\n\\\\nThis is an in-depth paper review, followed by an interview with the papers\\' authors!\\\\nSociety is ruled by norms, and most of these norms are very useful, such as washing your hands before cooking. However, there also exist plenty of social norms which are essentially arbitrary, such as what hairstyles are acceptable, or what words are rude. These are called \\\\\"",
    "lengthSeconds": "5799",
    "uploadDate": "2022-03-08",
    "thumbnail_url": "https://i.ytimg.com/vi/6dvcYx9hcbE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kl3aBni87jg",
    "title": "First Author Interview: AI & formal math (Formal Mathematics Statement Curriculum Learning)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openai, formal math, ai math, ai math prover, machine learning for math, ml math, artificial intelligence math, ai mathematics, automated proof search, mini f2f, ai imo, ai math olympiad, openai mathematics, openai formal math, language models formal math, lean, lean prover, lean proof, lean math, ai lean environment, ai proves theorems, ai theorem prover",
    "scraped_at": 1684582632.7623775,
    "genre": "Science",
    "views": "7482",
    "desc": "#openai #math #imo\\\\n\\\\nThis is an interview with Stanislas Polu, research engineer at OpenAI and first author of the paper \\\\\"",
    "lengthSeconds": "3488",
    "uploadDate": "2022-03-06",
    "thumbnail_url": "https://i.ytimg.com/vi/kl3aBni87jg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lvYVuOmUVs8",
    "title": "OpenAI tackles Math - Formal Mathematics Statement Curriculum Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openai, formal math, ai math, ai math prover, machine learning for math, ml math, artificial intelligence math, ai mathematics, automated proof search, mini f2f, ai imo, ai math olympiad, openai mathematics, openai formal math, language models formal math, lean, lean prover, lean proof, lean math, ai lean environment, ai proves theorems, ai theorem prover",
    "scraped_at": 1684582630.12038,
    "genre": "Science",
    "views": "9063",
    "desc": "#openai #math #imo\\\\n\\\\nFormal mathematics is a challenging area for both humans and machines. For humans, formal proofs require very tedious and meticulous specifications of every last detail and results in very long, overly cumbersome and verbose outputs. For machines, the discreteness and sparse reward nature of the problem presents a significant problem, which is classically tackled by brute force search, guided by a couple of heuristics. Previously, language models have been employed to better guide these proof searches and delivered significant improvements, but automated systems are still far from usable. This paper introduces another concept: An expert iteration procedure is employed to iteratively produce more and more challenging, but solvable problems for the machine to train on, which results in an automated curriculum, and a final algorithm that performs well above the previous models. OpenAI used this method to even solve two problems of the international math olympiad, which was previously infeasible for AI systems.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n2:35 - Paper Overview\\\\n5:50 - How do formal proofs work?\\\\n9:35 - How expert iteration creates a curriculum\\\\n16:50 - Model, data, and training procedure\\\\n25:30 - Predicting proof lengths for guiding search\\\\n29:10 - Bootstrapping expert iteration\\\\n34:10 - Experimental evaluation \\\\u0026 scaling properties\\\\n40:10 - Results on synthetic data\\\\n44:15 - Solving real math problems\\\\n47:15 - Discussion \\\\u0026 comments\\\\n\\\\nPaper: https://arxiv.org/abs/2202.01344\\\\nminiF2F benchmark: https://github.com/openai/miniF2F\\\\n\\\\nAbstract:\\\\nWe explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.\\\\n\\\\nAuthors: Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3040",
    "uploadDate": "2022-03-05",
    "thumbnail_url": "https://i.ytimg.com/vi/lvYVuOmUVs8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YOLL8dIhLJI",
    "title": "[ML News] DeepMind controls fusion | Yann LeCun's JEPA architecture | US: AI can't copyright its art",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deepmind, tokamak, topamak, tokamak fusion reactor, tokamak reactor, tokamak ai, machine learning fusion, ai nuclear fusion, yann lecun, lecun jepa, yann lecun jepa, yann lecun world model, ml news, mlnews, kilcher news, ai carbon emissions, machine learning carbon footprint, machine learning carbon emissions, ai nuclear reactor, deep mind, huggingface book, hugging face book",
    "scraped_at": 1684582633.5053456,
    "genre": "Science",
    "views": "15384",
    "desc": "#mlnews #deepmind #fusion\\\\nUpdates on what\\'s going on in the ML world!\\\\nCheck out w\\\\u0026b\\'s alerts feature: https://wandb.me/yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n2:35 - DeepMind uses Reinforcement Learning to control nuclear fusion\\\\n4:35 - Google responds to carbon emission estimates\\\\n8:40 - Yann LeCun proposes new architecture for world models\\\\n11:05 - Fruit fly neurons may perform multiplication\\\\n12:00 - Emojisearch App\\\\n12:30 - Ar5iv officially in arXiv labs\\\\n12:55 - Language Model Consciousness \\\\u0026 Media Hype\\\\n16:45 - Vision models are more fair when trained on uncurated data\\\\n18:30 - CLIPasso\\\\n19:15 - NLP with Transformers Book\\\\n20:15 - Helpful Things\\\\n26:00 - US Office: AI can\\'t copyright its art\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nReferences:\\\\nhttps://wandb.me/yannic\\\\n\\\\nDeepMind uses RL to control nuclear fusion\\\\nhttps://deepmind.com/blog/article/Accelerating-fusion-science-through-learned-plasma-control\\\\nhttps://www.nature.com/articles/s41586-021-04301-9/figures/1\\\\nhttps://www.nature.com/articles/s41586-021-04301-9.pdf\\\\nhttps://www.alexirpan.com/2018/02/14/rl-hard.html\\\\n\\\\nGoogle responds to carbon emission estimates\\\\nhttps://ai.googleblog.com/2022/02/good-news-about-carbon-footprint-of.html\\\\n\\\\nYann LeCun proposes new architecture for world models\\\\nhttps://ai.facebook.com/blog/yann-lecun-advances-in-ai-research\\\\n\\\\nFruit fly neurons may perform multiplication\\\\nhttps://www.nature.com/articles/s41586-022-04428-3\\\\n\\\\nEmojisearch App\\\\nhttps://twitter.com/lilianweng/status/1488791391358513153\\\\nhttps://www.emojisearch.app/\\\\nhttps://github.com/lilianweng/emoji-semantic-search/blob/main/server/app.py\\\\n\\\\nAr5iv officially in arXiv labs\\\\nhttps://blog.arxiv.org/2022/02/21/arxiv-articles-as-responsive-web-pages/\\\\n\\\\nTech media may be only slightly conscious\\\\nhttps://twitter.com/ilyasut/status/1491554478243258368\\\\nhttps://futurism.com/the-byte/openai-already-sentient\\\\nhttps://interestingengineering.com/ai-might-be-conscious\\\\nhttps://futurism.com/mit-researcher-conscious-ai\\\\nhttps://www.dailymail.co.uk/sciencetech/article-10503703/Artificial-Intelligence-expert-warns-slightly-conscious-AI.html\\\\nhttps://futurism.com/conscious-ai-backlash\\\\nhttps://www.dailystar.co.uk/tech/news/conscious-ai-already-exist-expert-26223303\\\\n\\\\nVision models are more fair when trained on uncurated data\\\\nhttps://arxiv.org/pdf/2202.08360.pdf\\\\n\\\\nCLIPasso\\\\nhttps://clipasso.github.io/clipasso/\\\\n\\\\nNLP with Transformers Book\\\\nhttps://www.amazon.de/dp/1098103246?linkCode=gs2\\\\u0026tag=oreilly200c-21\\\\n\\\\nHelpful Things\\\\nhttps://github.com/j3soon/tbparse\\\\nhttps://github.com/openvinotoolkit/anomalib\\\\nhttps://liuliu66.github.io/articulationobjects/\\\\nhttps://github.com/RobertTLange/evosax\\\\nhttps://github.com/google/evojax\\\\nhttps://github.com/google/evojax/pull/9\\\\nhttps://github.com/facebookresearch/textlesslib\\\\nhttps://standard-ai.github.io/Standard-Sim/\\\\nhttps://twitter.com/PatrickPlaten/status/1493916630967066626?utm_source=pocket_mylist\\\\nhttps://aimagelab.ing.unimore.it/imagelab/page.asp?IdPage=42\\\\u0026utm_source=pocket_mylist\\\\nhttps://github.com/yashbhalgat/HashNeRF-pytorch?utm_source=pocket_mylist\\\\nhttps://github.com/patrick-kidger/diffrax\\\\nhttps://github.com/AI4Finance-Foundation/FinRL\\\\nhttps://huggingface.co/AI-Nordics/bert-large-swedish-cased\\\\nhttps://huggingface.co/AI-Nordics/gpt-sw3\\\\nhttps://paperswithcode.com/dataset/muld\\\\nhttps://github.com/JonasGeiping/breaching\\\\nhttps://github.com/Weixin-Liang/MetaShift\\\\n\\\\nUS Office: AI can\\'t copyright its art\\\\nhttps://www.theverge.com/2022/2/21/22944335/us-copyright-office-reject-ai-generated-art-recent-entrance-to-paradise\\\\nhttps://www.urbasm.com/2016/05/artificial-intelligence-visions-art-of-a-dying-brain/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1700",
    "uploadDate": "2022-03-04",
    "thumbnail_url": "https://i.ytimg.com/vi/YOLL8dIhLJI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=C5sWbYwzKyg",
    "title": "AlphaCode - with the authors!",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, alphacode, alpha code, deepmind, deepmind code, deepmind alphacode, alphacoder, codex, copilot, ai code, ai programmer, ai competitive programming, ai leetcode, machine learning leetcode, deepmind leetcode, codeforces, large scale sampling, language models, language models for code, ai python programmer, deep mind, fuzzing, google deepmind, competitive programming ai, interview",
    "scraped_at": 1684582633.312372,
    "genre": "Science",
    "views": "11811",
    "desc": "#ai #alphacode #deepmind\\\\n\\\\nAn interview with the creators of AlphaCode!\\\\nPaper review video here: https://youtu.be/s9UAOmyah1A\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:10 - Media Reception\\\\n5:10 - How did the project go from start to finish?\\\\n9:15 - Does the model understand its own code?\\\\n14:45 - Are there plans to reduce the number of samples?\\\\n16:15 - Could one do smarter filtering of samples?\\\\n18:55 - How crucial are the public test cases?\\\\n21:55 - Could we imagine an adversarial method?\\\\n24:45 - How are coding problems even made?\\\\n27:40 - Does AlphaCode evaluate a solution\\'s asymptotic complexity?\\\\n33:15 - Are our sampling procedures inappropriate for diversity?\\\\n36:30 - Are all generated solutions as instructive as the example?\\\\n41:30 - How are synthetic examples created during training?\\\\n42:30 - What were high and low points during this research?\\\\n45:25 - What was the most valid criticism after publication?\\\\n47:40 - What are applications in the real world?\\\\n51:00 - Where do we go from here?\\\\n\\\\nPaper: https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf\\\\nCode: https://github.com/deepmind/code_contests\\\\n\\\\nAbstract: Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. Evaluated on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in programming competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\\\\n\\\\nAuthors: Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\\xc3\\xa9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\\xe2\\x80\\x99Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu and Oriol Vinyals\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3226",
    "uploadDate": "2022-03-02",
    "thumbnail_url": "https://i.ytimg.com/vi/C5sWbYwzKyg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=s9UAOmyah1A",
    "title": "Competition-Level Code Generation with AlphaCode (Paper Review)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, alphacode, alpha code, deepmind, deepmind code, deepmind alphacode, alphacoder, codex, copilot, ai code, ai programmer, ai competitive programming, ai leetcode, machine learning leetcode, deepmind leetcode, codeforces, large scale sampling, language models, language models for code, ai python programmer, deep mind, fuzzing, google deepmind, competitive programming ai",
    "scraped_at": 1684582632.852347,
    "genre": "Science",
    "views": "10279",
    "desc": "#ai #alphacode #deepmind\\\\n\\\\nAlphaCode is an automated system that can solve competitive programing exercises. The authors found an interesting combination of language models, large-scale sampling, and clever techniques to filter and subsequently cluster the resulting programs, which lets the system perform on the level of an average competitor in real competitions. In this video, we take a deep dive into AlphaCode\\'s design, architecture, and experimental evaluation. The paper is very well structured and the empirical results are super interesting!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n2:10 - Paper Overview\\\\n3:30 - An example problem from competitive programming\\\\n8:00 - AlphaCode system overview\\\\n14:00 - Filtering out wrong solutions\\\\n17:15 - Clustering equivalent generated programs\\\\n21:50 - Model configurations \\\\u0026 engineering choices\\\\n24:30 - Adding privileged information to the input \\\\u0026 more tricks\\\\n28:15 - Experimental Results (very interesting!)\\\\n\\\\nPaper: https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf\\\\nCode: https://github.com/deepmind/code_contests\\\\n\\\\nAbstract: Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. Evaluated on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in programming competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\\\\n\\\\nAuthors: Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\\xc3\\xa9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\\xe2\\x80\\x99Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu and Oriol Vinyals\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2725",
    "uploadDate": "2022-03-01",
    "thumbnail_url": "https://i.ytimg.com/vi/s9UAOmyah1A/maxresdefault.jpg"
  },
  {
    "link": "watch?v=FNDVy_BR8aA",
    "title": "Can Wikipedia Help Offline Reinforcement Learning? (Author Interview)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582635.0405138,
    "genre": "Science",
    "views": "4116",
    "desc": "#wikipedia #reinforcementlearning #languagemodels\\\\n\\\\nOriginal paper review here: https://youtu.be/XHGh19Hbx48\\\\n\\\\nMachel Reid and Yutaro Yamada join me to discuss their recent paper on langauge model pre-training for decision transformers in offline reinforcement learning.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:00 - Brief paper, setup \\\\u0026 idea recap\\\\n7:30 - Main experimental results \\\\u0026 high standard deviations\\\\n10:00 - Why is there no clear winner?\\\\n13:00 - Why are bigger models not a lot better?\\\\n14:30 - What\\xe2\\x80\\x99s behind the name ChibiT?\\\\n15:30 - Why is iGPT underperforming?\\\\n19:15 - How are tokens distributed in Reinforcement Learning?\\\\n22:00 - What other domains could have good properties to transfer?\\\\n24:20 - A deeper dive into the models\\' attention patterns\\\\n33:30 - Codebase, model sizes, and compute requirements\\\\n37:30 - Scaling behavior of pre-trained models\\\\n40:05 - What did not work out in this project?\\\\n42:00 - How can people get started and where to go next?\\\\n\\\\nPaper: https://arxiv.org/abs/2201.12122\\\\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\\\\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\\\\n\\\\nAbstract:\\\\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\\\\n\\\\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2687",
    "uploadDate": "2022-02-28",
    "thumbnail_url": "https://i.ytimg.com/vi/FNDVy_BR8aA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XHGh19Hbx48",
    "title": "Can Wikipedia Help Offline Reinforcement Learning? (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582634.3865116,
    "genre": "Science",
    "views": "11289",
    "desc": "#wikipedia #reinforcementlearning #languagemodels\\\\n\\\\nTransformers have come to overtake many domain-targeted custom models in a wide variety of fields, such as Natural Language Processing, Computer Vision, Generative Modelling, and recently also Reinforcement Learning. This paper looks at the Decision Transformer and shows that, surprisingly, pre-training the model on a language-modelling task significantly boosts its performance on Offline Reinforcement Learning. The resulting model achieves higher scores, can get away with less parameters, and exhibits superior scaling properties. This raises many questions about the fundamental connection between the domains of language and RL.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:35 - Paper Overview\\\\n7:35 - Offline Reinforcement Learning as Sequence Modelling\\\\n12:00 - Input Embedding Alignment \\\\u0026 other additions\\\\n16:50 - Main experimental results\\\\n20:45 - Analysis of the attention patterns across models\\\\n32:25 - More experimental results (scaling properties, ablations, etc.)\\\\n37:30 - Final thoughts\\\\n\\\\nPaper: https://arxiv.org/abs/2201.12122\\\\nCode: https://github.com/machelreid/can-wikipedia-help-offline-rl\\\\nMy Video on Decision Transformer: https://youtu.be/-buULmf7dec\\\\n\\\\nAbstract:\\\\nFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.\\\\n\\\\nAuthors: Machel Reid, Yutaro Yamada, Shixiang Shane Gu\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2315",
    "uploadDate": "2022-02-26",
    "thumbnail_url": "https://i.ytimg.com/vi/XHGh19Hbx48/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XjILIYVLFrI",
    "title": "[ML Olds] Meta Research Supercluster | OpenAI GPT-Instruct | Google LaMDA | Drones fight Pigeons",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gpt3, gpt",
    "scraped_at": 1684582634.8555126,
    "genre": "Science",
    "views": "12288",
    "desc": "#mlnews #rsc #gpt3\\\\n\\\\nSome things we\\'ve missed in recent weeks!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n0:40 - Meta builds AI Research Supercluster (RSC)\\\\n2:25 - OpenAI trains GPT-3 to follow instructions\\\\n4:10 - Meta AI releases multilingual language models\\\\n4:50 - Google LaMDA dialogue models\\\\n5:50 - Helpful Things\\\\n8:25 - Training the alpha matte generator for Pixel 6\\\\n10:15 - Drones used to deter pigeons on buildings\\\\n11:05 - IBM sells some Watson Health assets for USD 1B\\\\n\\\\nMerch: http://store.ykilcher.com\\\\n\\\\nReferences:\\\\nhttps://ai.facebook.com/blog/ai-rsc/?utm_source=pocket_mylist\\\\n\\\\nhttps://openai.com/blog/instruction-following/\\\\nhttps://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf\\\\nhttps://openai.com/blog/deep-reinforcement-learning-from-human-preferences/\\\\n\\\\nhttps://twitter.com/MetaAI/status/1486745968372551686?utm_source=pocket_mylist\\\\nhttps://arxiv.org/pdf/2112.10668.pdf\\\\nhttps://github.com/pytorch/fairseq/tree/main/examples/xglm\\\\n\\\\nhttps://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html?m=1\\\\u0026utm_source=pocket_mylist\\\\nhttps://arxiv.org/pdf/2201.08239.pdf\\\\n\\\\nhttps://evolutiongym.github.io/?utm_source=pocket_mylist\\\\nhttps://evolutiongym.github.io/all-tasks\\\\nhttps://evolutiongym.github.io/documentation\\\\nhttps://arxiv.org/pdf/2201.09863.pdf\\\\nhttps://github.com/EvolutionGym\\\\nhttps://huggingface.co/blog/sb3\\\\nhttps://twitter.com/Sentdex/status/1489991413005787139\\\\nhttps://github.com/lvwerra/trl?utm_source=pocket_mylist\\\\n\\\\nhttps://ai.googleblog.com/2022/01/accurate-alpha-matting-for-portrait.html\\\\nhttps://polyhaven.com/hdris\\\\n\\\\nhttps://ieeexplore.ieee.org/document/9656717\\\\n\\\\nhttps://www.bloomberg.com/news/articles/2022-01-21/ibm-is-said-to-near-sale-of-watson-health-to-francisco-partners\\\\nhttps://archive.ph/xadf9\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "759",
    "uploadDate": "2022-02-23",
    "thumbnail_url": "https://i.ytimg.com/vi/XjILIYVLFrI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=cO1nSnsH_CQ",
    "title": "Listening to You! - Channel Update (Author Interviews)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, with the authors, kilcher, kilcher interview, machine learning papers, machine learning interview, author interview, poster session, conference publication, paper explained, yannic with the authors, feedback, channel update",
    "scraped_at": 1684582628.9303048,
    "genre": "Science",
    "views": "6237",
    "desc": "#mlnews #kilcher #withtheauthors\\\\n\\\\nMany of you have given me feedback on what you did and didn\\'t like about the recent \\\\\"",
    "lengthSeconds": "270",
    "uploadDate": "2022-02-21",
    "thumbnail_url": "https://i.ytimg.com/vi/cO1nSnsH_CQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=VQoyypYTz2U",
    "title": "All about AI Accelerators: GPU, TPU, Dataflow, Near-Memory, Optical, Neuromorphic & more (w/ Author)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gpu, tpu, ipu, wave computing, dataflow, near memory compute, ai accelerators, deep learning hardware, sambanova, cerebras, graphcore, mythic, optical computing, lightmatter, groq, why are gpus so fast, why does deep learning need gpus, do i need a gpu for deep learning, transformers hardware, hardware matrix multiplication, fast deep learning, machine learning hardware",
    "scraped_at": 1684582634.0875137,
    "genre": "Science",
    "views": "59935",
    "desc": "#ai #gpu #tpu\\\\n\\\\nThis video is an interview with Adi Fuchs, author of a series called \\\\\"",
    "lengthSeconds": "3755",
    "uploadDate": "2022-02-20",
    "thumbnail_url": "https://i.ytimg.com/vi/VQoyypYTz2U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=fEKZC9mta8w",
    "title": "[ML News] Uber: Deep Learning for ETA | MuZero Video Compression  | Block-NeRF | EfficientNet-X",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, uber, uber eta, uber deep learning, deepmind, muzero, muzero video compression, muzero explained, machine learning tutorial, tech news, machine learning news, block nerf, blocknerf, learned soft prompts, gpt",
    "scraped_at": 1684582634.7635128,
    "genre": "Science",
    "views": "20805",
    "desc": "#mlnews #muzero #nerf\\\\n\\\\nYour regularly irregular updates on everything new in the ML world!\\\\nMerch: http://store.ykilcher.com\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: Weights \\\\u0026 Biases\\\\n2:15 - Uber switches from XGBoost to Deep Learning for ETA prediction\\\\n5:45 - MuZero advances video compression\\\\n10:10 - Learned Soft Prompts can steer large language models\\\\n12:45 - Block-NeRF captures entire city blocks\\\\n14:15 - Neural Architecture Search considers underlying hardware\\\\n16:50 - Mega-Blog on Self-Organizing Agents\\\\n18:40 - Know Your Data (for Tensorflow Datasets)\\\\n20:30 - Helpful Things\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nReferences:\\\\nhttps://docs.wandb.ai/guides/integrations/other/openai\\\\nhttps://colab.research.google.com/github/wandb/examples/blob/master/colabs/openai/Fine_tune_GPT_3_with_Weights_%26_Biases.ipynb#scrollTo=rJdQqrC8Ablo\\\\nhttps://wandb.ai/borisd13/GPT-3/reports/Fine-Tuning-Tips-and-Exploration-on-OpenAI-s-GPT-3---VmlldzoxNDYwODA2\\\\n\\\\nUber switches from XGBoost to Deep Learning for ETA prediction\\\\nhttps://eng.uber.com/deepeta-how-uber-predicts-arrival-times/?utm_source=pocket_mylist\\\\n\\\\nMuZero advances video compression\\\\nhttps://deepmind.com/blog/article/MuZeros-first-step-from-research-into-the-real-world\\\\nhttps://storage.googleapis.com/deepmind-media/MuZero/MuZero%20with%20self-competition.pdf\\\\n\\\\nLearned Soft Prompts can steer large language models\\\\nhttps://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html\\\\nhttps://aclanthology.org/2021.emnlp-main.243/\\\\n\\\\nBlock-NeRF captures entire city blocks\\\\nhttps://arxiv.org/abs/2202.05263\\\\nhttps://arxiv.org/pdf/2202.05263.pdf\\\\nhttps://waymo.com/intl/zh-cn/research/block-nerf/\\\\n\\\\nNeural Architecture Search considers underlying hardware\\\\nhttps://ai.googleblog.com/2022/02/unlocking-full-potential-of-datacenter.html\\\\nhttps://openaccess.thecvf.com/content/CVPR2021/papers/Li_Searching_for_Fast_Model_Families_on_Datacenter_Accelerators_CVPR_2021_paper.pdf\\\\n\\\\nMega-Blog on Self-Organizing Agents\\\\nhttps://developmentalsystems.org/sensorimotor-lenia/\\\\nhttps://flowers.inria.fr/\\\\n\\\\nKnow Your Data (for Tensorflow Datasets)\\\\nhttps://knowyourdata-tfds.withgoogle.com/#dataset=pass\\\\u0026filters=kyd%2Fcloud_vision%2Fface_probability:9\\\\u0026tab=RELATIONS\\\\u0026item=train%5B89%25%3A91%25%5D_27143\\\\u0026expanded_groups=cloud_vision\\\\nhttps://knowyourdata.withgoogle.com/\\\\n\\\\nHelpful Things\\\\nhttps://twitter.com/casualganpapers/status/1490318575873241091\\\\nhttps://www.reddit.com/r/MachineLearning/comments/snmtzn/r_phd_thesis_on_neural_differential_equations/\\\\nhttps://arxiv.org/abs/2202.02435\\\\nhttps://github.com/vicariousinc/PGMax\\\\nhttps://www.vicarious.com/posts/pgmax-factor-graphs-for-discrete-probabilistic-graphical-models-and-loopy-belief-propagation-in-jax/?utm_content=197542312\\\\u0026utm_medium=social\\\\u0026utm_source=twitter\\\\u0026hss_channel=tw-204185426\\\\nhttps://diambra.ai/tournaments\\\\nhttps://github.com/diambra/diambraArena\\\\nhttps://www.youtube.com/watch?v=dw72POyqcqk\\\\u0026t=271s\\\\nhttps://gitlab.com/deepcypher/python-fhez\\\\nhttps://python-fhez.readthedocs.io/en/latest/\\\\nhttps://joss.theoj.org/papers/10.21105/joss.04101?s=09\\\\u0026utm_source=pocket_mylist\\\\nhttps://github.com/PyTorchLightning/metrics\\\\nhttps://torchmetrics.readthedocs.io/en/latest/\\\\nhttps://twitter.com/alanyttian/status/1492027524909449221?utm_source=pocket_mylist\\\\nhttps://github.com/google/evojax\\\\nhttps://arxiv.org/abs/2202.05008\\\\nhttps://www.reddit.com/r/MachineLearning/comments/snod8f/n_gym_now_has_a_documentation_website/?utm_source=dlvr.it\\\\u0026utm_medium=twitter\\\\nhttps://www.gymlibrary.ml/pages/api/#initializing-environments\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1567",
    "uploadDate": "2022-02-18",
    "thumbnail_url": "https://i.ytimg.com/vi/fEKZC9mta8w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qNfCVGbvnJc",
    "title": "CM3: A Causal Masked Multimodal Model of the Internet (Paper Explained w/ Author Interview)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, cm3, facebook ai, fair, meta ai, language model, language modelling, gpt",
    "scraped_at": 1684582627.2866778,
    "genre": "Science",
    "views": "13055",
    "desc": "#cm3 #languagemodel #transformer\\\\n\\\\nThis video contains a paper explanation and an incredibly informative interview with first author Armen Aghajanyan.\\\\nAutoregressive Transformers have come to dominate many fields in Machine Learning, from text generation to image creation and many more. However, there are two problems. First, the collected data is usually scraped from the web and uni- or bi-modal and throws away a lot of structure of the original websites, and second, language modelling losses are uni-directional. CM3 addresses both problems: It directly operates on HTML and includes text, hyperlinks, and even images (via VQGAN tokenization) and can therefore be used in plenty of ways: Text generation, captioning, image creation, entity linking, and much more. It also introduces a new training strategy called Causally Masked Language Modelling, which brings a level of bi-directionality into autoregressive language modelling. In the interview after the paper explanation, Armen and I go deep into the how and why of these giant models, we go over the stunning results and we make sense of what they mean for the future of universal models.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n6:30 - Directly learning the structure of HTML\\\\n12:30 - Causally Masked Language Modelling\\\\n18:50 - A short look at how to use this model\\\\n23:20 - Start of interview\\\\n25:30 - Feeding language models with HTML\\\\n29:45 - How to get bi-directionality into decoder-only Transformers?\\\\n37:00 - Images are just tokens\\\\n41:15 - How does one train such giant models?\\\\n45:40 - CM3 results are amazing\\\\n58:20 - Large-scale dataset collection and content filtering\\\\n1:04:40 - More experimental results\\\\n1:12:15 - Why don\\'t we use raw HTML?\\\\n1:18:20 - Does this paper contain too many things?\\\\n\\\\nPaper: https://arxiv.org/abs/2201.07520\\\\n\\\\nAbstract:\\\\nWe introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.\\\\n\\\\nAuthors: Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer\\\\n\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "5060",
    "uploadDate": "2022-02-17",
    "thumbnail_url": "https://i.ytimg.com/vi/qNfCVGbvnJc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zcGOPqFZ4Tk",
    "title": "AI against Censorship: Genetic Algorithms, The Geneva Project, ML in Security, and more!",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, security, machine learning in security, ai security, ai network security, deep learning censorship, ai censorship, internet censorship, geneva, vpn, genetic algorithms, genetic algorithm, genetic algorithm example, real world genetic algorithm, ai in the real world, firewall, evolution, evolutionary search, maryland, breakerspace, encryption, amplification",
    "scraped_at": 1684582636.188448,
    "genre": "Science",
    "views": "7556",
    "desc": "#security #censorship #ai\\\\n\\\\nMost of us conceive the internet as a free and open space where we are able to send traffic between any two nodes, but for large parts of the world this is not the case. Entire nations have large machinery in place to survey all internet traffic and automated procedures to block any undesirable connections. Evading such censorship has been largely a cat-and-mouse game between security researchers and government actors. A new system, called Geneva, uses a Genetic Algorithm in combination with Evolutionary Search in order to dynamically evade such censorship and adjust itself in real-time to any potential response by its adversaries. In this video, I talk to Security researcher Kevin Bock, who is one of Geneva\\'s main contributors and member of the Breakerspace project. We talk about the evolution of internet censorship, how to evade it, how to mess with the censors\\' infrastructure, as well as the broader emerging connections between AI and Security.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n3:30 - What is automated censorship in networks?\\\\n7:20 - The evolution of censorship vs evasion\\\\n12:40 - Why do we need a dynamic, evolving system?\\\\n16:30 - The building blocks of Geneva\\\\n23:15 - Introducing evolution\\\\n28:30 - What\\'s the censors\\' response?\\\\n31:45 - How was Geneva\\'s media reception?\\\\n33:15 - Where do we go from here?\\\\n37:30 - Can we deliberately attack the censors?\\\\n47:00 - On responsible disclosure\\\\n49:40 - Breakerspace: Security research for undergrads\\\\n50:40 - How often do you get into trouble?\\\\n52:10 - How can I get started in security?\\\\n\\\\nLearn more at:\\\\n- Geneva (\\\\u0026 more) project page: https://censorship.ai\\\\n- Open Observatory of Network Interference: https://ooni.org\\\\n- Censored Planet: https://censoredplanet.org\\\\n- Breakerspace: https://breakerspace.cs.umd.edu\\\\n\\\\n\\\\nLinks:\\\\nMerch: http://store.ykilcher.com\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3297",
    "uploadDate": "2022-02-16",
    "thumbnail_url": "https://i.ytimg.com/vi/zcGOPqFZ4Tk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=D6osiiEoV0w",
    "title": "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning (w/ Author)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, metalearning, meta learning, neural network, unsupervised learning, few shot learning, google, google research, google ai, transformer, meta transformer, hypertransformer, hyper transformer, generate the weights of a neural network, privacy, personalization, interview, paper explained, semi",
    "scraped_at": 1684582630.2203722,
    "genre": "Science",
    "views": "16768",
    "desc": "#hypertransformer #metalearning #deeplearning\\\\n\\\\nThis video contains a paper explanation and an interview with author Andrey Zhmoginov!\\\\nFew-shot learning is an interesting sub-field in meta-learning, with wide applications, such as creating personalized models based on just a handful of data points. Traditionally, approaches have followed the BERT approach where a large model is pre-trained and then fine-tuned. However, this couples the size of the final model to the size of the model that has been pre-trained. Similar problems exist with \\\\\"",
    "lengthSeconds": "4696",
    "uploadDate": "2022-02-15",
    "thumbnail_url": "https://i.ytimg.com/vi/D6osiiEoV0w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=McpjrsHrEY4",
    "title": "[ML News] DeepMind AlphaCode | OpenAI math prover | Meta battles harmful content with AI",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ml news, machine learning news, tech news, artificial general intelligence, ai news, best ai, meta ai, harmful content, ai moderator, ai mod, ai harmful, openai, deepmind, deepmind alphacode, alphacode, alpha code, ai math, ai mathematics, ai math prove, ai theorem prover, expert iteration, langauge models, ai code, ai programmer, ai leetcode, stylegan xl",
    "scraped_at": 1684582627.633668,
    "genre": "Science",
    "views": "24763",
    "desc": "#mlnews #alphacode #openai\\\\n\\\\nThe latest and greatest from the world of Machine Learning!\\\\nMerch: http://store.ykilcher.com\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: Weights \\\\u0026 Biases\\\\n3:15 - DeepMind\\'s AlphaCode: AI competitive programmer\\\\n11:30 - OpenAI uses language models to prove math theorems\\\\n14:30 - StyleGAN XL: Scaling StyleGAN to diverse datasets\\\\n16:10 - ar5iv.org displays papers as HTML5\\\\n17:40 - Helpful Things\\\\n19:30 - ICML22 Review process changes\\\\n21:15 - Meta AI tackles harmful content classification using few-shot learning\\\\n23:55 - Company claims to produce face images from DNA\\\\n\\\\n\\\\nReferences:\\\\nhttps://deepmind.com/blog/article/Competitive-programming-with-AlphaCode\\\\nhttps://alphacode.deepmind.com/#layer=18,problem=34,heads=11111111111\\\\nhttps://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf\\\\nhttps://twitter.com/DBahdanau/status/1489009994007674881?utm_source=pocket_mylist\\\\n\\\\nhttps://openai.com/blog/formal-math/\\\\nhttps://arxiv.org/pdf/2202.01344.pdf\\\\n\\\\nhttps://blog.eleuther.ai/announcing-20b/?utm_source=pocket_mylist\\\\n\\\\nhttps://sites.google.com/view/stylegan-xl/\\\\nhttps://arxiv.org/pdf/2202.00273.pdf\\\\n\\\\nhttps://ar5iv.org/\\\\nhttps://ar5iv.org/html/1910.06709\\\\n\\\\nhttps://twitter.com/YiTayML/status/1488556619256328192?utm_source=pocket_mylist\\\\nhttps://ffcv.io/\\\\nhttps://github.com/ott-jax/ott\\\\nhttps://twitter.com/soumithchintala/status/1488206868573040641?utm_source=pocket_mylist\\\\nhttps://github.com/facebookresearch/dietgpu\\\\n\\\\nhttps://www.reddit.com/r/MachineLearning/comments/shazv1/n_changes_in_the_icml_2022_review_process/?utm_source=pocket_mylist\\\\nhttps://icml.cc/Conferences/2022/ReviewForm\\\\nhttps://icml.cc/Conferences/2022/CallForPapers\\\\n\\\\nhttps://ai.facebook.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/?utm_source=pocket_mylist\\\\n\\\\nhttps://www.technologyreview.com/2022/01/31/1044576/corsight-face-recognition-from-dna/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1599",
    "uploadDate": "2022-02-10",
    "thumbnail_url": "https://i.ytimg.com/vi/McpjrsHrEY4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=OUCwujwE7bA",
    "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (+Author)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, natural language processing, training data, deep learning tutorial, nlp, gpt3, gpt 3, codex, openai codex, large language models, gpt 3 planning, zero",
    "scraped_at": 1684582633.796538,
    "genre": "Science",
    "views": "15373",
    "desc": "#gpt3 #embodied #planning\\\\n\\\\nIn this video: Paper explanation, followed by first author interview with Wenlong Huang.\\\\nLarge language models contain extraordinary amounts of world knowledge that can be queried in various ways. But their output format is largely uncontrollable. This paper investigates the VirtualHome environment, which expects a particular set of actions, objects, and verbs to be used. Turns out, with proper techniques and only using pre-trained models (no fine-tuning), one can translate unstructured language model outputs into the structured grammar of the environment. This is potentially very useful anywhere where the models\\' world knowledge needs to be provided in a particular structured format.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:45 - The VirtualHome environment\\\\n6:25 - The problem of plan evaluation\\\\n8:40 - Contributions of this paper\\\\n16:40 - Start of interview\\\\n24:00 - How to use language models with environments?\\\\n34:00 - What does model size matter?\\\\n40:00 - How to fix the large models\\' outputs?\\\\n55:00 - Possible improvements to the translation procedure\\\\n59:00 - Why does Codex perform so well?\\\\n1:02:15 - Diving into experimental results\\\\n1:14:15 - Future outlook\\\\n\\\\nPaper: https://arxiv.org/abs/2201.07207\\\\nWebsite: https://wenlong.page/language-planner/\\\\nCode: https://github.com/huangwl18/language-planner\\\\nWenlong\\'s Twitter: https://twitter.com/wenlong_huang\\\\n\\\\nAbstract:\\\\nCan world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. \\\\\"",
    "lengthSeconds": "4625",
    "uploadDate": "2022-02-08",
    "thumbnail_url": "https://i.ytimg.com/vi/OUCwujwE7bA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5skIqoO3ku0",
    "title": "OpenAI Embeddings (and Controversy?!)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, natural language processing, mlnews, openai, openai embeddings, nils reimers, beir dataset, beir benchmark, text similarity, neural embeddings, gpt",
    "scraped_at": 1684582633.4053462,
    "genre": "Science",
    "views": "31905",
    "desc": "#mlnews #openai #embeddings\\\\n\\\\nCOMMENTS DIRECTLY FROM THE AUTHOR (thanks a lot for reaching out Arvind :) ):\\\\n1. The FIQA results you share also have code to reproduce the results in the paper using the API: https://twitter.com/arvind_io/status/1488257004783112192?s=20\\\\u0026t=gB3c79VEX8hGJl6WfZa2iA There\\'s no discrepancy AFAIK.\\\\n2. We leave out 6 not 7 BEIR datasets. Results on msmarco, nq and triviaqa are in a separate table (Table 5 in the paper). NQ is part of BEIR too and we didn\\'t want to repeat it. Finally, the 6 datasets we leave out are not readily available and it is common to leave them out in prior work too. For examples, see SPLADE v2 (https://arxiv.org/pdf/2109.10086.pdf) also evaluates on the same 12 BEIR datasets.\\\\n3. Finally, I\\'m now working on time travel so that I can cite papers from the future :)\\\\nEND COMMENTS FROM THE AUTHOR\\\\n\\\\nOpenAI launches an embeddings endpoint in their API, providing high-dimensional vector embeddings for use in text similarity, text search, and code search. While embeddings are universally recognized as a standard tool to process natural language, people have raised doubts about the quality of OpenAI\\'s embeddings, as one blog post found they are often outperformed by open-source models, which are much smaller and with which embedding would cost a fraction of what OpenAI charges. In this video, we examine the claims made and determine what it all means.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - Sponsor: Weights \\\\u0026 Biases\\\\n2:20 - What embeddings are available?\\\\n3:55 - OpenAI shows promising results\\\\n5:25 - How good are the results really?\\\\n6:55 - Criticism: Open models might be cheaper and smaller\\\\n10:05 - Discrepancies in the results\\\\n11:00 - The author\\'s response\\\\n11:50 - Putting things into perspective\\\\n13:35 - What about real world data?\\\\n14:40 - OpenAI\\'s pricing strategy: Why so expensive?\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nMerch: store.ykilcher.com\\\\n\\\\nERRATA: At 13:20 I say \\\\\"",
    "lengthSeconds": "957",
    "uploadDate": "2022-02-07",
    "thumbnail_url": "https://i.ytimg.com/vi/5skIqoO3ku0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vfBAUYpMCTU",
    "title": "Unsupervised Brain Models - How does Deep Learning inform Neuroscience? (w/ Patrick Mineault)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, xcorr, patrick mineault, unsupervised models, neuroscience, neuroscience and deep learning, deep learning brain, machine learning brain, brain models, how does the brain work, deep learning and neuroscience, self",
    "scraped_at": 1684582632.941347,
    "genre": "Science",
    "views": "27001",
    "desc": "#deeplearning #brain #neuroscience\\\\n\\\\nOriginally, Deep Learning sprang into existence inspired by how the brain processes information, but the two fields have diverged ever since. However, given that deep models can solve many perception tasks with remarkable accuracy, is it possible that we might be able to learn something about how the brain works by inspecting our models? I speak to Patrick Mineault about his blog post \\\\\"",
    "lengthSeconds": "4887",
    "uploadDate": "2022-02-06",
    "thumbnail_url": "https://i.ytimg.com/vi/vfBAUYpMCTU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=AJwnbSP_rq8",
    "title": "GPT-NeoX-20B - Open-Source huge language model by EleutherAI (Interview w/ co-founder Connor Leahy)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, leahy, eleuther, eleutherai, eleuther ai, connor leahy, coreweave, gooseai, goose ai, gpt neo, gpt",
    "scraped_at": 1684582634.9475133,
    "genre": "Science",
    "views": "20533",
    "desc": "#eleuther #gptneo #gptj\\\\n\\\\nEleutherAI announces GPT-NeoX-20B, a 20 billion parameter open-source language model, inspired by GPT-3. Connor joins me to discuss the process of training, how the group got their hands on the necessary hardware, what the new model can do, and how anyone can try it out!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:00 - Start of interview\\\\n2:00 - How did you get all the hardware?\\\\n3:50 - What\\'s the scale of this model?\\\\n6:00 - A look into the experimental results\\\\n11:15 - Why are there GPT-Neo, GPT-J, and GPT-NeoX?\\\\n14:15 - How difficult is training these big models?\\\\n17:00 - Try out the model on GooseAI\\\\n19:00 - Final thoughts\\\\n\\\\nRead the announcement: https://blog.eleuther.ai/announcing-20b/\\\\nTry out the model: https://goose.ai/\\\\nCheck out EleutherAI: https://www.eleuther.ai/\\\\nRead the code: https://github.com/EleutherAI/gpt-neox\\\\nHardware sponsor: https://www.coreweave.com/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1205",
    "uploadDate": "2022-02-04",
    "thumbnail_url": "https://i.ytimg.com/vi/AJwnbSP_rq8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1HEdXwEYrGM",
    "title": "Predicting the rules behind - Deep Symbolic Regression for Recurrent Sequences (w/ author interview)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, research, symbolic, symbolic regression, neuro symbolic computation, integer sequences, oeis, number sequences, ai number sequences, machine learning sequences, integer sequence rules, embedding space, transformers, attention mechanism, sequence generation, learning number sequences, predicting number sequences, facebook ai, meta ai, beam search, symbolic vs numeric",
    "scraped_at": 1684582635.7415378,
    "genre": "Science",
    "views": "18005",
    "desc": "#deeplearning #symbolic #research\\\\n\\\\nThis video includes an interview with first author St\\xc3\\xa9phane d\\'Ascoli (https://sdascoli.github.io/).\\\\nDeep neural networks are typically excellent at numeric regression, but using them for symbolic computation has largely been ignored so far. This paper uses transformers to do symbolic regression on integer and floating point number sequences, which means that given the start of a sequence of numbers, the model has to not only predict the correct continuation, but also predict the data generating formula behind the sequence. Through clever encoding of the input space and a well constructed training data generation process, this paper\\'s model can learn and represent many of the sequences in the OEIS, the online encyclopedia of integer sequences and it also features an interactive demo if you want to try it by yourself. \\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n2:20 - Summary of the Paper\\\\n16:10 - Start of Interview\\\\n17:15 - Why this research direction?\\\\n20:45 - Overview of the method\\\\n30:10 - Embedding space of input tokens\\\\n33:00 - Data generation process\\\\n42:40 - Why are transformers useful here?\\\\n46:40 - Beyond number sequences, where is this useful?\\\\n48:45 - Success cases and failure cases\\\\n58:10 - Experimental Results\\\\n1:06:30 - How did you overcome difficulties?\\\\n1:09:25 - Interactive demo\\\\n\\\\nPaper: https://arxiv.org/abs/2201.04600\\\\nInteractive demo: https://symbolicregression.metademolab.com/\\\\n\\\\nAbstract:\\\\nSymbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. bessel0(x)\\xe2\\x89\\x88sin(x)+cos(x)\\xcf\\x80x\\xe2\\x88\\x9a and 1.644934\\xe2\\x89\\x88\\xcf\\x802/6. An interactive demonstration of our models is provided at this https URL.\\\\n\\\\nAuthors: St\\xc3\\xa9phane d\\'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, Fran\\xc3\\xa7ois Charton\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "4270",
    "uploadDate": "2022-01-29",
    "thumbnail_url": "https://i.ytimg.com/vi/1HEdXwEYrGM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2v0xU2N1cdI",
    "title": "IT ARRIVED! YouTube sent me a package. (also: Limited Time Merch Deal)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, kilcher, silver plate, yannic kilcher subscribers, youtube silver plate, yannic kilcher merch, yannic kilcher merchandise, kilcher merch, machine learning merch, softmax merch, youtube silver award, kilcher silver award, 100k subscribers, kilcher 100k subscribers",
    "scraped_at": 1684582627.538673,
    "genre": "Science",
    "views": "6182",
    "desc": "LIMITED TIME MERCH DEAL: http://store.ykilcher.com\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "778",
    "uploadDate": "2022-01-27",
    "thumbnail_url": "https://i.ytimg.com/vi/2v0xU2N1cdI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yVKiMh2vEWQ",
    "title": "[ML News] ConvNeXt: Convolutions return | China regulates algorithms | Saliency cropping examined",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, deep learning ai, deep learning projects, mlnews, ml news, kilcher news, salicency cropping, twitter cropping, image cropping, twitter image cropping, convnext, facebook research, meta research, meta ai, convolutional neural networks, cnns vs transformers, mt3, yourtts, text to speech, ai for music, china regulation, china algorithms, china ai",
    "scraped_at": 1684582633.1233456,
    "genre": "Science",
    "views": "37833",
    "desc": "#mlnews #convnext #mt3\\\\n\\\\nYour update on what\\'s new in the Machine Learning world!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - ConvNeXt: Return of the Convolutions\\\\n2:50 - Investigating Saliency Cropping Algorithms\\\\n9:40 - YourTTS: SOTA zero-shot Text-to-Speech\\\\n10:40 - MT3: Multi-Track Music Transcription\\\\n11:35 - China regulates addictive algorithms\\\\n13:00 - A collection of Deep Learning interview questions \\\\u0026 solutions\\\\n13:35 - Helpful Things\\\\n16:05 - AlphaZero explained blog post\\\\n16:45 - Ru-DOLPH: HyperModal Text-to-Image-to-Text model\\\\n17:45 - Google AI 2021 Review\\\\n\\\\nReferences:\\\\nConvNeXt: Return of the Convolutions\\\\nhttps://arxiv.org/abs/2201.03545\\\\nhttps://github.com/facebookresearch/ConvNeXt\\\\nhttps://twitter.com/giffmana/status/1481054929573888005\\\\nhttps://twitter.com/wightmanr/status/1481150080765739009\\\\nhttps://twitter.com/tanmingxing/status/1481362887272636417\\\\n\\\\nInvestigating Saliency Cropping Algorithms\\\\nhttps://openaccess.thecvf.com/content/WACV2022/papers/Birhane_Auditing_Saliency_Cropping_Algorithms_WACV_2022_paper.pdf\\\\nhttps://vinayprabhu.github.io/Saliency_Image_Cropping/paper_html/main.html\\\\nhttps://vinayprabhu.medium.com/on-the-twitter-cropping-controversy-critique-clarifications-and-comments-7ac66154f687\\\\nhttps://vinayprabhu.github.io/Saliency_Image_Cropping/\\\\n\\\\nYourTTS: SOTA zero-shot Text-to-Speech\\\\nhttps://github.com/coqui-ai/TTS?utm_source=pocket_mylist\\\\nhttps://arxiv.org/abs/2112.02418?utm_source=pocket_mylist\\\\nhttps://coqui.ai/?utm_source=pocket_mylist\\\\nhttps://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages\\\\n\\\\nMT3: Multi-Track Music Transcription\\\\nhttps://arxiv.org/abs/2111.03017\\\\nhttps://github.com/magenta/mt3\\\\nhttps://huggingface.co/spaces/akhaliq/MT3\\\\nhttps://www.reddit.com/r/MachineLearning/comments/rtlx0r/r_mt3_multitask_multitrack_music_transcription/\\\\n\\\\nChina regulates addictive algorithms\\\\nhttps://technode.com/2022/01/05/china-issues-new-rules-to-regulate-algorithms-targeting-addiction-monopolies-and-overspending/\\\\nhttps://qz.com/2109618/china-reveals-new-algorithm-rules-to-weaken-platforms-control-of-users/\\\\n\\\\nA collection of Deep Learning interview questions \\\\u0026 solutions\\\\nhttps://arxiv.org/abs/2201.00650?utm_source=pocket_mylist\\\\nhttps://arxiv.org/pdf/2201.00650.pdf\\\\n\\\\nHelpful Things\\\\nhttps://docs.deepchecks.com/en/stable/index.html\\\\nhttps://github.com/deepchecks/deepchecks\\\\nhttps://docs.deepchecks.com/en/stable/examples/guides/quickstart_in_5_minutes.html\\\\nhttps://www.dagshub.com/\\\\nhttps://www.dagshub.com/docs/index.html\\\\nhttps://www.dagshub.com/blog/launching-dagshub-2-0/\\\\nhttps://bayesiancomputationbook.com/welcome.html\\\\nhttps://mlcontests.com/\\\\nhttps://github.com/Yard1/ray-skorch\\\\nhttps://github.com/skorch-dev/skorch\\\\nhttps://www.rumbledb.org/?utm_source=pocket_mylist\\\\nhttps://github.com/DarshanDeshpande/jax-models\\\\nhttps://github.com/s3prl/s3prl\\\\n\\\\nAlphaZero explained blog post\\\\nhttps://joshvarty.github.io/AlphaZero/?utm_source=pocket_mylist\\\\n\\\\nRu-DOLPH: HyperModal Text-to-Image-to-Text model\\\\nhttps://github.com/sberbank-ai/ru-dolph\\\\nhttps://colab.research.google.com/drive/1gmTDA13u709OXiAeXWGm7sPixRhEJCga?usp=sharing\\\\n\\\\nGoogle AI 2021 Review\\\\nhttps://ai.googleblog.com/2022/01/google-research-themes-from-2021-and.html\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1117",
    "uploadDate": "2022-01-25",
    "thumbnail_url": "https://i.ytimg.com/vi/yVKiMh2vEWQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=w3knicSHx5s",
    "title": "Dynamic Inference with Neural Interpreters (w/ author interview)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, neural interpreters, dynamic inference, neural programming, neural functions, recurrent networks, yoshua bengio, mila, schoelkopf, attention, modlin, modulated linear layer, weight sharing, recurrent modules, function modules, sparse neural networks, interview, first author interview, with the authors, dynamic inference with neural interpreters, deep neural interpreters",
    "scraped_at": 1684582632.580347,
    "genre": "Science",
    "views": "14507",
    "desc": "#deeplearning #neuralinterpreter #ai\\\\n\\\\nThis video includes an interview with the paper\\'s authors!\\\\nWhat if we treated deep networks like modular programs? Neural Interpreters divide computation into small modules and route data to them via a dynamic type inference system. The resulting model combines recurrent elements, weight sharing, attention, and more to tackle both abstract reasoning, as well as computer vision tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:00 - Model Overview\\\\n7:00 - Interpreter weights and function code\\\\n9:40 - Routing data to functions via neural type inference\\\\n14:55 - ModLin layers\\\\n18:25 - Experiments\\\\n21:35 - Interview Start\\\\n24:50 - General Model Structure\\\\n30:10 - Function code and signature\\\\n40:30 - Explaining Modulated Layers\\\\n49:50 - A closer look at weight sharing\\\\n58:30 - Experimental Results\\\\n\\\\nPaper: https://arxiv.org/abs/2110.06399\\\\n\\\\nGuests:\\\\nNasim Rahaman: https://twitter.com/nasim_rahaman\\\\nFrancesco Locatello: https://twitter.com/FrancescoLocat8\\\\nWaleed Gondal: https://twitter.com/Wallii_gondal\\\\n\\\\nAbstract:\\\\nModern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call \\\\\\\\emph{functions}. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization\\\\n\\\\nAuthors: Nasim Rahaman, Muhammad Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua Bengio, Francesco Locatello, Bernhard Sch\\xc3\\xb6lkopf\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "4956",
    "uploadDate": "2022-01-21",
    "thumbnail_url": "https://i.ytimg.com/vi/w3knicSHx5s/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Xp3jR-ttMfo",
    "title": "Noether Networks: Meta-Learning Useful Conserved Quantities (w/ the authors)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, noether networks, noether",
    "scraped_at": 1684582627.458673,
    "genre": "Science",
    "views": "11369",
    "desc": "#deeplearning #noether #symmetries\\\\n\\\\nThis video includes an interview with first author Ferran Alet!\\\\nEncoding inductive biases has been a long established methods to provide deep networks with the ability to learn from less data. Especially useful are encodings of symmetry properties of the data, such as the convolution\\'s translation invariance. But such symmetries are often hard to program explicitly, and can only be encoded exactly when done in a direct fashion. Noether Networks use Noether\\'s theorem connecting symmetries to conserved quantities and are able to dynamically and approximately enforce symmetry properties upon deep neural networks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n18:10 - Interview Start\\\\n21:20 - Symmetry priors vs conserved quantities\\\\n23:25 - Example: Pendulum\\\\n27:45 - Noether Network Model Overview\\\\n35:35 - Optimizing the Noether Loss\\\\n41:00 - Is the computation graph stable?\\\\n46:30 - Increasing the inference time computation\\\\n48:45 - Why dynamically modify the model?\\\\n55:30 - Experimental Results \\\\u0026 Discussion\\\\n\\\\nPaper: https://arxiv.org/abs/2112.03321\\\\nWebsite: https://dylandoblar.github.io/noether-networks/\\\\nCode: https://github.com/dylandoblar/noether-networks\\\\n\\\\nAbstract:\\\\nProgress in machine learning (ML) stems from a combination of data availability, computational resources, and an appropriate encoding of inductive biases. Useful biases often exploit symmetries in the prediction problem, such as convolutional networks relying on translation equivariance. Automatically discovering these useful symmetries holds the potential to greatly improve the performance of ML systems, but still remains a challenge. In this work, we focus on sequential prediction problems and take inspiration from Noether\\'s theorem to reduce the problem of finding inductive biases to meta-learning useful conserved quantities. We propose Noether Networks: a new type of architecture where a meta-learned conservation loss is optimized inside the prediction function. We show, theoretically and experimentally, that Noether Networks improve prediction quality, providing a general framework for discovering inductive biases in sequential problems.\\\\n\\\\nAuthors: Ferran Alet, Dylan Doblar, Allan Zhou, Joshua Tenenbaum, Kenji Kawaguchi, Chelsea Finn\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "4145",
    "uploadDate": "2022-01-19",
    "thumbnail_url": "https://i.ytimg.com/vi/Xp3jR"
  },
  {
    "link": "watch?v=a4P8v8lGFPw",
    "title": "This Team won the Minecraft RL BASALT Challenge! (Paper Explanation & Interview with the authors)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, minecraft, minerl, minerl basalt, minecraft machine learning, minecraft ai, human",
    "scraped_at": 1684582636.5730336,
    "genre": "Science",
    "views": "12784",
    "desc": "#minerl #minecraft #deeplearning\\\\n\\\\nThe MineRL BASALT challenge has no reward functions or technical descriptions of what\\'s to be achieved. Instead, the goal of each task is given as a short natural language string, and the agent is evaluated by a team of human judges who rate both how well the goal has been fulfilled, as well as how human-like the agent behaved. In this video, I interview KAIROS, the winning team of the 2021 challenge, and discuss how they used a combination of machine learning, efficient data collection, hand engineering, and a bit of knowledge about Minecraft to beat all other teams.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n4:10 - Paper Overview\\\\n11:15 - Start of Interview\\\\n17:05 - First Approach\\\\n20:30 - State Machine\\\\n26:45 - Efficient Label Collection\\\\n30:00 - Navigation Policy\\\\n38:15 - Odometry Estimation\\\\n46:00 - Pain Points \\\\u0026 Learnings\\\\n50:40 - Live Run Commentary\\\\n58:50 - What other tasks can be solved?\\\\n1:01:55 - What made the difference?\\\\n1:07:30 - Recommendations \\\\u0026 Conclusion\\\\n1:11:10 - Full Runs: Waterfall\\\\n1:12:40 - Full Runs: Build House\\\\n1:17:45 - Full Runs: Animal Pen\\\\n1:20:50 - Full Runs: Find Cave\\\\n\\\\nPaper: https://arxiv.org/abs/2112.03482\\\\nCode: https://github.com/viniciusguigo/kairos_minerl_basalt\\\\nChallenge Website: https://minerl.io/basalt/\\\\n\\\\nPaper Title: Combining Learning from Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft\\\\n\\\\nAbstract:\\\\nReal-world tasks of interest are generally poorly defined by human-readable descriptions and have no pre-defined reward signals unless it is defined by a human designer. Conversely, data-driven algorithms are often designed to solve a specific, narrowly defined, task with performance metrics that drives the agent\\'s learning. In this work, we present the solution that won first place and was awarded the most human-like agent in the 2021 NeurIPS Competition MineRL BASALT Challenge: Learning from Human Feedback in Minecraft, which challenged participants to use human data to solve four tasks defined only by a natural language description and no reward function. Our approach uses the available human demonstration data to train an imitation learning policy for navigation and additional human feedback to train an image classifier. These modules, together with an estimated odometry map, are then combined into a state-machine designed based on human knowledge of the tasks that breaks them down in a natural hierarchy and controls which macro behavior the learning agent should follow at any instant. We compare this hybrid intelligence approach to both end-to-end machine learning and pure engineered solutions, which are then judged by human evaluators. Codebase is available at this https URL.\\\\n\\\\nAuthors: Vinicius G. Goecks, Nicholas Waytowich, David Watkins, Bharat Prakash\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "5030",
    "uploadDate": "2022-01-11",
    "thumbnail_url": "https://i.ytimg.com/vi/a4P8v8lGFPw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rd3R_G6_UfY",
    "title": "Full Self-Driving is HARD! Analyzing Elon Musk re: Tesla Autopilot on Lex Fridman's Podcast",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, lex fridman, elon musk, elon, musk, tesla fsd, when will fsd ship, when will fsd be ready, tesla fsd release, tesla fsd release date, how does tesla autopilot work, does tesla use neural networks, andrej karpathy, self driving, tesla self driving, how good is tesla fsd, how safe is tesla, vector space, podcast, analysis, elon musk self",
    "scraped_at": 1684582630.5843456,
    "genre": "Science",
    "views": "18552",
    "desc": "#tesla #fsd #elon\\\\n\\\\nWatch the original podcast: https://www.youtube.com/watch?v=DxREm3s1scA\\\\n\\\\nAn analysis of Elon\\'s appearance on Lex Fridman. Very interesting conversation and a good overview of past, current, and future versions of Tesla\\'s Autopilot system.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:40 - Tesla Autopilot: How hard is it?\\\\n9:05 - Building an accurate understanding of the world\\\\n16:25 - History of Tesla\\'s neural network stack\\\\n26:00 - When is full self-driving ready?\\\\n29:55 - FSD 11: Less code, more neural networks\\\\n37:00 - Auto-labelling is essential\\\\n39:05 - Tesla Bot \\\\u0026 Discussion\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2510",
    "uploadDate": "2022-01-05",
    "thumbnail_url": "https://i.ytimg.com/vi/rd3R_G6_UfY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=U0mxx7AoNz0",
    "title": "Player of Games: All the games, one algorithm! (w/ author Martin Schmid)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, reinforcement learning, ai for go, ai go, ai chess, chess ai, stockfish, alphazero, alpha zero, muzero, player of games, pog, deepmind, deepmind games, imperfect information games, ai for poker, perfect vs imperfect information, public state, scotland yard, ai for scotland yard, reinforcement learning poker, ai no limit holdem, counterfactual regret minimization, tree search",
    "scraped_at": 1684582633.02838,
    "genre": "Science",
    "views": "17577",
    "desc": "#playerofgames #deepmind #alphazero\\\\n\\\\nSpecial Guest: First author Martin Schmid (https://twitter.com/Lifrordi)\\\\nGames have been used throughout research as testbeds for AI algorithms, such as reinforcement learning agents. However, different types of games usually require different solution approaches, such as AlphaZero for Go or Chess, and Counterfactual Regret Minimization (CFR) for Poker. Player of Games bridges this gap between perfect and imperfect information games and delivers a single algorithm that uses tree search over public information states, and is trained via self-play. The resulting algorithm can play Go, Chess, Poker, Scotland Yard, and many more games, as well as non-game environments.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n2:50 - What games can Player of Games be trained on?\\\\n4:00 - Tree search algorithms (AlphaZero)\\\\n8:00 - What is different in imperfect information games?\\\\n15:40 - Counterfactual Value- and Policy-Networks\\\\n18:50 - The Player of Games search procedure\\\\n28:30 - How to train the network?\\\\n34:40 - Experimental Results\\\\n47:20 - Discussion \\\\u0026 Outlook\\\\n\\\\nPaper: https://arxiv.org/abs/2112.03178\\\\n\\\\nAbstract:\\\\nGames have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold\\'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.\\\\n\\\\nAuthors: Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3251",
    "uploadDate": "2022-01-02",
    "thumbnail_url": "https://i.ytimg.com/vi/U0mxx7AoNz0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gwI6g1pBD84",
    "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openai, glide, diffusion, clip",
    "scraped_at": 1684582630.312372,
    "genre": "Science",
    "views": "30954",
    "desc": "#glide #openai #diffusion\\\\n\\\\nDiffusion models learn to iteratively reverse a noising process that is applied repeatedly during training. The result can be used for conditional generation as well as various other tasks such as inpainting. OpenAI\\'s GLIDE builds on recent advances in diffusion models and combines text-conditional diffusion with classifier-free guidance and upsampling to achieve unprecedented quality in text-to-image samples.\\\\n\\\\nTry it yourself: https://huggingface.co/spaces/valhalla/glide-text2im\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n6:10 - What is a Diffusion Model?\\\\n18:20 - Conditional Generation and Guided Diffusion\\\\n31:30 - Architecture Recap\\\\n34:05 - Training \\\\u0026 Result metrics\\\\n36:55 - Failure cases \\\\u0026 my own results\\\\n39:45 - Safety considerations\\\\n\\\\nPaper: https://arxiv.org/abs/2112.10741\\\\nCode \\\\u0026 Model: https://github.com/openai/glide-text2im\\\\n\\\\nMore diffusion papers:\\\\nhttps://arxiv.org/pdf/2006.11239.pdf\\\\nhttps://arxiv.org/pdf/2102.09672.pdf\\\\n\\\\nAbstract:\\\\nDiffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at this https URL.\\\\n\\\\nAuthors: Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2536",
    "uploadDate": "2021-12-28",
    "thumbnail_url": "https://i.ytimg.com/vi/gwI6g1pBD84/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GgHXGpQ60x0",
    "title": "[ML News] AI learns to search the Internet | Drawings come to life | New ML journal launches",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, webgpt, truthful, truthful qa, gpt",
    "scraped_at": 1684582636.2824786,
    "genre": "Science",
    "views": "22778",
    "desc": "#webgpt #aiart #mlnews\\\\n\\\\nThe latest and greatest from the Machine Learning world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n2:40 - WebGPT: When GPT-3 can search the Internet\\\\n15:45 - MetaAI brings children\\'s drawings to life\\\\n17:15 - OpenAI lets anyone fine-tune GPT-3\\\\n18:15 - New Journal: Transactions on Machine Learning Research\\\\n21:20 - Hugging Face buys Gradio\\\\n22:45 - Helpful Things\\\\n28:35 - NetHack Challenge winners announced\\\\n29:20 - Characters for good, created by AI\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nReferences:\\\\nWebGPT: When GPT-3 can search the Internet\\\\nhttps://openai.com/blog/improving-factual-accuracy/\\\\nhttps://cdn.openai.com/WebGPT.pdf\\\\n\\\\nMetaAI brings children\\'s drawings to life\\\\nhttps://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life\\\\nhttps://sketch.metademolab.com/canvas\\\\nhttps://tech.fb.com/ai-childrens-drawings/?utm_source=Twitter\\\\u0026utm_medium=organic_social\\\\u0026utm_campaign=TECH2021H2\\\\n\\\\nOpenAI lets anyone fine-tune GPT-3\\\\nhttps://openai.com/blog/customized-gpt3/\\\\nhttps://openai.com/api/pricing/\\\\n\\\\nNew Journal: Transactions on Machine Learning Research\\\\nhttps://medium.com/@hugo_larochelle_65309/announcing-the-transactions-on-machine-learning-research-3ea6101c936f\\\\nhttps://jmlr.org/tmlr/\\\\n\\\\nHugging Face buys Gradio\\\\nhttps://gradio.app/joining-huggingface/\\\\n\\\\nHelpful Things\\\\nhttps://github.com/kakaobrain/minDALL-E\\\\nhttps://github.com/borisdayma/dalle-mini\\\\nhttps://github.com/deepmind/arnheim\\\\nhttps://colab.research.google.com/github/deepmind/arnheim/blob/master/arnheim_3.ipynb\\\\nhttp://duebenchmark.com/leaderboard\\\\nhttps://github.com/due-benchmark\\\\nhttp://duebenchmark.com/data\\\\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/069059b7ef840f0c74a814ec9237b6ec-Abstract-round2.html\\\\nhttps://github.com/nyu-mll/quality\\\\nhttps://github.com/nyu-mll/quality/blob/main/quality_preprint.pdf\\\\nhttps://huggingface.co/blog/perceiver\\\\nhttps://arxiv.org/pdf/2112.05682.pdf\\\\nhttps://towardsdatascience.com/deriving-convolution-from-first-principles-4ff124888028\\\\nhttps://ai.googleblog.com/2021/12/training-machine-learning-models-more.html\\\\nhttps://github.com/huawei-noah/HEBO\\\\nhttps://www.sberbank.com/news-and-media/press-releases/article?newsID=a26a208d-6c72-4f8a-a3b7-aefe1112cbae\\\\u0026blockID=7\\\\u0026regionID=77\\\\u0026lang=en\\\\u0026type=NEWS\\\\nhttps://sbercloud.ru/ru/datahub/rugpt3family/rudall-e-12b?_ga=2.169749668.48600719.1639868013-1523472348.1639868013\\\\n\\\\nNetHack Challenge winners announced\\\\nhttps://nethackchallenge.com/report.html\\\\n\\\\nCharacters for good, created by AI\\\\nhttps://news.mit.edu/2021/ai-generated-characters-for-good-1216\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1875",
    "uploadDate": "2021-12-24",
    "thumbnail_url": "https://i.ytimg.com/vi/GgHXGpQ60x0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZOkvFf8JbkA",
    "title": "[ML News] DeepMind builds Gopher | Google builds GLaM | Suicide capsule uses AI to check access",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deepmind, gopher, retro, toxicity, ethical, machine learning ethics, ai ethics, retrofit, retrofit model, retro transformer, deepmind gopher, google glam, glam model, glam transformer, sparse transformer, mixture of experts, suicide capsule, ai suicide, ml news, mlnews, machine learning news, kilcher news, huggingface, hugging face, code parrot, synthesia, synthesia avatar",
    "scraped_at": 1684582635.1395142,
    "genre": "Science",
    "views": "23762",
    "desc": "#mlnews #gopher #glam\\\\n\\\\nYour updates on everything going on in the Machine Learning world.\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/yannic\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n3:05 - DeepMind releases 3 papers on large language models\\\\n11:45 - Hugging Face Blog: Training CodeParrot from scratch\\\\n14:25 - Paper: Pre-Training vision systems with noise\\\\n15:45 - DeepMind advances Quantum Mechanics\\\\n16:45 - GoogleAI trains GLaM: 1 Trillion Parameters Mixture of Experts Model\\\\n18:45 - Colin Raffel calls for building ML models like we build Open-Source software\\\\n22:05 - A rebuke of the hype around DeepMind\\'s math paper\\\\n24:45 - Helpful Things\\\\n32:25 - Suicide Capsule plans AI to assess your mental state before use\\\\n35:15 - Synthesia raises 50M to develop AI avatars\\\\n\\\\n\\\\nWeights \\\\u0026 Biases Embedding Projector\\\\nhttps://twitter.com/_ScottCondron/status/1469411468139536385?utm_source=pocket_mylist\\\\nhttps://docs.wandb.ai/ref/app/features/panels/weave/embedding-projector\\\\nhttps://wandb.ai/timssweeney/toy_datasets/reports/Feature-Report-W-B-Embeddings-Projector--VmlldzoxMjg2MjY4?accessToken=bo36zrgl0gref1th5nj59nrft9rc4r71s53zr2qvqlz68jwn8d8yyjdz73cqfyhq\\\\n\\\\nDeepMind releases 3 papers on large language models\\\\nhttps://deepmind.com/blog/article/language-modelling-at-scale\\\\nhttps://arxiv.org/pdf/2112.04426.pdf\\\\nhttps://kstatic.googleusercontent.com/files/b068c6c0e64d6f933068f7de30ea722359ef87c6c14d3065856b86d44fbdf2dea3ff373ed9eb751514f242d20df9d6a468622fad093f962563545e7d0cdb9dba\\\\nhttps://arxiv.org/pdf/2112.04359.pdf\\\\nhttps://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens\\\\n\\\\nHugging Face Blog: Training CodeParrot from scratch\\\\nhttps://huggingface.co/blog/codeparrot?utm_source=pocket_mylist\\\\n\\\\nPaper: Pre-Training vision systems with noise\\\\nhttps://mbaradad.github.io/learning_with_noise/\\\\n\\\\nDeepMind advances Quantum Mechanics\\\\nhttps://deepmind.com/blog/article/Simulating-matter-on-the-quantum-scale-with-AI\\\\nhttps://storage.googleapis.com/deepmind-media/papers/Data_Driven_Density_Functional_Design/data_driven_density_functional_design_unformatted.pdf\\\\nhttps://github.com/deepmind/deepmind-research/tree/master/density_functional_approximation_dm21\\\\n\\\\nGoogleAI trains GLaM: 1 Trillion Parameters Mixture of Experts Model\\\\nhttps://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html\\\\n\\\\nColin Raffel calls for building ML models like we build Open-Source software\\\\nhttps://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html\\\\n\\\\nA rebuke of the hype around DeepMind\\'s math paper\\\\nhttps://arxiv.org/abs/2112.04324?s=09\\\\n\\\\nHelpful Things\\\\nhttps://twitter.com/huggingface/status/1468996110207401992\\\\nhttps://docs.cohere.ai/prompt-engineering-wiki/?utm_source=pocket_mylist\\\\nhttps://github.blog/2021-12-08-improving-github-code-search/\\\\nhttps://huggingface.co/blog/data-measurements-tool\\\\nhttps://huggingface.co/spaces/huggingface/data-measurements-tool\\\\nhttps://blogs.microsoft.com/ai-for-business/building-ai-responsibly-from-research-to-practice/\\\\nhttps://techcommunity.microsoft.com/t5/azure-ai-blog/responsible-ai-dashboard-a-one-stop-shop-for-operationalizing/ba-p/3030944\\\\nhttps://github.com/minitorch/minitorch?utm_source=pocket_mylist\\\\nhttps://minitorch.github.io/\\\\nhttps://pandastutor.com/\\\\nhttps://pandastutor.com/vis.html\\\\nhttps://github.com/IAmPara0x/yuno\\\\nhttps://colab.research.google.com/drive/1WAewYgHDmDEWhPBBOvGgyLTiOaasVyOz?usp=sharing#scrollTo=hZamByTeBv3G\\\\nhttps://www.reddit.com/r/MachineLearning/comments/rbue4h/n_us_gov_launches_ml_competition_to_predict_snow/\\\\nhttps://www.drivendata.org/competitions/86/competition-reclamation-snow-water-dev/\\\\nhttps://www.reddit.com/r/MachineLearning/comments/rdb1uw/p_utttai_alphazerolike_solution_for_playing/\\\\nhttps://www.uttt.ai/\\\\nhttps://arxiv.org/abs/2112.02721?utm_source=pocket_mylist\\\\nhttps://arxiv.org/pdf/2112.02721.pdf\\\\nhttps://github.com/GEM-benchmark/NL-Augmenter\\\\nhttps://www.reddit.com/r/MachineLearning/comments/rdfdcv/p_collection_of_33_psychology_related_datasets/?utm_source=pocket_mylist\\\\n\\\\nSuicide Capsule plans AI to assess your mental state before use\\\\nhttps://www.swissinfo.ch/eng/sci-tech/sarco-suicide-capsule--passes-legal-review--in-switzerland/46966510\\\\n\\\\nSynthesia raises 50M to develop AI avatars\\\\nhttps://techcrunch.com/2021/12/08/synthesia-raises-50m-to-leverage-synthetic-avatars-for-corporate-training-and-more/\\\\nhttps://www.synthesia.io/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\n\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2242",
    "uploadDate": "2021-12-21",
    "thumbnail_url": "https://i.ytimg.com/vi/ZOkvFf8JbkA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Lg97gWXsiQ4",
    "title": "Resolution-robust Large Mask Inpainting with Fourier Convolutions (w/ Author Interview)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, lama, inpainting, gan, adversarial, loss function, fourier transform, fft, fast fourier transform, fourier convolution, fast fourier convolution, fourier convolution layer, global information, generative model, periodic strucutre, best inpainting, ai inpainting, first author interview, lama inpainting, mask filling, large mask inpainting, remove from picture, ai image editing",
    "scraped_at": 1684582635.22854,
    "genre": "Science",
    "views": "16183",
    "desc": "#lama #inpainting #deeplearning\\\\n\\\\nAt the end of the video is an interview with the paper authors!\\\\nLaMa is a system that is amazing at removing foreground objects from images, especially when those objects cover a large part of the image itself. LaMa is specifically trained to reconstruct large masked areas and includes global information throughout its forward propagation by using Fourier Convolutions in its layers. This makes it incredibly effective at reconstructing periodic structures with long-range consistency, compared to regular convolutions.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:45 - Sponsor: ClearML\\\\n3:30 - Inpainting Examples\\\\n5:05 - Live Demo\\\\n6:40 - Locality as a weakness of convolutions\\\\n10:30 - Using Fourier Transforms for global information\\\\n12:55 - Model architecture overview\\\\n14:35 - Fourier convolution layer\\\\n21:15 - Loss function\\\\n24:25 - Mask generation algorithm\\\\n25:40 - Experimental results\\\\n28:25 - Interview with the authors\\\\n\\\\nPaper: https://arxiv.org/abs/2109.07161\\\\nCode: https://github.com/saic-mdal/lama\\\\nOnline Demo: https://cleanup.pictures/\\\\n\\\\nSponsor: ClearML\\\\nhttps://clear.ml\\\\n\\\\nAbstract:\\\\nModern image inpainting systems, despite the significant progress, often struggle with large missing areas, complex geometric structures, and high-resolution images. We find that one of the main reasons for that is the lack of an effective receptive field in both the inpainting network and the loss function. To alleviate this issue, we propose a new method called large mask inpainting (LaMa). LaMa is based on i) a new inpainting network architecture that uses fast Fourier convolutions (FFCs), which have the image-wide receptive field; ii) a high receptive field perceptual loss; iii) large training masks, which unlocks the potential of the first two components. Our inpainting network improves the state-of-the-art across a range of datasets and achieves excellent performance even in challenging scenarios, e.g. completion of periodic structures. Our model generalizes surprisingly well to resolutions that are higher than those seen at train time, and achieves this at lower parameter\\\\u0026time costs than the competitive baselines. The code is available at \\\\\\\\url{this https URL}.\\\\n\\\\nAuthors: Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3242",
    "uploadDate": "2021-12-18",
    "thumbnail_url": "https://i.ytimg.com/vi/Lg97gWXsiQ4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=f2OgP49J7Pg",
    "title": "[ML News] DeepMind tackles Math | Microsoft does more with less | Timnit Gebru launches DAIR",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deepmind, ai math, machine learning math, deepmind math, topology, deepmind topology, knot theory, ai fundamental math, deepmind representation theory, deepmind mathematics, gebru, timnit gebru, gebru dair, timnit gebru research institute, microsoft turing, neurips, neurips ethics review, machine learning ethics, helpful things, sagemaker canvas, rtx 3090, nvidia",
    "scraped_at": 1684582636.0044477,
    "genre": "Science",
    "views": "35115",
    "desc": "#mlnews #deepmind #ai\\\\n\\\\nThe most trusted model in News!\\\\n\\\\nGet started with Weights \\\\u0026 Biases here: https://wandb.me/yannic\\\\n(it\\'s free forever for personal use)\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: Weights \\\\u0026 Biases\\\\n3:10 - DeepMind tackles fundamental math\\\\n6:45 - Microsoft focuses on scaling effectively and efficiently\\\\n10:15 - NeurIPS Anthology Visualization\\\\n13:30 - Timnit Gebru launches research institute independent from big tech\\\\n16:50 - SageMaker Canvas for no-code ML\\\\n17:50 - Help, Help!\\\\n21:40 - Cornelius Emde wins the 3090\\\\n21:55 - A retrospective on the NeurIPS 2021 ethics review process\\\\n\\\\nReferences:\\\\nDeepMind tackles fundamental math\\\\nhttps://deepmind.com/blog/article/exploring-the-beauty-of-pure-mathematics-in-novel-ways?utm_source=pocket_mylist\\\\nhttps://www.nature.com/articles/s41586-021-04086-x?utm_source=pocket_mylist\\\\n\\\\nMicrosoft focuses on scaling effectively and efficiently\\\\nhttps://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/?OCID=msr_blog_TNLRV5_tw\\\\n\\\\nNeurIPS Anthology Visualization\\\\nhttps://neuripsav.vizhub.ai/blog/\\\\nhttps://neuripsav.vizhub.ai/\\\\n\\\\nTimnit Gebru launches research institute independent from big tech\\\\nhttps://www.washingtonpost.com/technology/2021/12/02/timnit-gebru-dair/\\\\nhttps://www.dair-institute.org/about\\\\nhttps://www.theguardian.com/commentisfree/2021/dec/06/google-silicon-valley-ai-timnit-gebru\\\\n\\\\nSageMaker Canvas for no-code ML\\\\nhttps://aws.amazon.com/blogs/aws/announcing-amazon-sagemaker-canvas-a-visual-no-code-machine-learning-capability-for-business-analysts/\\\\n\\\\nHelp, Help!\\\\nhttps://macberth.netlify.app/\\\\nhttps://huggingface.co/emanjavacas/MacBERTh/tree/main\\\\nhttps://developer.nvidia.com/blog/nvidia-announces-tensorrt-8-2-and-integrations-with-pytorch-and-tensorflow/?ncid=so-twit-314589#cid=dl13_so-twit_en-us\\\\nhttps://opacus.ai/\\\\nhttps://twitter.com/naotokui_en/status/1466320722825920515\\\\nhttps://colab.research.google.com/drive/1H_g60Q_XELJ2VJu4GF7KY8111ce4VLwd?usp=sharing#scrollTo=JyNp3rwoWOQd\\\\nhttps://twitter.com/ThomasSimonini/status/1466437571303649301?utm_source=pocket_mylist\\\\nhttps://github.com/karpathy/arxiv-sanity-lite\\\\nhttps://arxiv-sanity-lite.com/\\\\nhttps://www.youtube.com/watch?v=01ENzpkjOCE\\\\nhttps://github.com/Felix-Petersen/algovision\\\\nhttps://github.com/rentruewang/koila?utm_source=pocket_mylist\\\\nhttps://github.com/YeWR/EfficientZero\\\\n\\\\nCornelius Emde wins the 3090\\\\nhttps://twitter.com/CorEmde/status/1466122212000374793\\\\n\\\\nA retrospective on the NeurIPS 2021 ethics review process\\\\nhttps://blog.neurips.cc/2021/12/03/a-retrospective-on-the-neurips-2021-ethics-review-process/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1540",
    "uploadDate": "2021-12-10",
    "thumbnail_url": "https://i.ytimg.com/vi/f2OgP49J7Pg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=InhMx1h0N40",
    "title": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion (ML Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, clearml, nuwa, n",
    "scraped_at": 1684582634.666538,
    "genre": "Science",
    "views": "15086",
    "desc": "#nuwa #microsoft #generative\\\\n\\\\nN\\xc3\\x9cWA is a unifying architecture that can ingest text, images, and videos and brings all of them into a quantized latent representation to support a multitude of visual generation tasks, such as text-to-image, text-guided video manipulation, or sketch-to-video. This paper details how the encoders for the different modalities are constructed, and how the latent representation is transformed using their novel 3D nearby self-attention layers. Experiments are shown on 8 different visual generation tasks that the model supports.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n1:20 - Sponsor: ClearML\\\\n3:35 - Tasks \\\\u0026 Naming\\\\n5:10 - The problem with recurrent image generation\\\\n7:35 - Creating a shared latent space w/ Vector Quantization\\\\n23:20 - Transforming the latent representation\\\\n26:25 - Recap: Self- and Cross-Attention\\\\n28:50 - 3D Nearby Self-Attention\\\\n41:20 - Pre-Training Objective\\\\n46:05 - Experimental Results\\\\n50:40 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2111.12417\\\\nGithub: https://github.com/microsoft/NUWA\\\\n\\\\nSponsor: ClearML\\\\nhttps://clear.ml\\\\n\\\\nAbstract:\\\\nThis paper presents a unified multimodal pre-trained model called N\\xc3\\x9cWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate N\\xc3\\x9cWA on 8 downstream tasks. Compared to several strong baselines, N\\xc3\\x9cWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.\\\\n\\\\nAuthors: Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3164",
    "uploadDate": "2021-12-08",
    "thumbnail_url": "https://i.ytimg.com/vi/InhMx1h0N40/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8f5xIMStqF4",
    "title": "[ML News] OpenAI removes GPT-3 waitlist | GauGAN2 is amazing | NYC regulates AI hiring tools",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, gaugan, gaugan2, nvidia, controllable gan, openai, gpt",
    "scraped_at": 1684582634.4865134,
    "genre": "Science",
    "views": "37327",
    "desc": "#mlnews #gaugan #gpt-3\\\\n\\\\nYour weekly dose of ML News!\\\\nMore GauGAN images here: https://drive.google.com/drive/folders/1tG1rpxP_mnspB1MWi9VZGScw5R-hxUdm?usp=sharing\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n2:20 - OpenAI\\'s removes GPT-3 Waitlist\\\\n4:55 - NVIDIA releases GauGAN2 Webapp\\\\n9:45 - Everyday Robots tackles real-life tasks\\\\n12:15 - MetNet-2: 12-hour Rain Forecasting\\\\n14:45 - TinyML Dog Bark Stopper\\\\n15:55 - AI learns to drive Mario Kart 64 on real hardware\\\\n17:40 - NYC regulates bias in AI hiring tools\\\\n21:05 - Beverage companies big into AI\\\\n21:50 - How does AlphaZero play Chess?\\\\n23:35 - Helpful Things\\\\n28:00 - ArXiv founder awarded Einstein Foundation Award\\\\n\\\\nReferences:\\\\n\\\\nOpenAI\\'s removes GPT-3 Waitlist\\\\nhttps://openai.com/blog/api-no-waitlist/\\\\nhttps://beta.openai.com/playground?model=davinci\\\\n\\\\nNVIDIA releases GauGAN2 Webapp\\\\nhttps://www.reddit.com/r/MachineLearning/comments/r0mok4/p_nvidia_releases_web_app_for_gaugan2_which/?utm_source=pocket_mylist\\\\nhttp://gaugan.org/gaugan2/\\\\nhttps://blogs.nvidia.com/blog/2021/11/22/gaugan2-ai-art-demo/?ncid=so-twit-261232-vt16#cid=nr01_so-twit_en-us\\\\nhttps://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/\\\\nhttps://arxiv.org/abs/1903.07291\\\\n\\\\nEveryday Robots tackles real-life tasks\\\\nhttps://everydayrobots.com/\\\\nhttps://www.wired.com/story/plaintext-alphabet-x-robots/\\\\nhttps://archive.ph/YC4XG#selection-925.354-925.397\\\\n\\\\nMetNet-2: 12-hour Rain Forecasting\\\\nhttps://ai.googleblog.com/2021/11/metnet-2-deep-learning-for-12-hour.html\\\\n\\\\nTinyML Dog Bark Stopper\\\\nhttps://www.hackster.io/NathanielF/tinyml-dog-bark-stopper-77e436\\\\n\\\\nAI learns to drive Mario Kart 64 on real hardwware\\\\nhttps://www.youtube.com/watch?v=z9E38sN5nRQ\\\\n\\\\nNYC regulates bias in AI hiring tools\\\\nhttps://www.nbcnewyork.com/news/local/nyc-aims-to-be-first-to-rein-in-artificial-intelligence-hiring-tools/3411736/\\\\n\\\\nBeverage companies big into AI\\\\nhttps://www.just-drinks.com/features/which-beverages-companies-are-leading-the-way-in-artificial-intelligence-data/\\\\n\\\\nHow does AlphaZero play Chess?\\\\nhttps://arxiv.org/pdf/2111.09259.pdf\\\\nhttps://storage.googleapis.com/uncertainty-over-space/alphachess/index.html?board=08\\\\n\\\\nHelpful Things\\\\nhttps://huggingface.co/sberbank-ai/rudalle-Emojich?utm_source=pocket_mylist\\\\nhttps://github.com/MathisFederico/OpenCodeBlocks?utm_source=pocket_mylist\\\\nhttps://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html?linkId=8008555\\\\nhttps://github.com/tensorflow/gnn\\\\nhttps://github.com/jurgisp/pydreamer?utm_source=pocket_mylist\\\\nhttps://danijar.com/project/dreamerv2/\\\\nhttps://github.com/danijar/dreamerv2\\\\nhttps://deepgenx.com/\\\\nhttps://github.com/DeepGenX/CodeGenX\\\\nhttps://devpost.com/software/heyoh-camera?utm_source=pocket_mylist\\\\nhttps://heyoh-app.github.io/heyoh-project-page/\\\\nhttps://github.com/heyoh-app/heyoh-project-page\\\\n\\\\nArXiv founder awarded Einstein Foundation Award\\\\nhttps://idw-online.de/en/news781515?utm_source=pocket_mylist\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1749",
    "uploadDate": "2021-12-02",
    "thumbnail_url": "https://i.ytimg.com/vi/8f5xIMStqF4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hgSGHusDx7M",
    "title": "Sparse is Enough in Scaling Transformers (aka Terraformer) | ML Research Paper Explained",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, terraformer, scaling transformers, nli, nlp, natural language processing, transformers memory, deep learning memory, fast transformer, fast transformers, attention, attention mechanism, attention is all you need, bert, gpt",
    "scraped_at": 1684582630.4013479,
    "genre": "Science",
    "views": "20584",
    "desc": "#scalingtransformers #terraformer #sparsity\\\\n\\\\nTransformers keep pushing the state of the art in language and other domains, mainly due to their ability to scale to ever more parameters. However, this scaling has made it prohibitively expensive to run a lot of inference requests against a Transformer, both in terms of compute and memory requirements. Scaling Transformers are a new kind of architecture that leverage sparsity in the Transformer blocks to massively speed up inference, and by including additional ideas from other architectures, they create the Terraformer, which is both fast, accurate, and consumes very little memory.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:10 - Recap: Transformer stack\\\\n6:55 - Sparse Feedforward layer\\\\n19:20 - Sparse QKV Layer\\\\n43:55 - Terraformer architecture\\\\n55:05 - Experimental Results \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2111.12763\\\\nCode: https://github.com/google/trax/blob/master/trax/examples/Terraformer_from_scratch.ipynb\\\\n\\\\nAbstract:\\\\nLarge Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.\\\\n\\\\nAuthors: Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, \\xc5\\x81ukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, Jonni Kanerva\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3426",
    "uploadDate": "2021-12-01",
    "thumbnail_url": "https://i.ytimg.com/vi/hgSGHusDx7M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=FbRcbM4T-50",
    "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582627.3666675,
    "genre": "Science",
    "views": "14810",
    "desc": "#ext5 #transferlearning #exmix\\\\n\\\\nThe T5 model has been a staple for NLP research for the last years. Both its size and its approach to formulate all NLP tasks as prompt-based language modeling make it a convenient choice to tackle new challenges and provides a strong baseline for most current datasets. ExT5 pushes T5 to its limits by pre-training not only on self-supervised mask filling, but also at the same time on 107 different supervised NLP tasks, which is their new ExMix dataset. The resulting model compares very favorably to T5 when fine-tuned to downstream tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:15 - Recap: The T5 model\\\\n3:55 - The ExT5 model and task formulations\\\\n8:10 - ExMix dataset\\\\n9:35 - Do different tasks help each other?\\\\n16:50 - Which tasks should we include?\\\\n20:30 - Pre-Training vs Pre-Finetuning\\\\n23:00 - A few hypotheses about what\\'s going on\\\\n27:20 - How much self-supervised data to use?\\\\n34:15 - More experimental results\\\\n38:40 - Conclusion \\\\u0026 Summary\\\\n\\\\nPaper: https://arxiv.org/abs/2111.10952\\\\n\\\\nAbstract:\\\\nDespite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.\\\\n\\\\nAuthors: Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, Donald Metzler\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2443",
    "uploadDate": "2021-11-29",
    "thumbnail_url": "https://i.ytimg.com/vi/FbRcbM4T"
  },
  {
    "link": "watch?v=W2UT8NjUqrk",
    "title": "Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, imle, implicit mle, maximum likelihood, backpropagation through algorithms, deep learning discrete, discrete deep learning, discrete backpropagation, gradient discrete, gradient of an algorithm",
    "scraped_at": 1684582637.3820622,
    "genre": "Science",
    "views": "19537",
    "desc": "#imle #backpropagation #discrete\\\\n\\\\nBackpropagation is the workhorse of deep learning, but unfortunately, it only works for continuous functions that are amenable to the chain rule of differentiation. Since discrete algorithms have no continuous derivative, deep networks with such algorithms as part of them cannot be effectively trained using backpropagation. This paper presents a method to incorporate a large class of algorithms, formulated as discrete exponential family distributions, into deep networks and derives gradient estimates that can easily be used in end-to-end backpropagation. This enables things like combinatorial optimizers to be part of a network\\'s forward propagation natively.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:25 - Sponsor: Weights \\\\u0026 Biases\\\\n6:15 - Problem Setup \\\\u0026 Contributions\\\\n8:50 - Recap: Straight-Through Estimator\\\\n13:25 - Encoding the discrete problem as an inner product\\\\n19:45 - From algorithm to distribution\\\\n23:15 - Substituting the gradient\\\\n26:50 - Defining a target distribution\\\\n38:30 - Approximating marginals via perturb-and-MAP\\\\n45:10 - Entire algorithm recap\\\\n56:45 - Github Page \\\\u0026 Example\\\\n\\\\nPaper: https://arxiv.org/abs/2106.01798\\\\nCode (TF): https://github.com/nec-research/tf-imle\\\\nCode (Torch): https://github.com/uclnlp/torch-imle\\\\n\\\\nOur Discord: https://discord.gg/4H8xxDF\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.com\\\\n\\\\nAbstract:\\\\nCombining discrete probability distributions and combinatorial optimization problems with neural network components has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable as it only requires the ability to compute the most probable states and does not rely on smooth relaxations. The framework encompasses several approaches such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.\\\\n\\\\nAuthors: Mathias Niepert, Pasquale Minervini, Luca Franceschi\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3558",
    "uploadDate": "2021-11-27",
    "thumbnail_url": "https://i.ytimg.com/vi/W2UT8NjUqrk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DEh1GR0t29k",
    "title": "Peer Review is still BROKEN! The NeurIPS 2021 Review Experiment (results are in)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, neurips, neurips experiment, peer review experiment, neurips peer review, peer review agreement, neurips conference, machine learning conference, ai conference, machine learning peer review, peer review process, peer review broken, peer review accuracy, reviewer number 2, neurips 2014",
    "scraped_at": 1684582637.4640307,
    "genre": "Science",
    "views": "24607",
    "desc": "#neurips #peerreview #machinelearning\\\\n\\\\nA look at the results of the 2021 NeurIPS peer review experiment.\\\\n\\\\nhttps://arxiv.org/abs/2109.09774\\\\nhttps://www.reddit.com/r/MachineLearning/comments/qzjuvk/discussion_neurips_2021_finally_accepted/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "669",
    "uploadDate": "2021-11-25",
    "thumbnail_url": "https://i.ytimg.com/vi/DEh1GR0t29k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3HUK2UWzlFA",
    "title": "Parameter Prediction for Unseen Deep Architectures (w/ First Author Boris Knyazev)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, neural architecture search, first author, boris knyazev, nas, metalearning, meta",
    "scraped_at": 1684582638.517031,
    "genre": "Science",
    "views": "15143",
    "desc": "#deeplearning #neuralarchitecturesearch #metalearning\\\\n\\\\nDeep Neural Networks are usually trained from a given parameter initialization using SGD until convergence at a local optimum. This paper goes a different route: Given a novel network architecture for a known dataset, can we predict the final network parameters without ever training them? The authors build a Graph-Hypernetwork and train on a novel dataset of various DNN-architectures to predict high-performing weights. The results show that not only can the GHN predict weights with non-trivial performance, but it can also generalize beyond the distribution of training architectures to predict weights for networks that are much larger, deeper, or wider than ever seen in training.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n6:20 - DeepNets-1M Dataset\\\\n13:25 - How to train the Hypernetwork\\\\n17:30 - Recap on Graph Neural Networks\\\\n23:40 - Message Passing mirrors forward and backward propagation\\\\n25:20 - How to deal with different output shapes\\\\n28:45 - Differentiable Normalization\\\\n30:20 - Virtual Residual Edges\\\\n34:40 - Meta-Batching\\\\n37:00 - Experimental Results\\\\n42:00 - Fine-Tuning experiments\\\\n45:25 - Public reception of the paper\\\\n\\\\nERRATA:\\\\n- Boris\\' name is obviously Boris, not Bori\\\\n- At 36:05, Boris mentions that they train the first variant, yet on closer examination, we decided it\\'s more like the second\\\\n\\\\nPaper: https://arxiv.org/abs/2110.13100\\\\nCode: https://github.com/facebookresearch/ppuda\\\\n\\\\nAbstract:\\\\nDeep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures - DeepNets-1M - and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.\\\\n\\\\nAuthors: Boris Knyazev, Michal Drozdzal, Graham W. Taylor, Adriana Romero-Soriano\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2887",
    "uploadDate": "2021-11-24",
    "thumbnail_url": "https://i.ytimg.com/vi/3HUK2UWzlFA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vVRC-0VKPrg",
    "title": "Learning Rate Grafting: Transferability of Optimizer Tuning (Machine Learning Research Paper Review)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, grafting, learning rate, deep learning learning rate, neural network learning rate, adaptive learning rate, adaptive optimizer, learning rate grafting, optimizer grafting, adam, sgd, adagrad, lars, lamb, openreview, reviewer, automatic learning rate, learning rate decay, learning rate warmup",
    "scraped_at": 1684582638.6050332,
    "genre": "Science",
    "views": "14813",
    "desc": "#grafting #adam #sgd\\\\n\\\\nThe last years in deep learning research have given rise to a plethora of different optimization algorithms, such as SGD, AdaGrad, Adam, LARS, LAMB, etc. which all claim to have their special peculiarities and advantages. In general, all algorithms modify two major things: The (implicit) learning rate schedule, and a correction to the gradient direction. This paper introduces grafting, which allows to transfer the induced learning rate schedule of one optimizer to another one. In that, the paper shows that much of the benefits of adaptive methods (e.g. Adam) are actually due to this schedule, and not necessarily to the gradient direction correction. Grafting allows for more fundamental research into differences and commonalities between optimizers, and a derived version of it makes it possible to computes static learning rate corrections for SGD, which potentially allows for large savings of GPU memory.\\\\n\\\\nOUTLINE\\\\n0:00 - Rant about Reviewer #2\\\\n6:25 - Intro \\\\u0026 Overview\\\\n12:25 - Adaptive Optimization Methods\\\\n20:15 - Grafting Algorithm\\\\n26:45 - Experimental Results\\\\n31:35 - Static Transfer of Learning Rate Ratios\\\\n35:25 - Conclusion \\\\u0026 Discussion\\\\n\\\\nPaper (OpenReview): https://openreview.net/forum?id=FpKgG31Z_i9\\\\nOld Paper (Arxiv): https://arxiv.org/abs/2002.11803\\\\n\\\\nOur Discord: https://discord.gg/4H8xxDF\\\\n\\\\nAbstract:\\\\nIn the empirical science of training large neural networks, the learning rate schedule is a notoriously challenging-to-tune hyperparameter, which can depend on all other properties (architecture, optimizer, batch size, dataset, regularization, ...) of the problem. In this work, we probe the entanglements between the optimizer and the learning rate schedule. We propose the technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of optimizer hyperparameter search. Using grafting, we discover a non-adaptive learning rate correction to SGD which allows it to train a BERT model to state-of-the-art performance. Besides providing a resource-saving tool for practitioners, the invariances discovered via grafting shed light on the successes and failure modes of optimizers in deep learning.\\\\n\\\\nAuthors: Anonymous (Under Review)\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2355",
    "uploadDate": "2021-11-20",
    "thumbnail_url": "https://i.ytimg.com/vi/vVRC"
  },
  {
    "link": "watch?v=FC-R4MlIqrc",
    "title": "[ML News] Cedille French Language Model | YOU Search Engine | AI Finds Profitable MEME TOKENS",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, machine learning news, ai news, ml news, cedille, french language model, gpt",
    "scraped_at": 1684582645.4397187,
    "genre": "Science",
    "views": "11389",
    "desc": "#mlnews #cedille #wmt\\\\n\\\\nOnly the greatest of news from the world of Machine Learning.\\\\n\\\\nOUTLINE:\\\\n0:00 - Sponsor: Weights \\\\u0026 Biases\\\\n1:50 - Cedille - French Language Model\\\\n3:55 - Facebook AI Multilingual model wins WMT\\\\n5:50 - YOU private search engine\\\\n10:35 - DeepMind\\'s Open-Source Arnheim\\\\n12:10 - Company sued for using AI to make website more accessible\\\\n18:05 - Alibaba DAMO Academy creates 10 Trillion M6 model\\\\n21:15 - AMD MI200 Family\\\\n22:30 - State of AI report 2021\\\\n24:15 - Andrew Ng\\'s Landing AI raises 57M\\\\n25:40 - Cerebras raises 250M\\\\n26:45 - Microsoft\\'s Varuna: Scalable Training of Huge Models\\\\n28:15 - Laura Ruis reproduces Extrapolation Paper\\\\n29:05 - Ian Charnas\\' Real-Life Punchout\\\\n30:00 - Helpful Things\\\\n33:10 - AI finds profitable Meme-Tokens\\\\n34:55 - This Sneaker Does Not Exist\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.com\\\\n\\\\nReferences:\\\\nCedille - French Language Model\\\\nhttps://en.cedille.ai/\\\\nhttps://github.com/coteries/cedille-ai\\\\nhttps://app.cedille.ai/\\\\nhttps://en.wikipedia.org/wiki/Cedilla\\\\n\\\\nFacebook AI Multilingual model wins WMT\\\\nhttps://ai.facebook.com/blog/the-first-ever-multilingual-model-to-win-wmt-beating-out-bilingual-models/\\\\n\\\\nYOU private search engine\\\\nhttps://you.com/\\\\nhttps://youdotcom.notion.site/FAQ-8c871d6c99d84e02955fda772a1da8d4\\\\n\\\\nDeepMind\\'s Open-Source Arnheim\\\\nhttps://deepmind.com/research/open-source/open-source-arnheim-a-learnable-visual-grammar-for-generating-paintings\\\\nhttps://twitter.com/OriolVinyalsML/status/1459231774068854785\\\\nhttps://github.com/deepmind/arnheim\\\\nhttps://colab.research.google.com/github/deepmind/arnheim/blob/master/arnheim_2.ipynb\\\\n\\\\nCompany sued for using AI to make website more accessible\\\\nhttps://www.wired.com/story/company-tapped-ai-website-landed-court/\\\\nhttps://archive.ph/kdvOM\\\\n\\\\nAlibaba DAMO Academy creates 10 Trillion M6 model\\\\nhttps://pandaily.com/alibaba-damo-academy-creates-worlds-largest-ai-pre-training-model-with-parameters-far-exceeding-google-and-microsoft/\\\\nhttps://www.infoq.cn/article/xIX9lekuuLcXewc5iphF\\\\n\\\\nAMD MI200 Family\\\\nhttps://www.anandtech.com/show/17054/amd-announces-instinct-mi200-accelerator-family-cdna2-exacale-servers?utm_source=pocket_mylist\\\\n\\\\nState of AI report 2021\\\\nhttps://www.stateof.ai/?utm_source=pocket_mylist\\\\n\\\\nAndrew Ng\\'s Landing AI raises 57M\\\\nhttps://techcrunch.com/2021/11/08/landing-ai-machine-learning-operations-tools/\\\\nhttps://www.forbes.com/sites/bernardmarr/2021/11/09/landing-ai-unlocking-the-power-of-data-centric-artificial-intelligence/\\\\nhttps://landing.ai/platform/\\\\n\\\\nCerebras raises 250M\\\\nhttps://cerebras.net/news/cerebras-systems-raises-250m-in-funding-for-over-4b-valuation-to-advance-the-future-of-artificial-intelligence-compute/\\\\nhttps://cerebras.net/news/cerebras-systems-announces-worlds-first-brain-scale-artificial-intelligence-solution/\\\\n\\\\nMicrosoft\\'s Varuna: Scalable Training of Huge Models\\\\nhttps://syncedreview.com/2021/11/10/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-142/\\\\n\\\\nLaura Ruis reproduces Extrapolation Paper\\\\nhttps://lauraruis.github.io/2021/11/06/extra.html?utm_source=pocket_mylist\\\\nhttps://github.com/LauraRuis\\\\n\\\\nIan Charnas\\' Real-Life Punchout\\\\nhttps://www.reddit.com/r/MachineLearning/comments/qpenkt/project_google_movenet_realtime_pose_estimation/\\\\nhttps://www.youtube.com/watch?v=07JibJJVNp8\\\\n\\\\nHelpful Things\\\\nhttps://www.marktechpost.com/2021/11/05/google-ai-introduces-goemotions-an-nlp-dataset-for-fine-grained-emotion-classification/\\\\nhttps://pair-code.github.io/lit/demos/\\\\nhttps://github.com/pair-code/lit\\\\nhttps://www.reddit.com/r/MachineLearning/comments/qsrdyk/p_texttoimage_rudalle_kandinsky_xxl_12_billion/\\\\nhttps://twitter.com/yeemachine/status/1457779633449934848?utm_source=pocket_mylist\\\\nhttps://github.com/yeemachine/kalidokit\\\\n\\\\nAI finds profitable Meme-Tokens\\\\nhttps://finance.yahoo.com/news/artificial-intelligence-now-makes-possible-104800931.html\\\\nhttps://finu.co/\\\\n\\\\nThis Sneaker Does Not Exist\\\\nhttps://thissneakerdoesnotexist.com/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2165",
    "uploadDate": "2021-11-18",
    "thumbnail_url": "https://i.ytimg.com/vi/FC"
  },
  {
    "link": "watch?v=EeMhj0sPrhE",
    "title": "Gradients are Not All You Need (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, backpropagation, all you need, gradients, machine learning gradients, differentiable environment, differentiable physics, differentiable simulation, when to use gradients, when not to use gradients, when to avoid gradients, google research, google ai",
    "scraped_at": 1684582637.561031,
    "genre": "Science",
    "views": "34802",
    "desc": "#deeplearning #backpropagation #simulation\\\\n\\\\nMore and more systems are made differentiable, which means that accurate gradients of these systems\\' dynamics can be computed exactly. While this development has led to a lot of advances, there are also distinct situations where backpropagation can be a very bad idea. This paper characterizes a few such systems in the domain of iterated dynamical systems, often including some source of stochasticity, resulting in chaotic behavior. In these systems, it is often better to use black-box estimators for gradients than computing them exactly.\\\\n\\\\nOUTLINE:\\\\n0:00 - Foreword\\\\n1:15 - Intro \\\\u0026 Overview\\\\n3:40 - Backpropagation through iterated systems\\\\n12:10 - Connection to the spectrum of the Jacobian\\\\n15:35 - The Reparameterization Trick\\\\n21:30 - Problems of reparameterization\\\\n26:35 - Example 1: Policy Learning in Simulation\\\\n33:05 - Example 2: Meta-Learning Optimizers\\\\n36:15 - Example 3: Disk packing\\\\n37:45 - Analysis of Jacobians\\\\n40:20 - What can be done?\\\\n45:40 - Just use Black-Box methods\\\\n\\\\nPaper: https://arxiv.org/abs/2111.05803\\\\n\\\\nAbstract:\\\\nDifferentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.\\\\n\\\\nAuthors: Luke Metz, C. Daniel Freeman, Samuel S. Schoenholz, Tal Kachman\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/2017636191\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2909",
    "uploadDate": "2021-11-15",
    "thumbnail_url": "https://i.ytimg.com/vi/EeMhj0sPrhE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=n622girLRNM",
    "title": "[ML News] Microsoft combines Images & Text | Meta makes artificial skin | Russians replicate DALL-E",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rudalle, rudall",
    "scraped_at": 1684582641.4400322,
    "genre": "Science",
    "views": "20320",
    "desc": "#mlnews #turing #reskin\\\\n\\\\nThe latest and greatest from the Machine Learning world\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: Weights \\\\u0026 Biases Tables\\\\n3:25 - Microsoft Turing Bletchley: Universal Image Language Representation Model\\\\n6:35 - Meta AI Tactile Sensing\\\\n9:55 - AnimeGANv2\\\\n11:35 - General In-Hand Object Re-Orientation\\\\n13:05 - Does Facebook score the \\\\\"",
    "lengthSeconds": "2273",
    "uploadDate": "2021-11-12",
    "thumbnail_url": "https://i.ytimg.com/vi/n622girLRNM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2h4tRsQzipQ",
    "title": "Autoregressive Diffusion Models (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, diffusion models, autoregressive models, generative models, nlp, natural language processing, gpt, image",
    "scraped_at": 1684582637.6530561,
    "genre": "Science",
    "views": "23379",
    "desc": "#machinelearning #ardm #generativemodels\\\\n\\\\nDiffusion models have made large advances in recent months as a new type of generative models. This paper introduces Autoregressive Diffusion Models (ARDMs), which are a mix between autoregressive generative models and diffusion models. ARDMs are trained to be agnostic to the order of autoregressive decoding and give the user a dynamic tradeoff between speed and performance at decoding time. This paper applies ARDMs to both text and image data, and as an extension, the models can also be used to perform lossless compression.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:15 - Decoding Order in Autoregressive Models\\\\n6:15 - Autoregressive Diffusion Models\\\\n8:35 - Dependent and Independent Sampling\\\\n14:25 - Application to Character-Level Language Models\\\\n18:15 - How Sampling \\\\u0026 Training Works\\\\n26:05 - Extension 1: Parallel Sampling\\\\n29:20 - Extension 2: Depth Upscaling\\\\n33:10 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2110.02037\\\\n\\\\nAbstract:\\\\nWe introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model\\'s adaptable parallel generation.\\\\n\\\\nAuthors: Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2063",
    "uploadDate": "2021-11-10",
    "thumbnail_url": "https://i.ytimg.com/vi/2h4tRsQzipQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=G7-fRGaCZts",
    "title": "[ML News] Google introduces Pathways | OpenAI solves Math Problems | Meta goes First Person",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, google ai, google pathways, jeff dean, pathways model, sparse neural network, meta, meta ai, ego4d, sam altman, openai, openai math, language model math, t0, tzero, bigscience, bigsciencew, deepmind, deepmind lecture series, huggingface, huggingface hub, dataset viewer, machine learning news, tech news",
    "scraped_at": 1684582641.5480325,
    "genre": "Science",
    "views": "58253",
    "desc": "#pathways #mlnews #ego4d\\\\n\\\\nYour irregular dose of Machine Learning News.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n2:10 - Google Introduces Pathways AI Architecture\\\\n6:30 - OpenAI trains Language Models to do High School Math\\\\n8:25 - Sam Altman says Neural Networks truly learn\\\\n9:35 - Google AI researchers frustrated with lawyers\\\\n12:10 - DeepMind RL Lecture Series 2021\\\\n12:40 - Fashion Store sells Adversarial Patches\\\\n13:15 - A viable method to remove the GIL from CPython\\\\n15:05 - BigScience Workshop releases T0\\\\n17:40 - Huggingface Hub Dataset Viewer\\\\n18:10 - Scite classifies scientific citations\\\\n19:25 - Facebook AI Ego4D dataset \\\\u0026 challenges\\\\n21:50 - Tesla Dojo Configurable Floating Point Spec\\\\n23:10 - Windows releases PyTorch-DirectML for Deep Learning on DirectX GPUs\\\\n23:50 - Helpful Things\\\\n33:00 - Traders use ML to analyze CEOs\\' language\\\\n34:20 - Cadbury creates DeepFake ads for local Indian businesses\\\\n35:25 - This Shoe Does Not Exist\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.com\\\\n\\\\nReferences:\\\\nGoogle Introduces Pathways AI Architecture\\\\nhttps://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/?utm_source=pocket_mylist\\\\n\\\\nOpenAI trains Language Models to do High School Math\\\\nhttps://openai.com/blog/grade-school-math/\\\\nhttps://arxiv.org/abs/2110.14168\\\\n\\\\nSam Altman says Neural Networks truly learn\\\\nhttps://twitter.com/sama/status/1450857134648823809?s=09\\\\u0026t=KazQPHo6Epn0M6ihs4DqHg\\\\u0026utm_source=pocket_mylist\\\\n\\\\nGoogle AI researchers frustrated with lawyers\\\\nhttps://archive.ph/lsQJJ#selection-2855.0-2855.294\\\\n\\\\nDeepMind RL Lecture Series 2021\\\\nhttps://deepmind.com/learning-resources/reinforcement-learning-series-2021\\\\n\\\\nFashion Store sells Adversarial Patches\\\\nhttps://twitter.com/naotokui/status/1450673712722702340\\\\n\\\\nA viable method to remove the GIL from CPython\\\\nhttps://lwn.net/Articles/872869/\\\\n\\\\nBigScience Workshop releases T0\\\\nhttps://bigscience.huggingface.co/\\\\nhttps://arxiv.org/abs/2110.08207\\\\nhttps://huggingface.co/bigscience/T0pp\\\\n\\\\nHuggingface Hub Dataset Viewer\\\\nhttps://twitter.com/huggingface/status/1454079471154257923\\\\n\\\\nScite classifies scientific citations\\\\nhttps://scite.ai\\\\nhttps://direct.mit.edu/qss/article/doi/10.1162/qss_a_00146/102990/scite-A-smart-citation-index-that-displays-the\\\\n\\\\nFacebook AI Ego4D dataset \\\\u0026 challenges\\\\nhttps://ai.facebook.com/blog/teaching-ai-to-perceive-the-world-through-your-eyes\\\\n\\\\nTesla Dojo Configurable Floating Point Spec\\\\nhttps://tesla-cdn.thron.com/static/SBY4B9_tesla-dojo-technology_OPNZ0M.pdf?xseo=\\\\u0026response-content-disposition=inline%3Bfilename%3D%22tesla-dojo-technology.pdf%22\\\\n\\\\nWindows releases PyTorch-DirectML for Deep Learning on DirectX GPUs\\\\nhttps://devblogs.microsoft.com/windowsai/introducing-pytorch-directml-train-your-machine-learning-models-on-any-gpu/\\\\n\\\\nHelpful Things\\\\nhttps://github.com/achaiah/pywick?utm_source=pocket_mylist\\\\nhttps://github.com/orybkin/lexa-benchmark?utm_source=pocket_mylist\\\\nhttps://orybkin.github.io/lexa/\\\\nhttps://twitter.com/danijarh/status/1438137568688807942?utm_source=pocket_mylist\\\\nhttps://github.com/RobertTLange/mle-hyperopt\\\\nhttps://keras.io/examples/vision/mobilevit/?utm_source=pocket_mylist\\\\nhttps://twitter.com/osanseviero/status/1451929248231563265?utm_source=pocket_mylist\\\\nhttps://huggingface.co/spaces/flax-community/image-captioning\\\\nhttps://huggingface.co/transformers/master/model_doc/visionencoderdecoder.html\\\\nhttps://github.com/facebookresearch/bitsandbytes\\\\nhttps://arxiv.org/abs/2110.11216\\\\nhttps://arxiv.org/pdf/2110.11216.pdf\\\\nhttps://github.com/facebookresearch/xformers\\\\nhttps://superbbenchmark.org/\\\\nhttps://arxiv.org/abs/2110.07731\\\\nhttps://github.com/BaguaSys/bagua?utm_source=pocket_mylist\\\\nhttps://github.com/cgarciae/treex\\\\nhttps://jax.readthedocs.io/en/latest/pytrees.html\\\\n\\\\nTraders use ML to analyze CEOs\\' language\\\\nhttps://www.reuters.com/technology/ai-can-see-through-you-ceos-language-under-machine-microscope-2021-10-20/\\\\n\\\\nCadbury creates DeepFake ads for local Indian businesses\\\\nhttps://www.bgr.in/entertainment/shah-rukh-khan-not-just-a-cadbury-ad-twitter-diwali-celebration-1016913/\\\\n\\\\nThis Shoe Does Not Exist\\\\nhttps://www.thisshoedoesnotexist.com/\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\"",
    "lengthSeconds": "2206",
    "uploadDate": "2021-11-05",
    "thumbnail_url": "https://i.ytimg.com/vi/G7"
  },
  {
    "link": "watch?v=NJCLUzkn-sA",
    "title": "EfficientZero: Mastering Atari Games with Limited Data (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, muzero, alphazero, berkeley, pieter abbeel, dreamer, dreamerv2, atari, reinforcement learning, deep reinforcement learning, world model, learned world model, latent world model, alphago, deep rl, model",
    "scraped_at": 1684582638.6950629,
    "genre": "Science",
    "views": "21806",
    "desc": "#efficientzero #muzero #atari\\\\n\\\\nReinforcement Learning methods are notoriously data-hungry. Notably, MuZero learns a latent world model just from scalar feedback of reward- and policy-predictions, and therefore relies on scale to perform well. However, most RL algorithms fail when presented with very little data. EfficientZero makes several improvements over MuZero that allows it to learn from astonishingly small amounts of data and outperform other methods by a large margin in the low-sample setting. This could be a staple algorithm for future RL research.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n2:30 - MuZero Recap\\\\n10:50 - EfficientZero improvements\\\\n14:15 - Self-Supervised consistency loss\\\\n17:50 - End-to-end prediction of the value prefix\\\\n20:40 - Model-based off-policy correction\\\\n25:45 - Experimental Results \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2111.00210\\\\nCode: https://github.com/YeWR/EfficientZero\\\\nNote: code not there yet as of release of this video\\\\n\\\\nAbstract:\\\\nReinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 190.4% mean human performance and 116.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero\\'s performance is also close to DQN\\'s performance at 200 million frames while we consume 500 times less data. EfficientZero\\'s low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at this https URL. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.\\\\n\\\\nAuthors: Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1765",
    "uploadDate": "2021-11-03",
    "thumbnail_url": "https://i.ytimg.com/vi/NJCLUzkn"
  },
  {
    "link": "watch?v=kEhEbVZQwjM",
    "title": "[YTalks] Siraj Raval - Stories about YouTube, Plagiarism, and the Dangers of Fame (Interview)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, siraj, siraj raval, ml youtube, fame, youtuber life, what happened to siraj, siraj raval plagiarism, siraj raval interview, siraj raval coursera, siraj raval apology, siraj raval paper, quantum door, ytalks, yannic siraj",
    "scraped_at": 1684582637.7420626,
    "genre": "Science",
    "views": "29028",
    "desc": "#ytalks #siraj #plagiarism\\\\n\\\\nA conversation with Siraj Raval about his journey on YouTube, and the perils of fame.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:30 - Welcome\\\\n3:15 - Starting out: From Economics to YouTube\\\\n13:00 - More Views: Plagiarizing Video Content\\\\n23:30 - One Step Up: Copying A Research Paper\\\\n29:15 - Was there another way?\\\\n39:00 - Clickbait Course: Make Money with Machine Learning\\\\n50:30 - Rock Bottom and the Way Forward\\\\n1:01:30 - Advice for Future Generations\\\\n\\\\nSiraj\\'s Channel: https://www.youtube.com/c/SirajRaval\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "4004",
    "uploadDate": "2021-10-31",
    "thumbnail_url": "https://i.ytimg.com/vi/kEhEbVZQwjM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=U8Rmfb8aZXE",
    "title": "[ML News] NVIDIA GTC'21 | DeepMind buys MuJoCo | Google predicts spreadsheet formulas",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, mujoco, nvidia, gtc21",
    "scraped_at": 1684582637.8340323,
    "genre": "Science",
    "views": "16765",
    "desc": "#gtc21 #mlnews #mujoco\\\\n\\\\nRegister to GTC\\'21 and Win a RTX 3090: https://nvda.ws/2Y2B5ni\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: NVIDIA GTC\\'21\\\\n5:35 - DeepMind buys \\\\u0026 Open-Sources MuJoCo\\\\n7:25 - PyTorch 1.10 Released\\\\n9:10 - Google Predicts Spreadsheet Formulas\\\\n11:25 - handtracking.io\\\\n12:25 - Cell Instance Segmentation Challenge\\\\n13:00 - Helpful Libraries\\\\n17:50 - Waymo cars keep turning into same dead-end\\\\n19:35 - BlueRiver balances tractors\\\\n\\\\nReferences:\\\\nDeepMind buys \\\\u0026 open-sources MuJoCo\\\\nhttps://deepmind.com/blog/announcements/mujoco\\\\n\\\\nPyTorch 1.10 released\\\\nhttps://pytorch.org/blog/pytorch-1.10-released/\\\\nhttps://developer.nvidia.com/blog/cuda-graphs/\\\\n\\\\nGoogleAI predicts spreadsheet formulas\\\\nhttps://ai.googleblog.com/2021/10/predicting-spreadsheet-formulas-from.html\\\\n\\\\nHandtracking in Browser\\\\nhttps://handtracking.io/\\\\nhttps://handtracking.io/draw_demo/\\\\n\\\\nSartorius Cell Instance Segmentation Competition\\\\nhttps://www.kaggle.com/c/sartorius-cell-instance-segmentation/\\\\n\\\\nHelpful Libraries\\\\nhttps://github.com/IntelLabs/control-flag\\\\nhttps://github.com/facebookresearch/salina\\\\nhttps://github.com/facebookresearch/salina/tree/main/salina_examples/rl/a2c/mono_cpu\\\\nhttps://github.com/ydataai/ydata-synthetic\\\\nhttps://syntheticdata.community/\\\\nhttps://github.com/ydataai/ydata-synthetic/blob/master/examples/regular/gan_example.ipynb\\\\nhttps://medium.com/aimstack/aim-3-0-0-the-foundations-for-open-source-open-metadata-ml-platform-f3969755d55\\\\nhttps://github.com/aimhubio/aim\\\\nhttps://robustbench.github.io/\\\\n\\\\nWaymo cars keep coming to same dead-end over and over\\\\nhttps://sanfrancisco.cbslocal.com/2021/10/14/dead-end-sf-street-plagued-with-confused-waymo-cars-trying-to-turn-around-every-5-minutes/\\\\n\\\\nBlueRiver balances tractors\\\\nhttps://www.linkedin.com/posts/lredden_blue-river-is-building-the-boston-dynamics-activity-6850873662959169536-8sue/\\\\nhttps://bluerivertechnology.com/ourmethods/\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1284",
    "uploadDate": "2021-10-29",
    "thumbnail_url": "https://i.ytimg.com/vi/U8Rmfb8aZXE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ch2O2fwWI-k",
    "title": "[ML News GERMAN] NVIDIA GTC'21 | DeepMind kauft MuJoCo | Google Lernt Spreadsheet Formeln",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, mujoco, nvidia, gtc21",
    "scraped_at": 1684582637.926058,
    "genre": "Science",
    "views": "6021",
    "desc": "#gtc21 #mlnews #mujoco\\\\n\\\\nRegistriere f\\xc3\\xbcr GTC\\'21 und gewinne eine RTX 3090: https://nvda.ws/2Y2B5ni\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: NVIDIA GTC\\'21\\\\n6:10 - DeepMind kauft \\\\u0026 Open-Sourct MuJoCo\\\\n9:05 - PyTorch 1.10 Ver\\xc3\\xb6ffentlicht\\\\n11:25 - Google Lernt Spreadsheet Formeln\\\\n14:15 - handtracking.io\\\\n15:25 - Zellinstanzsegmentierungswettbewerb\\\\n16:15 - Hilfreiche Bibliotheken\\\\n23:15 - Waymo autos verirren sich alle in der selben Sackgasse\\\\n24:50 - BlueRiver balanciert Traktoren\\\\n\\\\nReferences:\\\\nDeepMind kauft \\\\u0026 open-sourct MuJoCo\\\\nhttps://deepmind.com/blog/announcements/mujoco\\\\n\\\\nPyTorch 1.10 ver\\xc3\\xb6ffentlicht\\\\nhttps://pytorch.org/blog/pytorch-1.10-released/\\\\nhttps://developer.nvidia.com/blog/cuda-graphs/\\\\n\\\\nGoogleAI sagt Tabellen-Formeln voraus\\\\nhttps://ai.googleblog.com/2021/10/predicting-spreadsheet-formulas-from.html\\\\n\\\\nHandtracking im Browser\\\\nhttps://handtracking.io/\\\\nhttps://handtracking.io/draw_demo/\\\\n\\\\nSartorius Zellinstanzsegmentierungswettbewerb\\\\nhttps://www.kaggle.com/c/sartorius-cell-instance-segmentation/\\\\n\\\\nHilfreiche Bibliotheken\\\\nhttps://github.com/IntelLabs/control-flag\\\\nhttps://github.com/facebookresearch/salina\\\\nhttps://github.com/facebookresearch/salina/tree/main/salina_examples/rl/a2c/mono_cpu\\\\nhttps://github.com/ydataai/ydata-synthetic\\\\nhttps://syntheticdata.community/\\\\nhttps://github.com/ydataai/ydata-synthetic/blob/master/examples/regular/gan_example.ipynb\\\\nhttps://medium.com/aimstack/aim-3-0-0-the-foundations-for-open-source-open-metadata-ml-platform-f3969755d55\\\\nhttps://github.com/aimhubio/aim\\\\nhttps://robustbench.github.io/\\\\n\\\\nWaymo Autos verirren sich in dieselbe Sackgasse wieder und wieder\\\\nhttps://sanfrancisco.cbslocal.com/2021/10/14/dead-end-sf-street-plagued-with-confused-waymo-cars-trying-to-turn-around-every-5-minutes/\\\\n\\\\nBlueRiver balanciert Traktoren\\\\nhttps://www.linkedin.com/posts/lredden_blue-river-is-building-the-boston-dynamics-activity-6850873662959169536-8sue/\\\\nhttps://bluerivertechnology.com/ourmethods/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1617",
    "uploadDate": "2021-10-29",
    "thumbnail_url": "https://i.ytimg.com/vi/ch2O2fwWI"
  },
  {
    "link": "watch?v=xrYhDMqaa4U",
    "title": "I went to an AI Art Festival in Geneva (AiiA Festival Trip Report)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, aiia, festival, ai art, chimere, chimera, dai robot, clip guided diffusion, ai opera, ai generated art, artist ai, discussion panel, ai reality, impactai, ai festival, language models, gpt j, gpt",
    "scraped_at": 1684582638.0330327,
    "genre": "Science",
    "views": "6012",
    "desc": "#aiia #ai #art\\\\n\\\\nA trip report from the AiiA Festival in Geneva organized by the ImpactAI foundation.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:50 - Laura Tocmacov: The Festival\\\\n4:10 - Timothy O\\'Hear: The Tech\\\\n6:50 - Jonathan O\\'Hear: The Robot\\\\n11:50 - Cl\\xc3\\xa9a Chopard: The Artist\\\\n17:45 - Final Words\\\\n\\\\nWebsite: https://aiiafestival.org/en/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1132",
    "uploadDate": "2021-10-27",
    "thumbnail_url": "https://i.ytimg.com/vi/xrYhDMqaa4U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kP-dXK9JEhY",
    "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gpt",
    "scraped_at": 1684582638.7880576,
    "genre": "Science",
    "views": "22274",
    "desc": "#gpt3 #knowledge #symbolic\\\\n\\\\nSymbolic knowledge models are usually trained on human-generated corpora that are cumbersome and expensive to create. Such corpora consist of structured triples of symbolic knowledge. This paper takes a different approach and attempts to generate such a corpus by prompting GPT-3. Results show that clever prompting, combined with targeted small critic models trained on human ratings can outperform both human-generated data, as well as the teacher model (GPT-3) itself. The results of this paper give a general recipe for automatically building corpora for various NLP tasks by extracting samples from large language models.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:30 - Sponsor: Weights \\\\u0026 Biases\\\\n4:15 - Commonsense Knowledge Graphs\\\\n7:50 - ATOMIC dataset\\\\n10:00 - Generating the corpus from a model\\\\n13:00 - Prompting GPT-3\\\\n15:30 - Generating Events\\\\n18:40 - Generating Inferences\\\\n23:00 - Evaluating the created dataset\\\\n26:45 - Introducing the critic\\\\n31:25 - Using the critic to filter the data\\\\n36:30 - Training a student on the generated data\\\\n41:00 - Key Findings\\\\n44:45 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2110.07178\\\\nCode \\\\u0026 Corpus: https://github.com/peterwestai2/symbolic-knowledge-distillation\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.com\\\\nhttps://community.wandb.ai/\\\\n\\\\nAbstract:\\\\nThe common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model\\'s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.\\\\n\\\\nAuthors: Peter West, Chandra Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2721",
    "uploadDate": "2021-10-24",
    "thumbnail_url": "https://i.ytimg.com/vi/kP"
  },
  {
    "link": "watch?v=vxdcX0JTEr0",
    "title": "I took a Swiss train and it was awesome! Train Seat Review - SBB InterCity 1 - Geneva to St. Gallen",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, sbb, cff, sncf, swiss train, swiss train system, intercity train, intercity 1, durchmesserlinie, geneva, lausanne, bern, zurich, st gallen, train seat, 2nd class, switzerland train, schwerizerische bundesbahnen, seat review, train seat review, travel review, train travel, travel switzerland",
    "scraped_at": 1684582638.1290314,
    "genre": "Comedy",
    "views": "9863",
    "desc": "#sbb #seatreview #travel\\\\n\\\\nA friendly parody of Travel Vloggers and Airplane Seat Reviews :)\\\\nNo, SBB did not pay me for this (but they should ;) )\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "256",
    "uploadDate": "2021-10-21",
    "thumbnail_url": "https://i.ytimg.com/vi/vxdcX0JTEr0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=K3cmxn5znyU",
    "title": "[ML News] Microsoft trains 530B model | ConvMixer model fits into single tweet | DeepMind profitable",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, kilcher news, machine learning news, microsoft, turing nlg, convmixer, stylegan 3, stylegan v3, billion parameters, vqgan, gertel ai, deepmind, alphafold, schmidhuber, fukuhima, neocognitron, mosaicml, self",
    "scraped_at": 1684582645.5397193,
    "genre": "Science",
    "views": "30947",
    "desc": "#mlnews #turingnlg #convmixer\\\\n\\\\nYour latest upates on what\\'s happening in the Machine Learning world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:16 - Weights \\\\u0026 Biases raises on 1B valuation (sponsored)\\\\n2:30 - Microsoft trains 530 billion parameter model\\\\n5:15 - StyleGAN v3 released\\\\n6:45 - A few more examples may be worth billions of parameters\\\\n8:30 - ConvMixer fits into a tweet\\\\n9:45 - Improved VQGAN\\\\n11:25 - William Shatner AI chats about his life\\\\n12:35 - Google AI pushes material science\\\\n14:10 - Gretel AI raises 50M for privacy protection\\\\n16:05 - DeepMind\\'s push into ML for biology\\\\n19:00 - Schmidhuber laudates Kunihiko Fukushima for Bower Award\\\\n21:30 - Helpful Things\\\\n22:25 - Mosaic ML out of stealth mode\\\\n23:55 - First German self-driving train\\\\n24:45 - Ex-Pentagon Chief: China has already won\\\\n26:25 - DeepMind becomes profitable\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.com\\\\n\\\\nReferences:\\\\nMicrosoft Trains 530B Parameter Model\\\\nhttps://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\\\\n\\\\nStyleGAN 3 Code Released\\\\nhttps://nvlabs.github.io/stylegan3/\\\\nhttps://github.com/NVlabs/stylegan3\\\\nhttps://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2BCLIP.ipynb#scrollTo=V_rq-N2m0Tlb\\\\n\\\\nWhen do labels help?\\\\nhttps://arxiv.org/pdf/2110.04374.pdf\\\\n\\\\nml_paper.bruh\\\\nhttps://openreview.net/pdf?id=TVHS5Y4dNvM\\\\n\\\\nImproved VQGAN\\\\nhttps://openreview.net/pdf?id=pfNyExj7z2\\\\n\\\\nWilliam Shatner \\\\\"",
    "lengthSeconds": "1671",
    "uploadDate": "2021-10-20",
    "thumbnail_url": "https://i.ytimg.com/vi/K3cmxn5znyU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=NEkriziVYXo",
    "title": "[ML News] DeepMind does Nowcasting | The Guardian's shady reporting | AI finishes Beethoven's 10th",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nowcasting, mlnews, deepmind, weather prediction, ai weather, short term weather, rain prediction, rain when, ai nowcasting, ml weather prediction, deepmind weather, the guardian, truthfulqa, truthful qa, language models truthful, plato xl, beethoven 10, ai music, ai art, ai painting, painting authenticity, huggingface, huggingface infinity, neuromorphic chips",
    "scraped_at": 1684582641.6770327,
    "genre": "Science",
    "views": "21393",
    "desc": "#deepmind #nowcasting #machinelearning\\\\n\\\\nYour holy update on what\\'s new in the Machine Learning world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - DeepMind tackles Nowcasting\\\\n3:30 - The Guardian\\'s shady reporting on TruthfulQA\\\\n6:15 - Stochastic training not necessary for generalization\\\\n7:35 - Google AI\\'s efficient partitioning of road networks\\\\n9:15 - MiniHack Reinforcement Learning Environment\\\\n10:45 - Plato XL 11B dialog model\\\\n11:35 - AI finishes Beethoven\\'s 10th Symphony\\\\n13:10 - AI casts doubt on painting authenticity\\\\n15:55 - ShadowDragon social media surveillance\\\\n18:45 - Helpful Libraries\\\\n25:20 - Samsung to copy-paste brains onto chips\\\\n\\\\nReferences:\\\\nDeepMind improves Nowcasting\\\\nhttps://deepmind.com/blog/article/nowcasting\\\\nhttps://www.nature.com/articles/s41586-021-03854-z\\\\nhttps://github.com/deepmind/deepmind-research/tree/master/nowcasting\\\\nhttps://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nowcasting/Open_sourced_dataset_and_model_snapshot_for_precipitation_nowcasting.ipynb\\\\n\\\\nThe Guardian\\'s shady reporting on TruthfulQA\\\\nhttps://www.theguardian.com/commentisfree/2021/oct/02/the-truth-about-artificial-intelligence-it-isnt-that-honest?CMP=Share_iOSApp_Other\\\\n\\\\nStochastic Training is Not Necessary for Generalization\\\\nhttps://arxiv.org/pdf/2109.14119.pdf\\\\n\\\\nGoogle AI - Efficient Partitioning of Road Networks\\\\nhttps://ai.googleblog.com/2021/09/efficient-partitioning-of-road-networks.html\\\\n\\\\nMiniHack Reinforcement Learning Environment\\\\nhttps://ai.facebook.com/blog/minihack-a-new-sandbox-for-open-ended-reinforcement-learning\\\\n\\\\nBaidu PLATO-XL 11B Dialog Model\\\\nhttp://research.baidu.com/Blog/index-view?id=163\\\\n\\\\nAI finishes Beethoven\\'s 10th Symphony\\\\nhttps://thenextweb.com/news/computer-scientists-completed-beethoven-10th-symphony-syndication\\\\n\\\\nAI casts doubt on paining authenticity\\\\nhttps://www.smithsonianmag.com/smart-news/ai-casts-new-doubt-on-national-gallerys-prized-peter-paul-rubens-180978771/\\\\nhttps://art-recognition.com/\\\\nhttps://art-recognition.com/case-studies/\\\\nhttps://art-recognition.com/faq/\\\\n\\\\nShadowDragon Social Media Surveillance\\\\nhttps://www.rt.com/usa/535630-ai-surveillance-police-program-social-media/\\\\nhttps://theintercept.com/2021/09/21/surveillance-social-media-police-microsoft-shadowdragon-kaseware/\\\\n\\\\nHelpful Libraries / Datasets\\\\nhttps://huggingface.co/infinity\\\\nhttps://yanaiela.github.io/TNE/?s=09\\\\u0026utm_source=pocket_mylist\\\\nhttps://arxiv.org/abs/2109.10282\\\\nhttps://github.com/microsoft/unilm/tree/master/trocr\\\\nhttps://medium.com/people-ai-research/kaokore-exploring-the-intersection-of-humanities-and-ml-research-through-a-japanese-art-dataset-f6035ba1e4d\\\\nhttps://raft.elicit.org/\\\\nhttps://huggingface.co/spaces/ought/raft-leaderboard\\\\nhttps://huggingface.co/spaces/ought/raft-viewer?dataset=raft\\\\u0026config=ade_corpus_v2\\\\u0026raft=dataset\\\\u0026banking_77=config\\\\nhttps://arxiv.org/pdf/2109.14076.pdf\\\\nhttps://arxiv.org/pdf/2109.14394.pdf\\\\nhttps://www.robots.ox.ac.uk/~vgg/research/pass/\\\\nhttps://zenodo.org/record/5528345#.YVrtd0ZByDU\\\\nhttps://github.com/yukimasano/PASS/\\\\nhttps://openreview.net/pdf?id=BwzYI-KaHdr\\\\nhttps://github.com/pytorch/data?utm_source=pocket_mylist\\\\n\\\\nSamsung Method to copy paste brain onto chip\\\\nhttps://www.engadget.com/samsung-copy-and-paste-brain-neuromorphic-chips-185359994.html\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1660",
    "uploadDate": "2021-10-07",
    "thumbnail_url": "https://i.ytimg.com/vi/NEkriziVYXo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dND-7llwrpw",
    "title": "Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, grokking, openai, double descent, belkin, overfitting, bias variance, steps, training, binary tables, binary operations, binary operation, multiplication table, algorithmic datasets, groups, s5 group, deep learning algorithmic, deep learning generalization, generalization research, why do neural networks generalize",
    "scraped_at": 1684582638.883033,
    "genre": "Science",
    "views": "61617",
    "desc": "#grokking #openai #deeplearning\\\\n\\\\nGrokking is a phenomenon when a neural network suddenly learns a pattern in the dataset and jumps from random chance generalization to perfect generalization very suddenly. This paper demonstrates grokking on small algorithmic datasets where a network has to fill in binary tables. Interestingly, the learned latent spaces show an emergence of the underlying binary operations that the data were created with.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - The Grokking Phenomenon\\\\n3:50 - Related: Double Descent\\\\n7:50 - Binary Operations Datasets\\\\n11:45 - What quantities influence grokking?\\\\n15:40 - Learned Emerging Structure\\\\n17:35 - The role of smoothness\\\\n21:30 - Simple explanations win\\\\n24:30 - Why does weight decay encourage simplicity?\\\\n26:40 - Appendix\\\\n28:55 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf\\\\n\\\\nAbstract:\\\\nIn this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \\xe2\\x80\\x9cgrokking\\xe2\\x80\\x9d a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.\\\\n\\\\nAuthors: Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin \\\\u0026 Vedant Misra\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1787",
    "uploadDate": "2021-10-06",
    "thumbnail_url": "https://i.ytimg.com/vi/dND"
  },
  {
    "link": "watch?v=wTzvKB6D_34",
    "title": "How far can we scale up? Deep Learning's Diminishing Returns (Article Review)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, scale, co2, gpt",
    "scraped_at": 1684582638.9780333,
    "genre": "Science",
    "views": "22539",
    "desc": "#deeplearning #co2 #cost\\\\n\\\\nDeep Learning has achieved impressive results in the last years, not least due to the massive increases in computational power and data that has gone into these models. Scaling up currently promises to be a reliable way to create more performant systems, but how far can we go? This article explores the limits of exponential scaling in AI, and what people are doing to get around this problem\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:00 - Deep Learning at its limits\\\\n3:10 - The cost of overparameterization\\\\n5:40 - Extrapolating power usage and CO2 emissions\\\\n10:45 - We cannot just continue scaling up\\\\n13:25 - Current solution attempts\\\\n15:25 - Aside: ImageNet V2\\\\n17:50 - Are symbolic methods the way out?\\\\n\\\\nPaper: https://spectrum.ieee.org/deep-learning-computational-cost\\\\n\\\\nImage by Ralf Vetterle from Pixabay: https://pixabay.com/images/id-1752876/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1226",
    "uploadDate": "2021-10-02",
    "thumbnail_url": "https://i.ytimg.com/vi/wTzvKB6D_34/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tX1OolVxDzs",
    "title": "[ML News] Plagiarism Case w/ Plot Twist | CLIP for video surveillance | OpenAI summarizes books",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, clip, openai, summarizing books, darpa, darpa subt, schmidhuber, lbh, turing lecture, adversarial curiosity, gans, clip surveillance, ai surveillance, video search, plagiarism, machine learning plagiarism, dopamine, scikit learn, mlnews, ml news, machine learning news, unbias it, unbiasit, video surveillance, email surveillance, unconscious bias, phi flow, habitat, wikipedia",
    "scraped_at": 1684582641.774066,
    "genre": "Science",
    "views": "22714",
    "desc": "#plagiarism #surveillance #schmidhuber\\\\n\\\\nYour Mondaily updates of what\\'s going in the world of Machine Learning.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - New plagiarism case has plot twist\\\\n7:25 - CLIP for video surveillance\\\\n9:40 - DARPA SubTerranean Challenge\\\\n11:00 - Schmidhuber criticizing Turing Lecture\\\\n15:00 - OpenAI summarizes books\\\\n17:55 - UnBiasIt monitors employees\\' communications for bias\\\\n20:00 - iOS plans to detect depression\\\\n21:30 - UK 10 year plan to become AI superpower\\\\n23:30 - Helpful Libraries\\\\n29:00 - WIT: Wikipedia Image-Text dataset\\\\n\\\\nReferences:\\\\nNew plagiarism case with plot twist\\\\nhttps://www.reddit.com/r/MachineLearning/comments/pvgpfl/ndr_alleged_plagiarism_of_improve_object/\\\\nhttps://zhuanlan.zhihu.com/p/411800486\\\\nhttps://github.com/cybercore-co-ltd/CoLAD_paper/blob/master/PlagiarismClaim/README.md\\\\n\\\\nCLIP used for video surveillance\\\\nhttps://www.reddit.com/r/MachineLearning/comments/ps0d02/p_a_truck_with_the_text_jcn_clip_is_scarily_good/\\\\nhttps://github.com/johanmodin/clifs\\\\n\\\\nDARPA SubTerranean Challenge\\\\nhttps://twitter.com/BotJunkie/status/1441225455856615424\\\\nhttps://twitter.com/BotJunkie\\\\nhttps://www.subtchallenge.com/index.html\\\\nhttps://www.subtchallenge.com/resources/SubT_Challenge_Finals_Rules.pdf\\\\nhttps://twitter.com/dynamicrobots/status/1441481455830401028\\\\n\\\\nSchmidhuber Blog: Turing Lecture Errors\\\\nhttps://people.idsia.ch/~juergen/scientific-integrity-turing-award-deep-learning.html\\\\n\\\\nOpenAI on Summarizing Books\\\\nhttps://openai.com/blog/summarizing-books/\\\\nhttps://arxiv.org/pdf/2109.10862.pdf\\\\n\\\\nUnBiasIt to monitor employee language\\\\nhttps://edition.cnn.com/2021/09/20/tech/unbiasit-bias-surveillance-software/index.html\\\\nhttps://www.unbiasit.com/\\\\n\\\\niPhone to detect depression\\\\nhttps://www.wsj.com/articles/apple-wants-iphones-to-help-detect-depression-cognitive-decline-sources-say-11632216601\\\\nhttps://archive.ph/hRTnw\\\\n\\\\nUK 10-year plan to become AI-superpower\\\\nhttps://www.cnbc.com/2021/09/22/uk-publishes-plan-to-become-ai-superpower-and-rival-us-and-china.html\\\\nhttps://archive.ph/4gkKK\\\\n\\\\nHelpful Libraries\\\\nhttps://twitter.com/scikit_learn/status/1441443534184275969\\\\nhttps://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_0_0.html\\\\nhttps://twitter.com/pcastr/status/1441125505588084737\\\\nhttps://github.com/google/dopamine\\\\nhttps://github.com/microsoft/muzic\\\\nhttps://ai-muzic.github.io/muzic_logo/\\\\nhttps://ai.facebook.com/blog/dynatask-a-new-paradigm-of-ai-benchmarking-is-now-available-for-the-ai-community\\\\nhttps://github.com/tum-pbs/PhiFlow\\\\nhttps://github.com/facebookresearch/dora\\\\n\\\\nHabitat and Matterport 3D Dataset\\\\nhttps://github.com/facebookresearch/habitat-lab\\\\nhttps://aihabitat.org/\\\\nhttps://arxiv.org/pdf/2109.08238.pdf\\\\n\\\\nWIT: Wikipedia-Based Image-Text Dataset\\\\nhttps://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1852",
    "uploadDate": "2021-09-29",
    "thumbnail_url": "https://i.ytimg.com/vi/tX1OolVxDzs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=19Q-vMd9bYg",
    "title": "Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, neurips, nips, nips experiment, peer reviw, conference review, reviewer, machine learning reviewer, ml conference review, subjectivity in peer review, reviewer opinions, science review, science peer review, peer review fail",
    "scraped_at": 1684582638.2240574,
    "genre": "Science",
    "views": "15874",
    "desc": "#neurips #peerreview #nips\\\\n\\\\nThe peer-review system at Machine Learning conferences has come under much criticism over the last years. One major driver was the infamous 2014 NeurIPS experiment, where a subset of papers were given to two different sets of reviewers. This experiment showed that only about half of all accepted papers were consistently accepted by both committees and demonstrated significant influence of subjectivity. This paper revisits the data from the 2014 experiment and traces the fate of accepted and rejected papers during the 7 years since, and analyzes how well reviewers can assess future impact, among other things.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:20 - Recap: The 2014 NeurIPS Experiment\\\\n5:40 - How much of reviewing is subjective?\\\\n11:00 - Validation via simulation\\\\n15:45 - Can reviewers predict future impact?\\\\n23:10 - Discussion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2109.09774\\\\nCode: https://github.com/lawrennd/neurips2014/\\\\n\\\\nAbstract:\\\\nIn this paper we revisit the 2014 NeurIPS experiment that examined inconsistency in conference peer review. We determine that 50% of the variation in reviewer quality scores was subjective in origin. Further, with seven years passing since the experiment we find that for accepted papers, there is no correlation between quality scores and impact of the paper as measured as a function of citation count. We trace the fate of rejected papers, recovering where these papers were eventually published. For these papers we find a correlation between quality scores and impact. We conclude that the reviewing process for the 2014 conference was good for identifying poor papers, but poor for identifying good papers. We give some suggestions for improving the reviewing process but also warn against removing the subjective element. Finally, we suggest that the real conclusion of the experiment is that the community should place less onus on the notion of top-tier conference publications when assessing the quality of individual researchers. For NeurIPS 2021, the PCs are repeating the experiment, as well as conducting new ones.\\\\n\\\\nAuthors: Corinna Cortes, Neil D. Lawrence\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1559",
    "uploadDate": "2021-09-27",
    "thumbnail_url": "https://i.ytimg.com/vi/19Q"
  },
  {
    "link": "watch?v=DkojaN7_f4E",
    "title": "[ML News] New ImageNet SOTA | Uber's H3 hexagonal coordinate system | New text-image-pair dataset",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, laion, schmidhuber, coatnet, efficientnetv2, truthfulqa, gpt",
    "scraped_at": 1684582638.3190315,
    "genre": "Science",
    "views": "16734",
    "desc": "#truthfulqa #efficientnet #laion400M\\\\n\\\\nYour regularly irregular updates on what\\'s happening in the Machine Learning world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - TruthfulQA benchmark shines new light on GPT-3\\\\n2:00 - LAION-400M image-text-pair dataset\\\\n4:10 - GoogleAI\\'s EfficientNetV2 and CoAtNet\\\\n6:15 - Uber\\'s H3: A hexagonal coordinate system\\\\n7:40 - AWS NeurIPS 2021 DeepRacer Challenge\\\\n8:15 - Helpful Libraries\\\\n9:20 - State of PyTorch in September 2021\\\\n10:05 - Physics-Based Deep Learning Book\\\\n10:35 - Music-conditioned 3D dance generation\\\\n11:40 - Stallman\\'s take on legal issues with Codex\\\\n12:20 - Tensorflow DirectML on AMD GPUs\\\\n13:00 - Schmidhuber Blog: Turing Oversold\\\\n\\\\nERRATA:\\\\nUber\\'s H3 is actually not new, but from 2018\\\\n\\\\nReferences:\\\\nTruthfulQA - A benchmark assessing truthfulness of language models\\\\nhttps://owainevans.github.io/pdfs/truthfulQA_lin_evans.pdf\\\\n\\\\nLAION-400M image-text-pair dataset\\\\nhttps://laion.ai/laion-400-open-dataset/\\\\nhttps://laion.ai/#top\\\\nhttps://gogetfunding.com/help-us-build-the-worlds-largest-open-billion-scale-image-text-dataset-perfect-for-training-dall-e-clip-other-multimodal-models/\\\\nhttps://rom1504.github.io/clip-retrieval/?back=https%3A%2F%2Fsplunk.vra.ro\\\\u0026index=laion_400m_128G\\\\u0026query=yellow+train\\\\n\\\\nGooleAI releases EfficientNetV2 and CoAtNet\\\\nhttps://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html\\\\n\\\\nUber\\'s H3 hexagonal coordinate systems\\\\nhttps://eng.uber.com/h3/?utm_source=pocket_mylist\\\\n\\\\nNeurIPS 2021 DeepRacer Challenge\\\\nhttps://www.aicrowd.com/challenges/neurips-2021-aws-deepracer-ai-driving-olympics-challenge?utm_source=pocket_mylist\\\\nhttps://aws.amazon.com/deepracer/\\\\nhttps://gitlab.aicrowd.com/deepracer/neurips-2021-aws-deepracer-starter-kit/-/tree/master/deepracer-gym\\\\n\\\\nHelpful Libraries\\\\nhttps://github.com/rom1504/img2dataset\\\\nhttps://github.com/facebookresearch/vissl?utm_source=pocket_mylist\\\\nhttps://github.com/pyg-team/pytorch_geometric\\\\nhttps://aws.amazon.com/blogs/machine-learning/announcing-the-amazon-s3-plugin-for-pytorch/\\\\n\\\\nState of PyTorch in September 2021\\\\nhttps://dev-discuss.pytorch.org/t/state-of-pytorch-core-september-2021-edition/332\\\\n\\\\nPhysics-Based Deep Learning Book\\\\nhttp://physicsbaseddeeplearning.org/intro.html\\\\nhttps://arxiv.org/pdf/2109.05237.pdf\\\\n\\\\nMusic Conditioned 3D dance generation\\\\nhttps://ai.googleblog.com/2021/09/music-conditioned-3d-dance-generation.html\\\\n\\\\nRichard Stallman on Codex legal issues\\\\nhttps://news.slashdot.org/story/21/09/18/0432224/richard-stallman-shares-his-concerns-about-githubs-copilot----and-about-github\\\\n\\\\nTensorflow DirectML on AMD\\\\nhttps://wccftech.com/amd-microsoft-bring-tensorflow-directml-to-life-4x-improvement-with-rdna-2-gpus/\\\\n\\\\nSchmidhuber: Turing Oversold\\\\nhttps://people.idsia.ch//~juergen/turing-oversold.html\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "853",
    "uploadDate": "2021-09-24",
    "thumbnail_url": "https://i.ytimg.com/vi/DkojaN7_f4E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=aX8phGhG8VQ",
    "title": "Does GPT-3 lie? - Misinformation and fear-mongering around the TruthfulQA dataset",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gpt",
    "scraped_at": 1684582641.8627212,
    "genre": "Science",
    "views": "19070",
    "desc": "#gpt-3 #truth #conspiracy\\\\n\\\\nA new benchmark paper has created quite an uproar in the community. TruthfulQA is a dataset of 817 questions probing for imitative falsehoods where language models become less truthful, the larger they get. This surprising counter-intuitive finding validates many people\\'s criticisms of large language models, but is it really the correct conclusion?\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - Twitter Paper Announcement\\\\n4:10 - Large Language Models are to blame!\\\\n5:50 - How was the dataset constructed?\\\\n9:25 - The questions are adversarial\\\\n12:30 - Are you surprised?!\\\\n\\\\nPaper: https://arxiv.org/abs/2109.07958\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "799",
    "uploadDate": "2021-09-21",
    "thumbnail_url": "https://i.ytimg.com/vi/aX8phGhG8VQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=pBau7umFhjQ",
    "title": "Topographic VAEs learn Equivariant Capsules (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, vae, variational, bayesian, variational methods, variational autoencoder, max welling, elbo, prior, student t, reparameterization trick, log likelihood, encoder decoder",
    "scraped_at": 1684582639.0800314,
    "genre": "Science",
    "views": "17749",
    "desc": "#tvae #topographic #equivariant\\\\n\\\\nVariational Autoencoders model the latent space as a set of independent Gaussian random variables, which the decoder maps to a data distribution. However, this independence is not always desired, for example when dealing with video sequences, we know that successive frames are heavily correlated. Thus, any latent space dealing with such data should reflect this in its structure. Topographic VAEs are a framework for defining correlation structures among the latent variables and induce equivariance within the resulting model. This paper shows how such correlation structures can be built by correctly arranging higher-level variables, which are themselves independent Gaussians.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:40 - Architecture Overview\\\\n6:30 - Comparison to regular VAEs\\\\n8:35 - Generative Mechanism Formulation\\\\n11:45 - Non-Gaussian Latent Space\\\\n17:30 - Topographic Product of Student-t\\\\n21:15 - Introducing Temporal Coherence\\\\n24:50 - Topographic VAE\\\\n27:50 - Experimental Results\\\\n31:15 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2109.01394\\\\nCode: https://github.com/akandykeller/topographicvae\\\\n\\\\nAbstract:\\\\nIn this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. \\\\\"",
    "lengthSeconds": "1924",
    "uploadDate": "2021-09-20",
    "thumbnail_url": "https://i.ytimg.com/vi/pBau7umFhjQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-sNJd7bANTI",
    "title": "[ML News] Roomba Avoids Poop | Textless NLP | TikTok Algorithm Secrets | New Schmidhuber Blog",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, ml news, kilcher news, schmidhuber, schmidhuber blog, schmidhuber gan, schmidhuber transformer, textless nlp, facebook ai, vqvae, tiktok algorithm, how tiktok works, roomba, roomba poop, cohere, cohere ai, deepmind, deepmind google, deepmind mario, gpt",
    "scraped_at": 1684582646.9517214,
    "genre": "Science",
    "views": "16938",
    "desc": "#schmidhuber #tiktok #roomba\\\\n\\\\nYour regularly irregular update on what\\'s happening in the world of Machine Learning.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: Weights \\\\u0026 Biases\\\\n1:55 - ML YouTuber reaches 100k subscribers\\\\n2:40 - Facebook AI pushes Textless NLP\\\\n5:30 - Schmidhuber blog post: I invented everything\\\\n7:55 - TikTok algorithm rabbitholes users\\\\n10:45 - Roomba learns to avoid poop\\\\n11:50 - AI can spot art forgeries\\\\n14:55 - Deepmind\\'s plans to separate from Google\\\\n16:15 - Cohere raises 40M\\\\n16:55 - US Judge rejects AI inventor on patent\\\\n17:55 - Altman: GPT-4 not much bigger than GPT-3\\\\n18:45 - Salesforce CodeT5\\\\n19:45 - DeepMind Reinforcement Learning Lecture Series\\\\n20:15 - WikiGraphs Dataset\\\\n20:40 - LiveCell Dataset\\\\n21:00 - SpeechBrain\\\\n21:10 - AI-generated influencer gains 100 sponsorships\\\\n22:20 - AI News Questions\\\\n23:15 - AI hiring tools reject millions of valid applicants\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/start\\\\n\\\\nReferences:\\\\nFacebook AI creates Textless NLP\\\\nhttps://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio\\\\nhttps://speechbot.github.io/pgslm/?fbclid=IwAR1fbW6uKCMic9VyGEYqLTq-GrfcWU4VY43qJIywWV07eFi_sES1BxoLtIE\\\\n\\\\nSchmidhuber invented everything\\\\nhttps://people.idsia.ch/~juergen/most-cited-neural-nets.html?utm_source=pocket_mylist\\\\n\\\\nHow TikTok\\'s algorithm works\\\\nhttps://www.wsj.com/video/series/inside-tiktoks-highly-secretive-algorithm/investigation-how-tiktok-algorithm-figures-out-your-deepest-desires/6C0C2040-FF25-4827-8528-2BD6612E3796\\\\n\\\\nRoomba learns to avoid poop\\\\nhttps://edition.cnn.com/2021/09/09/tech/roomba-ai-avoids-dog-poop/index.html\\\\n\\\\nAmateur develops fake art detector\\\\nhttps://blogs.nvidia.com/blog/2021/08/27/da-vinci-rtx-2070/?linkId=100000066274217\\\\nhttps://spectrum.ieee.org/this-ai-can-spot-an-art-forgery\\\\n\\\\nDeepMind\\'s plan to break away from Google\\\\nhttps://www.businessinsider.com/deepmind-secret-plot-break-away-from-google-project-watermelon-mario-2021-9?IR=T\\\\u0026r=US\\\\u0026utm_source=pocket_mylist\\\\nhttps://archive.ph/8s5IK\\\\n\\\\nCohere raises USD 40M\\\\nhttps://www.fastcompany.com/90670635/ex-googlers-raise-40-million-to-democratize-natural-language-ai\\\\nhttps://cohere.ai/\\\\n\\\\nUS judge refuses AI patent\\\\nhttps://www.theregister.com/2021/09/04/ai_patent_ruling/\\\\n\\\\nSam Altman on GPT-4\\\\nhttps://www.reddit.com/r/OpenAI/comments/pj0nug/sam_altman_gpt4_will_remain_textonly_will_not_use/\\\\n\\\\nSalesforce releases CodeT5\\\\nhttps://blog.einstein.ai/codet5/\\\\n\\\\nDeepMind RL lecture series\\\\nhttps://deepmind.com/learning-resources/reinforcement-learning-series-2021\\\\n\\\\nWikiGraphs Dataset\\\\nhttps://github.com/deepmind/deepmind-research/tree/master/wikigraphs\\\\n\\\\nLiveCell Dataset\\\\nhttps://sartorius-research.github.io/LIVECell/?utm_source=pocket_mylist\\\\nhttps://www.nature.com/articles/s41592-021-01249-6\\\\n\\\\nSpeechBrain Library\\\\nhttps://speechbrain.github.io/\\\\n\\\\nAI generated influencer lands 100 sponsorships\\\\nhttps://www.allkpop.com/article/2021/09/social-media-influencer-model-created-from-artificial-intelligence-lands-100-sponsorships\\\\n\\\\nAI News Questions\\\\nhttps://www.forbes.com/sites/tomtaulli/2021/09/10/ai-artificial-intelligence-should-you-teach-it-to-your-employees/\\\\nhttps://mindmatters.ai/2021/09/isnt-it-time-for-an-artificial-intelligence-reality-check/\\\\nhttps://fortune.com/2021/09/07/deepmind-agi-eye-on-ai/\\\\nhttps://www.forbes.com/sites/anniebrown/2021/09/06/is-artificial-intelligence-set-to-take-over-the-art-industry/\\\\nhttps://www.cnbctv18.com/views/view-are-our-fears-of-artificial-intelligence-justified-10694741.htm\\\\nhttps://www.kcrw.com/culture/shows/life-examined/technology-artificial-intelligence-religion-faith/linda-kinstler-silicon-valley-ai-ethics-religious\\\\nhttps://techcrunch.com/2021/09/07/ai-as-a-service-to-solve-your-business-problems-guess-again/\\\\nhttps://www.forbes.com/sites/bernardmarr/2021/09/10/how-do-we-use-artificial-intelligence-ethically/\\\\n\\\\nAI hiring tools mistakenly reject millions of applicants\\\\nhttps://www.theverge.com/2021/9/6/22659225/automated-hiring-software-rejecting-viable-candidates-harvard-business-school\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1539",
    "uploadDate": "2021-09-16",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=ifBI2jTaAEo",
    "title": "Celebrating 100k Subscribers! (w/ Channel Statistics)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582639.1810315,
    "genre": "Science",
    "views": "6186",
    "desc": "#yannickilcher #machinelearning #100k\\\\n\\\\nOUTLINE:\\\\n0:00 - 100k!\\\\n1:00 - Announcements \\\\u0026 Thanks\\\\n3:55 - Channel Statistics\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "578",
    "uploadDate": "2021-09-14",
    "thumbnail_url": "https://i.ytimg.com/vi/ifBI2jTaAEo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=eROy3BrqEVk",
    "title": "[ML News] AI predicts race from X-Ray | Google kills HealthStreams | Boosting Search with MuZero",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, schmidhuber, kaust, saudi arabia, ai initiative, reading race, xray, race xray, ai race, ai bias, facebook primates, muzero, muzero code, muzero paper, google muzero, health streams, deepmind health, wandb, dmca, github dmca, distill, distill gnn, graph neural networks, ai depression, unconstrained scene generation, transformers",
    "scraped_at": 1684582641.9897199,
    "genre": "Science",
    "views": "17543",
    "desc": "#mlnews #schmidhuber #muzero\\\\n\\\\nYour regular updates on what\\'s happening in the ML world!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: Weights \\\\u0026 Biases\\\\n1:45 - Google shuts down health streams\\\\n4:25 - AI predicts race from blurry X-Rays\\\\n7:35 - Facebook labels black men as primates\\\\n11:05 - Distill papers on Graph Neural Networks\\\\n11:50 - J\\xc3\\xbcrgen Schmidhuber to lead KAUST AI Initiative\\\\n12:35 - GitHub brief on DMCA notices for source code\\\\n14:55 - Helpful Reddit Threads\\\\n19:40 - Simple Tricks to improve Transformers\\\\n20:40 - Apple\\'s Unconstrained Scene Generation\\\\n21:40 - Common Objects in 3D dataset\\\\n22:20 - WarpDrive Multi-Agent RL framework\\\\n23:10 - My new paper: Boosting Search Agents \\\\u0026 MuZero\\\\n25:15 - Can AI detect depression from speech?\\\\n\\\\nReferences:\\\\nGoogle shuts down Health Streams\\\\nhttps://techcrunch.com/2021/08/26/google-confirms-its-pulling-the-plug-on-streams-its-uk-clinician-support-app/\\\\n\\\\nAI predicts race from X-Rays\\\\nhttps://www.iflscience.com/technology/ai-makes-strangely-accurate-predictions-from-blurry-medical-scans-alarming-researchers/?fbclid=IwAR2ddIP4w0p6VNbMRoe_9OPXQS6NA365XdB22v7rMlVOcuqnxe1ST7ZuvtA\\\\u0026utm_source=pocket_mylist\\\\nhttps://arxiv.org/ftp/arxiv/papers/2107/2107.10356.pdf\\\\n\\\\nFacebook labels black men as primates\\\\nhttps://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html\\\\nhttps://en.wikipedia.org/wiki/Human\\\\n\\\\nDistill articles on GNNs\\\\nhttps://distill.pub/2021/gnn-intro/\\\\nhttps://distill.pub/2021/understanding-gnns/\\\\n\\\\nJ\\xc3\\xbcrgen Schmidhuber leads KAUST AI initiative\\\\nhttps://people.idsia.ch/~juergen/kaust-2021.html\\\\n\\\\nGitHub issues court brief on code DMCAs\\\\nhttps://github.blog/2021-08-31-vague-infringement-allegations-considered-harmful/\\\\n\\\\nUseful Reddit Threads\\\\nhttps://www.reddit.com/r/MachineLearning/comments/phvgzb/r_how_machine_learning_will_revolutionise_physics/\\\\nhttps://www.reddit.com/r/MachineLearning/comments/pe9jyt/d_what_are_the_most_important_problems_in_ml_today/\\\\nhttps://www.reddit.com/r/MachineLearning/comments/phnx8c/d_do_you_reproduce_a_method_for_sota_comparison/\\\\nhttps://www.reddit.com/r/MachineLearning/comments/pev04l/d_what_kind_of_hyperparameter_optimisation_do_you/\\\\n\\\\nTricks to improve Transformers\\\\nhttps://arxiv.org/pdf/2108.12284.pdf\\\\n\\\\nUnconstrained Scene Generation\\\\nhttps://apple.github.io/ml-gsn/\\\\n\\\\nCommon Objects in 3D dataset\\\\nhttps://ai.facebook.com/blog/common-objects-in-3d-dataset-for-3d-reconstruction\\\\n\\\\nWarpDrive Multi-Agent RL framework\\\\nhttps://blog.einstein.ai/warpdrive-fast-rl-on-a-gpu/\\\\n\\\\nBoosting Search Engines / MuZero Code\\\\nhttps://arxiv.org/abs/2109.00527\\\\nhttps://github.com/google-research/google-research/tree/master/muzero\\\\nhttps://github.com/google-research/language/tree/master/language/search_agents\\\\n\\\\nCan AI detect depression?\\\\nhttps://venturebeat.com/2021/08/31/ai-startups-claim-to-detect-depression-from-speech-but-the-jurys-out-on-their-accuracy/?utm_source=pocket_mylist\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1654",
    "uploadDate": "2021-09-10",
    "thumbnail_url": "https://i.ytimg.com/vi/eROy3BrqEVk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=0JlB9gufTw8",
    "title": "\u221e-former: Infinite Memory Transformer (aka Infty-Former / Infinity-Former, Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, inftyformer, infinityformer, infty former, infinity former, transformer, transformers, transformer linear, linear attention, unbounded memory transformer, continuous attention, attention mechanism, continuous attention mechanism, radial basis function, radial basis functions, ridge regression, long term memory, long term memory explained",
    "scraped_at": 1684582642.0837185,
    "genre": "Science",
    "views": "29547",
    "desc": "#inftyformer #infinityformer #transformer\\\\n\\\\nVanilla Transformers are excellent sequence models, but suffer from very harsch constraints on the length of the sequences they can process. Several attempts have been made to extend the Transformer\\'s sequence length, but few have successfully gone beyond a constant factor improvement. This paper presents a method, based on continuous attention mechanisms, to attend to an unbounded past sequence by representing the past as a continuous signal, rather than a sequence. This enables the Infty-Former to effectively enrich the current context with global information, which increases performance on long-range dependencies in sequence tasks. Further, the paper presents the concept of sticky memories, which highlight past events that are of particular importance and elevates their representation in the long-term memory.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:10 - Sponsor Spot: Weights \\\\u0026 Biases\\\\n3:35 - Problem Statement\\\\n8:00 - Continuous Attention Mechanism\\\\n16:25 - Unbounded Memory via concatenation \\\\u0026 contraction\\\\n18:05 - Does this make sense?\\\\n20:25 - How the Long-Term Memory is used in an attention layer\\\\n27:40 - Entire Architecture Recap\\\\n29:30 - Sticky Memories by Importance Sampling\\\\n31:25 - Commentary: Pros and cons of using heuristics\\\\n32:30 - Experiments \\\\u0026 Results\\\\n\\\\nPaper: https://arxiv.org/abs/2109.00301\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.me/start\\\\n\\\\nAbstract:\\\\nTransformers struggle when attending to long contexts, since the amount of computation grows with the context length, and therefore they cannot model long-term memories effectively. Several variations have been proposed to alleviate this problem, but they all have a finite memory capacity, being forced to drop old information. In this paper, we propose the \\xe2\\x88\\x9e-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \\xe2\\x88\\x9e-former\\'s attention complexity becomes independent of the context length. Thus, it is able to model arbitrarily long contexts and maintain \\\\\"",
    "lengthSeconds": "2197",
    "uploadDate": "2021-09-06",
    "thumbnail_url": "https://i.ytimg.com/vi/0JlB9gufTw8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=PFMtdR56Q4U",
    "title": "[ML News] Blind Chess AI Competition | Graph NNs for traffic | AI gift suggestions",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, rbc, reconaissence blind chess, blind chess neurips, chess neurips, chess neurips competition, ai chess, ai blind chess, nimblephysics, cerebras, cerebras cluster, cerebras wafer engine, cerebras large scale, ai gifts, ai gift, ai gift ideas, val kilmer voice, val kilmer ai voice, ai voice generated",
    "scraped_at": 1684582642.1747186,
    "genre": "Science",
    "views": "12156",
    "desc": "#mlnews #chess #neurips\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - Reconnaissance Blind Chess NeurIPS 2021 Competition\\\\n3:40 - Colab Pro no longer top priority for GPUs\\\\n4:45 - DeepMind uses Graph NNs to do traffic prediction\\\\n6:00 - Helpful Libraries: Isaac Gym, Differentiable Human, LVIS, BEHAVIOR\\\\n10:25 - Cerebras Wafer Scale Engine Cluster\\\\n12:15 - AI Voice Synthesis for Val Kilmer\\\\n14:20 - Can AI give thoughtful gifts?\\\\n\\\\nReferences:\\\\nReconnaissance Blind Chess NeurIPS 2021 Competition\\\\nhttps://rbc.jhuapl.edu/\\\\nhttps://rbc.jhuapl.edu/gameRules\\\\n\\\\nColab Pro no longer top priority\\\\nhttps://www.reddit.com/r/MachineLearning/comments/pdwxxz/d_colab_pro_no_longer_gives_you_a_v100_not_even_a/\\\\n\\\\nGoogle Maps ETA prediction using Graph Neural Networks\\\\nhttps://arxiv.org/pdf/2108.11482.pdf\\\\n\\\\nIsaac Gym: RL simulator on GPU\\\\nhttps://arxiv.org/abs/2108.10470\\\\nhttps://sites.google.com/view/isaacgym-nvidia\\\\nhttps://developer.nvidia.com/isaac-gym\\\\n\\\\nCerebras Cluster for massive AI models\\\\nhttps://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/?utm_source=pocket_mylist\\\\n\\\\nHelpful Libraries / Datasets\\\\nhttps://nimblephysics.org/docs/human-body.html?utm_source=pocket_mylist\\\\nhttps://www.lvisdataset.org/\\\\nhttps://arxiv.org/pdf/2108.03332.pdf\\\\n\\\\nAI Voice Reconstruction\\\\nhttps://www.washingtonpost.com/technology/2021/08/18/val-kilmer-ai-voice-cloning/\\\\n\\\\nCan AI make thoughtful gifts?\\\\nhttps://www.forbes.com/sites/anniebrown/2021/08/29/can-artificial-intelligence-give-thoughtful-gifts-an-exploration-of-the-possibilities-and-limits-of-ais-humanity/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1027",
    "uploadDate": "2021-09-03",
    "thumbnail_url": "https://i.ytimg.com/vi/PFMtdR56Q4U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-Kgxv64aG3o",
    "title": "ALiBi - Train Short, Test Long: Attention with linear biases enables input length extrapolation",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, alibi, transformer, position encoding, position embeddings, fair, google, attention is all you need, causal masking, causal attention, attentin matrix, attention matrix, vasvani, sinusoidal position encodings, learned position embeddings, train short test long, alibi position encodings, transformer position encodings, transformer position embeddings, transformer long sequences",
    "scraped_at": 1684582639.2990317,
    "genre": "Science",
    "views": "14818",
    "desc": "#alibi #transformers #attention\\\\n\\\\nTransformers are essentially set models that need additional inputs to make sense of sequence data. The most widespread additional inputs are position encodings or position embeddings, which add sequence index information in various forms. However, this has put a limit on the resulting model, which cannot run inference on sequences longer than it has been trained on, as it would encounter unfamiliar position encodings. ALiBi solves this by proposing simple linear fixed biases as position information, adding negligible overhead in time and memory, but surprisingly, the resulting model is able to handle inference on sequences many times as long as its training sequences.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - Position Encodings in Transformers\\\\n4:55 - Sinusoidial Position Encodings\\\\n11:50 - ALiBi Position Encodings\\\\n20:50 - How to choose the slope parameter\\\\n23:55 - Experimental Results\\\\n29:10 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://ofir.io/train_short_test_long.pdf\\\\nCode: https://github.com/ofirpress/attention_with_linear_biases\\\\n\\\\nAbstract:\\\\nSince the introduction of the transformer model by Vaswani et al. (2017), a fundamental question remains open: how to achieve extrapolation at inference time to longer sequences than seen during training? We first show that extrapolation can be improved by changing the position representation method, though we find that existing proposals do not allow efficient extrapolation. We introduce a simple and efficient method, Attention with Linear Biases (ALiBi), that allows for extrapolation. ALiBi does not add positional embeddings to the word embeddings; instead, it biases the query-key attention scores with a term that is proportional to their distance. We show that this method allows training a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048, 11% faster and using 11% less memory. ALiBi\\xe2\\x80\\x99s inductive bias towards recency allows it to outperform multiple strong position methods on the WikiText-103 benchmark. Finally, we provide analysis of ALiBi to understand why it leads to better performance.\\\\n\\\\nAuthors: Ofir Press, Noah A. Smith, Mike Lewis\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1882",
    "uploadDate": "2021-09-02",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=tunf2OunOKg",
    "title": "[ML News] Stanford HAI coins Foundation Models & High-profile case of plagiarism uncovered",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, plagiarism, research plagiarism, ml plagiarism, foundation models, tesla ai day, comma three, comma 3, george hotz, elon musk, stanford, stanford ai, stanford hai, resnet, momentum resnet, lux ai, neural mmo, lex fridman, dribnet, clip pixelart, pixelart, ai art, ai pixelart, deep learning tutorial, what is deep learning, introduction to deep learning, ml news, mlnews",
    "scraped_at": 1684582647.05272,
    "genre": "Science",
    "views": "28499",
    "desc": "#plagiarism #foundationmodels #tesla\\\\n\\\\nThe best place to keep up to date with the latest and greatest from the ML world!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Sponsor\\\\n3:15 - A high-profile case of plagiarism shocks the ML world\\\\n11:55 - Stanford AI releases paper on \\\\\"",
    "lengthSeconds": "1956",
    "uploadDate": "2021-08-27",
    "thumbnail_url": "https://i.ytimg.com/vi/tunf2OunOKg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qgUegkefocg",
    "title": "Fastformer: Additive Attention Can Be All You Need (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, attention mechanism, attention is all you need, fastformer, fast former, nlp, natural language processing, linear attention, linear transformer, query key value, additive attention, elementwise product, fast transformer, faster transformer, transformer memory, attention quadratic memory, fastformer explained",
    "scraped_at": 1684582642.2667484,
    "genre": "Science",
    "views": "26581",
    "desc": "#attention #transformer #fastformer\\\\n\\\\nTransformers have become the dominant model class in the last few years for large data, but their quadratic complexity in terms of sequence length has plagued them until now. Fastformer claims to be the fastest and most performant linear attention variant, able to consume long contexts at once. This is achieved by a combination of additive attention and elementwise products. While initial results look promising, I have my reservations...\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n2:15 - Fastformer description\\\\n5:20 - Baseline: Classic Attention\\\\n10:00 - Fastformer architecture\\\\n12:50 - Additive Attention\\\\n18:05 - Query-Key element-wise multiplication\\\\n21:35 - Redundant modules in Fastformer\\\\n25:00 - Problems with the architecture\\\\n27:30 - Is this even attention?\\\\n32:20 - Experimental Results\\\\n34:50 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2108.09084\\\\n\\\\nAbstract:\\\\nTransformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.\\\\n\\\\nAuthors: Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2130",
    "uploadDate": "2021-08-26",
    "thumbnail_url": "https://i.ytimg.com/vi/qgUegkefocg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=nQDZmf2Yb9k",
    "title": "PonderNet: Learning to Ponder (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, pondernet, deepmind, pondernet learning to ponder, deepmind pondernet, pondernet explained, dynamic computation, deep learning classic algorithms, halting probability, deep learning recurrent computation, dynamic recurrent network, broader impact, deep network learning to stop",
    "scraped_at": 1684582639.3970313,
    "genre": "Science",
    "views": "21832",
    "desc": "#pondernet #deepmind #machinelearning\\\\n\\\\nHumans don\\'t spend the same amount of mental effort on all problems equally. Instead, we respond quickly to easy tasks, and we take our time to deliberate hard tasks. DeepMind\\'s PonderNet attempts to achieve the same by dynamically deciding how many computation steps to allocate to any single input sample. This is done via a recurrent architecture and a trainable function that computes a halting probability. The resulting model performs well in dynamic computation tasks and is surprisingly robust to different hyperparameter settings.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:30 - Problem Statement\\\\n8:00 - Probabilistic formulation of dynamic halting\\\\n14:40 - Training via unrolling\\\\n22:30 - Loss function and regularization of the halting distribution\\\\n27:35 - Experimental Results\\\\n37:10 - Sensitivity to hyperparameter choice\\\\n41:15 - Discussion, Conclusion, Broader Impact\\\\n\\\\nPaper: https://arxiv.org/abs/2107.05407\\\\n\\\\nAbstract:\\\\nIn standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1\\\\n\\\\nAuthors: Andrea Banino, Jan Balaguer, Charles Blundell\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2659",
    "uploadDate": "2021-08-23",
    "thumbnail_url": "https://i.ytimg.com/vi/nQDZmf2Yb9k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6MUpWGeGMxs",
    "title": "NeuralHash is BROKEN - How to evade Apple's detection & craft hash collisions (w/ Open Source Code)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, neuralhash, neural hash, neural hash collision, neuralhash collision, neuralhash broken, break neuralhash, evade neuralhash, apple detection, icloud neuralhash, adversarial examples, neuralhash adversarial example, apple hash collision, how to neuralhash",
    "scraped_at": 1684582639.482052,
    "genre": "Science",
    "views": "10612",
    "desc": "#apple #icloud #neuralhash\\\\n\\\\nSend your Apple fanboy friends to prison with this one simple trick ;) We break Apple\\'s NeuralHash algorithm used to detect CSAM for iCloud photos. I show how it\\'s possible to craft arbitrary hash collisions from any source / target image pair using an adversarial example attack. This can be used for many purposes, such as evading detection, or forging false positives, triggering manual reviews.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:30 - Forced Hash Collisions via Adversarial Attacks\\\\n2:30 - My Successful Attack\\\\n5:40 - Results\\\\n7:15 - Discussion\\\\n\\\\nDISCLAIMER: This is for demonstration and educational purposes only. This is not an endorsement of illegal activity or circumvention of law.\\\\n\\\\nCode: https://github.com/yk/neural_hash_collision\\\\nExtract Model: https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX\\\\nMy Video on NeuralHash: https://youtu.be/z15JLtAuwVI\\\\n\\\\nADDENDUM:\\\\nThe application of framing people is a bit more intricate than I point out here. Apple has commented that there would be a second perceptual hashing scheme server-side, i.e. the model would not be released, which makes forging false positives harder. Nevertheless, evading the system remains fairly trivial.\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "495",
    "uploadDate": "2021-08-19",
    "thumbnail_url": "https://i.ytimg.com/vi/6MUpWGeGMxs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gu5UM99qaVc",
    "title": "[ML News] Nvidia renders CEO | Jurassic-1 larger than GPT-3 | Tortured Phrases reveal Plagiarism",
    "tags": "deep learning, machine learning, neural networks, ai, artificial intelligence, paper, introduction to deep learning, what is deep learning, deep learning tutorial, nvidia, jensen huang, nvidia keynote, nvidia keynote rendered, jensen huang rendered, jensen huang keynote, jurassic",
    "scraped_at": 1684582645.7827187,
    "genre": "Science",
    "views": "16892",
    "desc": "#mlnews #nvidia #openai\\\\n\\\\nAn in-depth look over what\\'s going on in the world of Machine Learning and Artificial intelligence. Subscribe now and make Monday the best day of the week!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n3:00 - Nvidia\\'s CEO was rendered during Keynote\\\\n5:00 - AI21 Labs releases Jurassic-1 language model\\\\n7:00 - Tortured Phrases reveal plagiarism\\\\n10:05 - Cortical neurons are computationally complex\\\\n11:55 - OpenAI Codex Update \\\\u0026 Challenge\\\\n13:30 - Automated drug abuse prevention gone wrong\\\\n17:55 - Rapid News Questions\\\\n18:40 - SoundStream learned neural audio codec\\\\n19:40 - RoboMimic framework for robotics research\\\\n20:05 - Droidlet framework for agent training\\\\n20:40 - Unidentified Video Objects Benchmark\\\\n21:45 - Grammatical Error Correction Dataset\\\\n22:15 - ColabPro Plus available\\\\n23:05 - BigBench Self-Awareness benchmark for language models\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.ai\\\\n\\\\nReferences:\\\\nNVIDIA renders CEO during keynote\\\\nhttps://www.vice.com/en/article/88nbpa/nvidia-reveals-its-ceo-was-computer-generated-in-keynote-speech\\\\nhttps://blogs.nvidia.com/blog/2021/08/11/omniverse-making-of-gtc/\\\\nhttps://www.youtube.com/watch?v=eAn_oiZwUXA\\\\u0026t=3760s\\\\n\\\\nAI21 Labs announces Jurassic-1 model\\\\nhttps://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1\\\\nhttps://studio.ai21.com/\\\\nhttps://twitter.com/yoavgo/status/1425584087016906752\\\\n\\\\nTortured Phrases point to plagiarism\\\\nhttps://www.nature.com/articles/d41586-021-02134-0\\\\n\\\\nReal Neurons are insanely complex\\\\nhttps://www.sciencedirect.com/science/article/pii/S0896627321005018?dgcid=coauthor\\\\n\\\\nOpenAI Codex Challenge \\\\u0026 Update\\\\nhttps://challenge.openai.com/\\\\nhttps://challenge.openai.com/codex/leaderboard\\\\nhttps://openai.com/blog/openai-codex/#helloworld\\\\n\\\\nAutomated drug abuse prevention goes wrong\\\\nhttps://www.wired.com/story/opioid-drug-addiction-algorithm-chronic-pain/\\\\n\\\\nNews Questions\\\\nhttps://www.imeche.org/news/news-article/feature-will-artificial-intelligence-replace-engineers\\\\nhttps://newseu.cgtn.com/news/2021-08-13/Can-artificial-intelligence-detect-COVID-19-from-the-sound-of-a-cough--12HnkO6lxMA/index.html\\\\nhttps://www.growingproduce.com/citrus/can-artificial-intelligence-predict-citrus-yields-better-than-humans/\\\\nhttps://www.cioreview.com/news/artificial-intelligence-%C3%A2%E2%82%AC%E2%80%9C-the-boon-or-the-bane-nid-34265-cid-145.html\\\\n\\\\nSoundStream Neural Audio Codec\\\\nhttps://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html\\\\n\\\\nRoboMimic Framework\\\\nhttps://arise-initiative.github.io/robomimic-web/\\\\n\\\\nDroidlet Framework\\\\nhttps://ai.facebook.com/blog/droidlet-a-one-stop-shop-for-modularly-building-intelligent-agents/\\\\n\\\\nUnidentified Video Objects Benchmark\\\\nhttps://ai.facebook.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation/\\\\n\\\\nGrammatical Error Correction Dataset\\\\nhttps://ai.googleblog.com/2021/08/the-c4200m-synthetic-dataset-for.html\\\\n\\\\nColab Pro Plus is \\\\\"",
    "lengthSeconds": "1598",
    "uploadDate": "2021-08-18",
    "thumbnail_url": "https://i.ytimg.com/vi/gu5UM99qaVc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=z15JLtAuwVI",
    "title": "How Apple scans your phone (and how to evade it) - NeuralHash CSAM Detection Algorithm Explained",
    "tags": "neural networks, artificial intelligence, what is deep learning, introduction to deep learning, deep learning tutorial, neuralhash, neural hash, apple privacy, icloud privacy, icloud encryption, icloud illegal, apple illegal, apple scan, apple scan illegal material, icloud illegal material, blinding step, hash function, private set intersection, adversarial attack, threshold secret sharing, icloud, csam, csam apple, csam apple scanning, csam detection, explained",
    "scraped_at": 1684582639.5720582,
    "genre": "Science",
    "views": "17254",
    "desc": "#apple #icloud #privacy\\\\n\\\\nApple recently announced scanning all images uploaded to iCloud for CSAM (child abuse material), and that this scan would happen locally on users\\' phones. We take a look at the technical report and explore how the system works in detail, how it is designed to preserve user privacy, and what weak points it still has.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n3:05 - System Requirements\\\\n9:15 - System Overview\\\\n14:00 - NeuralHash\\\\n20:45 - Private Set Intersection\\\\n31:15 - Threshold Secret Sharing\\\\n35:25 - Synthetic Match Vouchers\\\\n38:20 - Problem 1: Who controls the database?\\\\n42:40 - Problem 2: Adversarial Attacks\\\\n49:40 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf\\\\nML News Episode about CSAM: https://youtu.be/gFkBqD2hbnU\\\\n\\\\nAbstract:\\\\nCSAM Detection enables Apple to accurately identify and report iCloud users who store known Child Sexual Abuse Material (CSAM) in their iCloud Photos accounts. Apple servers flag accounts exceeding a threshold number of images that match a known database of CSAM image hashes so that Apple can provide relevant information to the National Center for Missing and Exploited Children (NCMEC). This process is secure, and is expressly designed to preserve user privacy.\\\\nCSAM Detection provides these privacy and security assurances:\\\\n\\xe2\\x80\\xa2 Apple does not learn anything about images that do not match the known CSAM database.\\\\n\\xe2\\x80\\xa2 Apple can\\xe2\\x80\\x99t access metadata or visual derivatives for matched CSAM images until a threshold of matches is exceeded for an iCloud Photos account.\\\\n\\xe2\\x80\\xa2 The risk of the system incorrectly flagging an account is extremely low. In addition, Apple manually reviews all reports made to NCMEC to ensure reporting accuracy.\\\\n\\xe2\\x80\\xa2 Users can\\xe2\\x80\\x99t access or view the database of known CSAM images.\\\\n\\xe2\\x80\\xa2 Users can\\xe2\\x80\\x99t identify which images were flagged as CSAM by the system.\\\\nFor detailed information about the cryptographic protocol and security proofs that the CSAM Detection process uses, see The Apple PSI System.\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3038",
    "uploadDate": "2021-08-16",
    "thumbnail_url": "https://i.ytimg.com/vi/z15JLtAuwVI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gFkBqD2hbnU",
    "title": "[ML NEWS] Apple scans your phone | Master Faces beat face recognition | WALL-E is real",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, ml news, machine learning news, kilcher news, mlnews, apple, privacy, european union, lsh, locality sensitive hashing, on device, adversarial attack, database, hash collision, wall",
    "scraped_at": 1684582639.6680603,
    "genre": "Science",
    "views": "10601",
    "desc": "#mlnews #apple #nolamarck\\\\n\\\\nYour update on the latest news in the AI and Machine Learning world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - Sponsor: Weights \\\\u0026 Biases\\\\n3:30 - Apple to scan iDevices for illegal content\\\\n14:10 - EU approves chatcontrol\\\\n15:20 - Machine Learning FAQ book\\\\n17:40 - TimeDial \\\\u0026 Disfl-QA Conversation Datasets\\\\n20:30 - VoxPopuli Speech Dataset\\\\n21:00 - Google Tensor chip coming to Pixel 6\\\\n21:30 - Pentagon uses AI to predict events\\\\n23:10 - Sketch your own GAN\\\\n24:45 - Can a Fruit Fly learn Word Embeddings?\\\\n26:00 - Master Faces beat facial recognition system\\\\n27:25 - PyTorch profiler 1.9\\\\n27:55 - 0 A.D. gets reinforcement learning interface\\\\n28:40 - BeatBot cleans up cigarette butts on the beach\\\\n\\\\nSponsor: Weights \\\\u0026 Biases\\\\nhttps://wandb.ai\\\\n\\\\nReferences:\\\\nApple to scan iDevices for illegal content\\\\nhttps://techcrunch.com/2021/08/05/apple-icloud-photos-scanning/\\\\nhttp://tylerneylon.com/a/lsh1/\\\\n\\\\nEU approves chatcontrol\\\\nhttps://european-pirateparty.eu/parliament-approves-chatcontrol/\\\\n\\\\nMachine Learning FAQ book\\\\nhttps://rentruewang.github.io/learning-machine/layers/emb/emb.html\\\\n\\\\nTimeDial \\\\u0026 Disfl-QA: New datasets for conversational NLP\\\\nhttps://ai.googleblog.com/2021/08/two-new-datasets-for-conversational-nlp.html\\\\n\\\\nVoxPopuli: Giant partially labeled speech dataset\\\\nhttps://github.com/facebookresearch/voxpopuli\\\\n\\\\nGoogle\\'s Tensor chip coming to Pixel 6\\\\nhttps://blog.google/products/pixel/google-tensor-debuts-new-pixel-6-fall/\\\\n\\\\nPentagon uses AI for predicting relevant events in advance\\\\nhttps://www.engadget.com/pentagon-ai-predicts-days-in-advance-135509604.html?utm_source=pocket_mylist\\\\n\\\\nSketch Your Own GAN\\\\nhttps://peterwang512.github.io/GANSketching/\\\\n\\\\nCan a fruit fly learn word embeddings?\\\\nhttps://arxiv.org/pdf/2101.06887.pdf\\\\n\\\\nMaster Faces for attacking facial recognition systems\\\\nhttps://arxiv.org/pdf/2108.01077.pdf\\\\n\\\\nPyTorch Profiler v1.9\\\\nhttps://www.marktechpost.com/2021/08/06/pytorch-releases-pytorch-profiler-v1-9-with-new-features-to-help-diagnose-and-fix-machine-learning-performance-issues/\\\\n\\\\n0 A.D. adds Reinforcement Learning interface\\\\nhttps://play0ad.com/media/screenshots/\\\\nhttps://trac.wildfiregames.com/wiki/GettingStartedReinforcementLearning\\\\n\\\\nBeachBot cleans up cigarette butts on the beach\\\\nhttps://news.yahoo.com/beachbot-rover-uses-artificial-intelligence-130031052.html\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1828",
    "uploadDate": "2021-08-13",
    "thumbnail_url": "https://i.ytimg.com/vi/gFkBqD2hbnU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=SPOqoI0zOPQ",
    "title": "[ML News] AI-generated patent approved | Germany gets an analog to OpenAI | ML cheats video games",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, what is deep learning, introduction to deep learning, ai inventor, dabus, thaler, steve thaler, stephen thaler, ai patent, creativity machine, aleph alpha, openai, german openai, aleph alpha openai, german aleph alpha, machine learning game cheat, ai cheat video games, machine learning video games, deepmind, wordcraft, neural flame",
    "scraped_at": 1684582647.1515052,
    "genre": "Science",
    "views": "16467",
    "desc": "#mlnews #dabus #alephalpha\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Sponsor: Weights \\\\u0026 Biases\\\\n3:45 - AI legally recognized as patent inventor\\\\n8:35 - Alpeh Alpha raises USD 27Mio to build European OpenAI\\\\n10:20 - AMP advances AI aided recycling\\\\n11:20 - DeepMind builds XLand RL environment\\\\n13:15 - Cognitive Behavioral Therapy as an app\\\\n16:15 - Wordcraft interactive AI text editor\\\\n17:05 - ML used to cheat in console games\\\\n18:10 - Google\\'s OpenBuildings Dataset\\\\n20:00 - Most ML COVID tools are flawed\\\\n21:10 - DALL-E mini released\\\\n21:55 - Helpful Libraries\\\\n25:20 - FSF funds papers discussing CoPilot\\\\n\\\\n\\\\nSPONSOR: Weights \\\\u0026 Biases\\\\nhttps://wandb.ai\\\\n\\\\nReferences:\\\\nAI legally recognized as patent inventor\\\\nhttps://www.globallegalpost.com/news/south-africa-issues-worlds-first-patent-listing-ai-as-inventor-161068982\\\\nhttps://www.abc.net.au/news/2021-08-01/historic-decision-allows-ai-to-be-recognised-as-an-inventor/100339264\\\\nhttps://artificialinventor.com/frequently-asked-questions/\\\\nhttps://artificialinventor.com/dabus/\\\\nhttps://www.worldscientific.com/doi/abs/10.1142/S2705078521500053\\\\nhttps://www.worldscientific.com/doi/epdf/10.1142/S2705078521500053\\\\nhttps://imagination-engines.com/dabus.html\\\\nhttps://imagination-engines.com/about.html\\\\nhttps://www.nextbigfuture.com/2016/03/sander-olson-interviewed-dr-stephen.html\\\\nhttps://www.actiac.org/system/files/Dawn19%20-%20Dr.%20Thaler.pdf\\\\n\\\\nAlpeh Alpha raises USD 27Mio to build European OpenAI\\\\nhttps://techcrunch.com/2021/07/27/german-startup-aleph-alpha-raises-27m-series-a-round-to-build-europes-openai/\\\\n\\\\nAMP advances AI aided recycling\\\\nhttps://www.robotics247.com/article/amp_robotics_marks_data_pick_rate_milestones_automated_recycling\\\\n\\\\nDeepMind builds XLand RL environment\\\\nhttps://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\\\\nhttps://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents\\\\n\\\\nCognitive Behavioral Therapy as an app\\\\nhttps://www.nytimes.com/2021/06/01/health/artificial-intelligence-therapy-woebot.html\\\\n\\\\nWordcraft interactive AI text editor\\\\nhttps://syncedreview.com/2021/07/21/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-66/\\\\nhttps://arxiv.org/abs/2107.07430\\\\nhttps://www.youtube.com/watch?v=9p4mfA0Fyd8\\\\n\\\\nML used to cheat in console games\\\\nhttps://au.pcmag.com/games/88121/machine-learning-is-now-being-used-to-cheat-in-multiplayer-games\\\\n\\\\nGoogle\\'s OpenBuildings Dataset\\\\nhttps://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html\\\\nhttps://sites.research.google/open-buildings/\\\\n\\\\nMost ML COVID tools are flawed\\\\nhttps://www.technologyreview.com/2021/07/30/1030329/machine-learning-ai-failed-covid-hospital-diagnosis-pandemic/\\\\n\\\\nDALL-E mini released\\\\nhttps://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA\\\\nhttps://huggingface.co/spaces/flax-community/dalle-mini\\\\n\\\\nHelpful Libraries\\\\nhttps://www.openai.com/blog/triton/\\\\nhttps://github.com/openai/triton\\\\nhttps://github.com/microsoft/FLAML\\\\nhttps://github.com/clip-italian/clip-italian\\\\nhttps://deepmind.com/research/open-source/melting-pot\\\\nhttps://github.com/deepmind/meltingpot\\\\nhttps://www.roboti.us/license.html\\\\nhttps://github.com/openai/gym/issues/2259\\\\nhttps://github.com/jkterry1\\\\n\\\\nFSF funds papers discussing CoPilot\\\\nhttps://www.fsf.org/blogs/licensing/fsf-funded-call-for-white-papers-on-philosophical-and-legal-questions-around-copilot\\\\nhttps://www.gnu.org/philosophy/who-does-that-server-really-serve.en.html\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1651",
    "uploadDate": "2021-08-06",
    "thumbnail_url": "https://i.ytimg.com/vi/SPOqoI0zOPQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4xklF7PZ-BY",
    "title": "[ML News] MMO Game destroys GPUs | OpenAI quits Robotics | Today w/ guest host Sanyam Bhutani",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, introduction to deep learning, what is deep learning, ml news, machine learning news, amazon mmo, game breaks gpu, google intrinsic, alphabet intrinsic, openai robotics, openai robotics team, chai time, chai time data science, chai time data science podcast, sanyam, sanyam bhutani, saynam ml news, nasa, nasa ai, ai common sense, common sense dataset, deep learning news",
    "scraped_at": 1684582647.8715136,
    "genre": "Science",
    "views": "9499",
    "desc": "#chai #mlnews #nvidia\\\\n\\\\nFollow Saynam here:\\\\nYouTube: https://www.youtube.com/c/ChaiTimeDataScience\\\\nTwitter: https://twitter.com/bhutanisanyam1\\\\nApple Podcasts: https://podcasts.apple.com/us/podcast/chai-time-data-science/id1473685440?uo=4\\\\nLinkedIn: https://www.linkedin.com/in/sanyambhutani/\\\\nSpotify: https://open.spotify.com/show/7IbEWJjeimwddhOZqWe0G1\\\\nAnchor.fm RSS: https://anchor.fm/s/c19772c/podcast/rss\\\\n\\\\nOutline:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:30 - Amazon\\'s MMO may destroy gaming GPUs\\\\n2:40 - OpenAI pivots away from Robotics\\\\n3:35 - Google parent Alphabet launches Intrinsic\\\\n4:55 - AI learns how vegetables taste\\\\n5:55 - NASA uses AI to better understand the sun\\\\n6:50 - Man used AI to bring back deceased fiancee\\\\n7:45 - Robot collision sparks warehouse fire\\\\n8:20 - AI deduces patients\\' racial identities from medical records\\\\n9:40 - AlphaFold protein structure database\\\\n10:15 - ICCV BEHAVIOR challenge\\\\n11:05 - IBM, MIT, Harvard release Common Sense database\\\\n11:35 - High quality image generation using diffusion models\\\\n12:50 - Conclusion\\\\n\\\\nReferences:\\\\n1 Amazon\\xe2\\x80\\x99s new MMO may be bricking Nvidia 3090s\\\\nhttps://www.theverge.com/2021/7/21/22587616/amazon-games-new-world-nvidia-rtx-3090-bricked-evga-closed-beta\\\\nhttps://www.youtube.com/watch?v=KLyNFrKyG74\\\\n\\\\n2 Open AI pivotes from Robots\\\\nhttps://venturebeat.com/2021/07/23/ai-weekly-openais-pivot-from-robotics-acknowledges-the-power-of-simulation/\\\\n\\\\n3 Google parent Alphabet launches Intrinsic: a new company to build software for industrial robots\\\\nhttps://www.theverge.com/2021/7/23/22590109/google-intrinsic-industrial-robotics-company-software\\\\nIntroducing Intrinsic\\\\nhttps://blog.x.company/introducing-intrinsic-1cf35b87651\\\\nhttps://x.company/projects/intrinsic/\\\\nhttps://www.forbes.com/sites/jenniferhicks/2021/07/20/ai-is-learning-to-understand-how-vegetables-taste/?sh=73e6f646e1b2\\\\n\\\\n4 Artificial Intelligence Helps Improve NASA\\xe2\\x80\\x99s Eyes on the Sun\\\\nhttps://www.nasa.gov/feature/goddard/2021/artificial-intelligence-helps-improve-nasa-s-eyes-on-the-sun\\\\n\\\\n5 A man used AI to bring back his deceased fianc\\xc3\\xa9. But the creators of the tech warn it could be dangerous\\\\nhttps://www.businessinsider.co.za/man-used-ai-to-talk-to-late-fiance-experts-warn-tech-could-be-misused-2021-7\\\\n\\\\n6 Robot collision at Ocado warehouse near London sparks fire, delaying customer orders https://www.theverge.com/2021/7/18/22582454/robot-collision-ocado-warehouse-england-fire-delayed-orders\\\\n\\\\n10 Reading Race: AI Recognizes Patient\\xe2\\x80\\x99s Racial Identity In Medical Images\\\\nhttps://arxiv.org/pdf/2107.10356.pdf\\\\n\\\\n11 AlphaFold Protein Structure Database\\\\nhttps://alphafold.ebi.ac.uk\\\\nhttps://www.theverge.com/2021/7/22/22586578/deepmind-alphafold-ai-protein-folding-human-proteome-released-for-free\\\\n\\\\n12 Behavior Challenge\\\\nhttp://svl.stanford.edu/behavior/challenge.html\\\\n\\\\n13 Researchers from IBM, MIT and Harvard Announced The Release Of DARPA \\xe2\\x80\\x9cCommon Sense AI\\xe2\\x80\\x9d Dataset Along With Two Machine Learning Models At ICML 2021\\\\nhttps://www.marktechpost.com/2021/07/20/researchers-from-ibm-mit-and-harvard-announced-the-release-of-its-darpa-common-sense-ai-dataset-along-with-two-machine-learning-models-at-icml-2021/\\\\nhttps://www.reddit.com/r/MachineLearning/comments/onxw90/n_researchers_from_ibm_mit_and_harvard_announced/\\\\n\\\\n14 Google uses diffusion model for image generation\\\\nhttps://www.reddit.com/r/MachineLearning/comments/ors7ht/r_using_the_diffusion_model_google_ai_is_able_to/\\\\nhttps://www.reddit.com/r/MachineLearning/comments/oo4cla/n_nvidia_launches_tensorrt_8_that_improves_ai/\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "800",
    "uploadDate": "2021-08-02",
    "thumbnail_url": "https://i.ytimg.com/vi/4xklF7PZ"
  },
  {
    "link": "watch?v=-cT-2xvaeks",
    "title": "[ML News] Facebook AI adapting robots | Baidu autonomous excavators | Happy Birthday EleutherAI",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, eleuther, eleutherai, eleuther gpt neo, gpt neo, gpt 3, open source gpt 3, boston dynamics, boston dynamics robot, baidu, autonomous excavator, robot excavator, elon musk, elon musk fsd, tesla fsd, clip art, vqgan clip, dalle clip, openai clip, vqgan art, ml news, machine learning news, minecraft ai, minerl, neurips challenge, expert reviewer, deep learning news",
    "scraped_at": 1684582642.3647187,
    "genre": "Science",
    "views": "12724",
    "desc": "A look into the happenings of the Machine Learning world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:25 - Facebook AI trains rapidly adapting robots\\\\n3:05 - Baidu presents autonomous excavator system\\\\n4:45 - EleutherAI turns 1\\\\n6:05 - Elon Musk says FSD harder than expected\\\\n8:10 - AI interview tools still fall short\\\\n11:10 - RunwayML AI-powered cloud video editor\\\\n11:55 - MineRL BASALT competition to learn from human feedback\\\\n13:15 - The Myth of the Expert Reviewer\\\\n15:55 - NVIDIA unveils Cambridge-1 supercomputer\\\\n17:10 - CLIP art sees rapid improvements\\\\n19:00 - AI demystifies boiling\\\\n21:20 - AI avatars for easier language learning\\\\n23:20 - Outro\\\\n\\\\nReferences:\\\\nFacebook AI trains rapidly adapting robots\\\\nhttps://ai.facebook.com/blog/ai-now-enables-robots-to-adapt-rapidly-to-changing-real-world-conditions/\\\\nhttps://ashish-kmr.github.io/rma-legged-robots/\\\\n\\\\nBaidu presents autonomous excavator system\\\\nhttp://research.baidu.com/Blog/index-view?id=159\\\\nhttps://www.youtube.com/watch?v=KFcNf_k0E_M\\\\n\\\\nEleutherAI turns 1\\\\nhttps://blog.eleuther.ai/year-one/\\\\n\\\\nElon Musk says FSD is harder than expected\\\\nhttps://www.theverge.com/2021/7/5/22563751/tesla-elon-musk-full-self-driving-admission-autopilot-crash\\\\n\\\\nAI interview tools still fall short\\\\nhttps://www.technologyreview.com/2021/07/07/1027916/we-tested-ai-interview-tools/\\\\n\\\\nRunwayML AI-powered cloud video editor\\\\nhttps://runwayml.com/\\\\n\\\\nMineRL BASALT competition to learn from human feedback\\\\nhttps://www.aicrowd.com/challenges/neurips-2021-minerl-basalt-competition\\\\n\\\\nThe Myth of the Expert Reviewer\\\\nhttps://parameterfree.com/2021/07/06/the-myth-of-the-expert-reviewer/\\\\n\\\\nNVIDIA unveils Cambridge-1 supercomputer\\\\nhttps://www.nvidia.com/en-us/industries/healthcare-life-sciences/cambridge-1/\\\\nhttps://nvidianews.nvidia.com/news/nvidia-launches-uks-most-powerful-supercomputer-for-research-in-ai-and-healthcare\\\\n\\\\nCLIP art sees rapid improvements\\\\nhttps://ml.berkeley.edu/blog/posts/clip-art/\\\\n\\\\nAI demystifies boiling\\\\nhttps://news.mit.edu/2021/infrared-cameras-artificial-intelligence-provide-insight-into-boiling-0707\\\\n\\\\nAI avatars for easier language learning\\\\nhttps://www.forbes.com/sites/petergreene/2021/07/07/language-lessons-from-ai/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1419",
    "uploadDate": "2021-07-15",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=PuOASKpiThY",
    "title": "I'm taking a break",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582639.755031,
    "genre": "Science",
    "views": "11569",
    "desc": "I\\'ll be back, don\\'t worry :)\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "74",
    "uploadDate": "2021-07-11",
    "thumbnail_url": "https://i.ytimg.com/vi/PuOASKpiThY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=TrLrBL1U8z0",
    "title": "[ML News] GitHub Copilot - Copyright, GPL, Patents & more | Brickit LEGO app | Distill goes on break",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, copilot, github copilot, github copilot copyright, github gpl, github copilot gpl, copilot copyright, copilot gpl, openai gpl, openai copilot, openai codex, github copilot codex, github automatic code, copilot public data, copilot code generation, distill pub, ml news, machine learning news, deep learning news, github copilot news, brickit, lego brickit, brickit app",
    "scraped_at": 1684582639.854031,
    "genre": "Science",
    "views": "19282",
    "desc": "#copilot #copyright #gpl\\\\n\\\\nGitHub and OpenAI release Copilot, an AI-powered code autocomplete system that can generate entire functions, classes, and modules from mere definitions and docstrings. Copilot was trained on all public GitHub repositories, and this has a lot of people upset about questions on copyright, code licenses, social obligations, and how much you can profit from other people\\'s work. I give my opinions on the issue in relation to copyright law, the GPL license, and terms of service. Further, we discuss the Brickit app to organize your LEGOs, Distill going on a break, and much more.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - GitHub Copilot\\\\n6:55 - My opinion on Copilot \\\\u0026 Copyright\\\\n17:25 - Facebook AI image similarity challenge\\\\n18:00 - Brickit app scans your LEGOs and suggests builds\\\\n18:40 - Distill journal goes on break\\\\n19:50 - Amazon uses algorithms to hire \\\\u0026 fire Flex drivers\\\\n23:20 - Helpful Libraries: TF Decision Forests, Habitat, Falken, Brax\\\\n24:20 - AI-generated papers give science a hard time\\\\n\\\\nReferences:\\\\nGitHub Copilot: AI pair programmer\\\\nhttps://twitter.com/gdb/status/1409890354132750336\\\\nhttps://twitter.com/rickhanlonii/status/1410020702028193798\\\\nhttps://copilot.github.com/\\\\nhttps://docs.github.com/en/github/copilot/research-recitation\\\\nhttps://docs.github.com/en/github/site-policy/github-terms-of-service#d-user-generated-content\\\\nhttps://tldrlegal.com/license/gnu-general-public-license-v3-(gpl-3)#fulltext\\\\nhttps://www.gnu.org/licenses/gpl-faq.en.html#CanIUseGPLToolsForNF\\\\nhttps://www.legalzoom.com/knowledge/copyright/topic/copyright-protection-scope\\\\nhttps://en.wikipedia.org/wiki/Derivative_work\\\\nhttps://twitter.com/giffmana/status/1410320795222654981\\\\nhttps://twitter.com/search?q=copilot\\\\u0026src=typed_query\\\\u0026f=image\\\\n\\\\nFacebook AI launches image similarity challenge\\\\nhttps://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/\\\\n\\\\nBrickit app sorts your LEGOs\\\\nhttps://brickit.app/?ref=producthunt\\\\u0026s=09\\\\nhttps://petapixel.com/2021/07/01/brickits-ai-camera-scans-your-lego-to-suggest-things-you-can-build/\\\\n\\\\nDistill goes on break\\\\nhttps://distill.pub/2021/distill-hiatus/\\\\n\\\\nAmazon uses Algorithms to fire Flex drivers\\\\nhttps://www.engadget.com/amazon-algorithms-fire-flex-delivery-drivers-055959081.html?guccounter=1\\\\n\\\\nTensorFlow decision forests\\\\nhttps://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html\\\\n\\\\nFacebook AI habitat 2.0\\\\nhttps://ai.facebook.com/blog/habitat-20-training-home-assistant-robots-with-faster-simulation-and-new-benchmarks/\\\\n\\\\nGoogle Falken trains game-playing agents\\\\nhttps://ai.googleblog.com/2021/06/quickly-training-game-playing-agents.html\\\\nhttps://github.com/google-research/falken\\\\n\\\\nGoogle Brax: differentiable physics simulator\\\\nhttps://github.com/google/brax\\\\nhttps://arxiv.org/pdf/2106.13281.pdf\\\\n\\\\nFake science is getting faker\\\\nhttps://thenextweb.com/news/fake-science-faker-thanks-ai-syndication\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1621",
    "uploadDate": "2021-07-08",
    "thumbnail_url": "https://i.ytimg.com/vi/TrLrBL1U8z0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=9MJTeOaSMTk",
    "title": "Self-driving from VISION ONLY - Tesla's self-driving progress by Andrej Karpathy (Talk Analysis)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, tesla, elon musk, karpathy, full self driving, tesla fsd, karpathy talk, tesla talk, tesla computer vision, how does tesla work, computer vision driving, lidar, radar, autonomous car, autonomous car tesla, autonomous driving, driverless car, driverless car tesla, tesla machine learning, tesla self driving, tesla ai, tesla research",
    "scraped_at": 1684582639.948032,
    "genre": "Science",
    "views": "33352",
    "desc": "#tesla #selfdriving #karpathy\\\\n\\\\nTesla is pushing the state-of-the-art in full self-driving, and interestingly, they explicitly switch from having multiple different sensors to a vision-only system. We discuss the highlights of Andrej Karpathy\\'s talk about Tesla\\'s FSD system, how to label petabytes of data, how to sample edge-cases, how to train a neural network that has to work in real-time, and why moving to having only cameras is superior to multi-sensor approaches.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:55 - Current Auto-Breaking system\\\\n3:20 - Full Self-Driving from vision only\\\\n4:55 - Auto-Labelling for collecting data\\\\n8:45 - How to get diverse data from edge-cases\\\\n12:15 - Neural network architecture\\\\n16:05 - Tesla\\'s in-house supercomputer\\\\n17:00 - Owning the whole pipeline\\\\n18:20 - Example results from vision only\\\\n23:10 - Conclusion \\\\u0026 Comments\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1427",
    "uploadDate": "2021-07-03",
    "thumbnail_url": "https://i.ytimg.com/vi/9MJTeOaSMTk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tDk10VTHwNo",
    "title": "[ML News] CVPR bans social media paper promotion | AI restores Rembrandt | GPU prices down",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, ml news, cvpr, social media, research discussion, peer review, bias, toxic language model, stochastic parrots, rembrandt painting, painting restoration, convolutional neural networks, nvidia, alias",
    "scraped_at": 1684582642.459719,
    "genre": "Science",
    "views": "13759",
    "desc": "#cvpr #socialmedia #machinelearning\\\\n\\\\nIn this week\\'s ML news we look at CVPR\\'s controversial action to ban paper promotions on social media during the review phase, among other things!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n0:25 - CVPR bans social media paper discussions\\\\n5:10 - WalMart uses AI to suggest substitutions\\\\n6:05 - NVIDIA releases Alias-Free GAN\\\\n7:30 - Confession Video in Myanmar possibly a DeepFake\\\\n8:50 - AI restores Rembrandt painting\\\\n10:40 - AI for healthcare not problem-free yet\\\\n11:50 - ML interviews book\\\\n12:15 - NVIDIA canvas turns sketches into paintings\\\\n13:00 - GPU prices down after crypto shock\\\\n13:30 - Facebook AI improves shopping experience\\\\n14:05 - DeepLab2 released on GitHub\\\\n14:35 - Toxic Language Models: Nobody cares\\\\n16:55 - Does AI have common sense?\\\\n\\\\nReferences:\\\\nCVPR forbids social media promotion\\\\nhttps://twitter.com/wjscheirer/status/1408507154219384834\\\\n\\\\nWalMart uses AI to substitute out-of-stock products\\\\nhttps://www.supermarketnews.com/technology/walmart-enlists-artificial-intelligence-online-grocery-substitutions\\\\n\\\\nNVIDIA releases Alias-Free GAN\\\\nhttps://nvlabs.github.io/alias-free-gan/\\\\n\\\\nMyanmar Politician\\'s confession could be DeepFake\\\\nhttps://www.wired.com/story/opinion-the-world-needs-deepfake-experts-to-stem-this-chaos/\\\\n\\\\nRembrandt restored using AI\\\\nhttps://www.smithsonianmag.com/smart-news/lost-edges-rembrandts-night-watch-are-restored-using-artificial-intelligence-180978056/\\\\n\\\\nAI in healthcare still shaky\\\\nhttp://www.greenvillebusinessmag.com/2021/06/22/360303/prisma-health-announces-artificial-intelligence-partnership\\\\nhttps://www.theverge.com/2021/6/22/22545044/algorithm-hospital-sepsis-epic-prediction\\\\n\\\\nML interviews book\\\\nhttps://huyenchip.com/ml-interviews-book/\\\\n\\\\nNVIDIA Canvas Beta available\\\\nhttps://blogs.nvidia.com/blog/2021/06/23/studio-canvas-app/\\\\n\\\\nGPU prices down as China cracks down on Crypto\\\\nhttps://www.theregister.com/2021/06/22/as_china_shutters_cryptomining_plants/\\\\n\\\\nFacebook AI\\'s big goal of improving shopping\\\\nhttps://ai.facebook.com/blog/advancing-ai-to-make-shopping-easier-for-everyone/\\\\n\\\\nGoogleAI releases DeepLab2\\\\nhttps://github.com/google-research/deeplab2\\\\n\\\\nToxic Language Model: Nobody cares\\\\nhttps://arxiv.org/pdf/2105.03023.pdf\\\\n\\\\nAI has no common sense\\\\nhttps://www.analyticsinsight.net/incapable-yes-artificial-intelligence-cant-do-these-things/\\\\nhttps://6b.eleuther.ai/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1107",
    "uploadDate": "2021-06-30",
    "thumbnail_url": "https://i.ytimg.com/vi/tDk10VTHwNo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=k_hUdZJNzkU",
    "title": "The Dimpled Manifold Model of Adversarial Examples in Machine Learning (Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, adversarial examples, goodfellow, goodfellow adversarial attacks, adversarial attacks on neural networks, features not bugs, madry, dimpled manifold, why do adversarial examples exist, adversarial examples explanation, adversarial attacks explanation, computer vision, decision boundary, data manifold, low dimensional manifold, what are adversarial examples, what is deep learning",
    "scraped_at": 1684582642.5507207,
    "genre": "Science",
    "views": "14876",
    "desc": "#adversarialexamples #dimpledmanifold #security\\\\n\\\\nAdversarial Examples have long been a fascinating topic for many Machine Learning researchers. How can a tiny perturbation cause the neural network to change its output by so much? While many explanations have been proposed over the years, they all appear to fall short. This paper attempts to comprehensively explain the existence of adversarial examples by proposing a view of the classification landscape, which they call the Dimpled Manifold Model, which says that any classifier will adjust its decision boundary to align with the low-dimensional data manifold, and only slightly bend around the data. This potentially explains many phenomena around adversarial examples. Warning: In this video, I disagree. Remember that I\\'m not an authority, but simply give my own opinions.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n7:30 - The old mental image of Adversarial Examples\\\\n11:25 - The new Dimpled Manifold Hypothesis\\\\n22:55 - The Stretchy Feature Model\\\\n29:05 - Why do DNNs create Dimpled Manifolds?\\\\n38:30 - What can be explained with the new model?\\\\n1:00:40 - Experimental evidence for the Dimpled Manifold Model\\\\n1:10:25 - Is Goodfellow\\'s claim debunked?\\\\n1:13:00 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2106.10151\\\\nMy replication code: https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64\\\\nGoodfellow\\'s Talk: https://youtu.be/CIfsB_EYsVI?t=4280\\\\n\\\\nAbstract:\\\\nThe extreme fragility of deep neural networks when presented with tiny perturbations in their inputs was independently discovered by several research groups in 2013, but in spite of enormous effort these adversarial examples remained a baffling phenomenon with no clear explanation. In this paper we introduce a new conceptual framework (which we call the Dimpled Manifold Model) which provides a simple explanation for why adversarial examples exist, why their perturbations have such tiny norms, why these perturbations look like random noise, and why a network which was adversarially trained with incorrectly labeled images can still correctly classify test images. In the last part of the paper we describe the results of numerous experiments which strongly support this new model, and in particular our assertion that adversarial perturbations are roughly perpendicular to the low dimensional manifold which contains all the training examples.\\\\n\\\\nAbstract: Adi Shamir, Odelia Melamed, Oriel BenShmuel\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "4461",
    "uploadDate": "2021-06-27",
    "thumbnail_url": "https://i.ytimg.com/vi/k_hUdZJNzkU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6_q9DbX35kk",
    "title": "[ML News] Hugging Face course | GAN Theft Auto | AI Programming Puzzles | PyTorch 1.9 Released",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, introduction to deep learning, deep learning news, machine learning news, facebook ai, augly, gan theft auto, gta ai, sentdex, huggingface, huggingface course, ubs ai, banking ai, banking machine learning, mcdonalds ai, mcdonalds ai drive thru, weather, antonio, antonio weather, mlnews, ml news, mayflower 400, boston dynamics, schmidhuber, schmidhuber blog",
    "scraped_at": 1684582640.071033,
    "genre": "Science",
    "views": "13038",
    "desc": "#mlnews #gta #weather\\\\n\\\\nIn this week\\'s ML News, we look at the latest developments in the Machine Learning and AI world with updates from research, industry, and society at large.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:20 - Hugging Face launches free course\\\\n1:30 - Sentdex releases GAN Theft Auto\\\\n2:25 - Facebook uses AI to help moderators\\\\n4:10 - Weather with Antonio\\\\n5:10 - Autonomous ship aborts mission\\\\n7:25 - PyTorch Release 1.9\\\\n8:30 - McDonald\\'s new AI drive thru\\\\n10:20 - UBS CEO says AI won\\'t replace humans\\\\n12:20 - G\\xc3\\xb6del paper has 90th birthday\\\\n12:55 - AugLy data augmentation library\\\\n13:20 - Programming Puzzles for autonomous coding\\\\n14:30 - Boston Dynamics\\' Spot turns 1\\\\n\\\\n\\\\nReferences:\\\\nPyTorch 1.9 Released\\\\nhttps://pytorch.org/blog/pytorch-1.9-released/?ref=mlnews\\\\nHugging Face launches course\\\\nhttps://huggingface.co/course/chapter1\\\\n90 years of G\\xc3\\xb6del\\'s theory\\\\nhttps://people.idsia.ch/~juergen/goedel-1931-founder-theoretical-computer-science-AI.html\\\\nAugLy: A data augmentation library\\\\nhttps://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/\\\\nSentdex builds GAN Theft Auto\\\\nhttps://github.com/sentdex/GANTheftAuto/\\\\nSpot turns 1\\\\nhttps://blog.bostondynamics.com/spots-year-in-the-real-world\\\\nAutonomous ship aborts mission\\\\nhttps://www.washingtonpost.com/technology/2021/06/18/mayflower-ibm-autonomous-ship/\\\\nhttps://mas400.com/dashboard#currentLocation\\\\nMcDonald\\'s tests AI drive thru\\\\nhttps://www.zdnet.com/article/i-just-watched-mcdonalds-new-ai-drive-thru-and-ive-lost-my-appetite/\\\\nFacebook uses AI to moderate conversations\\\\nhttps://edition.cnn.com/2021/06/16/tech/facebook-ai-conflict-moderation-groups/index.html\\\\nUBS CEO says AI won\\'t replace financial advisors\\\\nhttps://www.cnbc.com/2021/06/17/ai-wont-replace-financial-advisors-ubs-ceo-says.html\\\\nProgramming Puzzles\\\\nhttps://arxiv.org/abs/2106.05784\\\\nhttps://github.com/microsoft/PythonProgrammingPuzzles\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "952",
    "uploadDate": "2021-06-24",
    "thumbnail_url": "https://i.ytimg.com/vi/6_q9DbX35kk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=g08NkNWmZTA",
    "title": "XCiT: Cross-Covariance Image Transformers (Facebook AI Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, xcit, facebook ai, fair, transformer, transformer neural network, transformer computer vision, vision transformer, deit, self",
    "scraped_at": 1684582642.6377513,
    "genre": "Science",
    "views": "17551",
    "desc": "#xcit #transformer #attentionmechanism\\\\n\\\\nAfter dominating Natural Language Processing, Transformers have taken over Computer Vision recently with the advent of Vision Transformers. However, the attention mechanism\\'s quadratic complexity in the number of tokens means that Transformers do not scale well to high-resolution images. XCiT is a new Transformer architecture, containing XCA, a transposed version of attention, reducing the complexity from quadratic to linear, and at least on image data, it appears to perform on par with other models. What does this mean for the field? Is this even a transformer? What really matters in deep learning?\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:45 - Self-Attention vs Cross-Covariance Attention (XCA)\\\\n19:55 - Cross-Covariance Image Transformer (XCiT) Architecture\\\\n26:00 - Theoretical \\\\u0026 Engineering considerations\\\\n30:40 - Experimental Results\\\\n33:20 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2106.09681\\\\nCode: https://github.com/facebookresearch/xcit\\\\n\\\\nAbstract:\\\\nFollowing their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a \\\\\"",
    "lengthSeconds": "2140",
    "uploadDate": "2021-06-23",
    "thumbnail_url": "https://i.ytimg.com/vi/g08NkNWmZTA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=P38FZrbNHV4",
    "title": "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, reinforcement learning, imitation learning, uc berkeley, sergey levine, sergey levine reinforcement learning, pieter abbeel, pieter abbeel reinforcement learning, walk and punch, learning from demonstration, amp, adversarial motion priors, physics based reinforcement learning, 3d reinforcement learning",
    "scraped_at": 1684582642.7277186,
    "genre": "Science",
    "views": "8428",
    "desc": "#reiforcementlearning #gan #imitationlearning\\\\n\\\\nLearning from demonstrations is a fascinating topic, but what if the demonstrations are not exactly the behaviors we want to learn? Can we adhere to a dataset of demonstrations and still achieve a specified goal? This paper uses GANs to combine goal-achieving reinforcement learning with imitation learning and learns to perform well at a given task while doing so in the style of a given presented dataset. The resulting behaviors include many realistic-looking transitions between the demonstrated movements.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:25 - Problem Statement\\\\n6:10 - Reward Signals\\\\n8:15 - Motion Prior from GAN\\\\n14:10 - Algorithm Overview\\\\n20:15 - Reward Engineering \\\\u0026 Experimental Results\\\\n30:40 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2104.02180\\\\nMain Video: https://www.youtube.com/watch?v=wySUxZN_KbM\\\\nSupplementary Video: https://www.youtube.com/watch?v=O6fBSMxThR4\\\\n\\\\nAbstract:\\\\nSynthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion tracking are a prominent class of techniques for producing high fidelity motions for a wide range of behaviors. However, the effectiveness of these tracking-based methods often hinges on carefully designed objective functions, and when applied to large and diverse motion datasets, these methods require significant additional machinery to select the appropriate motion for the character to track in a given scenario. In this work, we propose to obviate the need to manually design imitation objectives and mechanisms for motion selection by utilizing a fully automated approach based on adversarial imitation learning. High-level task objectives that the character should perform can be specified by relatively simple reward functions, while the low-level style of the character\\'s behaviors can be specified by a dataset of unstructured motion clips, without any explicit clip selection or sequencing. These motion clips are used to train an adversarial motion prior, which specifies style-rewards for training the character through reinforcement learning (RL). The adversarial RL procedure automatically selects which motion to perform, dynamically interpolating and generalizing from the dataset. Our system produces high-quality motions that are comparable to those achieved by state-of-the-art tracking-based techniques, while also being able to easily accommodate large datasets of unstructured motion clips. Composition of disparate skills emerges automatically from the motion prior, without requiring a high-level motion planner or other task-specific annotations of the motion clips. We demonstrate the effectiveness of our framework on a diverse cast of complex simulated characters and a challenging suite of motor control tasks.\\\\n\\\\nAuthors: Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, Angjoo Kanazawa\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/ykilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2084",
    "uploadDate": "2021-06-19",
    "thumbnail_url": "https://i.ytimg.com/vi/P38FZrbNHV4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Ihg4XDWOy68",
    "title": "[ML News] De-Biasing GPT-3 | RL cracks chip design | NetHack challenge | Open-Source GPT-J",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, tensorflow forum, tensorflow discussion forum, what is deep learning, deep learning tutorial, introduction to deep learning, deep learning news, machine learning news, weekly news machine learning, ml news, yannic kilcher news, nethack challenge, gpt",
    "scraped_at": 1684582640.1770322,
    "genre": "Science",
    "views": "14358",
    "desc": "OUTLINE:\\\\n0:00 - Intro\\\\n0:30 - Google RL creates next-gen TPUs\\\\n2:15 - Facebook launches NetHack challenge\\\\n3:50 - OpenAI mitigates bias by fine-tuning\\\\n9:05 - Google AI releases browseable reconstruction of human cortex\\\\n9:50 - GPT-J 6B Transformer in JAX\\\\n12:00 - Tensorflow launches Forum\\\\n13:50 - Text style transfer from a single word\\\\n15:45 - ALiEn artificial life simulator\\\\n\\\\nMy Video on Chip Placement: https://youtu.be/PDRtyrVskMU\\\\n\\\\nReferences:\\\\nRL creates next-gen TPUs\\\\nhttps://www.nature.com/articles/s41586-021-03544-w\\\\nhttps://www.youtube.com/watch?v=PDRtyrVskMU\\\\nFacebook launches NetHack challenge\\\\nhttps://ai.facebook.com/blog/launching-the-nethack-challenge-at-neurips-2021/\\\\nMitigating bias by fine-tuning\\\\nhttps://openai.com/blog/improving-language-model-behavior/?s=09\\\\nHuman Cortex 3D Reconstruction\\\\nhttps://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html\\\\nGPT-J: An open-source 6B transformer\\\\nhttps://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/\\\\nhttps://6b.eleuther.ai/\\\\nhttps://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b\\\\nTensorflow launches \\\\\"",
    "lengthSeconds": "1022",
    "uploadDate": "2021-06-16",
    "thumbnail_url": "https://i.ytimg.com/vi/Ihg4XDWOy68/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8Oy7o3Yu-Xo",
    "title": "Efficient and Modular Implicit Differentiation (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, implicit differentiation, implicit function theorem, imaml, inner optimization, inner optimization procedure, how to backpropagate through sgd, backpropagate through optimizer, outer optimization loop, bi",
    "scraped_at": 1684582642.8207514,
    "genre": "Science",
    "views": "17217",
    "desc": "#implicitfunction #jax #autodiff\\\\n\\\\nMany problems in Machine Learning involve loops of inner and outer optimization. Finding update steps for the outer loop is usually difficult, because of the.need to differentiate through the inner loop\\'s procedure over multiple steps. Such loop unrolling is very limited and constrained to very few steps. Other papers have found solutions around unrolling in very specific, individual problems. This paper proposes a unified framework for implicit differentiation of inner optimization procedures without unrolling and provides implementations that integrate seamlessly into JAX.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:05 - Automatic Differentiation of Inner Optimizations\\\\n4:30 - Example: Meta-Learning\\\\n7:45 - Unrolling Optimization\\\\n13:00 - Unified Framework Overview \\\\u0026 Pseudocode\\\\n21:10 - Implicit Function Theorem\\\\n25:45 - More Technicalities\\\\n28:45 - Experiments\\\\n\\\\nERRATA:\\\\n- Dataset Distillation is done with respect to the training set, not the validation or test set.\\\\n\\\\nPaper: https://arxiv.org/abs/2105.15183\\\\nCode coming soon\\\\n\\\\nAbstract:\\\\nAutomatic differentiation (autodiff) has revolutionized machine learning. It allows expressing complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization as a layer, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, the formulas for these derivatives often involve case-by-case tedious mathematical derivations. In this paper, we propose a unified, efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines (in Python in the case of our implementation) a function F capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of F and implicit differentiation to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many recently proposed implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.\\\\n\\\\nAuthors: Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-L\\xc3\\xb3pez, Fabian Pedregosa, Jean-Philippe Vert\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1966",
    "uploadDate": "2021-06-11",
    "thumbnail_url": "https://i.ytimg.com/vi/8Oy7o3Yu"
  },
  {
    "link": "watch?v=bw1kiLMQFKU",
    "title": "[ML News] EU regulates AI, China trains 1.75T model, Google's oopsie, Everybody cheers for fraud.",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning news, machine learning news, academic fraud, geoffrey hinton, please commit more academic fraud, wudao, wudao china, baai, google, kannada, ugliest language, mcdonalds machine learning, ai predicts stock market, european union ai, eu ai regulation, ai regulation, machine learning regulation, this week in machine learning",
    "scraped_at": 1684582642.90872,
    "genre": "Science",
    "views": "18362",
    "desc": "#mlnews #wudao #academicfraud\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:25 - EU seeks to regulate AI\\\\n2:45 - AI COVID detection systems are all flawed\\\\n5:05 - Chinese lab trains model 10x GPT-3 size\\\\n6:55 - Google error identifies \\\\\"",
    "lengthSeconds": "1014",
    "uploadDate": "2021-06-09",
    "thumbnail_url": "https://i.ytimg.com/vi/bw1kiLMQFKU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=RZ7JiAk9azY",
    "title": "My GitHub (Trash code I wrote during PhD)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, github, my github, phd code, code during phd, deep learning tutorial, what is deep learning, introduction to deep learning, deep learning phd coding",
    "scraped_at": 1684582642.9957194,
    "genre": "Science",
    "views": "18317",
    "desc": "#phdlife #github #researchcode\\\\n\\\\nA brief browse through my public GitHub and musings about my old code.\\\\n\\\\nLink: https//github.com/yk\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "649",
    "uploadDate": "2021-06-08",
    "thumbnail_url": "https://i.ytimg.com/vi/RZ7JiAk9azY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-buULmf7dec",
    "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling (Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, decisiontransformer, decision transformer, berkeley, uc berkeley, facebook ai language, fair, deep learning tutorial, what is deep learning, introduction to deep learning, transformers for reinforcement learning, transformers for rl, transformer reinforcement learning, sequence modeling, sequence modelling, sequence modeling reinforcement learning, reinforcement learning with transformers",
    "scraped_at": 1684582640.3430326,
    "genre": "Science",
    "views": "45559",
    "desc": "#decisiontransformer #reinforcementlearning #transformer\\\\n\\\\nProper credit assignment over long timespans is a fundamental problem in reinforcement learning. Even methods designed to combat this problem, such as TD-learning, quickly reach their limits when rewards are sparse or noisy. This paper reframes offline reinforcement learning as a pure sequence modeling problem, with the actions being sampled conditioned on the given history and desired future rewards. This allows the authors to use recent advances in sequence modeling using Transformers and achieve competitive results in Offline RL benchmarks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:15 - Offline Reinforcement Learning\\\\n10:10 - Transformers in RL\\\\n14:25 - Value Functions and Temporal Difference Learning\\\\n20:25 - Sequence Modeling and Reward-to-go\\\\n27:20 - Why this is ideal for offline RL\\\\n31:30 - The context length problem\\\\n34:35 - Toy example: Shortest path from random walks\\\\n41:00 - Discount factors\\\\n45:50 - Experimental Results\\\\n49:25 - Do you need to know the best possible reward?\\\\n52:15 - Key-to-door toy experiment\\\\n56:00 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2106.01345\\\\nWebsite: https://sites.google.com/berkeley.edu/decision-transformer\\\\nCode: https://github.com/kzl/decision-transformer\\\\n\\\\nTrajectory Transformer: https://trajectory-transformer.github.io/\\\\nUpside-Down RL: https://arxiv.org/abs/1912.02875\\\\n\\\\nAbstract:\\\\nWe present a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.\\\\n\\\\nAuthors: Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3408",
    "uploadDate": "2021-06-05",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=oxsdp--ULRo",
    "title": "[ML News] Anthropic raises $124M, ML execs clueless, collusion rings, ELIZA source discovered & more",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, mlnews, machine learning news, anthropic, eliza, peer review, collusion, collusion ring, openai fund, tech news, technology news, deep learning news, ai safety, steerable ai",
    "scraped_at": 1684582640.4460552,
    "genre": "Science",
    "views": "14318",
    "desc": "#mlnews #anthropic #eliza\\\\n\\\\nAnthropic raises $124M for steerable AI, peer review is threatened by collusion rings, and the original ELIZA source code was discovered.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:40 - Anthropic raises $124M\\\\n3:25 - 65% of execs can\\'t explain AI predictions\\\\n4:25 - DeepMind releases AndroidEnv\\\\n6:10 - Collusion rings in ML Conferences\\\\n7:30 - ELIZA\\'s original source code discovered\\\\n10:45 - OpenAI raises $100M fund\\\\n11:25 - Outro\\\\n\\\\nReferences:\\\\nhttps://techcrunch.com/2021/05/28/anthropic-is-the-new-ai-research-outfit-from-openais-dario-amodei-and-it-has-124m-to-burn/\\\\nhttps://www.anthropic.com/news/announcement\\\\nhttps://www.anthropic.com/\\\\nhttps://openai.com/blog/introducing-openai/\\\\nhttps://deepmind.com/research/publications/androidenv\\\\nhttps://cacm.acm.org/magazines/2021/6/252840-collusion-rings-threaten-the-integrity-of-computer-science-research/fulltext#FNA\\\\nhttps://venturebeat.com/2021/05/25/65-of-execs-cant-explain-how-their-ai-models-make-decisions-survey-finds/\\\\nhttps://techcrunch.com/2021/05/26/openais-100m-startup-fund-will-make-big-early-bets-with-microsoft-as-partner/\\\\nhttps://sites.google.com/view/elizagen-org/the-original-eliza\\\\nhttp://psych.fullerton.edu/mbirnbaum/psych101/eliza.htm\\\\nhttps://en.wikipedia.org/wiki/Carl_Rogers\\\\nhttps://openai.com/fund/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "717",
    "uploadDate": "2021-06-02",
    "thumbnail_url": "https://i.ytimg.com/vi/oxsdp"
  },
  {
    "link": "watch?v=dmH1ZpcROMk",
    "title": "Reward Is Enough (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, introduction to deep learning, what is deep learning, how to achieve agi, artificial general intelligence, how to create intelligence, reward maximisation, reward maximization, reinforcement learning, is alphago intelligence, is gpt 3 self aware, is gpt 3 intelligent, how to create ai, how to achieve ai, general ai, agent environment, deepmind",
    "scraped_at": 1684582640.5460317,
    "genre": "Science",
    "views": "27939",
    "desc": "#reinforcementlearning #deepmind #agi\\\\n\\\\nWhat\\'s the most promising path to creating Artificial General Intelligence (AGI)? This paper makes the bold claim that a learning agent maximizing its reward in a sufficiently complex environment will necessarily develop intelligence as a by-product, and that Reward Maximization is the best way to move the creation of AGI forward. The paper is a mix of philosophy, engineering, and futurism, and raises many points of discussion.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n4:10 - Reward Maximization\\\\n10:10 - The Reward-is-Enough Hypothesis\\\\n13:15 - Abilities associated with intelligence\\\\n16:40 - My Criticism\\\\n26:15 - Reward Maximization through Reinforcement Learning\\\\n31:30 - Discussion, Conclusion \\\\u0026 My Comments\\\\n\\\\nPaper: https://www.sciencedirect.com/science/article/pii/S0004370221000862\\\\n\\\\nAbstract:\\\\nIn this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.\\\\n\\\\nAuthors: David Silver, Satinder Singh, Doina Precup, Richard S. Sutton\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2149",
    "uploadDate": "2021-05-31",
    "thumbnail_url": "https://i.ytimg.com/vi/dmH1ZpcROMk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zWFkUGXjbdo",
    "title": "[Rant] Can AI read your emotions? (No, but ...)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ai face recognition, face recognition, face recognition emotion detection, can ai read your mind, can ai read your emotions, ai emotion analysis, ai analyzes emotion, government ai face detection, ai emotion recognition, ai emotion detection",
    "scraped_at": 1684582640.636033,
    "genre": "Science",
    "views": "7715",
    "desc": "#facerecognition #emotiondetection #mindreading\\\\n\\\\nFace recognition has a bad rep in the ML community. While the technology continuously advances, so does the resistance against its applications, with good reasons: AI emotion analysis hints at a dystopian future where our lives are completely governed by algorithms. However, we must be realistic about what is and isn\\'t possible with AI, and while current systems are not the most accurate, denying the link between your facial expression and your emotions is not productive either.\\\\n\\\\nhttps://twitter.com/jblefevre60/status/1395617615964475392\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "413",
    "uploadDate": "2021-05-30",
    "thumbnail_url": "https://i.ytimg.com/vi/zWFkUGXjbdo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kU-tWy_wr78",
    "title": "Fast and Slow Learning of Recurrent Independent Mechanisms (Machine Learning Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, recurrent independent mechanisms, metarim, deep learning tutorial, introduction to deep learning, what is deep learning, machine learning paper, deep reinforcement learning, reinforcement learning meta learning, yoshua bengio, bentio mila, grid world, fast and slow learning, reinforcement learning attention, catastrophic forgetting, lifelong learning, multitask learning",
    "scraped_at": 1684582640.7330325,
    "genre": "Science",
    "views": "9149",
    "desc": "#metarim #deeprl #catastrophicforgetting\\\\n\\\\nReinforcement Learning is very tricky in environments where the objective shifts over time. This paper explores agents in multi-task environments that are usually subject to catastrophic forgetting. Building on the concept of Recurrent Independent Mechanisms (RIM), the authors propose to separate the learning procedures for the mechanism parameters (fast) and the attention parameters (slow) and achieve superior results and more stability, and even better zero-shot transfer performance.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:30 - Recombining pieces of knowledge\\\\n11:30 - Controllers as recurrent neural networks\\\\n14:20 - Recurrent Independent Mechanisms\\\\n21:20 - Learning at different time scales\\\\n28:40 - Experimental Results \\\\u0026 My Criticism\\\\n44:20 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2105.08710\\\\nRIM Paper: https://arxiv.org/abs/1909.10893\\\\n\\\\nAbstract:\\\\nDecomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.\\\\n\\\\nAuthors: Kanika Madan, Nan Rosemary Ke, Anirudh Goyal, Bernhard Sch\\xc3\\xb6lkopf, Yoshua Bengio\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2706",
    "uploadDate": "2021-05-29",
    "thumbnail_url": "https://i.ytimg.com/vi/kU"
  },
  {
    "link": "watch?v=dWGjoInRaAs",
    "title": "[ML News] DeepMind fails to get independence from Google",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ml news, machine learning news, tech news, technology news, deep learning news, google deepmind, does google own deepmind, deepmind offices, does deepmind make profit, who pays for deepmind, when did google buy deepmind, how much did google pay for deepmind, alphago, alphafold",
    "scraped_at": 1684582640.8240316,
    "genre": "Science",
    "views": "13443",
    "desc": "#deepmind #google #mlnews\\\\n\\\\nDeepMind has reportedly failed to negotiate for greater independence from Google/Alphabet. While DeepMind wanted to set up a non-profit-like structure, Google seems to go for the opposite approach and seek tight integration. How is AI best served?\\\\n\\\\nOriginal Article: https://www.wsj.com/articles/google-unit-deepmind-triedand-failedto-win-ai-autonomy-from-parent-11621592951\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "241",
    "uploadDate": "2021-05-26",
    "thumbnail_url": "https://i.ytimg.com/vi/dWGjoInRaAs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2PYLNHqxd5A",
    "title": "Expire-Span: Not All Memories are Created Equal: Learning to Forget by Expiring (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, expire span, facebook ai, transformers, long sequence models, transformers long sequence, large context language models, language model sequence length, transformer xl, learning to forget, lstm, schmidhuber, learning to remember, not all memories are created equal, linear attention, attention mechanism, linear attention mechanism, transformer memory, deep learning tutorial",
    "scraped_at": 1684582643.0867188,
    "genre": "Science",
    "views": "10466",
    "desc": "#expirespan #nlp #facebookai\\\\n\\\\nFacebook AI (FAIR) researchers present Expire-Span, a variant of Transformer XL that dynamically assigns expiration dates to previously encountered signals. Because of this, Expire-Span can handle sequences of many thousand tokens, while keeping the memory and compute requirements at a manageable level. It severely matches or outperforms baseline systems, while consuming much less resources. We discuss its architecture, advantages, and shortcomings.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:30 - Remembering the past in sequence models\\\\n5:45 - Learning to expire past memories\\\\n8:30 - Difference to local attention\\\\n10:00 - Architecture overview\\\\n13:45 - Comparison to Transformer XL\\\\n18:50 - Predicting expiration masks\\\\n32:30 - Experimental Results\\\\n40:00 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2105.06548\\\\nCode: https://github.com/facebookresearch/transformer-sequential\\\\n\\\\nADDENDUM: I mention several times that the gradient signal of the e quantity only occurs inside the R ramp. By that, I mean the gradient stemming from the model loss. The regularization loss acts also outside the R ramp.\\\\n\\\\nAbstract:\\\\nAttention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.\\\\n\\\\nAuthors: Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, Angela Fan\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2504",
    "uploadDate": "2021-05-24",
    "thumbnail_url": "https://i.ytimg.com/vi/2PYLNHqxd5A/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JJR3pBl78zw",
    "title": "FNet: Mixing Tokens with Fourier Transforms (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, fnet, fnets, fourier nets, fourier neural networks, attention fourier, fourier attention, deep learning fft, machine learning fft, deep learning fourier transform, attention mechanism fourier transform, fourier transform in deep learning, attention networks, do we need attention in deep learning",
    "scraped_at": 1684582640.9400325,
    "genre": "Science",
    "views": "25800",
    "desc": "#fnet #attention #fourier\\\\n\\\\nDo we even need Attention? FNets completely drop the Attention mechanism in favor of a simple Fourier transform. They perform almost as well as Transformers, while drastically reducing parameter count, as well as compute and memory requirements. This highlights that a good token mixing heuristic could be as valuable as a learned attention matrix.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n0:45 - Giving up on Attention\\\\n5:00 - FNet Architecture\\\\n9:00 - Going deeper into the Fourier Transform\\\\n11:20 - The Importance of Mixing\\\\n22:20 - Experimental Results\\\\n33:00 - Conclusions \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2105.03824\\\\n\\\\nADDENDUM:\\\\nOf course, I completely forgot to discuss the connection between Fourier transforms and Convolutions, and that this might be interpreted as convolutions with very large kernels.\\\\n\\\\nAbstract:\\\\nWe show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \\\\\"",
    "lengthSeconds": "2062",
    "uploadDate": "2021-05-21",
    "thumbnail_url": "https://i.ytimg.com/vi/JJR3pBl78zw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rR5_emVeyBk",
    "title": "AI made this music video | What happens when OpenAI's CLIP meets BigGAN?",
    "tags": "deep learning, machine learning, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, what is deep learning, introduction to deep learning, ai generated music video, ai music video, deep learning music video, ai music video generator, music video generator, openai clip, openai clip music video, biggan music video, clip biggan, biggan clip, stylegan clip, imagenet song, imagenet classes lyrics, stylegan music, gan interpolation, be my weasel",
    "scraped_at": 1684582643.175746,
    "genre": "Science",
    "views": "91564",
    "desc": "#artificialintelligence #musicvideo #clip\\\\n\\\\nI used OpenAI\\'s CLIP model and BigGAN to create a music video that goes along with the lyrics of a song that I wrote. The song lyrics are made from ImageNet class labels, and the song itself is performed by me on a looper.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:00 - AI-generated music video for \\\\\"",
    "lengthSeconds": "833",
    "uploadDate": "2021-05-18",
    "thumbnail_url": "https://i.ytimg.com/vi/rR5_emVeyBk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=W-O7AZNzbzQ",
    "title": "DDPM - Diffusion Models Beat GANs on Image Synthesis (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, diffusion models, diffusion model, ddpm, ddim, denoising autoencoders, generative models, generative models deep learning, gan alternatives, alternatives to gans, computer vision generative, machine learning image generation, openai diffusion, openai gan, variational autoencoder, log likelihood, variational lower bound",
    "scraped_at": 1684582643.267744,
    "genre": "Science",
    "views": "117658",
    "desc": "#ddpm #diffusionmodels #openai\\\\n\\\\nGANs have dominated the image generation space for the majority of the last decade. This paper shows for the first time, how a non-GAN model, a DDPM, can be improved to overtake GANs at standard evaluation metrics for image generation. The produced samples look amazing and other than GANs, the new model has a formal probabilistic foundation. Is there a future for GANs or are Diffusion Models going to overtake them for good?\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:10 - Denoising Diffusion Probabilistic Models\\\\n11:30 - Formal derivation of the training loss\\\\n23:00 - Training in practice\\\\n27:55 - Learning the covariance\\\\n31:25 - Improving the noise schedule\\\\n33:35 - Reducing the loss gradient noise\\\\n40:35 - Classifier guidance\\\\n52:50 - Experimental Results\\\\n\\\\nPaper (this): https://arxiv.org/abs/2105.05233\\\\nPaper (previous): https://arxiv.org/abs/2102.09672\\\\nCode: https://github.com/openai/guided-diffusion\\\\n\\\\nAbstract:\\\\nWe show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for sample quality using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\\xc3\\x97128, 4.59 on ImageNet 256\\xc3\\x97256, and 7.72 on ImageNet 512\\xc3\\x97512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.85 on ImageNet 512\\xc3\\x97512. We release our code at this https URL\\\\n\\\\nAuthors: Alex Nichol, Prafulla Dhariwal\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3273",
    "uploadDate": "2021-05-15",
    "thumbnail_url": "https://i.ytimg.com/vi/W"
  },
  {
    "link": "watch?v=WknN4E-y44E",
    "title": "Research Conference ICML drops their acceptance rate | Area Chairs instructed to be more picky",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, icml, peer review, machine learning conference, icml conference, icml submission, icml paper accepted, how to write machine learning papers, how to publish a paper, how to publish in machine learning, how to do a phd in machine learning, deep learning conference, machine learning research conference, icml acceptance rate, icml submissions, icml area chairs, machine learning news",
    "scraped_at": 1684582643.3507187,
    "genre": "Science",
    "views": "12817",
    "desc": "#icml #machinelearning #conference\\\\n\\\\nIn a controversial move, ICML Area Chairs were instructed to raise the bar on acceptance to drop the acceptance rate by 10% from the previous trajectory. This raises a lot of questions about the pains of an academic peer review system under the load of an exponentially increasing field of study. Who draws the short stick? Usually not the big corporations.\\\\n\\\\nReferences:\\\\nhttps://www.reddit.com/r/MachineLearning/comments/n243qw/d_icml_conference_we_plan_to_reduce_the_number_of/\\\\nhttps://twitter.com/tomgoldsteincs/status/1388156022112624644\\\\nhttps://twitter.com/ryan_p_adams/status/1388164670410866692\\\\nhttps://github.com/lixin4ever/Conference-Acceptance-Rate\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "428",
    "uploadDate": "2021-05-11",
    "thumbnail_url": "https://i.ytimg.com/vi/WknN4E"
  },
  {
    "link": "watch?v=pH2jZun8MoY",
    "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition (Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, computer vision, convolutional neural network, convolutions alternative, cnn attention, self attention, attention mechanism for vision, weight sharing neural networks, convolutions vision, cnn vision, involution vision, image segmentation, rednet, resnet, residual neural networks, bytedance ai",
    "scraped_at": 1684582643.4397185,
    "genre": "Science",
    "views": "23599",
    "desc": "#involution #computervision #attention\\\\n\\\\nConvolutional Neural Networks (CNNs) have dominated computer vision for almost a decade by applying two fundamental principles: Spatial agnosticism and channel-specific computations. Involution aims to invert these principles and presents a spatial-specific computation, which is also channel-agnostic. The resulting Involution Operator and RedNet architecture are a compromise between classic Convolutions and the newer Local Self-Attention architectures and perform favorably in terms of computation accuracy tradeoff when compared to either.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:00 - Principles of Convolution\\\\n10:50 - Towards spatial-specific computations\\\\n17:00 - The Involution Operator\\\\n20:00 - Comparison to Self-Attention\\\\n25:15 - Experimental Results\\\\n30:30 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2103.06255\\\\nCode: https://github.com/d-li14/involution\\\\n\\\\nAbstract:\\\\nConvolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at this https URL.\\\\n\\\\nAuthors: Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, Qifeng Chen\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1853",
    "uploadDate": "2021-05-08",
    "thumbnail_url": "https://i.ytimg.com/vi/pH2jZun8MoY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=7K4Z8RqjWIk",
    "title": "MLP-Mixer: An all-MLP Architecture for Vision (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, google mixer, google ai mixer, vit, bit, mlp mixer, mlpmixer, imagenet mixer, imagenet only feedforward, no convolutions, imagenet without convolutions, image patches, attention mechanism, multilayer perceptron, transfer learning, linear classifier, state of the art, tradeoff",
    "scraped_at": 1684582647.9925382,
    "genre": "Science",
    "views": "42169",
    "desc": "#mixer #google #imagenet\\\\n\\\\nConvolutional Neural Networks have dominated computer vision for nearly 10 years, and that might finally come to an end. First, Vision Transformers (ViT) have shown remarkable performance, and now even simple MLP-based models reach competitive accuracy, as long as sufficient data is used for pre-training. This paper presents MLP-Mixer, using MLPs in a particular weight-sharing arrangement to achieve a competitive, high-throughput model and it raises some interesting questions about the nature of learning and inductive biases and their interaction with scale for future research.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:20 - MLP-Mixer Architecture\\\\n13:20 - Experimental Results\\\\n17:30 - Effects of Scale\\\\n24:30 - Learned Weights Visualization\\\\n27:25 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2105.01601\\\\n\\\\nAbstract:\\\\nConvolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. \\\\\"",
    "lengthSeconds": "1691",
    "uploadDate": "2021-05-06",
    "thumbnail_url": "https://i.ytimg.com/vi/7K4Z8RqjWIk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hsOMCwvFv80",
    "title": "I'm out of Academia",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582643.518751,
    "genre": "Science",
    "views": "19086",
    "desc": "#machinelearning #ai #phd\\\\n\\\\nDone with my PhD in Machine Learning at ETH Zurich.\\\\nOn to new lands!\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "268",
    "uploadDate": "2021-05-04",
    "thumbnail_url": "https://i.ytimg.com/vi/hsOMCwvFv80/maxresdefault.jpg"
  },
  {
    "link": "watch?v=h3ij3F3cPIk",
    "title": "DINO: Emerging Properties in Self-Supervised Vision Transformers (Facebook AI Research Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, what is deep learning, introduction to deep learning, facebook, facebook ai, fair, byol, swav, self supervised learning, unsupervised feature learning, unsupervised machine learning, feature engineering, stop gradient, dino, self distillation, self",
    "scraped_at": 1684582641.0860329,
    "genre": "Science",
    "views": "89145",
    "desc": "#dino #facebook #selfsupervised\\\\n\\\\nSelf-Supervised Learning is the final frontier in Representation Learning: Getting useful features without any labels. Facebook AI\\'s new system, DINO, combines advances in Self-Supervised Learning for Computer Vision with the new Vision Transformer (ViT) architecture and achieves impressive results without any labels. Attention maps can be directly interpreted as segmentation maps, and the obtained representations can be used for image retrieval and zero-shot k-nearest neighbor classifiers (KNNs).\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n6:20 - Vision Transformers\\\\n9:20 - Self-Supervised Learning for Images\\\\n13:30 - Self-Distillation\\\\n15:20 - Building the teacher from the student by moving average\\\\n16:45 - DINO Pseudocode\\\\n23:10 - Why Cross-Entropy Loss?\\\\n28:20 - Experimental Results\\\\n33:40 - My Hypothesis why this works\\\\n38:45 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2104.14294\\\\nBlog: https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training\\\\nCode: https://github.com/facebookresearch/dino\\\\n\\\\nMy Video on ViT: https://youtu.be/TrdevFK_am4\\\\nMy Video on BYOL: https://youtu.be/YPfUiOMYOEE\\\\n\\\\nAbstract:\\\\nIn this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\\\\n\\\\nAuthors: Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\\xc3\\xa9 J\\xc3\\xa9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2352",
    "uploadDate": "2021-05-01",
    "thumbnail_url": "https://i.ytimg.com/vi/h3ij3F3cPIk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=uwfVxckuq50",
    "title": "Why AI is Harder Than We Think (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, introduction to deep learning, what is deep learning, ai winter, ai spring, why is ai hard, can machines think, can machines be conscious, alan turing, elon musk artificial intelligence, self driving cars, marvin minsky, expert systems, deep learning artificial intelligence, are neural networks artificial intelligence, why is deep learning important",
    "scraped_at": 1684582641.1870315,
    "genre": "Science",
    "views": "46603",
    "desc": "#aiwinter #agi #embodiedcognition\\\\n\\\\nThe AI community has gone through regular cycles of AI Springs, where rapid progress gave rise to massive overconfidence, high funding, and overpromise, followed by these promises being unfulfilled, subsequently diving into periods of disenfranchisement and underfunding, called AI Winters. This paper examines the reasons for the repeated periods of overconfidence and identifies four fallacies that people make when they see rapid progress in AI.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:10 - AI Springs \\\\u0026 AI Winters\\\\n5:40 - Is the current AI boom overhyped?\\\\n15:35 - Fallacy 1: Narrow Intelligence vs General Intelligence\\\\n19:40 - Fallacy 2: Hard for humans doesn\\'t mean hard for computers\\\\n21:45 - Fallacy 3: How we call things matters\\\\n28:15 - Fallacy 4: Embodied Cognition\\\\n35:30 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2104.12871\\\\n\\\\nMy Video on Shortcut Learning: https://youtu.be/D-eg7k8YSfs\\\\n\\\\nAbstract:\\\\nSince its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (\\\\\"",
    "lengthSeconds": "2201",
    "uploadDate": "2021-04-30",
    "thumbnail_url": "https://i.ytimg.com/vi/uwfVxckuq50/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hIoCn_9QTVU",
    "title": "I COOKED A RECIPE MADE BY A.I. | Cooking with GPT-3 (Don't try this at home)",
    "tags": "deep learning, machine learning, neural networks, artificial intelligence, deep learning tutorial, introduction to deep learning, cooking by ai, can ai cook, ai recipe, ai recipe generator, gpt 3, gpt 3 recipe, gpt",
    "scraped_at": 1684582643.603749,
    "genre": "Science",
    "views": "12249",
    "desc": "#gpt3 #airecipe #cooking\\\\n\\\\nWe went to the store and bought a set of completely random ingredients and had OpenAI\\'s GPT-3 come up with a recipe, which we then cooked and ate.\\\\n\\\\nOur Rules:\\\\n1. All Vegan\\\\n2. Follow the recipe as closely as possible\\\\n3. We must finish our plates\\\\n\\\\nThe Recipe:\\\\n1. Boil the potatoes and carrots.\\\\n2. In the meantime, prepare the VEGAN minced meat, or use pre-cooked soy meat. \\\\n3. Then fry the VEGAN butter, add the garlic, and the mushrooms, and stir for 2 minutes. \\\\n4. Add the soy cream, stir and cook for three minutes. \\\\n5. Add the pickles, tomatoes, and beans, stir and simmer for five minutes. \\\\n6. Cut the bread in small squares and fry in the vegan butter until golden brown.\\\\n7. Cut the limes into cubes and squeeze the juice into the bean mixture. \\\\n8. Add the soy sauce, parsley, salt, pepper, cumin, cilantro, and dried figs. Stir, and add the kale.\\\\n9. Pour the bean mix into a blender. \\\\n10. Bake for 5 minutes in the oven at 180C. \\\\n11. Cut the sweet potatoes in cubes, and add to a pot with the remaining butter. Add the red beans mixture. \\\\n12. Cut the bell pepper into cubes and add to the pot. \\\\n13. Add the VEGAN minced meat, and cook in the oven at 180C for 10 minutes. \\\\n14. Add the avocado. \\\\n15. Add the chickpeas. \\\\n16. Add the chocolate.\\\\n17. Serve on bread with mustard and pommegrenade on top.\\\\n\\\\nOUTLINE:\\\\n0:00 - The Plan\\\\n2:15 - Ingredients\\\\n4:05 - What is GPT-3?\\\\n6:10 - Let\\'s cook\\\\n12:25 - The Taste Test\\\\n\\\\nGPT-3 on Wikipedia: https://en.wikipedia.org/wiki/GPT-3\\\\nGPT-3 Paper: https://arxiv.org/abs/2005.14165\\\\n\\\\nJonas\\' Scholar: https://scholar.google.de/citations?user=a1rCLUMAAAAJ\\\\nEdit by Ryan\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "930",
    "uploadDate": "2021-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/hIoCn_9QTVU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=CRlN-cYFxTk",
    "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ML Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nerf network, nerf neural network, deep learning tutorial, what is deep learning, introduction to deep learning, deep learning explanation, nerf network explanation, neural rendering, differentiable rendering, differentiable neural rendering, volume rendering, nerf view synthesis, view synthesis, view synthesis nerf, view synthesis neural, novel view synthesis, nerf",
    "scraped_at": 1684582643.6987503,
    "genre": "Science",
    "views": "108542",
    "desc": "#nerf #neuralrendering #deeplearning\\\\n\\\\nView Synthesis is a tricky problem, especially when only given a sparse set of images as an input. NeRF embeds an entire scene into the weights of a feedforward neural network, trained by backpropagation through a differential volume rendering procedure, and achieves state-of-the-art view synthesis. It includes directional dependence and is able to capture fine structural details, as well as reflection effects and transparency.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:50 - View Synthesis Task Description\\\\n5:50 - The fundamental difference to classic Deep Learning\\\\n7:00 - NeRF Core Concept\\\\n15:30 - Training the NeRF from sparse views\\\\n20:50 - Radiance Field Volume Rendering\\\\n23:20 - Resulting View Dependence\\\\n24:00 - Positional Encoding\\\\n28:00 - Hierarchical Volume Sampling\\\\n30:15 - Experimental Results\\\\n33:30 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2003.08934\\\\nWebsite \\\\u0026 Code: https://www.matthewtancik.com/nerf\\\\n\\\\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\\\\n\\\\nAbstract:\\\\nWe present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z) and viewing direction (\\xce\\xb8,\\xcf\\x95)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.\\\\n\\\\nAuthors: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2036",
    "uploadDate": "2021-04-19",
    "thumbnail_url": "https://i.ytimg.com/vi/CRlN"
  },
  {
    "link": "watch?v=7OdhtAiPfWY",
    "title": "I BUILT A NEURAL NETWORK IN MINECRAFT | Analog Redstone Network w/ Backprop & Optimizer (NO MODS)",
    "tags": "deep learning, machine learning, neural networks, ai, artificial intelligence, minecraft, neural networks explained, what is deep learning, deep learning tutorial, introduction to deep learning, deep learning in minecraft, minecraft machine learning, redstone neural network, minecraft redstone neural network, gaming neural network, neural network explained, machine learning in minecraft, vanilla minecraft computer, minecraft vanilla redstone computer, minecraft backpropagation",
    "scraped_at": 1684582641.284032,
    "genre": "Science",
    "views": "30476",
    "desc": "#minecraft #neuralnetwork #backpropagation\\\\n\\\\nI built an analog neural network in vanilla Minecraft without any mods or command blocks. The network uses Redstone wire power strengths to carry the signal through one hidden layer, including nonlinearities, and then do automatic backpropagation and even weight updates.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:50 - Redstone Components Explained\\\\n5:00 - Analog Multiplication in Redstone\\\\n7:00 - Gradient Descent for Square Root Computation\\\\n9:35 - Neural Network Demonstration\\\\n10:45 - Network Schema Explained\\\\n18:35 - The Network Learns a Datapoint\\\\n20:20 - Outro \\\\u0026 Conclusion\\\\n\\\\nI built this during a series of live streams and want to thank everyone who helped me and cheered for me in the chat!\\\\n\\\\nWorld saves here: https://github.com/yk/minecraft-neural-network\\\\nGame here: https://www.minecraft.net\\\\nMultiplier Inspiration: https://www.youtube.com/channel/UCLmzk4TlnLXCXCHcjuJe2ag\\\\n\\\\nCredits to Lanz for editing!\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1306",
    "uploadDate": "2021-04-14",
    "thumbnail_url": "https://i.ytimg.com/vi/7OdhtAiPfWY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qtu0aSTDE2I",
    "title": "DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, artificial intelligence, wake sleep algorithm, program synthesis, ai program synthesis, program synthesis deep learning, dreamcoder, dream coder, mit dream coder, bayesian program search, neural guided search, learning to sort a list, neural networks learn sorting, deep learning physical laws, deep learning symbolic reasoning, symbolic machine learning, symbolic artificial intelligence, deep learning tutorial",
    "scraped_at": 1684582643.7877202,
    "genre": "Science",
    "views": "26458",
    "desc": "#dreamcoder #programsynthesis #symbolicreasoning\\\\n\\\\nClassic Machine Learning struggles with few-shot generalization for tasks where humans can easily generalize from just a handful of examples, for example sorting a list of numbers. Humans do this by coming up with a short program, or algorithm, that explains the few data points in a compact way. DreamCoder emulates this by using neural guided search over a language of primitives, a library, that it builds up over time. By doing this, it can iteratively construct more and more complex programs by building on its own abstractions and therefore solve more and more difficult tasks in a few-shot manner by generating very short programs that solve the few given datapoints. The resulting system can not only generalize quickly but also delivers an explainable solution to its problems in form of a modular and hierarchical learned library. Combining this with classic Deep Learning for low-level perception is a very promising future direction.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:55 - DreamCoder System Architecture\\\\n9:00 - Wake Phase: Neural Guided Search\\\\n19:15 - Abstraction Phase: Extending the Internal Library\\\\n24:30 - Dreaming Phase: Training Neural Search on Fictional Programs and Replays\\\\n30:55 - Abstraction by Compressing Program Refactorings\\\\n32:40 - Experimental Results on LOGO Drawings\\\\n39:00 - Ablation Studies\\\\n39:50 - Re-Discovering Physical Laws\\\\n42:25 - Discovering Recursive Programming Algorithms\\\\n44:20 - Conclusions \\\\u0026 Discussion\\\\n\\\\nPaper: https://arxiv.org/abs/2006.08381\\\\nCode: https://github.com/ellisk42/ec\\\\n\\\\nAbstract:\\\\nExpert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep\\'\\' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton\\'s and Coulomb\\'s laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.\\\\n\\\\nAuthors: Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke Hewitt, Armando Solar-Lezama, Joshua B. Tenenbaum\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2898",
    "uploadDate": "2021-04-11",
    "thumbnail_url": "https://i.ytimg.com/vi/qtu0aSTDE2I/maxresdefault.jpg"
  },
  {
    "link": "watch?v=M2-BE5JotjA",
    "title": "PAIR AI Explorables | Is the problem in the data? Examples on Fairness, Diversity, and Bias.",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, bias in machine learning, ai bias, algorithmic bias, bias in algorithms, garbage in garbage out, the problem is in the data, the problem is not in the data, twitter machine learning, machine learning bias, machine learning in society, ethical ai, ai ethics, ai ethics bias, where does bias come from, google ai",
    "scraped_at": 1684582643.8727195,
    "genre": "Science",
    "views": "12321",
    "desc": "In the recurring debate about bias in Machine Learning models, there is a growing argument saying that \\\\\"",
    "lengthSeconds": "1413",
    "uploadDate": "2021-04-07",
    "thumbnail_url": "https://i.ytimg.com/vi/M2"
  },
  {
    "link": "watch?v=rHQPBqMULXo",
    "title": "Machine Learning PhD Survival Guide 2021 | Advice on Topic Selection, Papers, Conferences & more!",
    "tags": "machine learning phd, how to do a phd in machine learning, phd advice, machine learning phd thesis topics, machine learning phd topics, how to machine learning phd, how to select a thesis topic, how to machine learning conferences, how to write a machine learning paper, advice for phd students, advice for new phd students, how to survive a phd, what to do in a machine learning phd, deep learning phd advice, machine learning phd thesis, machine learning phd thesis topic",
    "scraped_at": 1684582643.9667501,
    "genre": "Science",
    "views": "67841",
    "desc": "#machinelearning #phd #howto\\\\n\\\\nThis video is advice for new PhD students in the field of Machine Learning in 2021 and after. The field has shifted dramatically in the last few years and navigating grad school can be very hard, especially when you\\'re as clueless as I was when I started. The video is a personal recount of my mistakes and what I\\'ve learned from them. If you already have several published papers and know what to do, this video is not for you. However, if you are not even sure where to start, how to select a topic, or what goes in a paper, you might benefit from this video, because that\\'s exactly how I felt.\\\\n\\\\nMain Takeaways:\\\\n- Select niche topics rather than hype topics\\\\n- Write papers that can\\'t be rejected\\\\n- Don\\'t be discouraged by bad reviews\\\\n- Take reviewing \\\\u0026 teaching seriously\\\\n- Keep up your focus\\\\n- Conferences are for networking\\\\n- Internships are great opportunities\\\\n- Team up with complementary skills\\\\n- Don\\'t work too hard\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:25 - Thesis Topic Selection\\\\n4:25 - How To Publish Papers\\\\n5:35 - Dealing With Reviewers\\\\n6:30 - How To Be A Reviewer\\\\n7:40 - Take Teaching Seriously\\\\n8:30 - Maintain Focus\\\\n10:20 - Navigating Conferences\\\\n12:40 - Internships\\\\n13:40 - Collaborations\\\\n14:55 - Don\\'t Forget To Enjoy\\\\n\\\\nTranscript: https://www.notion.so/Yannic-Kilcher-s-PhD-Survival-Guide-Transcript-c507ab8e963e496fbb185cdfdb8d65ae\\\\n\\\\nCredits to Lanz for editing\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "986",
    "uploadDate": "2021-03-30",
    "thumbnail_url": "https://i.ytimg.com/vi/rHQPBqMULXo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=J7CrtblmMnU",
    "title": "Is Google Translate Sexist? Gender Stereotypes in Statistical Machine Translation",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google translate, gender stereotype, machine learning biased, debiasing, debiasing machine learning, algorithmic fairness, machine learning social justice, machine learning bias, deep learning bias, deep learning gender, what is deep learning, deep learning tutorial, introduction to deep learning, hungarian translate, translate gender stereotype",
    "scraped_at": 1684582645.8817215,
    "genre": "Science",
    "views": "12344",
    "desc": "#genderbias #algorithmicfairness #debiasing\\\\n\\\\nA brief look into gender stereotypes in Google Translate. The origin is a Tweet containing a Hungarian text. Hungarian is a gender-neutral language, so translating gender pronouns is ambiguous. Turns out that Google Translate assigns very stereotypical pronouns. In this video, we\\'ll have a look at the origins and possible solutions to this problem.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:10 - Digging Deeper\\\\n2:30 - How does Machine Translation work?\\\\n3:50 - Training Data Problems\\\\n4:40 - Learning Algorithm Problems\\\\n5:45 - Argmax Output Problems\\\\n6:45 - Pragmatics\\\\n7:50 - More on Google Translate\\\\n9:40 - Social Engineering\\\\n11:15 - Conclusion\\\\n\\\\nSongs:\\\\nLike That - Anno Domini Beats\\\\nSubmarine - Dyalla\\\\nDude - Patrick Patrikios\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "723",
    "uploadDate": "2021-03-23",
    "thumbnail_url": "https://i.ytimg.com/vi/J7CrtblmMnU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=P_xeshTnPZg",
    "title": "Perceiver: General Perception with Iterative Attention (Google DeepMind Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, what is deep learning, introduction to deep learning, deepmind, perceiver, cross attention, attention mechanism, attention is all you need, google deepmind, deepmind perceiver, perceiver model, perciever model, perciever, self attention, rnn, recurrent neural network, weight sharing, computer vision, natural language processing, fourier features",
    "scraped_at": 1684582644.0587451,
    "genre": "Science",
    "views": "50026",
    "desc": "#perceiver #deepmind #transformer\\\\n\\\\nInspired by the fact that biological creatures attend to multiple modalities at the same time, DeepMind releases its new Perceiver model. Based on the Transformer architecture, the Perceiver makes no assumptions on the modality of the input data and also solves the long-standing quadratic bottleneck problem. This is achieved by having a latent low-dimensional Transformer, where the input data is fed multiple times via cross-attention. The Perceiver\\'s weights can also be shared across layers, making it very similar to an RNN. Perceivers achieve competitive performance on ImageNet and state-of-the-art on other modalities, all while making no architectural adjustments to input data.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:20 - Built-In assumptions of Computer Vision Models\\\\n5:10 -  The Quadratic Bottleneck of Transformers\\\\n8:00 - Cross-Attention in Transformers\\\\n10:45 - The Perceiver Model Architecture \\\\u0026 Learned Queries\\\\n20:05 - Positional Encodings via Fourier Features\\\\n23:25 - Experimental Results \\\\u0026 Attention Maps\\\\n29:05 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2103.03206\\\\n\\\\nMy Video on Transformers (Attention is All You Need): https://youtu.be/iDulhoQ2pro\\\\n\\\\nAbstract:\\\\nBiological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.\\\\n\\\\nAuthors: Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1776",
    "uploadDate": "2021-03-22",
    "thumbnail_url": "https://i.ytimg.com/vi/P_xeshTnPZg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Elxn8rS88bI",
    "title": "Pretrained Transformers as Universal Computation Engines (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, artificial intelligence, paper, what is deep learning, deep learning tutorial, introduction to deep learning, berkeley, google brain, facebook ai research, pretrained transformers, gpt",
    "scraped_at": 1684582644.153745,
    "genre": "Science",
    "views": "22027",
    "desc": "#universalcomputation #pretrainedtransformers #finetuning\\\\n\\\\nLarge-scale pre-training and subsequent fine-tuning is a common recipe for success with transformer models in machine learning. However, most such transfer learning is done when a model is pre-trained on the same or a very similar modality to the final task to be solved. This paper demonstrates that transformers can be fine-tuned to completely different modalities, such as from language to vision. Moreover, they demonstrate that this can be done by freezing all attention layers, tuning less than .1% of all parameters. The paper further claims that language modeling is a superior pre-training task for such cross-domain transfer. The paper goes through various ablation studies to make its point.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:00 - Frozen Pretrained Transformers\\\\n4:50 - Evaluated Tasks\\\\n10:05 - The Importance of Training LayerNorm\\\\n17:10 - Modality Transfer\\\\n25:10 - Network Architecture Ablation\\\\n26:10 - Evaluation of the Attention Mask\\\\n27:20 - Are FPTs Overfitting or Underfitting?\\\\n28:20 - Model Size Ablation\\\\n28:50 - Is Initialization All You Need?\\\\n31:40 - Full Model Training Overfits\\\\n32:15 - Again the Importance of Training LayerNorm\\\\n33:10 - Conclusions \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2103.05247\\\\nCode: https://github.com/kzl/universal-computation\\\\n\\\\nAbstract:\\\\nWe investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks.\\\\n\\\\nAuthors: Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2042",
    "uploadDate": "2021-03-16",
    "thumbnail_url": "https://i.ytimg.com/vi/Elxn8rS88bI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Ag1bw8MfHGQ",
    "title": "Yann LeCun - Self-Supervised Learning: The Dark Matter of Intelligence (FAIR Blog Post Explained)",
    "tags": "deep learning, machine learning, explained, neural networks, artificial intelligence, deep learning tutorial, what is deep learning, introduction to deep learning, what is self supervised learning, self supervised learning, self",
    "scraped_at": 1684582645.9777188,
    "genre": "Science",
    "views": "92047",
    "desc": "#selfsupervisedlearning #yannlecun #facebookai\\\\n\\\\nDeep Learning systems can achieve remarkable, even super-human performance through supervised learning on large, labeled datasets. However, there are two problems: First, collecting ever more labeled data is expensive in both time and money. Second, these deep neural networks will be high performers on their task, but cannot easily generalize to other, related tasks, or they need large amounts of data to do so. In this blog post, Yann LeCun and Ishan Misra of Facebook AI Research (FAIR) describe the current state of Self-Supervised Learning (SSL) and argue that it is the next step in the development of AI that uses fewer labels and can transfer knowledge faster than current systems. They suggest as a promising direction to build non-contrastive latent-variable predictive models, like VAEs, but ones that also provide high-quality latent representations for downstream tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:15 - Supervised Learning, Self-Supervised Learning, and Common Sense\\\\n7:35 - Predicting Hidden Parts from Observed Parts\\\\n17:50 - Self-Supervised Learning for Language vs Vision\\\\n26:50 - Energy-Based Models\\\\n30:15 - Joint-Embedding Models\\\\n35:45 - Contrastive Methods\\\\n43:45 - Latent-Variable Predictive Models and GANs\\\\n55:00 - Summary \\\\u0026 Conclusion\\\\n\\\\nPaper (Blog Post): https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence\\\\nMy Video on BYOL: https://www.youtube.com/watch?v=YPfUiOMYOEE\\\\n\\\\nERRATA:\\\\n- The difference between loss and energy: Energy is for inference, loss is for training.\\\\n- The R(z) term is a regularizer that restricts the capacity of the latent variable. I think I said both of those things, but never together.\\\\n- The way I explain why BERT is contrastive is wrong. I haven\\'t figured out why just yet, though :)\\\\n\\\\nVideo approved by Antonio.\\\\n\\\\nAbstract:\\\\nWe believe that self-supervised learning (SSL) is one of the most promising ways to build such background knowledge and approximate a form of common sense in AI systems.\\\\n\\\\nAuthors: Yann LeCun, Ishan Misra\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3517",
    "uploadDate": "2021-03-11",
    "thumbnail_url": "https://i.ytimg.com/vi/Ag1bw8MfHGQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Z_kWZpgEZ7w",
    "title": "Multimodal Neurons in Artificial Neural Networks (w/ OpenAI Microscope, Research Paper Explained)",
    "tags": "deep learning, machine learning, explained, neural networks, ai, artificial intelligence, paper, openai, openai emotions, openai dalle, openai clip, openai microscope, openai clip microscope, alec radford, emotion neuron, deep learning emotion, chris olah, chris olah openai, neural network feature visualization, multimodal neural network, what does a neural network learn, what do neural networks learn, how do neural networks work, what does openai do, faceted visualization",
    "scraped_at": 1684582646.0907195,
    "genre": "Science",
    "views": "20181",
    "desc": "#openai #clip #microscope\\\\n\\\\nOpenAI does a huge investigation into the inner workings of their recent CLIP model via faceted feature visualization and finds amazing things: Some neurons in the last layer respond to distinct concepts across multiple modalities, meaning they fire for photographs, drawings, and signs depicting the same concept, even when the images are vastly distinct. Through manual examination, they identify and investigate neurons corresponding to persons, geographical regions, religions, emotions, and much more. In this video, I go through the publication and then I present my own findings from digging around in the OpenAI Microscope.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:35 - OpenAI Microscope\\\\n7:10 - Categories of found neurons\\\\n11:10 - Person Neurons\\\\n13:00 - Donald Trump Neuron\\\\n17:15 - Emotion Neurons\\\\n22:45 - Region Neurons\\\\n26:40 - Sparse Mixture of Emotions\\\\n28:05 - Emotion Atlas\\\\n29:45 - Adversarial Typographic Attacks\\\\n31:55 - Stroop Test\\\\n33:10 - My Findings in OpenAI Microscope\\\\n33:30 - Superman Neuron\\\\n33:50 - Resting B*tchface Neuron\\\\n34:10 - Trash Bag Neuron\\\\n35:25 - God Weightlifting Neuron\\\\n36:40 - Organ Neuron\\\\n38:35 - Film Spool Neuron\\\\n39:05 - Feather Neuron\\\\n39:20 - Spartan Neuron\\\\n40:25 - Letter E Neuron\\\\n40:35 - Cleanin Neuron\\\\n40:45 - Frown Neuron\\\\n40:55 - Lion Neuron\\\\n41:05 - Fashion Model Neuron\\\\n41:20 - Baseball Neuron\\\\n41:50 - Bride Neuron\\\\n42:00 - Navy Neuron\\\\n42:30 - Hemp Neuron\\\\n43:25 - Staircase Neuron\\\\n43:45 - Disney Neuron\\\\n44:15 - Hillary Clinton Neuron\\\\n44:50 - God Neuron\\\\n45:15 - Blurry Neuron\\\\n45:35 - Arrow Neuron\\\\n45:55 - Trophy Presentation Neuron\\\\n46:10 - Receding Hairline Neuron\\\\n46:30 - Traffic Neuron\\\\n46:40 - Raised Hand Neuron\\\\n46:50 - Google Maps Neuron\\\\n47:15 - Nervous Smile Neuron\\\\n47:30 - Elvis Neuron\\\\n47:55 - The Flash Neuron\\\\n48:05 - Beard Neuron\\\\n48:15 - Kilt Neuron\\\\n48:25 - Rainy Neuron\\\\n48:35 - Electricity Neuron\\\\n48:50 - Droplets Neuron\\\\n49:00 - Escape Neuron\\\\n49:25 - King Neuron\\\\n49:35 - Country Neuron\\\\n49:45 - Overweight Men Neuron\\\\n49:55 - Wedding\\\\n50:05 - Australia Neuron\\\\n50:15 - Yawn Neuron\\\\n50:30 - Bees \\\\u0026 Simpsons Neuron\\\\n50:40 - Mussles Neuron\\\\n50:50 - Spice Neuron\\\\n51:00 - Conclusion\\\\n\\\\nPaper: https://distill.pub/2021/multimodal-neurons/\\\\nMy Findings: https://www.notion.so/CLIP-OpenAI-Microscope-Findings-27465eac373c451d8083428443e0837c\\\\nMy Video on CLIP: https://youtu.be/T9XSU0pKX2E\\\\nMy Video on Feature Visualizations \\\\u0026 The OpenAI Microscope: https://youtu.be/Ok44otx90D4\\\\n\\\\nAbstract:\\\\nIn 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry. The exciting thing wasn\\xe2\\x80\\x99t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person\\xe2\\x80\\x99s name. The neurons were multimodal. As the lead author would put it: \\\\\"",
    "lengthSeconds": "3089",
    "uploadDate": "2021-03-05",
    "thumbnail_url": "https://i.ytimg.com/vi/Z_kWZpgEZ7w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=cllFzkvrYmE",
    "title": "GLOM: How to represent part-whole hierarchies in a neural network (Geoff Hinton's Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, geoff hinton, geoff hinton capsule networks, geoff hinton neural networks, geoffrey hinton, geoffrey hinton deep learning, geoffrey hinton glom, hinton glom, glom model, deep learning tutorial, introduction to deep learning, capsule networks, computer vision, capsule networks explained, google brain, google ai, schmidhuber, transformer, attention mechanism, consensus algorithm, column",
    "scraped_at": 1684582646.1827195,
    "genre": "Science",
    "views": "43228",
    "desc": "#glom #hinton #capsules\\\\n\\\\nGeoffrey Hinton describes GLOM, a Computer Vision model that combines transformers, neural fields, contrastive learning, capsule networks, denoising autoencoders and RNNs. GLOM decomposes an image into a parse tree of objects and their parts. However, unlike previous systems, the parse tree is constructed dynamically and differently for each input, without changing the underlying neural network. This is done by a multi-step consensus algorithm that runs over different levels of abstraction at each location of an image simultaneously. GLOM is just an idea for now but suggests a radically new approach to AI visual scene understanding.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:10 - Object Recognition as Parse Trees\\\\n5:40 - Capsule Networks\\\\n8:00 - GLOM Architecture Overview\\\\n13:10 - Top-Down and Bottom-Up communication\\\\n18:30 - Emergence of Islands\\\\n22:00 - Cross-Column Attention Mechanism\\\\n27:10 - My Improvements for the Attention Mechanism\\\\n35:25 - Some Design Decisions\\\\n43:25 - Training GLOM as a Denoising Autoencoder \\\\u0026 Contrastive Learning\\\\n52:20 - Coordinate Transformations \\\\u0026 Representing Uncertainty\\\\n57:05 - How GLOM handles Video\\\\n1:01:10 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2102.12627\\\\n\\\\nAbstract:\\\\nThis paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language\\\\n\\\\nAuthors: Geoffrey Hinton\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3805",
    "uploadDate": "2021-02-27",
    "thumbnail_url": "https://i.ytimg.com/vi/cllFzkvrYmE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=RSSVWpBak6s",
    "title": "Linear Transformers Are Secretly Fast Weight Memory Systems (Machine Learning Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, fast weights, fast weights hinton, fast weights neural network, schmidhuber, j",
    "scraped_at": 1684582646.2767515,
    "genre": "Science",
    "views": "17149",
    "desc": "#fastweights #deeplearning #transformers\\\\n\\\\nTransformers are dominating Deep Learning, but their quadratic memory and compute requirements make them expensive to train and hard to use. Many papers have attempted to linearize the core module: the attention mechanism, using kernels - for example, the Performer. However, such methods are either not satisfactory or have other downsides, such as a reliance on random features. This paper establishes an intrinsic connection between linearized (kernel) attention and the much older Fast Weight Memory Systems, in part popularized by J\\xc3\\xbcrgen Schmidhuber in the 90s. It shows the fundamental limitations of these algorithms and suggests new update rules and new kernels in order to fix these problems. The resulting model compares favorably to Performers on key synthetic experiments and real-world tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - Fast Weight Systems\\\\n7:00 - Distributed Storage of Symbolic Values\\\\n12:30 - Autoregressive Attention Mechanisms\\\\n18:50 - Connecting Fast Weights to Attention Mechanism\\\\n22:00 - Softmax as a Kernel Method (Performer)\\\\n25:45 - Linear Attention as Fast Weights\\\\n27:50 - Capacity Limitations of Linear Attention\\\\n29:45 - Synthetic Data Experimental Setup\\\\n31:50 - Improving the Update Rule\\\\n37:30 - Deterministic Parameter-Free Projection (DPFP) Kernel\\\\n46:15 - Experimental Results\\\\n50:50 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2102.11174\\\\nCode: https://github.com/ischlag/fast-weight-transformers\\\\nMachine Learning Street Talk on Kernels: https://youtu.be/y_RjsDHl5Y4\\\\n\\\\nAbstract:\\\\nWe show the formal equivalence of linearised self-attention mechanisms and fast weight memories from the early \\'90s. From this observation we infer a memory capacity limitation of recent linearised softmax attention variants. With finite memory, a desirable behaviour of fast weight memory models is to manipulate the contents of memory and dynamically interact with it. Inspired by previous work on fast weights, we propose to replace the update rule with an alternative rule yielding such behaviour. We also propose a new kernel function to linearise attention, balancing simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.\\\\n\\\\nAuthors: Imanol Schlag, Kazuki Irie, J\\xc3\\xbcrgen Schmidhuber\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3098",
    "uploadDate": "2021-02-26",
    "thumbnail_url": "https://i.ytimg.com/vi/RSSVWpBak6s/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_c6A33Fg5Ns",
    "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention (Machine Learning Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, huggingface, huggingface transformers, microsoft, microsoft research, bert, roberta, deberta, nlp, natural language processing, glue, superglue, state of the art, transformers, attention, attention mechanism, disentanglement, disentangled representation, positional encodings, position embeddings, masked language modelling, pretraining, open source",
    "scraped_at": 1684582644.2617388,
    "genre": "Science",
    "views": "15946",
    "desc": "#deberta #bert #huggingface\\\\n\\\\nDeBERTa by Microsoft is the next iteration of BERT-style Self-Attention Transformer models, surpassing RoBERTa in State-of-the-art in multiple NLP tasks. DeBERTa brings two key improvements: First, they treat content and position information separately in a new form of disentangled attention mechanism. Second, they resort to relative positional encodings throughout the base of the transformer, and provide absolute positional encodings only at the very end. The resulting model is both more accurate on downstream tasks and needs less pretraining steps to reach good accuracy. Models are also available in Huggingface and on Github.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:15 - Position Encodings in Transformer\\'s Attention Mechanism\\\\n9:55 - Disentangling Content \\\\u0026 Position Information in Attention\\\\n21:35 - Disentangled Query \\\\u0026 Key construction in the Attention Formula\\\\n25:50 - Efficient Relative Position Encodings\\\\n28:40 - Enhanced Mask Decoder using Absolute Position Encodings\\\\n35:30 - My Criticism of EMD\\\\n38:05 - Experimental Results\\\\n40:30 - Scaling up to 1.5 Billion Parameters\\\\n44:20 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.03654\\\\nCode: https://github.com/microsoft/DeBERTa\\\\nHuggingface models: https://huggingface.co/models?search=deberta\\\\n\\\\nAbstract:\\\\nRecent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models\\' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).\\\\n\\\\nAuthors: Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2713",
    "uploadDate": "2021-02-25",
    "thumbnail_url": "https://i.ytimg.com/vi/_c6A33Fg5Ns/maxresdefault.jpg"
  },
  {
    "link": "watch?v=o75ybZ-6Uu8",
    "title": "Dreamer v2: Mastering Atari with Discrete World Models (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, reinforcement learning, deep reinforcement learning, dreamer, dreamer v2, dreamer rl, dreamer reinforcement learning, google reinforcement learning, deepmind reinforcement learning, google ai, world model, world model reinforcement learning, google deepmind world model, google deepmind reinforcement learning, atari reinforcement learning, atari world model, rainbow, muzero",
    "scraped_at": 1684582644.3657186,
    "genre": "Science",
    "views": "21892",
    "desc": "#dreamer #deeprl #reinforcementlearning\\\\n\\\\nModel-Based Reinforcement Learning has been lagging behind Model-Free RL on Atari, especially among single-GPU algorithms. This collaboration between Google AI, DeepMind, and the University of Toronto (UofT) pushes world models to the next level. The main contribution is a learned latent state consisting of one discrete part and one stochastic part, whereby the stochastic part is a set of 32 categorical variables, each with 32 possible values. The world model can freely decide how it wants to use these variables to represent the input, but is tasked with the prediction of future observations and rewards. This procedure gives rise to an informative latent representation and in a second step, reinforcement learning (A2C Actor-Critic) can be done purely - and very efficiently - on the basis of the world-model\\'s latent states. No observations needed! This paper combines this with straight-through estimators, KL balancing, and many other tricks to achieve state-of-the-art single-GPU performance in Atari.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:50 - Short Recap of Reinforcement Learning\\\\n6:05 - Problems with Model-Free Reinforcement Learning\\\\n10:40 - How World Models Help\\\\n12:05 - World Model Learner Architecture\\\\n16:50 - Deterministic \\\\u0026 Stochastic Hidden States\\\\n18:50 - Latent Categorical Variables\\\\n22:00 - Categorical Variables and Multi-Modality\\\\n23:20 - Sampling \\\\u0026 Stochastic State Prediction\\\\n30:55 - Actor-Critic Learning in Dream Space\\\\n32:05 - The Incompleteness of Learned World Models\\\\n34:15 - How General is this Algorithm?\\\\n37:25 - World Model Loss Function\\\\n39:20 - KL Balancing\\\\n40:35 - Actor-Critic Loss Function\\\\n41:45 - Straight-Through Estimators for Sampling Backpropagation\\\\n46:25 - Experimental Results\\\\n52:00 - Where Does It Fail?\\\\n54:25 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2010.02193\\\\nCode: https://github.com/danijar/dreamerv2\\\\nAuthor Blog: https://danijar.com/project/dreamerv2/\\\\nGoogle AI Blog: https://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html\\\\n\\\\nERRATA (from the authors): \\\\n- KL balancing (prior vs posterior within the KL) is different from beta VAEs (reconstruction vs KL)\\\\n- The vectors of categoricals can in theory represent 32^32 different images so their capacity is quite large\\\\n\\\\nAbstract:\\\\nIntelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, DreamerV2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.\\\\n\\\\nAuthors: Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3298",
    "uploadDate": "2021-02-19",
    "thumbnail_url": "https://i.ytimg.com/vi/o75ybZ"
  },
  {
    "link": "watch?v=R5DiLFOMZrc",
    "title": "TransGAN: Two Transformers Can Make One Strong GAN (Machine Learning Research Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, neural networks, ai, artificial intelligence, attention neural networks, attention is all you need, transformer gan, transformer gans, transformer generative adversarial network, generative adversarial network, attention mechanism, self attention, vision transformer, pixelshuffle, superresolution, local attention, multihead attention, transformer generator, google, machine learning explained, deep learning explained, paper explained, transgan",
    "scraped_at": 1684582644.4577193,
    "genre": "Science",
    "views": "30685",
    "desc": "#transformer #gan #machinelearning\\\\n\\\\nGenerative Adversarial Networks (GANs) hold the state-of-the-art when it comes to image generation. However, while the rest of computer vision is slowly taken over by transformers or other attention-based architectures, all working GANs to date contain some form of convolutional layers. This paper changes that and builds TransGAN, the first GAN where both the generator and the discriminator are transformers. The discriminator is taken over from ViT (an image is worth 16x16 words), and the generator uses pixelshuffle to successfully up-sample the generated resolution. Three tricks make training work: Data augmentations using DiffAug, an auxiliary superresolution task, and a localized initialization of self-attention. Their largest model reaches competitive performance with the best convolutional GANs on CIFAR10, STL-10, and CelebA.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction \\\\u0026 Overview\\\\n3:05 - Discriminator Architecture\\\\n5:25 - Generator Architecture\\\\n11:20 - Upsampling with PixelShuffle\\\\n15:05 - Architecture Recap\\\\n16:00 - Vanilla TransGAN Results\\\\n16:40 - Trick 1: Data Augmentation with DiffAugment\\\\n19:10 - Trick 2: Super-Resolution Co-Training\\\\n22:20 - Trick 3: Locality-Aware Initialization for Self-Attention\\\\n27:30 - Scaling Up \\\\u0026 Experimental Results\\\\n28:45 - Recap \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2102.07074\\\\nCode: https://github.com/VITA-Group/TransGAN\\\\nMy Video on ViT: https://youtu.be/TrdevFK_am4\\\\n\\\\nAbstract:\\\\nThe recent explosive interest on transformers has suggested their potential to become powerful \\\\\"",
    "lengthSeconds": "1793",
    "uploadDate": "2021-02-17",
    "thumbnail_url": "https://i.ytimg.com/vi/R5DiLFOMZrc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rNkHjZtH0RQ",
    "title": "NFNets: High-Performance Large-Scale Image Recognition Without Normalization (ML Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deep learning tutorial, machine learning tutorial, machine learning explained, batch normalization, jax, layer normalization, gradient clipping, weight standardization, normalizer",
    "scraped_at": 1684582646.366751,
    "genre": "Science",
    "views": "36915",
    "desc": "#nfnets #deepmind #machinelearning\\\\n\\\\nBatch Normalization is a core component of modern deep learning. It enables training at higher batch sizes, prevents mean shift, provides implicit regularization, and allows networks to reach higher performance than without. However, BatchNorm also has disadvantages, such as its dependence on batch size and its computational overhead, especially in distributed settings. Normalizer-Free Networks, developed at Google DeepMind, are a class of CNNs that achieve state-of-the-art classification accuracy on ImageNet without batch normalization. This is achieved by using adaptive gradient clipping (AGC), combined with a number of improvements in general network architecture. The resulting networks train faster, are more accurate, and provide better transfer learning performance. Code is provided in Jax.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:40 - What\\'s the problem with BatchNorm?\\\\n11:00 - Paper contribution Overview\\\\n13:30 - Beneficial properties of BatchNorm\\\\n15:30 - Previous work: NF-ResNets\\\\n18:15 - Adaptive Gradient Clipping\\\\n21:40 - AGC and large batch size\\\\n23:30 - AGC induces implicit dependence between training samples\\\\n28:30 - Are BatchNorm\\'s problems solved?\\\\n30:00 - Network architecture improvements\\\\n31:10 - Comparison to EfficientNet\\\\n33:00 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2102.06171\\\\nCode: https://github.com/deepmind/deepmind-research/tree/master/nfnets\\\\n\\\\nMy Video on BatchNorm: https://www.youtube.com/watch?v=OioFONrSETc\\\\nMy Video on ResNets: https://www.youtube.com/watch?v=GWt6Fu05voI\\\\n\\\\nERRATA (from Lucas Beyer): \\\\\"",
    "lengthSeconds": "2066",
    "uploadDate": "2021-02-14",
    "thumbnail_url": "https://i.ytimg.com/vi/rNkHjZtH0RQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=m-zrcmRd7E4",
    "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention (AI Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, machine learning explained, transformers explained, nystrom, nystromformer, nystromer, nystrom approximation, self attention, attention mechanism, attention is all you need, transformer, linear transformer, linformer, linear attention, machine learning tutorial, quadratic attention, matrix approximation, low rank, landmark points, landmarks, matrix reconstruction, fast attention",
    "scraped_at": 1684582644.566721,
    "genre": "Science",
    "views": "15971",
    "desc": "#transformer #nystromer #nystromformer\\\\n\\\\nThe Nystr\\xc3\\xb6mformer (or Nystromformer, Nystr\\xc3\\xb6mer, Nystromer), is a new drop-in replacement for approximating the Self-Attention matrix in Transformers with linear memory and time requirements. Most importantly, it uses the Nystrom-Method to subselect (or segment mean) queries and keys as so-called landmarks and uses those to reconstruct the inherently low-rank attention matrix. This is relevant for many areas of Machine Learning, especially Natural Language processing, where it enables longer sequences of text to be processed at once.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:30 - The Quadratic Memory Bottleneck in Self-Attention\\\\n7:20 - The Softmax Operation in Attention\\\\n11:15 - Nystr\\xc3\\xb6m-Approximation\\\\n14:00 - Getting Around the Softmax Problem\\\\n18:05 - Intuition for Landmark Method\\\\n28:05 - Full Algorithm\\\\n30:20 - Theoretical Guarantees\\\\n35:55 - Avoiding the Large Attention Matrix\\\\n36:55 - Subsampling Keys vs Negative Sampling\\\\n43:15 - Experimental Results\\\\n47:00 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2102.03902\\\\nCode: https://github.com/mlpen/Nystromformer\\\\nAppendix: https://github.com/mlpen/Nystromformer/blob/main/doc/Nystromformer_Supplement.pdf\\\\nLRA Results: https://twitter.com/tanmingxing/status/1359301186734620675\\\\nTwitter lucidrains w/ author: https://twitter.com/lucidrains/status/1359597104075661312\\\\nTwitter lucidrains w/ _clashluke: https://twitter.com/_clashluke/status/1359483460851802115\\\\n\\\\nAbstract:\\\\nTransformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences -- a topic being actively studied in the community. To address this limitation, we propose Nystr\\xc3\\xb6mformer -- a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\\xc3\\xb6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\\xc3\\xb6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\\xc3\\xb6mformer performs comparably, or in a few cases, even slightly better, than standard Transformer. Our code is at this https URL.\\\\n\\\\nAuthors: Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2892",
    "uploadDate": "2021-02-11",
    "thumbnail_url": "https://i.ytimg.com/vi/m"
  },
  {
    "link": "watch?v=ahRPdiCop3E",
    "title": "Deep Networks Are Kernel Machines (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, what is deep learning, deep neural networks, neural networks gradient descent, kernel machines, kernel trick, svm, support vector machine, sgd, stochastic gradient descent, machine learning theory, pedro domingos, linear regression, nearest neighbor, representations, data representations, representation learning, proof, math proof, learning theory, representer theorem",
    "scraped_at": 1684582646.456751,
    "genre": "Science",
    "views": "55027",
    "desc": "#deeplearning #kernels #neuralnetworks\\\\n\\\\nFull Title: Every Model Learned by Gradient Descent Is Approximately a Kernel Machine\\\\n\\\\nDeep Neural Networks are often said to discover useful representations of the data. However, this paper challenges this prevailing view and suggest that rather than representing the data, deep neural networks store superpositions of the training data in their weights and act as kernel machines at inference time. This is a theoretical paper with a main theorem and an understandable proof and the result leads to many interesting implications for the field.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n4:50 - What is a Kernel Machine?\\\\n10:25 - Kernel Machines vs Gradient Descent\\\\n12:40 - Tangent Kernels\\\\n22:45 - Path Kernels\\\\n25:00 - Main Theorem\\\\n28:50 - Proof of the Main Theorem\\\\n39:10 - Implications \\\\u0026 My Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2012.00152\\\\nStreet Talk about Kernels: https://youtu.be/y_RjsDHl5Y4\\\\n\\\\nERRATA: I simplify a bit too much when I pit kernel methods against gradient descent. Of course, you can even learn kernel machines using GD, they\\'re not mutually exclusive. And it\\'s also not true that you \\\\\"",
    "lengthSeconds": "2583",
    "uploadDate": "2021-02-04",
    "thumbnail_url": "https://i.ytimg.com/vi/ahRPdiCop3E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zdb8MM94A5c",
    "title": "Feedback Transformers: Addressing Some Limitations of Transformers with Feedback Memory (Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, transformer, rnn, lstm, seq2seq, gpt3, gpt",
    "scraped_at": 1684582644.7307205,
    "genre": "Science",
    "views": "14739",
    "desc": "#ai #science #transformers\\\\n\\\\nAutoregressive Transformers have taken over the world of Language Modeling (GPT-3). However, in order to train them, people use causal masking and sample parallelism, which means computation only happens in a feedforward manner. This results in higher layer information, which would be available, to not be used in the lower layers of subsequent tokens, and leads to a loss in the computational capabilities of the overall model. Feedback Transformers trade-off training speed for access to these representations and demonstrate remarkable improvements in complex reasoning and long-range dependency tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:55 - Problems of Autoregressive Processing\\\\n3:30 - Information Flow in Recurrent Neural Networks\\\\n7:15 - Information Flow in Transformers\\\\n9:10 - Solving Complex Computations with Neural Networks\\\\n16:45 - Causal Masking in Transformers\\\\n19:00 - Missing Higher Layer Information Flow\\\\n26:10 - Feedback Transformer Architecture\\\\n30:00 - Connection to Attention-RNNs\\\\n36:00 - Formal Definition\\\\n37:05 - Experimental Results\\\\n43:10 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2002.09402\\\\n\\\\nMy video on Attention: https://youtu.be/iDulhoQ2pro\\\\n\\\\nERRATA: Sometimes I say \\\\\"",
    "lengthSeconds": "2631",
    "uploadDate": "2021-02-02",
    "thumbnail_url": "https://i.ytimg.com/vi/zdb8MM94A5c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yFAuXmcGk2Y",
    "title": "SingularityNET - A Decentralized, Open Market and Network for AIs (Whitepaper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, singularity, singularitynet, agi, ben goertzel, goertzel, hanson, hanson robotics, sophia, network, api, offercoin, offernetworks, offer networks, offer coin, agi token, erc20, ethereum, cardano, governance, benefit, reputation, reputation system, liquid rank, liquidrank, deoldify, inflation, ico, matchmaking, graph, opencog, open cog, tontoni phi, intelligence, artificial general intelligence, blockchain",
    "scraped_at": 1684582644.8357205,
    "genre": "Science",
    "views": "17088",
    "desc": "#ai #research #blockchain\\\\n\\\\nBig Tech is currently dominating the pursuit of ever more capable AI. This happens behind closed doors and results in a monopoly of power. SingularityNET is an open, decentralized network where anyone can offer and consume AI services, and where AI agents can interlink with each other to provide ever more sophisticated AI, with the goal to create a singularity that\\'s beneficial for humanity. This video takes a look at the basics behind SingularityNET and some of its core components.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:55 - Document Summarization Example Workflow\\\\n5:50 - Why AI needs a Marketplace?\\\\n9:20 - A network of APIs\\\\n12:30 - AI Evaluators \\\\u0026 Matchmakers\\\\n15:00 - My criticisms of the Marketplace\\\\n17:45 - What is on the Blockchain?\\\\n20:45 - AI Marketplace Demo\\\\n22:00 - The AGI Token \\\\u0026 Inflation\\\\n26:30 - Reputation System \\\\u0026 other features\\\\n30:00 - Democratic Governance\\\\n33:00 - Benefit Tasks\\\\n36:15 - My general thoughts on the application examples\\\\n38:05 - Measuring Intelligence on SingularityNET\\\\n45:15 - OfferNet Economy\\\\n50:00 - Summary \\\\u0026 Comments\\\\n\\\\nWhitepaper: https://public.singularitynet.io/whitepaper.pdf\\\\nWebsite: https://singularitynet.io/\\\\nAI Marketplace: https://beta.singularitynet.io/aimarketplace\\\\n\\\\nReferences:\\\\nhttps://www.hansonrobotics.com/wp-content/uploads/2018/12/Using-Tononi-Phi-to-Measure-Consciousness-of-a-Cognitive-System-While-Reading-and-Conversing.pdf\\\\nhttps://arxiv.org/pdf/1601.02626.pdf\\\\nhttps://blog.singularitynet.io/singularitynet-the-past-the-present-and-the-future-7bacb2b8e7f0\\\\nhttps://blog.singularitynet.io/singularitynet-supervisory-council-e7c513fd3ea6\\\\nhttps://blog.singularitynet.io/singularitynet-phase-two-massive-token-utilization-toward-decentralized-beneficial-agi-6e3ac5a5b44a\\\\n\\\\nADDENDUM:\\\\nI forgot to mention one important example for the utility of dynamic matchmaking: If I have a German text to summarize, and there is a German summarizer, but there is also a better English one, a clever AI could figure out for me whether to use the German one or whether to use a translator to English, then the English summarizer, then a backtranslator. And it could even do so depending on the input text.\\\\n\\\\nAbstract:\\\\n[...] Most AI research today is controlled by a handful of corporations\\xe2\\x80\\x94those with\\\\nthe resources to fund development. Independent developers of AI tools have no\\\\nreadily available way to monetize their creations. Usually, their most lucrative\\\\noption is to sell their tool to one of the big tech companies, leading to control of\\\\nthe technology becoming even more concentrated. SingularityNET\\xe2\\x80\\x99s open-source\\\\nprotocol and collection of smart contracts are designed to address these problems.\\\\nDevelopers can launch their AI tools on the network, where they can interoperate\\\\nwith other AIs and with paying users.\\\\nNot only does the SingularityNET platform give developers a commercial\\\\nlaunchpad (much like app stores give mobile app developers an easy path to\\\\nmarket), it also allows the AIs to interoperate, creating a more synergistic, broadly\\\\ncapable intelligence. For example, if a text-to-speech AI and an Italian-to-English\\\\ntranslation AI were both on the network, then the network as a whole would be\\\\ncapable of using Italian text to produce English speech.\\\\nWithin this framework, AI transforms from a corporate asset to a global\\\\ncommons; anyone can access AI tech or become a stakeholder in its development.\\\\nAlso, anyone can add an AI/machine learning service to SingularityNET for use\\\\nby the network and receive network payment tokens in exchange. [...]\\\\n\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3129",
    "uploadDate": "2021-01-29",
    "thumbnail_url": "https://i.ytimg.com/vi/yFAuXmcGk2Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=iAR8LkkMMIM",
    "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, attention, transformer, attention mechanism, google, google brain, shazeer, trillion, trillion parameter, language model, gpt3, gpt",
    "scraped_at": 1684582646.54372,
    "genre": "Science",
    "views": "25851",
    "desc": "#ai #technology #switchtransformer\\\\n\\\\nScale is the next frontier for AI. Google Brain uses sparsity and hard routing to massively increase a model\\'s parameters, while keeping the FLOPs per forward pass constant. The Switch Transformer compares favorably to its dense counterparts in terms of speed and sample efficiency and breaks the next magic number: One Trillion Parameters.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:30 - Performance Gains from Scale\\\\n8:30 - Switch Transformer Architecture\\\\n17:00 - Model-, Data- and Expert-Parallelism\\\\n25:30 - Experimental Results\\\\n29:00 - Stabilizing Training\\\\n32:20 - Distillation into Dense Models\\\\n33:30 - Final Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2101.03961\\\\nCodebase T5: https://github.com/google-research/text-to-text-transfer-transformer\\\\n\\\\nAbstract:\\\\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \\\\\"",
    "lengthSeconds": "2027",
    "uploadDate": "2021-01-22",
    "thumbnail_url": "https://i.ytimg.com/vi/iAR8LkkMMIM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hHZSA9z_abE",
    "title": "STOCHASTIC MEME DESCENT - Deep Learning Meme Review - Episode 2 (Part 2 of 2)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, funny, meme, memes, meme review, gpt",
    "scraped_at": 1684582644.9207196,
    "genre": "Science",
    "views": "13085",
    "desc": "#memes #science #ai\\\\n\\\\nPart 2 of Antonio and me examining the latest and greatest of deep learning memes.\\\\n\\\\nMusic:\\\\nSunshower - LATASH\\xc3\\x81\\\\nPapov - Yung Logos\\\\nSunny Days - Anno Domini Beats\\\\nTrinity - Jeremy Blake\\\\n\\\\nMore memes:\\\\nfacebook.com/convolutionalmemes\\\\n\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "834",
    "uploadDate": "2021-01-17",
    "thumbnail_url": "https://i.ytimg.com/vi/hHZSA9z_abE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=T9XSU0pKX2E",
    "title": "OpenAI CLIP: ConnectingText and Images (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openai, sutskever, radford, meme, dalle, dall",
    "scraped_at": 1684582645.0187192,
    "genre": "Science",
    "views": "83102",
    "desc": "#ai #openai #technology\\\\n\\\\nPaper Title: Learning Transferable Visual Models From Natural Language Supervision\\\\nCLIP trains on 400 million images scraped from the web, along with text descriptions to learn a model that can connect the two modalities. The core idea is a contrastive objective combined with a large batch size. The resulting model can be turned into arbitrary zero-shot classifiers for new image \\\\u0026 text tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n3:15 - Overview\\\\n4:40 - Connecting Images \\\\u0026 Text\\\\n9:00 - Building Zero-Shot Classifiers\\\\n14:40 - CLIP Contrastive Training Objective\\\\n22:25 - Encoder Choices\\\\n25:00 - Zero-Shot CLIP vs Linear ResNet-50\\\\n31:50 - Zero-Shot vs Few-Shot\\\\n35:35 - Scaling Properties\\\\n36:35 - Comparison on different tasks\\\\n37:40 - Robustness to Data Shift\\\\n44:20 - Broader Impact Section\\\\n47:00 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf\\\\nBlog: https://openai.com/blog/clip/\\\\nCode: https://github.com/openai/CLIP\\\\n\\\\nAbstract:\\\\nState-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.\\\\n\\\\nAuthors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2886",
    "uploadDate": "2021-01-12",
    "thumbnail_url": "https://i.ytimg.com/vi/T9XSU0pKX2E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=j4xgkjWlfL4",
    "title": "OpenAI DALL\u00b7E: Creating Images from Text (Blog Post Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gpt, gpt",
    "scraped_at": 1684582647.471547,
    "genre": "Science",
    "views": "101195",
    "desc": "#openai #science #gpt3\\\\n\\\\nOpenAI\\'s newest model, DALL\\xc2\\xb7E, shows absolutely amazing abilities in generating high-quality images from arbitrary text descriptions. Like GPT-3, the range of applications and the diversity of outputs is astonishing, given that this is a single model, trained on a purely autoregressive task. This model is a significant step towards the combination of text and images in future AI applications.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n2:45 - Overview\\\\n4:20 - Dataset\\\\n5:35 - Comparison to GPT-3\\\\n7:00 - Model Architecture\\\\n13:20 - VQ-VAE\\\\n21:00 - Combining VQ-VAE with GPT-3\\\\n27:30 - Pre-Training with Relaxation\\\\n32:15 - Experimental Results\\\\n33:00 - My Hypothesis about DALL\\xc2\\xb7E\\'s inner workings\\\\n36:15 - Sparse Attention Patterns\\\\n38:00 - DALL\\xc2\\xb7E can\\'t count\\\\n39:35 - DALL\\xc2\\xb7E can\\'t global order\\\\n40:10 - DALL\\xc2\\xb7E renders different views\\\\n41:10 - DALL\\xc2\\xb7E is very good at texture\\\\n41:40 - DALL\\xc2\\xb7E can complete a bust\\\\n43:30 - DALL\\xc2\\xb7E can do some reflections, but not others\\\\n44:15 - DALL\\xc2\\xb7E can do cross-sections of some objects\\\\n45:50 - DALL\\xc2\\xb7E is amazing at style\\\\n46:30 - DALL\\xc2\\xb7E can generate logos\\\\n47:40 - DALL\\xc2\\xb7E can generate bedrooms\\\\n48:35 - DALL\\xc2\\xb7E can combine unusual concepts\\\\n49:25 - DALL\\xc2\\xb7E can generate illustrations\\\\n50:15 - DALL\\xc2\\xb7E sometimes understands complicated prompts\\\\n50:55 - DALL\\xc2\\xb7E can pass part of an IQ test\\\\n51:40 - DALL\\xc2\\xb7E probably does not have geographical / temporal knowledge\\\\n53:10 - Reranking dramatically improves quality\\\\n53:50 - Conclusions \\\\u0026 Comments\\\\n\\\\nBlog: https://openai.com/blog/dall-e/\\\\n\\\\nLinks:\\\\nTabNine Code Completion (Referral): http://bit.ly/tabnine-yannick\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3345",
    "uploadDate": "2021-01-06",
    "thumbnail_url": "https://i.ytimg.com/vi/j4xgkjWlfL4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=plK2WVdLTOY",
    "title": "Extracting Training Data from Large Language Models (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, apple, openai, berkeley, stanford, carlini, dawn song, google ai, nlp, natural language processing, gpt, gpt2, gpt",
    "scraped_at": 1684582646.6347516,
    "genre": "Science",
    "views": "18217",
    "desc": "#ai #privacy #tech\\\\n\\\\nThis paper demonstrates a method to extract verbatim pieces of the training data from a trained language model. Moreover, some of the extracted pieces only appear a handful of times in the dataset. This points to serious security and privacy implications for models like GPT-3. The authors discuss the risks and propose mitigation strategies.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n9:15 - Personal Data Example\\\\n12:30 - Eidetic Memorization \\\\u0026 Language Models\\\\n19:50 - Adversary\\'s Objective \\\\u0026 Outlier Data\\\\n24:45 - Ethical Hedging\\\\n26:55 - Two-Step Method Overview\\\\n28:20 - Perplexity Baseline\\\\n30:30 - Improvement via Perplexity Ratios\\\\n37:25 - Weights for Patterns \\\\u0026 Weights for Memorization\\\\n43:40 - Analysis of Main Results\\\\n1:00:30 - Mitigation Strategies\\\\n1:01:40 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2012.07805\\\\n\\\\nAbstract:\\\\nIt has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.\\\\nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model\\'s training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.\\\\nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.\\\\n\\\\nAuthors: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3798",
    "uploadDate": "2020-12-26",
    "thumbnail_url": "https://i.ytimg.com/vi/plK2WVdLTOY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=7DGlElSVYGo",
    "title": "MEMES IS ALL YOU NEED - Deep Learning Meme Review - Episode 2 (Part 1 of 2)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, funny, meme, memes, meme review, gpt",
    "scraped_at": 1684582645.1067193,
    "genre": "Science",
    "views": "22341",
    "desc": "#memes #science #ai\\\\n\\\\nAntonio and I critique the creme de la creme of Deep Learning memes.\\\\n\\\\nMusic:\\\\nSunshower - LATASH\\xc3\\x81\\\\nPapov - Yung Logos\\\\nSunny Days - Anno Domini Beats\\\\nTrinity - Jeremy Blake\\\\n\\\\nMore memes:\\\\nfacebook.com/convolutionalmemes\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "868",
    "uploadDate": "2020-12-24",
    "thumbnail_url": "https://i.ytimg.com/vi/7DGlElSVYGo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BhUWvQmLzSk",
    "title": "ReBeL - Combining Deep Reinforcement Learning and Search for Imperfect-Information Games (Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, poker, deep neural networks, facebook, facebook ai, rebel, holdem, texas holdem, rock paper scissors, liars dice, liar dice, self play, nash equilibrium, alpha go, alphazero, zero sum, policy, cfr, counterfactual regret minimization, tree search, monte carlo tree search, mcts, public belief state, infostate, value function, supergradient, strategy, actor critic, imperfect information",
    "scraped_at": 1684582645.1977203,
    "genre": "Science",
    "views": "32438",
    "desc": "#ai #technology #poker\\\\n\\\\nThis paper does for Poker what AlphaZero has done for Chess \\\\u0026 Go. The combination of Self-Play Reinforcement Learning and Tree Search has had tremendous success in perfect-information games, but transferring such techniques to imperfect information games is a hard problem. Not only does ReBeL solve this problem, but it provably converges to a Nash Equilibrium and delivers a superhuman Heads Up No-Limit Hold\\'em bot with very little domain knowledge.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:20 - Rock, Paper, and Double Scissor\\\\n10:00 - AlphaZero Tree Search\\\\n18:30 - Notation Setup: Infostates \\\\u0026 Nash Equilibria\\\\n31:45 - One Card Poker: Introducing Belief Representations\\\\n45:00 - Solving Games in Belief Representation\\\\n55:20 - The ReBeL Algorithm\\\\n1:04:00 - Theory \\\\u0026 Experiment Results\\\\n1:07:00 - Broader Impact\\\\n1:10:20 - High-Level Summary\\\\n\\\\nPaper: https://arxiv.org/abs/2007.13544\\\\nCode: https://github.com/facebookresearch/rebel\\\\nBlog: https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/\\\\n\\\\nERRATA: As someone last video pointed out: This is not the best Poker algorithm, but the best one that uses very little expert knowledge.\\\\n\\\\nAbstract:\\\\nThe combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold\\'em poker, while using far less domain knowledge than any prior poker AI.\\\\n\\\\nAuthors: Noam Brown, Anton Bakhtin, Adam Lerer, Qucheng Gong\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "4341",
    "uploadDate": "2020-12-16",
    "thumbnail_url": "https://i.ytimg.com/vi/BhUWvQmLzSk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=R07CVhWbAXc",
    "title": "2M All-In into $5 Pot! WWYD? Daniel Negreanu's No-Limit Hold'em Challenge! (Poker Hand Analysis)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, poker, negreanu, daniel negreanu, realkidpoker, flop, turn, river, holdem, libratus, pluribus, rebel, facebook, poker bot, nash equilibrium, overbet, hand range, level, raise, hole cards, aces, quads, twitter, analysis, bluff, nuts",
    "scraped_at": 1684582645.2867186,
    "genre": "Science",
    "views": "6947",
    "desc": "#ai #technology #poker\\\\n\\\\nDaniel Negreanu posted a set of very interesting No-Limit Hold\\'em situations on Twitter. I try to analyze them from the perspective of a poker bot. See how such bots think about the game and approximate Nash equilibria.\\\\n\\\\nhttps://twitter.com/RealKidPoker/status/1337887509397741568\\\\nhttps://twitter.com/RealKidPoker/status/1337899147337244673\\\\nhttps://twitter.com/RealKidPoker/status/1337904860721606656\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nBiliBili: https://space.bilibili.com/1824646584\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1670",
    "uploadDate": "2020-12-13",
    "thumbnail_url": "https://i.ytimg.com/vi/R07CVhWbAXc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=B9PL__gVxLI",
    "title": "DeepMind's AlphaFold 2 Explained! AI Breakthrough in Protein Folding! What we know (& what we don't)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, deepmind, deep mind, alphago, alphazero, alphafold, protein, dna, rna, folding, casp, casp14, alphafold 2, blog, hassabis, biology, translation, amino acid, transformer, convolution, residual, spatial graph, refine, gradient descent, van der waals, torsion angles, google ai, google brain, nobel prize, msa, multiple sequence alignment, covariation, evolution, contact prediction, distogram",
    "scraped_at": 1684582646.728719,
    "genre": "Science",
    "views": "213805",
    "desc": "#deepmind #biology #ai\\\\n\\\\nThis is Biology\\'s AlexNet moment! DeepMind solves a 50-year old problem in Protein Folding Prediction. AlphaFold 2 improves over DeepMind\\'s 2018 AlphaFold system with a new architecture and massively outperforms all competition. In this Video, we take a look at how AlphaFold 1 works and what we can gather about AlphaFold 2 from the little information that\\'s out there.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:10 - Proteins \\\\u0026 Protein Folding\\\\n14:20 - AlphaFold 1 Overview\\\\n18:20 - Optimizing a differentiable geometric model at inference\\\\n25:40 - Learning the Spatial Graph Distance Matrix\\\\n31:20 - Multiple Sequence Alignment of Evolutionarily Similar Sequences\\\\n39:40 - Distance Matrix Output Results\\\\n43:45 - Guessing AlphaFold 2 (it\\'s Transformers)\\\\n53:30 - Conclusion \\\\u0026 Comments\\\\n\\\\nAlphaFold 2 Blog: https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology\\\\nAlphaFold 1 Blog: https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery\\\\nAlphaFold 1 Paper: https://www.nature.com/articles/s41586-019-1923-7\\\\nMSA Reference: https://arxiv.org/abs/1211.1281\\\\nCASP14 Challenge: https://predictioncenter.org/casp14/index.cgi\\\\nCASP14 Result Bar Chart: https://www.predictioncenter.org/casp14/zscores_final.cgi\\\\n\\\\nPaper Title: High Accuracy Protein Structure Prediction Using Deep Learning\\\\n\\\\nAbstract:\\\\nProteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and what a protein does largely depends on its unique 3D structure. Figuring out what shapes proteins fold into is known as the \\xe2\\x80\\x9cprotein folding problem\\xe2\\x80\\x9d, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.\\\\n\\\\nAuthors: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin \\xc5\\xbd\\xc3\\xaddek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3277",
    "uploadDate": "2020-12-01",
    "thumbnail_url": "https://i.ytimg.com/vi/B9PL__gVxLI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=LB4B5FYvtdI",
    "title": "Predictive Coding Approximates Backprop along Arbitrary Computation Graphs (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, backpropagation, computation, autograph, tensorflow, pytorch, torch, autodiff, differentiation, backprop, biologically plausible, neurons, error signal, predictive coding, variational, gaussian, iterative, local updates, distributed, inner loop, brain, neuroscience, deep neural networks, analyzed, hand drawing, cnn, rnn, lstm, convolutional neural network, recurrent neural network, hebian",
    "scraped_at": 1684582647.5625124,
    "genre": "Science",
    "views": "24459",
    "desc": "#ai #biology #neuroscience\\\\n\\\\nBackpropagation is the workhorse of modern deep learning and a core component of most frameworks, but it has long been known that it is not biologically plausible, driving a divide between neuroscience and machine learning. This paper shows that Predictive Coding, a much more biologically plausible algorithm, can approximate Backpropagation for any computation graph, which they verify experimentally by building and training CNNs and LSTMs using Predictive Coding. This suggests that the brain and deep neural networks could be much more similar than previously believed.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:00 - Backpropagation \\\\u0026 Biology\\\\n7:40 - Experimental Results\\\\n8:40 - Predictive Coding\\\\n29:00 - Pseudocode\\\\n32:10 - Predictive Coding approximates Backprop\\\\n35:00 - Hebbian Updates\\\\n36:35 - Code Walkthrough\\\\n46:30 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.04182\\\\nCode: https://github.com/BerenMillidge/PredictiveCodingBackprop\\\\n\\\\nAbstract:\\\\nBackpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.\\\\n\\\\nAuthors: Beren Millidge, Alexander Tschantz, Christopher L. Buckley\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2905",
    "uploadDate": "2020-11-29",
    "thumbnail_url": "https://i.ytimg.com/vi/LB4B5FYvtdI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=IaS72aHrJKE",
    "title": "Fourier Neural Operator for Parametric Partial Differential Equations (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, berkeley, purdue, mc hammer, mchammer, mit, technology review, pde, partial differential equation, navier stokes, darcy flow, burgers, convolutions, fft, dfft, fourier transform, fourier neural operator, neural operator, fast fourier transform, fourier modes, flow, turbulent flow, fluid dynamics, residual, aerodynamics, wind tunnel, neural network, layers, numerical, discretization",
    "scraped_at": 1684582648.080513,
    "genre": "Science",
    "views": "49327",
    "desc": "#ai #research #engineering\\\\n\\\\nNumerical solvers for Partial Differential Equations are notoriously slow. They need to evolve their state by tiny steps in order to stay accurate, and they need to repeat this for each new problem. Neural Fourier Operators, the architecture proposed in this paper, can evolve a PDE in time by a single forward pass, and do so for an entire family of PDEs, as long as the training set covers them well. By performing crucial operations only in Fourier Space, this new architecture is also independent of the discretization or sampling of the underlying signal and has the potential to speed up many scientific applications.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n6:15 - Navier Stokes Problem Statement\\\\n11:00 - Formal Problem Definition\\\\n15:00 - Neural Operator\\\\n31:30 - Fourier Neural Operator\\\\n48:15 - Experimental Examples\\\\n50:35 - Code Walkthrough\\\\n1:01:00 - Summary \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2010.08895\\\\nBlog: https://zongyi-li.github.io/blog/2020/fourier-pde/\\\\nCode: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_3d.py\\\\nMIT Technology Review: https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/\\\\n\\\\nAbstract:\\\\nThe classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers\\' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.\\\\n\\\\nAuthors: Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3932",
    "uploadDate": "2020-11-22",
    "thumbnail_url": "https://i.ytimg.com/vi/IaS72aHrJKE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=i_p5wLoCCiw",
    "title": "[News] Soccer AI FAILS and mixes up ball and referee's bald head.",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, soccer, camera, fail, head, bald, ball, tracking, computer vision, hough transform, ethics, broader impact statement",
    "scraped_at": 1684582648.1605432,
    "genre": "Science",
    "views": "8018",
    "desc": "#ai #tech #news\\\\n\\\\nThis soccer camera is operated by an AI to track the ball. However, the AI has an interesting failure mode and repeatedly mixes up the ball with the bald head of a referee. This raises some interesting questions about the role of ethics in AI research.\\\\n\\\\nFootage from SPFL Championship : ICTFC 1 v 1 AYR : 24/10/2020\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "228",
    "uploadDate": "2020-11-15",
    "thumbnail_url": "https://i.ytimg.com/vi/i_p5wLoCCiw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gch94ttuy5s",
    "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, pipeline, ml pipeline, deep networks, epidemiology, theoretical, underspecification, overparameterization, overfitting, generalization, out of distribution, bert, gender, stereotypes, distribution shift, analysis, performance, bias, correlation, problems, quality assurance",
    "scraped_at": 1684582648.748512,
    "genre": "Science",
    "views": "18813",
    "desc": "#ai #research #machinelearning\\\\n\\\\nDeep Learning models are often overparameterized and have many degrees of freedom, which leads to many local minima that all perform equally well on the test set. But it turns out that even though they all generalize in-distribution, the performance of these models can be drastically different when tested out-of-distribution. Notably, in many cases, a good model can actually be found among all these candidates, but it seems impossible to select it. This paper describes this problem, which it calls underspecification, and gives several theoretical and practical examples.\\\\n\\\\nOUTLINE:\\\\n0:00 - Into \\\\u0026 Overview\\\\n2:00 - Underspecification of ML Pipelines\\\\n11:15 - Stress Tests\\\\n12:40 - Epidemiological Example\\\\n20:45 - Theoretical Model\\\\n26:55 - Example from Medical Genomics\\\\n34:00 - ImageNet-C Example\\\\n36:50 - BERT Models\\\\n56:55 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2011.03395\\\\n\\\\nAbstract:\\\\nML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.\\\\n\\\\nAuthors: Alexander D\\'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3561",
    "uploadDate": "2020-11-10",
    "thumbnail_url": "https://i.ytimg.com/vi/gch94ttuy5s/maxresdefault.jpg"
  },
  {
    "link": "watch?v=NAJOZTNkhlI",
    "title": "Language Models are Open Knowledge Graphs (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nlp, natural language processing, bert, gpt, gpt2, gpt",
    "scraped_at": 1684582648.2525432,
    "genre": "Science",
    "views": "28636",
    "desc": "#ai #research #nlp\\\\n\\\\nKnowledge Graphs are structured databases that capture real-world entities and their relations to each other. KGs are usually built by human experts, which costs considerable amounts of time and money. This paper hypothesizes that language models, which have increased their performance dramatically in the last few years, contain enough knowledge to use them to construct a knowledge graph from a given corpus, without any fine-tuning of the language model itself. The resulting system can uncover new, unknown relations and outperforms all baselines in automated KG construction, even trained ones!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - TabNine Promotion\\\\n4:20 - Title Misnomer\\\\n6:45 - From Corpus To Knowledge Graph\\\\n13:40 - Paper Contributions\\\\n15:50 - Candidate Fact Finding Algorithm\\\\n25:50 - Causal Attention Confusion\\\\n31:25 - More Constraints\\\\n35:00 - Mapping Facts To Schemas\\\\n38:40 - Example Constructed Knowledge Graph\\\\n40:10 - Experimental Results\\\\n47:25 - Example Discovered Facts\\\\n50:40 - Conclusion \\\\u0026 My Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2010.11967\\\\n\\\\nAbstract:\\\\nThis paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.\\\\n\\\\nAuthors: Chenguang Wang, Xiao Liu, Dawn Song\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3136",
    "uploadDate": "2020-11-02",
    "thumbnail_url": "https://i.ytimg.com/vi/NAJOZTNkhlI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=xJrKIPwVwGM",
    "title": "Rethinking Attention with Performers (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nlp, natural language processing, natural language understanding, data science, transformer, attention, attention mechanism, transformers, attention is all you need, gpus, tpu, linformer, reformer, explanation, imagenet64, kernels, gaussian kernel, softmax, softmax kernel, approximation, random features, random positive features, random fourier features, google, favor, machine translation",
    "scraped_at": 1684582648.3465455,
    "genre": "Science",
    "views": "51235",
    "desc": "#ai #research #attention\\\\n\\\\nTransformers have huge memory and compute requirements because they construct an Attention matrix, which grows quadratically in the size of the input. The Performer is a model that uses random positive orthogonal features to construct an unbiased estimator to the Attention matrix and obtains an arbitrarily good approximation in linear time! The method generalizes beyond attention and opens the door to the next generation of deep learning architectures.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n6:15 - Quadratic Bottleneck in Attention Mechanisms\\\\n10:00 - Decomposing the Attention Matrix\\\\n15:30 - Approximating the Softmax Kernel\\\\n24:45 - Different Choices, Different Kernels\\\\n28:00 - Why the Naive Approach does not work!\\\\n31:30 - Better Approximation via Positive Features\\\\n36:55 - Positive Features are Infinitely Better\\\\n40:10 - Orthogonal Features are Even Better\\\\n43:25 - Experiments\\\\n49:20 - Broader Impact Statement\\\\n50:00 - Causal Attention via Prefix Sums\\\\n52:10 - Code\\\\n53:50 - Final Remarks \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2009.14794\\\\nCode: https://github.com/google-research/google-research/tree/master/performer\\\\nBlog: https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html\\\\n\\\\nKernels on ML Street Talk: https://www.youtube.com/watch?v=y_RjsDHl5Y4\\\\nMy Video on Linformer: https://www.youtube.com/watch?v=-_2AF9Lhweo\\\\nMy Video on Reformer: https://www.youtube.com/watch?v=i4H0kjxrias\\\\nMy Video on Attention: https://www.youtube.com/watch?v=iDulhoQ2pro\\\\n\\\\nAbstract:\\\\nWe introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\\\\n\\\\nAuthors: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller\\\\n\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3278",
    "uploadDate": "2020-10-26",
    "thumbnail_url": "https://i.ytimg.com/vi/xJrKIPwVwGM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3qxJ2WD8p4w",
    "title": "LambdaNetworks: Modeling long-range Interactions without Attention (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, attention, attention mechanism, lambda, lambdaresnet, residual networks, local attention, quadratic, memory, transformer, transformers, keys, values, queries, architecture, input size, iclr, lambdanet, lambdanets, lambdaresnets, efficientnet, tradeoff, routing, linear function, functional programming",
    "scraped_at": 1684582648.8365364,
    "genre": "Science",
    "views": "46913",
    "desc": "#ai #research #attention\\\\n\\\\nTransformers, having already captured NLP, have recently started to take over the field of Computer Vision. So far, the size of images as input has been challenging, as the Transformers\\' Attention Mechanism\\'s memory requirements grows quadratic in its input size. LambdaNetworks offer a way around this requirement and capture long-range interactions without the need to build expensive attention maps. They reach a new state-of-the-art in ImageNet and compare favorably to both Transformers and CNNs in terms of efficiency.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction \\\\u0026 Overview\\\\n6:25 - Attention Mechanism Memory Requirements\\\\n9:30 - Lambda Layers vs Attention Layers\\\\n17:10 - How Lambda Layers Work\\\\n31:50 - Attention Re-Appears in Lambda Layers\\\\n40:20 - Positional Encodings\\\\n51:30 - Extensions and Experimental Comparisons\\\\n58:00 - Code\\\\n\\\\nPaper: https://openreview.net/forum?id=xTJEN-ggl1b\\\\nLucidrains\\' Code: https://github.com/lucidrains/lambda-networks\\\\n\\\\nAbstract:\\\\nWe present a general framework for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Our method, called the lambda layer, captures such interactions by transforming available contexts into linear functions,  termed lambdas,  and applying these linear functions to each input separately.  Lambda layers are versatile and may be implemented to model content and position-based interactions in global, local or masked contexts.  As they bypass the need for expensive attention maps, lambda layers can routinely be applied to inputs of length in the thousands, en-abling their applications to long sequences or high-resolution images. The resulting neural network architectures, LambdaNetworks, are computationally efficient and simple to implement using direct calls to operations available in modern neural network libraries.  Experiments on ImageNet classification and COCO object detection  and  instance  segmentation  demonstrate  that  LambdaNetworks  significantly  outperform  their  convolutional  and  attentional  counterparts  while  being more computationally efficient. Finally, we introduce LambdaResNets, a family of LambdaNetworks, that considerably improve the speed-accuracy tradeoff of image classification models. LambdaResNets reach state-of-the-art accuracies on ImageNet while being \\xe2\\x88\\xbc4.5x faster than the popular EfficientNets on modern machine learning accelerators.\\\\n\\\\nAuthors: Anonymous\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3573",
    "uploadDate": "2020-10-17",
    "thumbnail_url": "https://i.ytimg.com/vi/3qxJ2WD8p4w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DiNzQP7kK-s",
    "title": "Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, optimization, polyak, nesterov, benchmark, cnn, cifar, mnist, adam, adagrad, adadelta, momentum, sgd, gradient, learning rate, tuning, budget, default parameters, comparison, grid search, random search, random seed, vae, learning rate schedule, cosine decay, trapezoid, improvement, best optimizer, best optimizer for deep learning, stochastic gradient descent",
    "scraped_at": 1684582648.4375415,
    "genre": "Science",
    "views": "13581",
    "desc": "#ai #research #optimization\\\\n\\\\nDeep Learning famously gives rise to very complex, non-linear optimization problems that cannot be solved analytically. Therefore, the choice of a suitable optimization algorithm can often make or break the training of a Deep Neural Network. Yet, the literature is full with hundreds of different algorithms, each claiming to be superior and selecting one of them is mostly done based on popular opinion or anecdotes. This paper investigates 14 of the most popular optimizers in a standardized benchmark and even though there is no clear winner, it can give some recommendations as a result.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction \\\\u0026 Overview\\\\n2:15 - The Overwhelming Amount of Optimizers\\\\n5:50 - Compared Optimizers\\\\n6:50 - Default Parameters \\\\u0026 Tuning Distribution\\\\n13:10 - Deep Learning Problems Considered\\\\n16:45 - Tuning on Single Seeds\\\\n23:15 - Results \\\\u0026 Interpretation\\\\n34:00 - Learning Rate Schedules \\\\u0026 Noise\\\\n36:10 - Conclusions \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2007.01547\\\\nRaw Results: https://github.com/SirRob1997/Crowded-Valley---Results\\\\n\\\\nAbstract:\\\\nChoosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.\\\\n\\\\nAuthors: Robin M. Schmidt, Frank Schneider, Philipp Hennig\\\\n\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2459",
    "uploadDate": "2020-10-11",
    "thumbnail_url": "https://i.ytimg.com/vi/DiNzQP7kK"
  },
  {
    "link": "watch?v=TrdevFK_am4",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, attention mechanism, convolutional neural network, data science, cnn, transformer, attention is all you need, vaswani, beyer, google, google brain, google research, tpu, tpu v3, iclr, iclr 2021, peer review, anonymous, karpathy, andrej karpathy, twitter, review, under submission, big transfer, bit, vit, vision transformer, visual transformer, transformer images, transformer computer vision",
    "scraped_at": 1684582648.9295142,
    "genre": "Science",
    "views": "265018",
    "desc": "#ai #research #transformers\\\\n\\\\nTransformers are Ruining Convolutions. This paper, under review at ICLR, shows that given enough data, a standard Transformer can outperform Convolutional Neural Networks in image recognition tasks, which are classically tasks where CNNs excel. In this Video, I explain the architecture of the Vision Transformer (ViT), the reason why it works better and rant about why double-bline peer review is broken.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n0:30 - Double-Blind Review is Broken\\\\n5:20 - Overview\\\\n6:55 - Transformers for Images\\\\n10:40 - Vision Transformer Architecture\\\\n16:30 - Experimental Results\\\\n18:45 - What does the Model Learn?\\\\n21:00 - Why Transformers are Ruining Everything\\\\n27:45 - Inductive Biases in Transformers\\\\n29:05 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper (Under Review): https://openreview.net/forum?id=YicbFdNTTy\\\\nArxiv version: https://arxiv.org/abs/2010.11929\\\\n\\\\nBiT Paper: https://arxiv.org/pdf/1912.11370.pdf\\\\nImageNet-ReaL Paper: https://arxiv.org/abs/2006.07159\\\\n\\\\nMy Video on BiT (Big Transfer): https://youtu.be/k1GOF2jmX7c\\\\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\\\\nMy Video on BERT: https://youtu.be/-9evrZnBorM\\\\nMy Video on ResNets: https://youtu.be/GWt6Fu05voI\\\\n\\\\n\\\\nAbstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer can perform very well on image classification tasks when applied directly to sequences of image patches. When pre-trained on large amounts of data and transferred to multiple recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc), Vision Transformer attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\\\\n\\\\nAuthors: Anonymous / Under Review\\\\n\\\\nErrata:\\\\n- Patches are not flattened, but vectorized\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1796",
    "uploadDate": "2020-10-04",
    "thumbnail_url": "https://i.ytimg.com/vi/TrdevFK_am4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3baFTP0uYOc",
    "title": "Training more effective learned optimizers, and using them to train themselves (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, optimization, lstm, taskset, google, google research, compute, outer optimization, adam, adamw, sgd, momentum, learning rate, gradient, learned optimizer, second moment, cnn, rnn, paper explained, neural network, gradient descent, hyper parameters, grid search, mnist, cifar10, imagenet",
    "scraped_at": 1684582649.024538,
    "genre": "Science",
    "views": "18498",
    "desc": "#ai #research #optimization\\\\n\\\\nOptimization is still the domain of hand-crafted, simple algorithms. An ML engineer not only has to pick a suitable one for their problem but also often do grid-search over various hyper-parameters. This paper proposes to learn a single, unified optimization algorithm, given not by an equation, but by an LSTM-based neural network, to act as an optimizer for any deep learning problem, and ultimately to optimize itself.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n2:20 - From Hand-Crafted to Learned Features\\\\n4:25 - Current Optimization Algorithm\\\\n9:40 - Learned Optimization\\\\n15:50 - Optimizer Architecture\\\\n22:50 - Optimizing the Optimizer using Evolution Strategies\\\\n30:30 - Task Dataset\\\\n34:00 - Main Results\\\\n36:50 - Implicit Regularization in the Learned Optimizer\\\\n41:05 - Generalization across Tasks\\\\n41:40 - Scaling Up\\\\n45:30 - The Learned Optimizer Trains Itself\\\\n47:20 - Pseudocode\\\\n49:45 - Broader Impact Statement\\\\n52:55 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2009.11243\\\\n\\\\nAbstract:\\\\nMuch as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.\\\\n\\\\nAuthors: Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, Jascha Sohl-Dickstein\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3215",
    "uploadDate": "2020-10-03",
    "thumbnail_url": "https://i.ytimg.com/vi/3baFTP0uYOc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=MQ89be_685o",
    "title": "The Hardware Lottery (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, hardware, gpus, tpus, gpu, tpu, convolutional neural networks, yann lecun, history, historic, ai winter, expert systems, babbage, google, accelerators, cuda, nvidia, flops, von neumann architecture, bottleneck, parallelize, research, funding, society, cost, competition, general purpose, fpga",
    "scraped_at": 1684582649.1145396,
    "genre": "Science",
    "views": "10291",
    "desc": "#ai #research #hardware\\\\n\\\\nWe like to think that ideas in research succeed because of their merit, but this story is likely incomplete. The term \\\\\"",
    "lengthSeconds": "3131",
    "uploadDate": "2020-09-18",
    "thumbnail_url": "https://i.ytimg.com/vi/MQ89be_685o/maxresdefault.jpg"
  },
  {
    "link": "watch?v=O1b0cbgpRBw",
    "title": "Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deepmind, chess, kramnik, fide, rules, alphago, alpha go, alphazero, alpha zero, mu zero, muzero, google, reinforcement learning, mcts, rule change, other rules, alternate rules, torpedo, no castling, pawn sideways, self capture, entropy, opening theory, rule based systems, berlin defense, opening, stalemate, deep rl, deep reinforcement learning, alphazero chess, alphazero analysis",
    "scraped_at": 1684582648.5315113,
    "genre": "Science",
    "views": "5968",
    "desc": "#ai #chess #alphazero\\\\n\\\\nChess is a very old game and both its rules and theory have evolved over thousands of years in the collective effort of millions of humans. Therefore, it is almost impossible to predict the effect of even minor changes to the game rules, because this collective process cannot be easily replicated. This paper proposes to use AlphaZero\\'s ability to achieve superhuman performance in board games within one day of training to assess the effect of a series of small, but consequential rule changes. It analyzes the resulting strategies and sets the stage for broader applications of reinforcement learning to study rule-based systems.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:30 - Alternate Chess Rules\\\\n4:20 - Using AlphaZero to assess rule change outcomes\\\\n6:00 - How AlphaZero works\\\\n16:40 - Alternate Chess Rules continued\\\\n18:50 - Game outcome distributions\\\\n31:45 - e4 and Nf3 in classic vs no-castling chess\\\\n36:40 - Conclusions \\\\u0026 comments\\\\n\\\\nPaper: https://arxiv.org/abs/2009.04374\\\\n\\\\nMy Video on AI Economist: https://youtu.be/F5aaXrIMWyU\\\\n\\\\nAbstract:\\\\nIt is non-trivial to design engaging and balanced sets of game rules. Modern chess has evolved over centuries, but without a similar recourse to history, the consequences of rule changes to game dynamics are difficult to predict. AlphaZero provides an alternative in silico means of game balance assessment. It is a system that can learn near-optimal strategies for any rule set from scratch, without any human supervision, by continually learning from its own experience. In this study we use AlphaZero to creatively explore and design new chess variants. There is growing interest in chess variants like Fischer Random Chess, because of classical chess\\'s voluminous opening theory, the high percentage of draws in professional play, and the non-negligible number of games that end while both players are still in their home preparation. We compare nine other variants that involve atomic changes to the rules of chess. The changes allow for novel strategic and tactical patterns to emerge, while keeping the games close to the original. By learning near-optimal strategies for each variant with AlphaZero, we determine what games between strong human players might look like if these variants were adopted. Qualitatively, several variants are very dynamic. An analytic comparison show that pieces are valued differently between variants, and that some variants are more decisive than classical chess. Our findings demonstrate the rich possibilities that lie beyond the rules of modern chess.\\\\n\\\\nAuthors: Nenad Toma\\xc5\\xa1ev, Ulrich Paquet, Demis Hassabis, Vladimir Kramnik\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2560",
    "uploadDate": "2020-09-13",
    "thumbnail_url": "https://i.ytimg.com/vi/O1b0cbgpRBw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vLTmnaMpQCs",
    "title": "Learning to summarize from human feedback (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openai, nlp, transformer, gpt, gpt3, gpt",
    "scraped_at": 1684582649.204512,
    "genre": "Science",
    "views": "17436",
    "desc": "#summarization #gpt3 #openai\\\\n\\\\nText Summarization is a hard task, both in training and evaluation. Training is usually done maximizing the log-likelihood of a human-generated reference summary, while evaluation is performed using overlap-based metrics like ROUGE. Both significantly undervalue the breadth and intricacies of language and the nature of the information contained in text summaries. This paper by OpenAI includes direct human feedback both in evaluation and - via reward model proxies - in training. The final model even outperforms single humans when judged by other humans and is an interesting application of using reinforcement learning together with humans in the loop.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n5:35 - Summarization as a Task\\\\n7:30 - Problems with the ROUGE Metric\\\\n10:10 - Training Supervised Models\\\\n12:30 - Main Results\\\\n16:40 - Including Human Feedback with Reward Models \\\\u0026 RL\\\\n26:05 - The Unknown Effect of Better Data\\\\n28:30 - KL Constraint \\\\u0026 Connection to Adversarial Examples\\\\n37:15 - More Results\\\\n39:30 - Understanding the Reward Model\\\\n41:50 - Limitations \\\\u0026 Broader Impact\\\\n\\\\nPaper: https://arxiv.org/abs/2009.01325\\\\nBlog: https://openai.com/blog/learning-to-summarize-with-human-feedback/\\\\nCode: https://github.com/openai/summarize-from-feedback\\\\nSamples: https://openaipublic.blob.core.windows.net/summarize-from-feedback/website/index.html#/\\\\n\\\\nMy Video on GPT-3: https://youtu.be/SY5PvZrJhLE\\\\nMy Video on GPT-2: https://youtu.be/u1_qMdb0kYU\\\\n\\\\nAbstract:\\\\nAs language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.\\\\n\\\\nAuthors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2730",
    "uploadDate": "2020-09-07",
    "thumbnail_url": "https://i.ytimg.com/vi/vLTmnaMpQCs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=EbHUU-gLyRA",
    "title": "Self-classifying MNIST Digits (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, biology, biological, alive, living, message passing, global state, local state, information, cellular automata, neural cellular automata, neural ca, convolution, recurrent, rnn, pixels, cell state, latent state, distill, distill pub, mnist, neural network, digit classification",
    "scraped_at": 1684582650.7304928,
    "genre": "Science",
    "views": "12738",
    "desc": "#ai #biology #machinelearning\\\\n\\\\nNeural Cellular Automata are models for how living creatures can use local message passing to reach global consensus without a central authority. This paper teaches pixels of an image to communicate with each other and figure out as a group which digit they represent. On the way, the authors have to deal with pesky side-effects that come from applying the Cross-Entropy Loss in combination with a Softmax layer, but ultimately achieve a self-sustaining, stable and continuous algorithm that models living systems.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:10 - Neural Cellular Automata\\\\n7:30 - Global Agreement via Message-Passing\\\\n11:05 - Neural CAs as Recurrent Convolutions\\\\n14:30 - Training Continuously Alive Systems\\\\n17:30 - Problems with Cross-Entropy\\\\n26:10 - Out-of-Distribution Robustness\\\\n27:10 - Chimeric Digits\\\\n27:45 - Visualizing Latent State Dimensions\\\\n29:05 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://distill.pub/2020/selforg/mnist/\\\\n\\\\nMy Video on Neural CAs: https://youtu.be/9Kec_7WFyp0\\\\n\\\\nAbstract:\\\\nGrowing Neural Cellular Automata [1] demonstrated how simple cellular automata (CAs) can learn to self-organise into complex shapes while being resistant to perturbations. Such a computational model approximates a solution to an open question in biology, namely, how do cells cooperate to create a complex multicellular anatomy and work to regenerate it upon damage? The model parameterizing the cells\\xe2\\x80\\x99 rules is parameter-efficient, end-to-end differentiable, and illustrates a new approach to modeling the regulation of anatomical homeostasis. In this work, we use a version of this model to show how CAs can be applied to a common task in machine learning: classification. We pose the question: can CAs use local message passing to achieve global agreement on what digit they compose?\\\\n\\\\nAuthors: Ettore Randazzo, Alexander Mordvintsev, Eyvind Niklasson, Michael Levin, Sam Greydanus\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1830",
    "uploadDate": "2020-09-02",
    "thumbnail_url": "https://i.ytimg.com/vi/EbHUU"
  },
  {
    "link": "watch?v=hv3UO3G0Ofo",
    "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, cnn, resnet, big bird, bigbird, attention, attention mechanism, attention for images, transformer for images, transformer, bert, convolutions, window, neighbors, axial attention, position embeddings, positional encodings, quadratic, memory, panoptic segmentation, coco, imagenet, cityscapes, softmax, routing",
    "scraped_at": 1684582650.8191702,
    "genre": "Science",
    "views": "14318",
    "desc": "#ai #machinelearning #attention\\\\n\\\\nConvolutional Neural Networks have dominated image processing for the last decade, but transformers are quickly replacing traditional models. This paper proposes a fully attentional model for images by combining learned Positional Embeddings with Axial Attention. This new model can compete with CNNs on image classification and achieve state-of-the-art in various image segmentation tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:10 - This Paper\\'s Contributions\\\\n6:20 - From Convolution to Self-Attention for Images\\\\n16:30 - Learned Positional Embeddings\\\\n24:20 - Propagating Positional Embeddings through Layers\\\\n27:00 - Traditional vs Position-Augmented Attention\\\\n31:10 - Axial Attention\\\\n44:25 - Replacing Convolutions in ResNet\\\\n46:10 - Experimental Results \\\\u0026 Examples\\\\n\\\\nPaper: https://arxiv.org/abs/2003.07853\\\\nCode: https://github.com/csrhddlam/axial-deeplab\\\\n\\\\nMy Video on BigBird: https://youtu.be/WVPE62Gk3EM\\\\nMy Video on ResNet: https://youtu.be/GWt6Fu05voI\\\\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\\\\n\\\\nAbstract:\\\\nConvolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.\\\\n\\\\nAuthors: Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh Chen\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3344",
    "uploadDate": "2020-08-28",
    "thumbnail_url": "https://i.ytimg.com/vi/hv3UO3G0Ofo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=G2sr1g6rLdE",
    "title": "Radioactive data: tracing through training (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, cnn, imagenet, resnet, radioactive, fake, feature, feature space, feature extractor, facebook ai, fair, deep neural networks, classifier, classes, backpropagation, black box, white box, detect, features, privacy, adversarial examples, tagging, inria",
    "scraped_at": 1684582649.2975452,
    "genre": "Science",
    "views": "4557",
    "desc": "#ai #research #privacy\\\\n\\\\nData is the modern gold. Neural classifiers can improve their performance by training on more data, but given a trained classifier, it\\'s difficult to tell what data it was trained on. This is especially relevant if you have proprietary or personal data and you want to make sure that other people don\\'t use it to train their models. This paper introduces a method to mark a dataset with a hidden \\\\\"",
    "lengthSeconds": "2162",
    "uploadDate": "2020-08-26",
    "thumbnail_url": "https://i.ytimg.com/vi/G2sr1g6rLdE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=9-o2aAoN0rY",
    "title": "Fast reinforcement learning with generalized policy updates (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, deep rl, q learning, deep reinforcement learning, q learning machine learning, deep q learning, successor features, deep mind, zero shot, environment, agent, task, linear, regression, reward, mila, neural network, reinforcement learning, value function, state value function, state value",
    "scraped_at": 1684582649.3925383,
    "genre": "Science",
    "views": "10245",
    "desc": "#ai #research #reinforcementlearning\\\\n\\\\nReinforcement Learning is a powerful tool, but it is also incredibly data-hungry. Given a new task, an RL agent has to learn a good policy entirely from scratch. This paper proposes a new framework that allows an agent to carry over knowledge from previous tasks into solving new tasks, even deriving zero-shot policies that perform well on completely new reward functions.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:25 - Problem Statement\\\\n6:25 - Q-Learning Primer\\\\n11:40 - Multiple Rewards, Multiple Policies\\\\n14:25 - Example Environment\\\\n17:35 - Tasks as Linear Mixtures of Features\\\\n24:15 - Successor Features\\\\n28:00 - Zero-Shot Policy for New Tasks\\\\n35:30 - Results on New Task W3\\\\n37:00 - Inferring the Task via Regression\\\\n39:20 - The Influence of the Given Policies\\\\n48:40 - Learning the Feature Functions\\\\n50:30 - More Complicated Tasks\\\\n51:40 - Life-Long Learning, Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://www.pnas.org/content/early/2020/08/13/1907370117\\\\n\\\\nMy Video on Successor Features: https://youtu.be/KXEEqcwXn8w\\\\n\\\\nAbstract:\\\\nThe combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.\\\\n\\\\nAuthors:\\\\nAndr\\xc3\\xa9 Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3311",
    "uploadDate": "2020-08-23",
    "thumbnail_url": "https://i.ytimg.com/vi/9"
  },
  {
    "link": "watch?v=a4VvcmqnkhY",
    "title": "What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, rl, deep rl, deep reinforcement learning, on",
    "scraped_at": 1684582650.9091394,
    "genre": "Science",
    "views": "8656",
    "desc": "#ai #research #machinelearning\\\\n\\\\nOnline Reinforcement Learning is a flourishing field with countless methods for practitioners to choose from. However, each of those methods comes with a plethora of hyperparameter choices. This paper builds a unified framework for five continuous control tasks and investigates in a large-scale study the effects of these choices. As a result, they come up with a set of recommendations for future research and applications.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:55 - Parameterized Agents\\\\n7:00 - Unified Online RL and Parameter Choices\\\\n14:10 - Policy Loss\\\\n16:40 - Network Architecture\\\\n20:25 - Initial Policy\\\\n24:20 - Normalization \\\\u0026 Clipping\\\\n26:30 - Advantage Estimation\\\\n28:55 - Training Setup\\\\n33:05 - Timestep Handling\\\\n34:10 - Optimizers\\\\n35:05 - Regularization\\\\n36:10 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.05990\\\\n\\\\nAbstract:\\\\nIn recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress (Engstrom\\'20). As a step towards filling that gap, we implement over 50 such \\\\\"",
    "lengthSeconds": "2309",
    "uploadDate": "2020-08-20",
    "thumbnail_url": "https://i.ytimg.com/vi/a4VvcmqnkhY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=VgqHitvEbR0",
    "title": "[Rant] REVIEWER #2: How Peer Review is FAILING in Machine Learning",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, research, ml, conference, nips, neurips, icml, iclr, review, peer review, publishing, accept, reject, citations, conflict, reviewer, rebuttal, area chair, money, free, experiments, theory, crisis, boom, overloaded, incentives, incentive, revise, quality",
    "scraped_at": 1684582650.99814,
    "genre": "Science",
    "views": "14990",
    "desc": "#ai #research #peerreview\\\\n\\\\nMachine Learning research is in dire straits as more people flood into the field and competent reviewers are scarce and overloaded. This video takes a look at the incentive structures behind the current system and describes how they create a negative feedback loop. In the end, I\\'ll go through some proposed solutions and add my own thoughts.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:05 - The ML Boom\\\\n3:10 - Author Incentives\\\\n7:00 - Conference Incentives\\\\n8:00 - Reviewer Incentives\\\\n13:10 - Proposed Solutions\\\\n17:20 - A Better Solution\\\\n23:50 - The Road Ahead\\\\n\\\\nPS: If it is not entirely clear to anyone already, stealing ideas as a reviewer is against most conferences\\' code of ethics and I disapprove of any such behavior. I mention it because it is being done regularly and good luck proving it in any particular case.\\\\n\\\\nSources:\\\\nhttps://thecognitivevortex.wordpress.com/category/phd/\\\\nhttps://susannapaasonen.org/2019/05/31/observations-on-peer-reviewing/\\\\nhttps://www.radicalhistoryreview.org/abusablepast/forum-1-1-on-peer-review/\\\\nhttps://www.meme-arsenal.com/en/create/meme/2012988\\\\nhttps://imgflip.com/i/1pydon\\\\nhttps://uqkdhanj.wordpress.com/2015/02/18/10-best-reviewer-comments-in-meme-part-2/\\\\nhttps://susannapaasonen.org/2019/05/31/observations-on-peer-reviewing/\\\\nhttps://www.memecreator.org/meme/what-if-i-told-you-reviewer-2-wanted-more-experiments/\\\\nhttps://www.emaze.com/@ATFTTRRF\\\\nhttps://thegradient.pub/neurips-2019-too-big/\\\\nhttps://www.videezy.com/backgrounds/6199-switzerland-flag-4k-motion-loop-stock-video\\\\nhttp://blog.mrtz.org/2014/12/15/the-nips-experiment.html\\\\nhttps://twitter.com/tdietterich/status/1292217162103316481\\\\nhttps://www.pinterest.de/pin/192951165261323337/\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1520",
    "uploadDate": "2020-08-18",
    "thumbnail_url": "https://i.ytimg.com/vi/VgqHitvEbR0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lj-LGrnh1oU",
    "title": "REALM: Retrieval-Augmented Language Model Pre-Training (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, orqa, qa, question answering, google, kenton, wikipedia, mlm, bert, masked language modeling, realm, t5, transformer, inner product, mips, index, pretraining, ict, inverse cloze task, google ai, search, retrieval, documents, natural questions, open domain, attention, salient, masking, encoder",
    "scraped_at": 1684582649.4925435,
    "genre": "Science",
    "views": "10928",
    "desc": "#ai #tech #science\\\\n\\\\nOpen Domain Question Answering is one of the most challenging tasks in NLP. When answering a question, the model is able to retrieve arbitrary documents from an indexed corpus to gather more information. REALM shows how Masked Language Modeling (MLM) pretraining can be used to train a retriever for relevant documents in an end-to-end fashion and improves over state-of-the-art by a significant margin.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction \\\\u0026 Overview\\\\n4:30 - World Knowledge in Language Models\\\\n8:15 - Masked Language Modeling for Latent Document Retrieval\\\\n14:50 - Problem Formulation\\\\n17:30 - Knowledge Retriever Model using MIPS\\\\n23:50 - Question Answering Model\\\\n27:50 - Architecture Recap\\\\n29:55 - Analysis of the Loss Gradient\\\\n34:15 - Initialization using the Inverse Cloze Task\\\\n41:40 - Prohibiting Trivial Retrievals\\\\n44:05 - Null Document\\\\n45:00 - Salient Span Masking\\\\n50:15 - My Idea on Salient Span Masking\\\\n51:50 - Experimental Results and Ablations\\\\n57:30 - Concrete Example from the Model\\\\n\\\\nPaper: https://arxiv.org/abs/2002.08909\\\\nCode: https://github.com/google-research/language/tree/master/language/realm\\\\n\\\\nMy Video on GPT-3: https://www.youtube.com/watch?v=SY5PvZrJhLE\\\\nMy Video on BERT: https://www.youtube.com/watch?v=-9evrZnBorM\\\\nMy Video on Word2Vec: https://www.youtube.com/watch?v=yexR53My2O4\\\\n\\\\nAbstract:\\\\nLanguage model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\\\\nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\\\\nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.\\\\n\\\\nAuthors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "3640",
    "uploadDate": "2020-08-14",
    "thumbnail_url": "https://i.ytimg.com/vi/lj"
  },
  {
    "link": "watch?v=v2GRWzIhaqQ",
    "title": "Meta-Learning through Hebbian Plasticity in Random Networks (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, hebbian, vision, car, ant, quadruped, neuroplasticity, fire together wire together, reinforcement learning, deep rl, deep reinforcement learning, policy network, policy gradient, evolutionary methods, evolution step, population, correlation, gradient, episode, random, adaptive, reconfigure, damage, injury, agent",
    "scraped_at": 1684582649.5795455,
    "genre": "Science",
    "views": "11344",
    "desc": "#ai #neuroscience #rl\\\\n\\\\nReinforcement Learning is a powerful tool, but it lacks biological plausibility because it learns a fixed policy network. Animals use neuroplasticity to reconfigure their policies on the fly and quickly adapt to new situations. This paper uses Hebbian Learning, a biologically inspired technique, to have agents adapt random networks to high-performing solutions as an episode is progressing, leading to agents that can reconfigure themselves in response to new observations.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:30 - Reinforcement Learning vs Hebbian Plasticity\\\\n9:00 - Episodes in Hebbian Learning\\\\n10:00 - Hebbian Plasticity Rules\\\\n18:10 - Quadruped Experiment Results\\\\n21:20 - Evolutionary Learning of Hebbian Plasticity\\\\n29:10 - More Experimental Results\\\\n34:50 - Conclusions\\\\n35:30 - Broader Impact Statement\\\\n\\\\nVideos: https://twitter.com/risi1979/status/1280544779630186499\\\\nPaper: https://arxiv.org/abs/2007.02686\\\\n\\\\nAbstract:\\\\nLifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to different morphological damage in the absence of any explicit reward or error signal.\\\\n\\\\nAuthors: Elias Najarro, Sebastian Risi\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2362",
    "uploadDate": "2020-08-12",
    "thumbnail_url": "https://i.ytimg.com/vi/v2GRWzIhaqQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=nv6oFDp6rNQ",
    "title": "Hopfield Networks is All You Need (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, schmidhuber, hochreiter, lstm, gru, rnn, hopfield, attention, attention is all you need, transformer, bert, query, key, value, routing, pattern, retrieval, store, error, exponental, binary, continuous, hopfield network, lse, energy function, update rule, metastable, separation",
    "scraped_at": 1684582649.6685114,
    "genre": "Science",
    "views": "75243",
    "desc": "#ai #transformer #attention\\\\n\\\\nHopfield Networks are one of the classic models of biological memory networks. This paper generalizes modern Hopfield Networks to continuous states and shows that the corresponding update rule is equal to the attention mechanism used in modern Transformers. It further analyzes a pre-trained BERT model through the lens of Hopfield Networks and uses a Hopfield Attention Layer to perform Immune Repertoire Classification.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:35 - Binary Hopfield Networks\\\\n5:55 - Continuous Hopfield Networks\\\\n8:15 - Update Rules \\\\u0026 Energy Functions\\\\n13:30 - Connection to Transformers\\\\n14:35 - Hopfield Attention Layers\\\\n26:45 - Theoretical Analysis\\\\n48:10 - Investigating BERT\\\\n1:02:30 - Immune Repertoire Classification\\\\n\\\\nPaper: https://arxiv.org/abs/2008.02217\\\\nCode: https://github.com/ml-jku/hopfield-layers\\\\nImmune Repertoire Classification Paper: https://arxiv.org/abs/2007.13505\\\\n\\\\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\\\\nMy Video on BERT: https://youtu.be/-9evrZnBorM\\\\n\\\\nAbstract:\\\\nWe show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called \\\\\"",
    "lengthSeconds": "3915",
    "uploadDate": "2020-08-09",
    "thumbnail_url": "https://i.ytimg.com/vi/nv6oFDp6rNQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=udS2OPohs_s",
    "title": "I TRAINED AN AI TO SOLVE 2+2 (w/ Live Coding)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, 2",
    "scraped_at": 1684582649.751513,
    "genre": "Science",
    "views": "10946",
    "desc": "#ai #tech #code\\\\n\\\\nA whole bunch of humans are arguing whether 2+2=4 or 2+2=5. Pointless! Let the machines handle this!\\\\n\\\\nColab: https://colab.research.google.com/drive/1tDjFW7CFGQG8vHdUAVNpr2EG9z0JZGYC?usp=sharing\\\\n\\\\nDisclaimer: This is a joke.\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "698",
    "uploadDate": "2020-08-06",
    "thumbnail_url": "https://i.ytimg.com/vi/udS2OPohs_s/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ml3Y1ljVSQ8",
    "title": "PCGRL: Procedural Content Generation via Reinforcement Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, reinforcement learning, level design, game design, video game, sobokan, sokoban, zelda, maze, agent, turtle, observation, reward, action, space, deep rl, deep reinforcement learning, content, minecraft",
    "scraped_at": 1684582649.8378453,
    "genre": "Science",
    "views": "7273",
    "desc": "#ai #research #gaming\\\\n\\\\nDeep RL is usually used to solve games, but this paper turns the process on its head and applies RL to game level creation. Compared to traditional approaches, it frames level design as a sequential decision making progress and ends up with a fast and diverse level generator.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:30 - Level Design via Reinforcement Learning\\\\n3:00 - Reinforcement Learning\\\\n4:45 - Observation Space\\\\n5:40 - Action Space\\\\n15:40 - Change Percentage Limit\\\\n20:50 - Quantitative Results\\\\n22:10 - Conclusion \\\\u0026 Outlook\\\\n\\\\nPaper: https://arxiv.org/abs/2001.09212\\\\nCode: https://github.com/amidos2006/gym-pcgrl\\\\n\\\\nAbstract:\\\\nWe investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.\\\\n\\\\nAuthors: Ahmed Khalifa, Philip Bontrager, Sam Earle, Julian Togelius\\\\n\\\\nERRATA:\\\\n- The reward is given after each step.\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1476",
    "uploadDate": "2020-08-04",
    "thumbnail_url": "https://i.ytimg.com/vi/ml3Y1ljVSQ8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=WVPE62Gk3EM",
    "title": "Big Bird: Transformers for Longer Sequences (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, google research, bigbird, big bird, bert, attention, attention is all you need, longformer, random attention, quadratic attention, attention mechanism, qa, natural questions, hotpot qa, genomics, nlp, natural language processing, transformer, transformers, fully connected, sparse attention, graph, star graph, turing complete, universal approximation, window attention, convolution",
    "scraped_at": 1684582649.9278133,
    "genre": "Science",
    "views": "21586",
    "desc": "#ai #nlp #attention\\\\n\\\\nThe quadratic resource requirements of the attention mechanism are the main roadblock in scaling up transformers to long sequences. This paper replaces the full quadratic attention mechanism by a combination of random attention, window attention, and global attention. Not only does this allow the processing of longer sequences, translating to state-of-the-art experimental results, but also the paper shows that BigBird comes with theoretical guarantees of universal approximation and turing completeness.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:50 - Quadratic Memory in Full Attention\\\\n4:55 - Architecture Overview\\\\n6:35 - Random Attention\\\\n10:10 - Window Attention\\\\n13:45 - Global Attention\\\\n15:40 - Architecture Summary\\\\n17:10 - Theoretical Result\\\\n22:00 - Experimental Parameters\\\\n25:35 - Structured Block Computations\\\\n29:30 - Recap\\\\n31:50 - Experimental Results\\\\n34:05 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2007.14062\\\\n\\\\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\\\\nMy Video on BERT: https://youtu.be/-9evrZnBorM\\\\nMy Video on Longformer: https://youtu.be/_8KNb5iqblE\\\\n... and its memory requirements: https://youtu.be/gJR28onlqzs\\\\n\\\\nAbstract:\\\\nTransformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\\\\n\\\\nAuthors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2069",
    "uploadDate": "2020-08-02",
    "thumbnail_url": "https://i.ytimg.com/vi/WVPE62Gk3EM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=q7PjrmGNx5A",
    "title": "Self-training with Noisy Student improves ImageNet classification (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ssl, semi",
    "scraped_at": 1684582650.017814,
    "genre": "Science",
    "views": "15779",
    "desc": "The abundance of data on the internet is vast. Especially unlabeled images are plentiful and can be collected with ease. This model investigates a new method for incorporating unlabeled data into a supervised learning pipeline. First, a teacher model is trained in a supervised fashion. Then, that teacher is used to label the unlabeled data. Next, a larger student model is trained on the combination of all data and achieves better performance than the teacher by itself.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:05 - Semi-Supervised \\\\u0026 Transfer Learning\\\\n5:45 - Self-Training \\\\u0026 Knowledge Distillation\\\\n10:00 - Noisy Student Algorithm Overview\\\\n20:20 - Noise Methods\\\\n22:30 - Dataset Balancing\\\\n25:20 - Results\\\\n30:15 - Perturbation Robustness\\\\n34:35 - Ablation Studies\\\\n39:30 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/1911.04252\\\\nCode: https://github.com/google-research/noisystudent\\\\nModels: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\\\\n\\\\nAbstract:\\\\nWe present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.\\\\nNoisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.\\\\n\\\\nAuthors: Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2435",
    "uploadDate": "2020-07-29",
    "thumbnail_url": "https://i.ytimg.com/vi/q7PjrmGNx5A/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rFwQDDbYTm4",
    "title": "[Classic] Playing Atari with Deep Reinforcement Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, dqn, deep q learning, deep q networks, q learning, qlearning, rl, drl, deep rl, deep reinforcement learning, deepmind, david silver, atari, pong, breakout, space invaders, agent, cnn, convolutional neural network, bellman",
    "scraped_at": 1684582651.0851712,
    "genre": "Science",
    "views": "34702",
    "desc": "#ai #dqn #deepmind\\\\n\\\\nAfter the initial success of deep neural networks, especially convolutional neural networks on supervised image processing tasks, this paper was the first to demonstrate their applicability to reinforcement learning. Deep Q Networks learn from pixel input to play seven different Atari games and outperform baselines that require hand-crafted features. This paper kicked off the entire field of deep reinforcement learning and positioned DeepMind as one of the leading AI companies in the world.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:50 - Arcade Learning Environment\\\\n4:25 - Deep Reinforcement Learning\\\\n9:20 - Deep Q-Learning\\\\n26:30 - Experience Replay\\\\n32:25 - Network Architecture\\\\n33:50 - Experiments\\\\n37:45 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/1312.5602\\\\n\\\\nAbstract:\\\\nWe present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.\\\\n\\\\nAuthors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2351",
    "uploadDate": "2020-07-26",
    "thumbnail_url": "https://i.ytimg.com/vi/rFwQDDbYTm4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Nq3auVtvd9Q",
    "title": "[Classic] ImageNet Classification with Deep Convolutional Neural Networks (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, classic, alexnet, hinton, geoff hinton, imagenet, convolution, convolutional neural network, architecture, dropout, data augmentation, cnns, computer vision, image classification, object recognition, classifier, max pool, pretraining, deep neural networks",
    "scraped_at": 1684582650.109492,
    "genre": "Science",
    "views": "31354",
    "desc": "#ai #research #alexnet\\\\n\\\\nAlexNet was the start of the deep learning revolution. Up until 2012, the best computer vision systems relied on hand-crafted features and highly specialized algorithms to perform object classification. This paper was the first to successfully train a deep convolutional neural network on not one, but two GPUs and managed to outperform the competition on ImageNet by an order of magnitude.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:00 - The necessity of larger models\\\\n6:20 - Why CNNs?\\\\n11:05 - ImageNet\\\\n12:05 - Model Architecture Overview\\\\n14:35 - ReLU Nonlinearities\\\\n18:45 - Multi-GPU training\\\\n21:30 - Classification Results\\\\n24:30 - Local Response Normalization\\\\n28:05 - Overlapping Pooling\\\\n32:25 - Data Augmentation\\\\n38:30 - Dropout\\\\n40:30 - More Results\\\\n43:50 - Conclusion\\\\n\\\\nPaper: http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf\\\\n\\\\nAbstract:\\\\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \\xe2\\x80\\x9cdropout\\xe2\\x80\\x9d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\\\\n\\\\nAuthors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2767",
    "uploadDate": "2020-07-23",
    "thumbnail_url": "https://i.ytimg.com/vi/Nq3auVtvd9Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=a6v92P0EbJc",
    "title": "Neural Architecture Search without Training (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nas, nas",
    "scraped_at": 1684582650.1954935,
    "genre": "Science",
    "views": "24823",
    "desc": "#ai #research #machinelearning\\\\n\\\\nNeural Architecture Search is typically very slow and resource-intensive. A meta-controller has to train many hundreds or thousands of different models to find a suitable building plan. This paper proposes to use statistics of the Jacobian around data points to estimate the performance of proposed architectures at initialization. This method does not require training and speeds up NAS by orders of magnitude.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n0:50 - Neural Architecture Search\\\\n4:15 - Controller-based NAS\\\\n7:35 - Architecture Search Without Training\\\\n9:30 - Linearization Around Datapoints\\\\n14:10 - Linearization Statistics\\\\n19:00 - NAS-201 Benchmark\\\\n20:15 - Experiments\\\\n34:15 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.04647\\\\nCode: https://github.com/BayesWatch/nas-without-training\\\\n\\\\nAbstract:\\\\nThe time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be extremely slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network\\'s trained accuracy from its initial state. In this work, we examine how the linear maps induced by data points correlate for untrained network architectures in the NAS-Bench-201 search space, and motivate how this can be used to give a measure of modelling flexibility which is highly indicative of a network\\'s trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU. Code to reproduce our experiments is available at this https URL.\\\\n\\\\nAuthors: Joseph Mellor, Jack Turner, Amos Storkey, Elliot J. Crowley\\\\n\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar (preferred to Patreon): https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2106",
    "uploadDate": "2020-07-21",
    "thumbnail_url": "https://i.ytimg.com/vi/a6v92P0EbJc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=eyxmSmjmNS0",
    "title": "[Classic] Generative Adversarial Networks (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gan, generator, discriminator, convolution, deconvolution, goodfellow, bengio, convolutional neural network, mnist, cifar10, generative, generative model, image generation, face model, latent space, interpolation, minmax, nash equilibrium, game theory",
    "scraped_at": 1684582651.1731415,
    "genre": "Science",
    "views": "48625",
    "desc": "#ai #deeplearning #gan\\\\n\\\\nGANs are of the main models in modern deep learning. This is the paper that started it all! While the task of image classification was making progress, the task of image generation was still cumbersome and prone to artifacts. The main idea behind GANs is to pit two competing networks against each other, thereby creating a generative model that only ever has implicit access to the data through a second, discriminative, model. The paper combines architecture, experiments, and theoretical analysis beautifully.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:50 - Motivation\\\\n8:40 - Minimax Loss Function\\\\n13:20 - Intuition Behind the Loss\\\\n19:30 - GAN Algorithm\\\\n22:05 - Theoretical Analysis\\\\n27:00 - Experiments\\\\n33:10 - Advantages \\\\u0026 Disadvantages\\\\n35:00 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/1406.2661\\\\n\\\\nAbstract:\\\\nWe propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\\\\n\\\\nAuthors: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "2224",
    "uploadDate": "2020-07-19",
    "thumbnail_url": "https://i.ytimg.com/vi/eyxmSmjmNS0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yexR53My2O4",
    "title": "[Classic] Word2Vec: Distributed Representations of Words and Phrases and their Compositionality",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, jeff dean, mikolov, word2vec, word vectors, word representations, nlp, natural language processing, sentiment classification, king, queen, man, woman, arithmetic, latent space, distributed, country, capital, semantic, synonyms, skip gram, negative sampling, nce, noise contrastive estimation",
    "scraped_at": 1684582651.259141,
    "genre": "Science",
    "views": "19030",
    "desc": "#ai #research #word2vec\\\\n\\\\nWord vectors have been one of the most influential techniques in modern NLP to date. This paper describes Word2Vec, which the most popular technique to obtain word vectors. The paper introduces the negative sampling technique as an approximation to noise contrastive estimation and shows that this allows the training of word vectors from giant corpora on a single machine in a very short time.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n1:50 - Distributed Word Representations\\\\n5:40 - Skip-Gram Model\\\\n12:00 - Hierarchical Softmax\\\\n14:55 - Negative Sampling\\\\n22:30 - Mysterious 3/4 Power\\\\n25:50 - Frequent Words Subsampling\\\\n28:15 - Empirical Results\\\\n29:45 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/1310.4546\\\\nCode: https://code.google.com/archive/p/word2vec/\\\\n\\\\nAbstract:\\\\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \\\\\"",
    "lengthSeconds": "1882",
    "uploadDate": "2020-07-16",
    "thumbnail_url": "https://i.ytimg.com/vi/yexR53My2O4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GWt6Fu05voI",
    "title": "[Classic] Deep Residual Learning for Image Recognition (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, vision, computer vision, kaiming he, google, resnet, resnet50, resnet151, deep neural network, imagenet, residual, identity function, very deep, convolutional neural network, bottleneck, overfitting",
    "scraped_at": 1684582651.3491402,
    "genre": "Science",
    "views": "56935",
    "desc": "#ai #research #resnet\\\\n\\\\nResNets are one of the cornerstones of modern Computer Vision. Before their invention, people were not able to scale deep neural networks beyond 20 or so layers, but with this paper\\'s invention of residual connections, all of a sudden networks could be arbitrarily deep. This led to a big spike in the performance of convolutional neural networks and rapid adoption in the community. To this day, ResNets are the backbone of most vision models and residual connections appear all throughout deep learning.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:45 - The Problem with Depth\\\\n3:15 - VGG-Style Networks\\\\n6:00 - Overfitting is Not the Problem\\\\n7:25 - Motivation for Residual Connections\\\\n10:25 - Residual Blocks\\\\n12:10 - From VGG to ResNet\\\\n18:50 - Experimental Results\\\\n23:30 - Bottleneck Blocks\\\\n24:40 - Deeper ResNets\\\\n28:15 - More Results\\\\n29:50 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/1512.03385\\\\n\\\\nAbstract:\\\\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\\\\nThe depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \\\\u0026 COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\\\\n\\\\nAuthors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\\\\n\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1881",
    "uploadDate": "2020-07-14",
    "thumbnail_url": "https://i.ytimg.com/vi/GWt6Fu05voI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GwItCHOifG8",
    "title": "I'M TAKING A BREAK... (Channel Update July 2020)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582656.8305502,
    "genre": "Science",
    "views": "8305",
    "desc": "Past, Present \\\\u0026 Future of this Channel.\\\\n\\\\nOUTLINE:\\\\n0:00 - I\\'m going on a break\\\\n0:20 - Channel Stats\\\\n1:20 - Other Platforms\\\\n4:20 - Drama Videos\\\\n5:30 - Flatland\\\\n8:40 - SpineNet Thumbnail\\\\n9:55 - Future Content\\\\n12:55 - How do I select papers?\\\\n15:50 - Financial Support, Ads \\\\u0026 Merch\\\\n18:50 - Conclusion\\\\n\\\\nOur Flatland Repo: https://github.com/yk/youtube-flatland\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\\\\nLinkedIn: https://www.linkedin.com/in/yannic-kilcher-488534136/\\\\n\\\\nIf you want to support me, the best thing to do is to share out the content :)\\\\n\\\\nIf you want to support me financially (completely optional and voluntary, but a lot of people have asked for this):\\\\nSubscribeStar: https://www.subscribestar.com/yannickilcher\\\\nPatreon: https://www.patreon.com/yannickilcher\\\\nBitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq\\\\nEthereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2\\\\nLitecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m\\\\nMonero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\"",
    "lengthSeconds": "1176",
    "uploadDate": "2020-07-12",
    "thumbnail_url": "https://i.ytimg.com/vi/GwItCHOifG8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5IRlUVrEVL8",
    "title": "Deep Ensembles: A Loss Landscape Perspective (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ensembles, bayesian, modes, loss function, nonconvex, google, deepmind, stan fort, foundational, weight space, labels, agreement, minima, loss landscape, trajectory, local minima, optimization",
    "scraped_at": 1684582650.292491,
    "genre": "Science",
    "views": "20581",
    "desc": "#ai #research #optimization\\\\n\\\\nDeep Ensembles work surprisingly well for improving the generalization capabilities of deep neural networks. Surprisingly, they outperform Bayesian Networks, which are - in theory - doing the same thing. This paper investigates how Deep Ensembles are especially suited to capturing the non-convex loss landscape of neural networks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:05 - Deep Ensembles\\\\n4:15 - The Solution Space of Deep Networks\\\\n7:30 - Bayesian Models\\\\n9:00 - The Ensemble Effect\\\\n10:25 - Experiment Setup\\\\n11:30 - Solution Equality While Training\\\\n19:40 - Tracking Multiple Trajectories\\\\n21:20 - Similarity of Independent Solutions\\\\n24:10 - Comparison to Baselines\\\\n30:10 - Weight Space Cross-Sections\\\\n35:55 - Diversity vs Accuracy\\\\n41:00 - Comparing Ensembling Methods\\\\n44:55 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/1912.02757\\\\n\\\\nAbstract:\\\\nDeep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.\\\\n\\\\nAuthors: Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2791",
    "uploadDate": "2020-07-11",
    "thumbnail_url": "https://i.ytimg.com/vi/5IRlUVrEVL8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=v-ZxzTSpmk4",
    "title": "Gradient Origin Networks (Paper Explained w/ Live Coding)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gon, gradient, negative gradient, implicit, implicit representation, siren, sirens, deep neural networks, convolutional neural network, dnns, mnist, cifar10, fashion mnist, gradient descent, sgd, inner loop, backpropagation, live code, code, machine learning code, research, research paper",
    "scraped_at": 1684582650.3804927,
    "genre": "Science",
    "views": "9987",
    "desc": "Neural networks for implicit representations, such as SIRENs, have been very successful at modeling natural signals. However, in the classical approach, each data point requires its own neural network to be fit. This paper extends implicit representations to an entire dataset by introducing latent vectors of data points to SIRENs. Interestingly, the paper shows that such latent vectors can be obtained without the need for an explicit encoder, by simply looking at the negative gradient of the zero-vector through the representation function.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:10 - Implicit Generative Models\\\\n5:30 - Implicitly Represent a Dataset\\\\n11:00 - Gradient Origin Networks\\\\n23:55 - Relation to Gradient Descent\\\\n28:05 - Messing with their Code\\\\n37:40 - Implicit Encoders\\\\n38:50 - Using GONs as classifiers\\\\n40:55 - Experiments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2007.02798\\\\nCode: https://github.com/cwkx/GON\\\\nProject Page: https://cwkx.github.io/data/GON/\\\\n\\\\nMy Video on SIREN: https://youtu.be/Q5g3p9Zwjrk\\\\n\\\\nAbstract:\\\\nThis paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks.\\\\n\\\\nAuthors: Sam Bond-Taylor, Chris G. Willcocks\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\"",
    "lengthSeconds": "2535",
    "uploadDate": "2020-07-10",
    "thumbnail_url": "https://i.ytimg.com/vi/v"
  },
  {
    "link": "watch?v=x6T1zMSE4Ts",
    "title": "NVAE: A Deep Hierarchical Variational Autoencoder (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gan, vae, kl, elbo, autoencoder, variational, latent, sampling, hierarchical, scales, faces, mnist, cifar10, swish, batch norm, generative, nvidia, mixed precision, memory, deep, layers, depthwise convolutions, cnn, convolutional, generation, generative model",
    "scraped_at": 1684582650.4684935,
    "genre": "Science",
    "views": "31544",
    "desc": "VAEs have been traditionally hard to train at high resolutions and unstable when going deep with many layers. In addition, VAE samples are often more blurry and less crisp than those from GANs. This paper details all the engineering choices necessary to successfully train a deep hierarchical VAE that exhibits global consistency and astounding sharpness at high resolutions.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:55 - Variational Autoencoders\\\\n8:25 - Hierarchical VAE Decoder\\\\n12:45 - Output Samples\\\\n15:00 - Hierarchical VAE Encoder\\\\n17:20 - Engineering Decisions\\\\n22:10 - KL from Deltas\\\\n26:40 - Experimental Results\\\\n28:40 - Appendix\\\\n33:00 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2007.03898\\\\n\\\\nAbstract:\\\\nNormalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\\xc3\\x97256 pixels.\\\\n\\\\nAuthors: Arash Vahdat, Jan Kautz\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\"",
    "lengthSeconds": "2051",
    "uploadDate": "2020-07-09",
    "thumbnail_url": "https://i.ytimg.com/vi/x6T1zMSE4Ts/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Jqvb7jp4Nm8",
    "title": "Addendum for Supermasks in Superposition: A Closer Look (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, supsup, supermasks, lottery ticket, lottery ticket hypothesis, gradient, entropy, surplus, superfluous neurons, lifelong learning, multitask learning, catastrophic forgetting, continuous learning, binary mask, random network, optimization, hopfield network, gradient descent, superposition",
    "scraped_at": 1684582650.5534909,
    "genre": "Science",
    "views": "2698",
    "desc": "I take a closer look at \\\\\"",
    "lengthSeconds": "2931",
    "uploadDate": "2020-07-08",
    "thumbnail_url": "https://i.ytimg.com/vi/Jqvb7jp4Nm8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3jT1qJ8ETzk",
    "title": "SupSup: Supermasks in Superposition (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, supsup, supermasks, lottery ticket, lottery ticket hypothesis, gradient, entropy, surplus, superfluous neurons, lifelong learning, multitask learning, catastrophic forgetting, continuous learning, binary mask, random network, optimization, hopfield network, gradient descent, superposition",
    "scraped_at": 1684582651.4441664,
    "genre": "Science",
    "views": "7732",
    "desc": "Supermasks are binary masks of a randomly initialized neural network that result in the masked network performing well on a particular task. This paper considers the problem of (sequential) Lifelong Learning and trains one Supermask per Task, while keeping the randomly initialized base network constant. By minimizing the output entropy, the system can automatically derive the Task ID of a data point at inference time and distinguish up to 2500 tasks automatically.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:20 - Catastrophic Forgetting\\\\n5:20 - Supermasks\\\\n9:35 - Lifelong Learning using Supermasks\\\\n11:15 - Inference Time Task Discrimination by Entropy\\\\n15:05 - Mask Superpositions\\\\n24:20 - Proof-of-Concept, Task Given at Inference\\\\n30:15 - Binary Maximum Entropy Search\\\\n32:00 - Task Not Given at Inference\\\\n37:15 - Task Not Given at Training\\\\n41:35 - Ablations\\\\n45:05 - Superfluous Neurons\\\\n51:10 - Task Selection by Detecting Outliers\\\\n57:40 - Encoding Masks in Hopfield Networks\\\\n59:40 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2006.14769\\\\nCode: https://github.com/RAIVNLab/supsup\\\\n\\\\nMy Video about Lottery Tickets: https://youtu.be/ZVVnvZdUMUk\\\\nMy Video about Supermasks: https://youtu.be/jhCInVFE2sc\\\\n\\\\nAbstract:\\\\nWe present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.\\\\n\\\\nAuthors: Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\"",
    "lengthSeconds": "3606",
    "uploadDate": "2020-07-07",
    "thumbnail_url": "https://i.ytimg.com/vi/3jT1qJ8ETzk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=z_3Qv4In2ac",
    "title": "[Live Machine Learning Research] Plain Self-Ensembles (I actually DISCOVER SOMETHING) - Part 1",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ensemble, pytorch, lightning, cifar10, github, vim, code, cuda, gpu, research, ml, ml research, how to, implement, live coding, python, self, distillation, born again, deep ensembles, cnn, resnet, vgg, torchvision, imagenet",
    "scraped_at": 1684582651.5301743,
    "genre": "Science",
    "views": "12092",
    "desc": "I share my progress of implementing a research idea from scratch. I attempt to build an ensemble model out of students of label-free self-distillation without any additional data or augmentation. Turns out, it actually works, and interestingly, the more students I employ, the better the accuracy. This leads to the hypothesis that the ensemble effect is not a process of extracting more information from labels.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n2:10 - Research Idea\\\\n4:15 - Adjusting the Codebase\\\\n25:00 - Teacher and Student Models\\\\n52:30 - Shipping to the Server\\\\n1:03:40 - Results\\\\n1:14:50 - Conclusion\\\\n\\\\nCode: https://github.com/yk/PyTorch_CIFAR10\\\\n\\\\nReferences:\\\\nMy Video on SimCLRv2: https://youtu.be/2lkUNDZld-4\\\\nBorn-Again Neural Networks: https://arxiv.org/abs/1805.04770\\\\nDeep Ensembles: A Loss Landscape Perspective: https://arxiv.org/abs/1912.02757\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nParler: https://parler.com/profile/YannicKilcher\"",
    "lengthSeconds": "4723",
    "uploadDate": "2020-07-06",
    "thumbnail_url": "https://i.ytimg.com/vi/z_3Qv4In2ac/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qFRfnIRMNlk",
    "title": "SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, vision, recognition, localization, resnet, resnet50, fpn, backbone, permuation, upsampling, stride, convolution, convolutional neural network, google, spine, spine net, imagenet, coco, segmentation, bounding box, skip connections, residual, bottleneck",
    "scraped_at": 1684582651.617139,
    "genre": "Science",
    "views": "10166",
    "desc": "#machinelearning #ai #google\\\\n\\\\nThe high-level architecture of CNNs has not really changed over the years. We tend to build high-resolution low-dimensional layers first, followed by ever more coarse, but deep layers. This paper challenges this decades-old heuristic and uses neural architecture search to find an alternative, called SpineNet that employs multiple rounds of re-scaling and long-range skip connections.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:00 - Problem Statement\\\\n2:30 - The Problem with Current Architectures\\\\n8:20 - Scale-Permuted Networks\\\\n11:40 - Neural Architecture Search\\\\n14:00 - Up- and Downsampling\\\\n19:10 - From ResNet to SpineNet\\\\n24:20 - Ablations\\\\n27:00 - My Idea: Attention Routing for CNNs\\\\n29:55 - More Experiments\\\\n34:45 - Conclusion \\\\u0026 Comments\\\\n\\\\nPapers: https://arxiv.org/abs/1912.05027\\\\nCode: https://github.com/tensorflow/tpu/tree/master/models/official/detection\\\\n\\\\nAbstract:\\\\nConvolutional neural networks typically encode an input image into a series of intermediate features with decreasing resolutions. While this structure is suited to classification tasks, it does not perform well for tasks requiring simultaneous recognition and localization (e.g., object detection). The encoder-decoder architectures are proposed to resolve this by applying a decoder network onto a backbone model designed for classification tasks. In this paper, we argue encoder-decoder architecture is ineffective in generating strong multi-scale features because of the scale-decreased backbone. We propose SpineNet, a backbone with scale-permuted intermediate features and cross-scale connections that is learned on an object detection task by Neural Architecture Search. Using similar building blocks, SpineNet models outperform ResNet-FPN models by ~3% AP at various scales while using 10-20% fewer FLOPs. In particular, SpineNet-190 achieves 52.5% AP with a MaskR-CNN detector and achieves 52.1% AP with a RetinaNet detector on COCO for a single model without test-time augmentation, significantly outperforms prior art of detectors. SpineNet can transfer to classification tasks, achieving 5% top-1 accuracy improvement on a challenging iNaturalist fine-grained dataset. Code is at: this https URL.\\\\n\\\\nAuthors: Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, Xiaodan Song\\\\n\\\\nThumbnail art by Lucas Ferreira\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2152",
    "uploadDate": "2020-07-05",
    "thumbnail_url": "https://i.ytimg.com/vi/qFRfnIRMNlk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hAooAOFRsYc",
    "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nlp, natural language processing, attention, attention mechanism, linear, linear transformer, linformer, reformer, idiap, epfl, queries, keys, softmax, kernel, routing, inner product, rnn, recurrent neural network, transformer, bert, autoregressive, dimensions, topic modeling, language model",
    "scraped_at": 1684582653.5856354,
    "genre": "Science",
    "views": "23626",
    "desc": "#ai #attention #transformer #deeplearning\\\\n\\\\nTransformers are famous for two things: Their superior performance and their insane requirements of compute and memory. This paper reformulates the attention mechanism in terms of kernel functions and obtains a linear formulation, which reduces these requirements. Surprisingly, this formulation also surfaces an interesting connection between autoregressive transformers and RNNs.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:35 - Softmax Attention \\\\u0026 Transformers\\\\n8:40 - Quadratic Complexity of Softmax Attention\\\\n9:40 - Generalized Attention Mechanism\\\\n13:45 - Kernels\\\\n20:40 - Linear Attention\\\\n25:20 - Experiments\\\\n28:30 - Intuition on Linear Attention\\\\n33:55 - Connecting Autoregressive Transformers and RNNs\\\\n41:30 - Caveats with the RNN connection\\\\n46:00 - More Results \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2006.16236\\\\nWebsite: https://linear-transformers.com/\\\\nCode: https://github.com/idiap/fast-transformers\\\\n\\\\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\\\\nMy Video on BERT: https://youtu.be/-9evrZnBorM\\\\n\\\\nAbstract:\\\\nTransformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input\\'s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \\xee\\x88\\xbb(N2) to \\xee\\x88\\xbb(N), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\\\\n\\\\nAuthors: Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\\xc3\\xa7ois Fleuret\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2885",
    "uploadDate": "2020-07-04",
    "thumbnail_url": "https://i.ytimg.com/vi/hAooAOFRsYc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=O9kFX33nUcU",
    "title": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 4: The ARC Challenge (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, chollet, keras, google, francois, intelligence, iq, iq test, deep neural networks, prior, skill, performance, measurement, measure, test, number, intelligent, smart, learning, generalization, ability, experience, humans, evolution, nature, nurture, psychometrics, range, adaptability, arc, kaggle, difficulty, entropy, core knowledge, objectness, navigation, contact, agent, goal",
    "scraped_at": 1684582653.6726365,
    "genre": "Science",
    "views": "4626",
    "desc": "In this part, we look at the ARC challenge as a proposed test of machine intelligence. The dataset features 1000 tasks that test rapid generalization based on human core knowledge priors, such as object-ness, symmetry, and navigation.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:55 - What is ARC?\\\\n6:30 - The Goals of ARC\\\\n10:40 - Assumed Priors \\\\u0026 Examples\\\\n21:50 - An Imagined Solution\\\\n28:15 - Consequences of a Solution\\\\n31:00 - Weaknesses\\\\n31:25 - My Comments \\\\u0026 Ideas\\\\n\\\\nPaper: https://arxiv.org/abs/1911.01547\\\\nARC: https://github.com/fchollet/ARC\\\\n\\\\nAbstract:\\\\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \\\\\"",
    "lengthSeconds": "2036",
    "uploadDate": "2020-07-03",
    "thumbnail_url": "https://i.ytimg.com/vi/O9kFX33nUcU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=q6Kyvy1zLwQ",
    "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, bert, transformer, mlm, language model, masked language modeling, proteins, protein, amino acid, primary, secondary, tertiary, structure, helix, strand, band, sheet, turn, binding site, contact map, dna, rna, amino acids, proline, phenylalanine",
    "scraped_at": 1684582653.7616322,
    "genre": "Science",
    "views": "13431",
    "desc": "Proteins are the workhorses of almost all cellular functions and a core component of life. But despite their versatility, all proteins are built as sequences of the same 20 amino acids. These sequences can be analyzed with tools from NLP. This paper investigates the attention mechanism of a BERT model that has been trained on protein sequence data and discovers that the language model has implicitly learned non-trivial higher-order biological properties of proteins.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - From DNA to Proteins\\\\n5:20 - BERT for Amino Acid Sequences\\\\n8:50 - The Structure of Proteins\\\\n12:40 - Investigating Biological Properties by Inspecting BERT\\\\n17:45 - Amino Acid Substitution\\\\n24:55 - Contact Maps\\\\n30:15 - Binding Sites\\\\n33:45 - Linear Probes\\\\n35:25 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.15222\\\\nCode: https://github.com/salesforce/provis\\\\n\\\\nMy Video on BERT: https://youtu.be/-9evrZnBorM\\\\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\\\\n\\\\nAbstract:\\\\nTransformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at this https URL.\\\\n\\\\nAuthors: Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2209",
    "uploadDate": "2020-07-02",
    "thumbnail_url": "https://i.ytimg.com/vi/q6Kyvy1zLwQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1VdEw_mGjFk",
    "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nlp, billion, parameters, float32, attention mechanism, transformer, scale, gpt",
    "scraped_at": 1684582651.711139,
    "genre": "Science",
    "views": "14849",
    "desc": "Google builds a 600 billion parameter transformer to do massively multilingual, massive machine translation. Interestingly, the larger model scale does not come from increasing depth of the transformer, but from increasing width in the feedforward layers, combined with a hard routing to parallelize computations on up to 2048 TPUs. A very detailed engineering paper!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n4:10 - Main Results\\\\n5:10 - Mixture-of-Experts\\\\n16:00 - Difference to Scaling Classic Transformers\\\\n18:50 - Backpropagation in Mixture-of-Experts\\\\n20:05 - MoE Routing Algorithm in GShard\\\\n38:20 - GShard Einsum Examples\\\\n47:40 - Massively Multilingual Translation\\\\n56:00 - Results\\\\n1:11:30 - Conclusion \\\\u0026 Comments\\\\n\\\\nERRATA:\\\\nI said the computation of MoE scales linearly, but actually, it\\'s sub(!)-linear.\\\\n\\\\nPaper: https://arxiv.org/abs/2006.16668\\\\n\\\\nAbstract:\\\\nNeural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.\\\\n\\\\nAuthors:\\\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "4384",
    "uploadDate": "2020-07-01",
    "thumbnail_url": "https://i.ytimg.com/vi/1VdEw_mGjFk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DYBmD88vpiA",
    "title": "Object-Centric Learning with Slot Attention (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, ethz, vision, objects, slots, attention mechanism, gru, lstm, routing, capsules, permutation invariant, encoder, set, detr, embeddings, transformer, weight sharing, disentanglement, render, tetris, clevr, cnn, convolutional neural network, attention",
    "scraped_at": 1684582653.8476298,
    "genre": "Science",
    "views": "14346",
    "desc": "Visual scenes are often comprised of sets of independent objects. Yet, current vision models make no assumptions about the nature of the pictures they look at. By imposing an objectness prior, this paper a module that is able to recognize permutation-invariant sets of objects from pixels in both supervised and unsupervised settings. It does so by introducing a slot attention module that combines an attention mechanism with dynamic routing.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - Problem Formulation\\\\n4:30 - Slot Attention Architecture\\\\n13:30 - Slot Attention Algorithm\\\\n21:30 - Iterative Routing Visualization\\\\n29:15 - Experiments\\\\n36:20 - Inference Time Flexibility\\\\n38:35 - Broader Impact Statement\\\\n42:05 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.15055\\\\n\\\\nMy Video on Facebook\\'s DETR: https://youtu.be/T35ba_VXkMY\\\\nMy Video on Attention: https://youtu.be/iDulhoQ2pro\\\\nMy Video on Capsules: https://youtu.be/nXGHJTtFYRU\\\\n\\\\nAbstract:\\\\nLearning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.\\\\n\\\\nAuthors: Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2558",
    "uploadDate": "2020-06-30",
    "thumbnail_url": "https://i.ytimg.com/vi/DYBmD88vpiA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=V79rRI05Lj4",
    "title": "Set Distribution Networks: a Generative Model for Sets of Images (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, sets, images, cnn, convolutional neural network, gan, generator, encoder, discriminator, prior, mean, made, latent, binary, conditional, noise, distribution, probability, energy",
    "scraped_at": 1684582653.9336054,
    "genre": "Science",
    "views": "5170",
    "desc": "We\\'ve become very good at making generative models for images and classes of images, but not yet of sets of images, especially when the number of sets is unknown and can contain sets that have never been encountered during training. This paper builds a probabilistic framework and a practical implementation of a generative model for sets of images based on variational methods.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:25 - Problem Statement\\\\n8:05 - Architecture Overview\\\\n20:05 - Probabilistic Model\\\\n33:50 - Likelihood Function\\\\n40:30 - Model Architectures\\\\n44:20 - Loss Function \\\\u0026 Optimization\\\\n47:30 - Results\\\\n58:45 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2006.10705\\\\n\\\\nAbstract:\\\\nImages with shared characteristics naturally form sets. For example, in a face verification benchmark, images of the same identity form sets. For generative models, the standard way of dealing with sets is to represent each as a one hot vector, and learn a conditional generative model p(x|y). This representation assumes that the number of sets is limited and known, such that the distribution over sets reduces to a simple multinomial distribution. In contrast, we study a more generic problem where the number of sets is large and unknown. We introduce Set Distribution Networks (SDNs), a novel framework that learns to autoencode and freely generate sets. We achieve this by jointly learning a set encoder, set discriminator, set generator, and set prior. We show that SDNs are able to reconstruct image sets that preserve salient attributes of the inputs in our benchmark datasets, and are also able to generate novel objects/identities. We examine the sets generated by SDN with a pre-trained 3D reconstruction network and a face verification network, respectively, as a novel way to evaluate the quality of generated sets of images.\\\\n\\\\nAuthors: Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista, Carlos Guestrin, Josh M. Susskind\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "3557",
    "uploadDate": "2020-06-29",
    "thumbnail_url": "https://i.ytimg.com/vi/V79rRI05Lj4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=eI8xTdcZ6VY",
    "title": "Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, vision, cnn, convolutional neural network, coco, object detection, region of interest, rcnn, r",
    "scraped_at": 1684582651.8008416,
    "genre": "Science",
    "views": "12067",
    "desc": "Object detection often does not occur in a vacuum. Static cameras, such as wildlife traps, collect lots of irregularly sampled data over a large time frame and often capture repeating or similar events. This model learns to dynamically incorporate other frames taken by the same camera into its object detection pipeline.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:10 - Problem Formulation\\\\n2:10 - Static Camera Data\\\\n6:45 - Architecture Overview\\\\n10:00 - Short-Term Memory\\\\n15:40 - Long-Term Memory\\\\n20:10 - Quantitative Results\\\\n22:30 - Qualitative Results\\\\n30:10 - False Positives\\\\n32:50 - Appendix \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/1912.03538\\\\n\\\\nMy Video On Attention Is All You Need: https://youtu.be/iDulhoQ2pro\\\\n\\\\nAbstract:\\\\nIn static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame.\\\\nWe apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP.\\\\n\\\\nAuthors: Sara Beery, Guanhang Wu, Vivek Rathod, Ronny Votel, Jonathan Huang\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2062",
    "uploadDate": "2020-06-28",
    "thumbnail_url": "https://i.ytimg.com/vi/eI8xTdcZ6VY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Hdo81GtLC_4",
    "title": "Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gnn, transformer, graph, biology, neurons, axon, dendrites, plausible, biologically plausible, backprop, backpropagation, dfa, feedback alignment, random projections",
    "scraped_at": 1684582651.8838212,
    "genre": "Science",
    "views": "16097",
    "desc": "Backpropagation is one of the central components of modern deep learning. However, it\\'s not biologically plausible, which limits the applicability of deep learning to understand how the human brain works. Direct Feedback Alignment is a biologically plausible alternative and this paper shows that, contrary to previous research, it can be successfully applied to modern deep architectures and solve challenging tasks.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - The Problem with Backpropagation\\\\n10:25 - Direct Feedback Alignment\\\\n21:00 - My Intuition why DFA works\\\\n31:20 - Experiments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.12878\\\\nCode: https://github.com/lightonai/dfa-scales-to-modern-deep-learning\\\\nReferenced Paper by Arild N\\xc3\\xb8kland: https://arxiv.org/abs/1609.01596\\\\n\\\\nAbstract:\\\\nDespite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.\\\\n\\\\nAuthors: Julien Launay, Iacopo Poli, Fran\\xc3\\xa7ois Boniface, Florent Krzakala\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2098",
    "uploadDate": "2020-06-27",
    "thumbnail_url": "https://i.ytimg.com/vi/Hdo81GtLC_4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=cuyM63ugsxI",
    "title": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 3: The Math (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, chollet, keras, google, francois, intelligence, iq, iq test, deep neural networks, prior, skill, performance, measurement, measure, test, number, intelligent, smart, learning, generalization, ability, experience, humans, evolution, nature, nurture, psychometrics, range, adaptability, arc, kaggle, difficulty, entropy, core knowledge, objectness, navigation, contact, agent, goal",
    "scraped_at": 1684582651.9738429,
    "genre": "Science",
    "views": "5491",
    "desc": "In this part, we go over the formal definition of the measure of intelligence. In order to do this, we have to frame and quantify the notions of generalization difficulty, priors, and experience in terms of algorithmic complexity.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Recap\\\\n2:50 - Concept Schema\\\\n10:00 - Algorithmic Complexity\\\\n13:00 - Definitions\\\\n15:25 - Generalization Difficulty\\\\n18:55 - Developer Aware Generalization Difficulty\\\\n22:40 - Priors\\\\n25:10 - Experience\\\\n30:50 - The Measure Of Intelligence\\\\n38:00 - An Ideal Intelligence Benchmark\\\\n42:30 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/1911.01547\\\\n\\\\nPart 1: https://youtu.be/3_qGrmD6iQY\\\\nPart 2: https://youtu.be/THcuTJbeD34\\\\n\\\\nAbstract:\\\\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \\\\\"",
    "lengthSeconds": "2604",
    "uploadDate": "2020-06-26",
    "thumbnail_url": "https://i.ytimg.com/vi/cuyM63ugsxI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=LMb5tvW-UoQ",
    "title": "Discovering Symbolic Models from Deep Learning with Inductive Biases (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, graph networks, graph neural networks, gnn, physics, newtonian, hamiltonian, dynamics, cosmology, dark matter, symbolic regression, edge, vertex, regularization",
    "scraped_at": 1684582654.0246062,
    "genre": "Science",
    "views": "40994",
    "desc": "Neural networks are very good at predicting systems\\' numerical outputs, but not very good at deriving the discrete symbolic equations that govern many physical systems. This paper combines Graph Networks with symbolic regression and shows that the strong inductive biases of these models can be used to derive accurate symbolic equations from observation data.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Outline\\\\n1:10 - Problem Statement\\\\n4:25 - Symbolic Regression\\\\n6:40 - Graph Neural Networks\\\\n12:05 - Inductive Biases for Physics\\\\n15:15 - How Graph Networks compute outputs\\\\n23:10 - Loss Backpropagation\\\\n24:30 - Graph Network Recap\\\\n26:10 - Analogies of GN to Newtonian Mechanics\\\\n28:40 - From Graph Network to Equation\\\\n33:50 - L1 Regularization of Edge Messages\\\\n40:10 - Newtonian Dynamics Example\\\\n43:10 - Cosmology Example\\\\n44:45 - Conclusions \\\\u0026 Appendix\\\\n\\\\nPaper: https://arxiv.org/abs/2006.11287\\\\nCode: https://github.com/MilesCranmer/symbolic_deep_learning\\\\n\\\\nAbstract:\\\\nWe develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.\\\\n\\\\nAuthors: Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, Shirley Ho\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2773",
    "uploadDate": "2020-06-25",
    "thumbnail_url": "https://i.ytimg.com/vi/LMb5tvW"
  },
  {
    "link": "watch?v=Uumd2zOOz60",
    "title": "How I Read a Paper: Facebook's DETR (Video Tutorial)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ml, reading, papers, understanding, quickly, quick, fast, ultralearning, research, facebook, detr, object detection, transformers, how to",
    "scraped_at": 1684582652.0638225,
    "genre": "Science",
    "views": "38791",
    "desc": "I retrace my first reading of Facebook AI\\'s DETR paper and explain my process of understanding it.\\\\n\\\\nOUTLINE:\\\\n0:00 - Introduction\\\\n1:25 - Title\\\\n4:10 - Authors\\\\n5:55 - Affiliation\\\\n7:40 - Abstract\\\\n13:50 - Pictures\\\\n20:30 - Introduction\\\\n22:00 - Related Work\\\\n24:00 - Model\\\\n30:00 - Experiments\\\\n41:50 - Conclusions \\\\u0026 Abstract\\\\n42:40 - Final Remarks\\\\n\\\\nOriginal Video about DETR: https://youtu.be/T35ba_VXkMY\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2724",
    "uploadDate": "2020-06-24",
    "thumbnail_url": "https://i.ytimg.com/vi/Uumd2zOOz60/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qSArFEIoSbo",
    "title": "RepNet: Counting Out Time - Class Agnostic Video Repetition Counting in the Wild (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, vision, counting, self",
    "scraped_at": 1684582654.1136045,
    "genre": "Science",
    "views": "9321",
    "desc": "Counting repeated actions in a video is one of the easiest tasks for humans, yet remains incredibly hard for machines. RepNet achieves state-of-the-art by creating an information bottleneck in the form of a temporal self-similarity matrix, relating video frames to each other in a way that forces the model to surface the information relevant for counting. Along with that, the authors produce a new dataset for evaluating counting models.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:30 - Problem Statement\\\\n5:15 - Output \\\\u0026 Loss\\\\n6:25 - Per-Frame Embeddings\\\\n11:20 - Temporal Self-Similarity Matrix\\\\n19:00 - Periodicity Predictor\\\\n25:50 - Architecture Recap\\\\n27:00 - Synthetic Dataset\\\\n30:15 - Countix Dataset\\\\n31:10 - Experiments\\\\n33:35 - Applications\\\\n35:30 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper Website: https://sites.google.com/view/repnet\\\\nColab: https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\\\\n\\\\nAbstract:\\\\nWe present an approach for estimating the period with which an action is repeated in a video. The crux of the approach lies in constraining the period prediction module to use temporal self-similarity as an intermediate representation bottleneck that allows generalization to unseen repetitions in videos in the wild. We train this model, called RepNet, with a synthetic dataset that is generated from a large unlabeled video collection by sampling short clips of varying lengths and repeating them with different periods and counts. This combination of synthetic data and a powerful yet constrained model, allows us to predict periods in a class-agnostic fashion. Our model substantially exceeds the state of the art performance on existing periodicity (PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new challenging dataset called Countix (~90 times larger than existing datasets) which captures the challenges of repetition counting in real-world videos.\\\\n\\\\nAuthors: Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2202",
    "uploadDate": "2020-06-23",
    "thumbnail_url": "https://i.ytimg.com/vi/qSArFEIoSbo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=n1SXlK5rhR8",
    "title": "[Drama] Yann LeCun against Twitter on Dataset Bias",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ylc, yann, lecun, convnet, face, pulse, github, colab, jeff dean, hardmaru, charles sutton, soumith, meredith, timnit, bias, noise, dataset, systems, twitter, mob",
    "scraped_at": 1684582652.146847,
    "genre": "Science",
    "views": "29906",
    "desc": "Yann LeCun points out an instance of dataset bias and proposes a sensible solution. People are not happy about it.\\\\n\\\\nOriginal Tweet: https://twitter.com/ylecun/status/1274782757907030016\\\\n\\\\nERRATA:\\\\n- My specific example of the L1 regularizer wrt to Porsches and Ferraris does not actually work in this particular case. What I mean is a general sparsity-inducing regularizer.\\\\n- When I claim that an L1 regularizer would make the problem worse, this only holds in certain circumstances, for example when the data is Gaussian iid.\\\\n\\\\nThumbnail: https://commons.wikimedia.org/wiki/File:Yann_LeCun_-_2018_(cropped).jpg by J\\xc3\\xa9r\\xc3\\xa9my Barande / Ecole polytechnique Universit\\xc3\\xa9 Paris-Saclay / CC BY-SA 2.0\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "881",
    "uploadDate": "2020-06-22",
    "thumbnail_url": "https://i.ytimg.com/vi/n1SXlK5rhR8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Q5g3p9Zwjrk",
    "title": "SIREN: Implicit Neural Representations with Periodic Activation Functions (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, implicit, nerf, neural processes, optimization, curve fitting, audio, signal processing, surfaces, point clouds, oriented, signed distance function, mlp, layers, hypernetworks, representation, function, sin, sinus, sinusoid, fourier, initialization, relu, nonlinearity, derivative, gradient, laplacian, wave",
    "scraped_at": 1684582654.2016308,
    "genre": "Science",
    "views": "39089",
    "desc": "Implicit neural representations are created when a neural network is used to represent a signal as a function. SIRENs are a particular type of INR that can be applied to a variety of signals, such as images, sound, or 3D shapes. This is an interesting departure from regular machine learning and required me to think differently.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:15 - Implicit Neural Representations\\\\n9:40 - Representing Images\\\\n14:30 - SIRENs\\\\n18:05 - Initialization\\\\n20:15 - Derivatives of SIRENs\\\\n23:05 - Poisson Image Reconstruction\\\\n28:20 - Poisson Image Editing\\\\n31:35 - Shapes with Signed Distance Functions\\\\n45:55 - Paper Website\\\\n48:55 - Other Applications\\\\n50:45 - Hypernetworks over SIRENs\\\\n54:30 - Broader Impact\\\\n\\\\nPaper: https://arxiv.org/abs/2006.09661\\\\nWebsite: https://vsitzmann.github.io/siren/\\\\n\\\\nAbstract:\\\\nImplicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal\\'s spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.\\\\n\\\\nAuthors: Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "3365",
    "uploadDate": "2020-06-21",
    "thumbnail_url": "https://i.ytimg.com/vi/Q5g3p9Zwjrk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2lkUNDZld-4",
    "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, cnn, resnet, simclr, simclr2, simclrv2, simclr v2, v2, hinton, geoff, brain, wide, deep, convolutional, convolutions, self",
    "scraped_at": 1684582652.2348213,
    "genre": "Science",
    "views": "30867",
    "desc": "This paper proposes SimCLRv2 and shows that semi-supervised learning benefits a lot from self-supervised pre-training. And stunningly, that effect gets larger the fewer labels are available and the more parameters the model has.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - Semi-Supervised Learning\\\\n3:50 - Pre-Training via Self-Supervision\\\\n5:45 - Contrastive Loss\\\\n10:50 - Retaining Projection Heads\\\\n13:10 - Supervised Fine-Tuning\\\\n13:45 - Unsupervised Distillation \\\\u0026 Self-Training\\\\n18:45 - Architecture Recap\\\\n22:25 - Experiments\\\\n34:15 - Broader Impact\\\\n\\\\nPaper: https://arxiv.org/abs/2006.10029\\\\nCode: https://github.com/google-research/simclr\\\\n\\\\nAbstract:\\\\nOne paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\\\\\\\\% ImageNet top-1 accuracy with just 1\\\\\\\\% of the labels (\\xe2\\x89\\xa413 labeled images per class) using ResNet-50, a 10\\xc3\\x97 improvement in label efficiency over the previous state-of-the-art. With 10\\\\\\\\% of labels, ResNet-50 trained with our method achieves 77.5\\\\\\\\% top-1 accuracy, outperforming standard supervised training with all of the labels.\\\\n\\\\nAuthors: Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2251",
    "uploadDate": "2020-06-20",
    "thumbnail_url": "https://i.ytimg.com/vi/2lkUNDZld"
  },
  {
    "link": "watch?v=THcuTJbeD34",
    "title": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 2: Human Priors (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, chollet, keras, google, francois, intelligence, iq, iq test, deep neural networks, prior, skill, performance, measurement, measure, test, number, intelligent, smart, learning, generalization, ability, experience, humans, evolution, nature, nurture, psychometrics, range, adaptability, arc, kaggle, difficulty, entropy, core knowledge, objectness, navigation, contact, agent, goal",
    "scraped_at": 1684582654.2896063,
    "genre": "Science",
    "views": "7281",
    "desc": "In this part, we go much more in-depth into the relationship between intelligence, generality, skill, experience, and prior knowledge and take a close look at what priors are built into humans. This will form the basis for comparing the intelligence of humans and AI systems.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Recap\\\\n3:00 - Optimize for Generality\\\\n5:45 - Buying Skill with Data and Priors\\\\n12:40 - The Human Scope\\\\n17:30 - Human Priors\\\\n24:05 - Core Knowledge\\\\n28:50 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/1911.01547\\\\nTim Scarfe\\'s Video: https://youtu.be/GpWLZUbPhr0\\\\n\\\\nAbstract:\\\\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \\\\\"",
    "lengthSeconds": "1850",
    "uploadDate": "2020-06-19",
    "thumbnail_url": "https://i.ytimg.com/vi/THcuTJbeD34/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YBlNQK0Ao6g",
    "title": "Image GPT: Generative Pretraining from Pixels (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openai, gpt2, gpt3, bert, transformer, attention is all you need, attention mechanism, multi",
    "scraped_at": 1684582652.321823,
    "genre": "Science",
    "views": "28686",
    "desc": "BERT and GPT-2/3 have shown the enormous power of using generative models as pre-training for classification tasks. However, for images, pre-training is usually done with supervised or self-supervised objectives. This paper investigates how far you can get when applying the principles from the world of NLP to the world of images.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n2:50 - Generative Models for Pretraining\\\\n4:50 - Pretraining for Visual Tasks\\\\n7:40 - Model Architecture\\\\n15:15 - Linear Probe Experiments\\\\n24:15 - Fine-Tuning Experiments\\\\n30:25 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper:\\\\nhttps://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf\\\\nBlog: https://openai.com/blog/image-gpt/\\\\nCode: https://github.com/openai/image-gpt\\\\n\\\\nAbstract:\\\\nInspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.\\\\n\\\\nAuthors: Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1907",
    "uploadDate": "2020-06-18",
    "thumbnail_url": "https://i.ytimg.com/vi/YBlNQK0Ao6g/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YPfUiOMYOEE",
    "title": "BYOL: Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, deepmind, ucl, representation, moco, momentum contrast, simclr, encoder, augmentation, mixup, randaugment, crop, random crop, jitter, flip, unsupervised, self",
    "scraped_at": 1684582654.3746054,
    "genre": "Science",
    "views": "49995",
    "desc": "Self-supervised representation learning relies on negative samples to keep the encoder from collapsing to trivial solutions. However, this paper shows that negative samples, which are a nuisance to implement, are not necessary for learning good representation, and their algorithm BYOL is able to outperform other baselines using just positive samples.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:10 - Image Representation Learning\\\\n3:55 - Self-Supervised Learning\\\\n5:35 - Negative Samples\\\\n10:50 - BYOL\\\\n23:20 - Experiments\\\\n30:10 - Conclusion \\\\u0026 Broader Impact\\\\n\\\\nPaper: https://arxiv.org/abs/2006.07733\\\\n\\\\nAbstract:\\\\nWe introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.\\\\n\\\\nAuthors: Jean-Bastien Grill, Florian Strub, Florent Altch\\xc3\\xa9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\\xc3\\xa9mi Munos, Michal Valko\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2025",
    "uploadDate": "2020-06-17",
    "thumbnail_url": "https://i.ytimg.com/vi/YPfUiOMYOEE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=sEG8hD64c_Q",
    "title": "TUNIT: Rethinking the Truly Unsupervised Image-to-Image Translation (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, image translation, style transfer, unsupervised, clustering, self",
    "scraped_at": 1684582652.410822,
    "genre": "Science",
    "views": "11216",
    "desc": "Image-to-Image translation usually requires corresponding samples or at least domain labels of the dataset. This paper removes that restriction and allows for fully unsupervised image translation of a source image to the style of one or many reference images. This is achieved by jointly training a guiding network that provides style information and pseudo-labels.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:20 - Unsupervised Image-to-Image Translation\\\\n7:05 - Architecture Overview\\\\n14:15 - Pseudo-Label Loss\\\\n19:30 - Encoder Style Contrastive Loss\\\\n25:30 - Adversarial Loss\\\\n31:20 - Generator Style Contrastive Loss\\\\n35:15 - Image Reconstruction Loss\\\\n36:55 - Architecture Recap\\\\n39:55 - Full Loss\\\\n42:05 - Experiments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.06500\\\\nCode: https://github.com/clovaai/tunit\\\\n\\\\nAbstract:\\\\nEvery recent image-to-image translation model uses either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision at minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose the truly unsupervised image-to-image translation method (TUNIT) that simultaneously learns to separate image domains via an information-theoretic approach and generate corresponding images using the estimated domain labels. Experimental results on various datasets show that the proposed method successfully separates domains and translates images across those domains. In addition, our model outperforms existing set-level supervised methods under a semi-supervised setting, where a subset of domain labels is provided. The source code is available at this https URL\\\\n\\\\nAuthors: Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, Hyunjung Shim\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2981",
    "uploadDate": "2020-06-16",
    "thumbnail_url": "https://i.ytimg.com/vi/sEG8hD64c_Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DLq1DUcMh1Q",
    "title": "A bio-inspired bistable recurrent cell allows for long-lasting memory (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, gru, lstm, schmidhuber, bistable, bistability, neurons, biological, spiking, tanh, stable, attractor, fixed points, memory, memorize, sparse, long sequence, history, storage, remember, rnn, recurrent neural network, gated recurrent unit, forget, backpropagation, biologically inspired",
    "scraped_at": 1684582654.4606037,
    "genre": "Science",
    "views": "7945",
    "desc": "Even though LSTMs and GRUs solve the vanishing and exploding gradient problems, they have trouble learning to remember things over very long time spans. Inspired from bistability, a property of biological neurons, this paper constructs a recurrent cell with an inherent memory property, with only minimal modification to existing architectures.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:10 - Recurrent Neural Networks\\\\n6:00 - Gated Recurrent Unit\\\\n14:40 - Neuronal Bistability\\\\n22:50 - Bistable Recurrent Cell\\\\n31:00 - Neuromodulation\\\\n32:50 - Copy First Benchmark\\\\n37:35 - Denoising Benchmark\\\\n48:00 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.05252\\\\nCode: https://github.com/nvecoven/BRC\\\\n\\\\nAbstract:\\\\nRecurrent neural networks (RNNs) provide state-of-the-art performances in a wide variety of tasks that require memory. These performances can often be achieved thanks to gated recurrent cells such as gated recurrent units (GRU) and long short-term memory (LSTM). Standard gated cells share a layer internal state to store information at the network level, and long term memory is shaped by network-wide recurrent connection weights. Biological neurons on the other hand are capable of holding information at the cellular level for an arbitrary long amount of time through a process called bistability. Through bistability, cells can stabilize to different stable states depending on their own past state and inputs, which permits the durable storing of past information in neuron state. In this work, we take inspiration from biological neuron bistability to embed RNNs with long-lasting memory at the cellular level. This leads to the introduction of a new bistable biologically-inspired recurrent cell that is shown to strongly improves RNN performance on time-series which require very long memory, despite using only cellular connections (all recurrent connections are from neurons to themselves, i.e. a neuron state is not influenced by the state of other neurons). Furthermore, equipping this cell with recurrent neuromodulation permits to link them to standard GRU cells, taking a step towards the biological plausibility of GRU.\\\\n\\\\nAuthors: Nicolas Vecoven, Damien Ernst, Guillaume Drion\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2952",
    "uploadDate": "2020-06-15",
    "thumbnail_url": "https://i.ytimg.com/vi/DLq1DUcMh1Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8l-TDqpoUQs",
    "title": "SynFlow: Pruning neural networks without any data by iteratively conserving synaptic flow",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, initialization, lottery ticket hypothesis, pruning, training, magnitude, snip, grasp, init, xavier, glorot, he, flow, layer collapse, iterative, recompute, stepwise, memory, fast, prune, weights, feedforward, layer, neural network",
    "scraped_at": 1684582652.48385,
    "genre": "Science",
    "views": "16214",
    "desc": "The Lottery Ticket Hypothesis has shown that it\\'s theoretically possible to prune a neural network at the beginning of training and still achieve good performance, if we only knew which weights to prune away. This paper does not only explain where other attempts at pruning fail, but provides an algorithm that provably reaches maximum compression capacity, all without looking at any data!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:00 - Pruning Neural Networks\\\\n3:40 - Lottery Ticket Hypothesis\\\\n6:00 - Paper Story Overview\\\\n9:45 - Layer Collapse\\\\n18:15 - Synaptic Saliency Conservation\\\\n23:25 - Connecting Layer Collapse \\\\u0026 Saliency Conservation\\\\n28:30 - Iterative Pruning avoids Layer Collapse\\\\n33:20 - The SynFlow Algorithm\\\\n40:45 - Experiments\\\\n43:35 - Conclusion \\\\u0026 Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.05467\\\\nCode: https://github.com/ganguli-lab/Synaptic-Flow\\\\nMy Video on the Lottery Ticket Hypothesis: https://youtu.be/ZVVnvZdUMUk\\\\nStreet Talk about LTH: https://youtu.be/SfjJoevBbjU\\\\n\\\\nAbstract:\\\\nPruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identified, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design. We first mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.9 percent). Thus our data-agnostic pruning algorithm challenges the existing paradigm that data must be used to quantify which synapses are important.\\\\n\\\\nAuthors: Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2692",
    "uploadDate": "2020-06-14",
    "thumbnail_url": "https://i.ytimg.com/vi/8l"
  },
  {
    "link": "watch?v=l12GXD0t_RE",
    "title": "Deep Differential System Stability - Learning advanced computations from examples (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, math, derivative, ode, pde, solution, integral, gradient, jacobian, mathematics, language model, transformer, symbolic, numeric, stability, equilibrium, attention, tokens, dataset, abstract",
    "scraped_at": 1684582652.5668533,
    "genre": "Science",
    "views": "6757",
    "desc": "Determining the stability properties of differential systems is a challenging task that involves very advanced symbolic and numeric mathematical manipulations. This paper shows that given enough training data, a simple language model with no underlying knowledge of mathematics can learn to solve these problems with remarkably high accuracy.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n3:15 - Differential System Tasks\\\\n11:30 - Datasets \\\\u0026 Models\\\\n15:15 - Experiments\\\\n21:00 - Discussion \\\\u0026 My Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2006.06462\\\\nMy Video on Deep Learning for Symbolic Mathematics: https://youtu.be/p3sAF3gVMMA\\\\n\\\\nAbstract:\\\\nCan advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.\\\\n\\\\nAuthors: Fran\\xc3\\xa7ois Charton, Amaury Hayat, Guillaume Lample\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2209",
    "uploadDate": "2020-06-13",
    "thumbnail_url": "https://i.ytimg.com/vi/l12GXD0t_RE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZfDZRX3WiJg",
    "title": "VirTex: Learning Visual Representations from Textual Annotations (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, cnn, visual, resnet, caption, nlp, transformer, vasvani, attention, text, coco, imagenet, convolutional neural network, adaptation, transfer learning, quality, unsupervised, self",
    "scraped_at": 1684582654.5306356,
    "genre": "Science",
    "views": "5918",
    "desc": "Pre-training a CNN backbone for visual transfer learning has recently seen a big push into the direction of incorporating more data, at the cost of less supervision. This paper investigates the opposite: Visual transfer learning by pre-training from very few, but very high-quality samples on an image captioning task.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:00 - Pre-Training for Visual Tasks\\\\n3:40 - Quality-Quantity Tradeoff\\\\n5:50 - Image Captioning\\\\n8:35 - VirTex Method\\\\n14:30 - Linear Classification\\\\n20:30 - Ablations\\\\n22:05 - Fine-Tuning\\\\n25:45 - Attention Visualization\\\\n27:30 - Conclusion \\\\u0026 Remarks\\\\n\\\\nPaper: https://arxiv.org/abs/2006.06666\\\\nCode: https://github.com/kdexd/virtex\\\\n\\\\nAbstract:\\\\nThe de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.\\\\n\\\\nAuthors: Karan Desai, Justin Johnson\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1782",
    "uploadDate": "2020-06-12",
    "thumbnail_url": "https://i.ytimg.com/vi/ZfDZRX3WiJg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-_2AF9Lhweo",
    "title": "Linformer: Self-Attention with Linear Complexity (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, facebook, linear, quadratic, transformer, attention, self",
    "scraped_at": 1684582652.657853,
    "genre": "Science",
    "views": "27613",
    "desc": "Transformers are notoriously resource-intensive because their self-attention mechanism requires a squared number of memory and computations in the length of the input sequence. The Linformer Model gets around that by using the fact that often, the actual information in the attention matrix is of lower rank and can be approximated.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:40 - The Complexity of Self-Attention\\\\n4:50 - Embedding Dimension \\\\u0026 Multiple Heads\\\\n8:45 - Formal Attention\\\\n10:30 - Empirical Investigation into RoBERTa\\\\n20:00 - Theorem: Self-Attention is Low Rank\\\\n28:10 - Linear Self-Attention Method\\\\n36:15 - Theorem: Linear Self-Attention\\\\n44:10 - Language Modeling\\\\n46:40 - NLP Benchmarks\\\\n47:50 - Compute Time \\\\u0026 Memory Gains\\\\n48:20 - Broader Impact Statement\\\\n49:55 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2006.04768\\\\n\\\\nAbstract:\\\\nLarge transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the \\\\\\\\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\\\\n\\\\nAuthors: Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "3023",
    "uploadDate": "2020-06-11",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=WTB2p4bqtXU",
    "title": "End-to-End Adversarial Text-to-Speech (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, tts, text",
    "scraped_at": 1684582652.7438211,
    "genre": "Science",
    "views": "12189",
    "desc": "Text-to-speech engines are usually multi-stage pipelines that transform the signal into many intermediate representations and require supervision at each step. When trying to train TTS end-to-end, the alignment problem arises: Which text corresponds to which piece of sound? This paper uses an alignment module to tackle this problem and produces astonishingly good sound.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:55 - Problems with Text-to-Speech\\\\n3:55 - Adversarial Training\\\\n5:20 - End-to-End Training\\\\n7:20 - Discriminator Architecture\\\\n10:40 - Generator Architecture\\\\n12:20 - The Alignment Problem\\\\n14:40 - Aligner Architecture\\\\n24:00 - Spectrogram Prediction Loss\\\\n32:30 - Dynamic Time Warping\\\\n38:30 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2006.03575\\\\nWebsite: https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech\\\\n\\\\nAbstract:\\\\nModern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable monotonic interpolation scheme to predict the duration of each input token. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.\\\\n\\\\nAuthors: Jeff Donahue, Sander Dieleman, Miko\\xc5\\x82aj Bi\\xc5\\x84kowski, Erich Elsen, Karen Simonyan\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2449",
    "uploadDate": "2020-06-10",
    "thumbnail_url": "https://i.ytimg.com/vi/WTB2p4bqtXU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=xTzFJIknh7E",
    "title": "TransCoder: Unsupervised Translation of Programming Languages (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582654.621606,
    "genre": "Science",
    "views": "144842",
    "desc": "Code migration between languages is an expensive and laborious task. To translate from one language to the other, one needs to be an expert at both. Current automatic tools often produce illegible and complicated code. This paper applies unsupervised neural machine translation to source code of Python, C++, and Java and is able to translate between them, without ever being trained in a supervised fashion.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:15 - The Transcompiling Problem\\\\n5:55 - Neural Machine Translation\\\\n8:45 - Unsupervised NMT\\\\n12:55 - Shared Embeddings via Token Overlap\\\\n20:45 - MLM Objective\\\\n25:30 - Denoising Objective\\\\n30:10 - Back-Translation Objective\\\\n33:00 - Evaluation Dataset\\\\n37:25 - Results\\\\n41:45 - Tokenization\\\\n42:40 - Shared Embeddings\\\\n43:30 - Human-Aware Translation\\\\n47:25 - Failure Cases\\\\n48:05 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2006.03511\\\\n\\\\nAbstract:\\\\nA transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.\\\\n\\\\nAuthors: Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2917",
    "uploadDate": "2020-06-09",
    "thumbnail_url": "https://i.ytimg.com/vi/xTzFJIknh7E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=cvkeWwDQr0A",
    "title": "JOIN ME for the NeurIPS 2020 Flatland Multi-Agent RL Challenge!",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582652.8296058,
    "genre": "Science",
    "views": "9152",
    "desc": "Join me to solve the NeurIPS 2020 challenge on multi-agent reinforcement learning in the flatland environment. This challenge has participants optimize a complex train scheduling system, subject to accidents, delays and re-routing. The plan is to solve this as a community with no expectations of winning and fully in the open.\\\\n\\\\nDiscord: https://discord.gg/4H8xxDF \\\\nCommunity GitHub Repo: https://github.com/yk/youtube-flatland\\\\nNeurips 2020 Flatland Challenge: https://www.aicrowd.com/challenges/neurips-2020-flatland-challenge\\\\nFlatland Environment: https://gitlab.aicrowd.com/flatland/flatland\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:00 - The Flatland Environment\\\\n2:00 - The NeurIPS 2020 Flatland Challenge\\\\n3:20 - Let\\'s do this as a Community\\\\n4:10 - Ground Rules\\\\n6:15 - Conclusion\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nDiscord: https://discord.gg/4H8xxDF\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "407",
    "uploadDate": "2020-06-08",
    "thumbnail_url": "https://i.ytimg.com/vi/cvkeWwDQr0A/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rl4nUngiR2k",
    "title": "BLEURT: Learning Robust Metrics for Text Generation (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nlp, natural language processing, mt, machine translation, transformer, bert, lstm, attention, wmt, wikipedia, backtranslation, bleu, rouge, ngrams, score, metric, comparison, human raters, google, google research, automatic, overlap, distribution shift",
    "scraped_at": 1684582654.7086344,
    "genre": "Science",
    "views": "6376",
    "desc": "Proper evaluation of text generation models, such as machine translation systems, requires expensive and slow human assessment. As these models have gotten better in previous years, proxy-scores, like BLEU, are becoming less and less useful. This paper proposes to learn a proxy score and demonstrates that it correlates well with human raters, even as the data distribution shifts.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 High-Level Overview\\\\n1:00 - The Problem with Evaluating Machine Translation\\\\n5:10 - Task Evaluation as a Learning Problem\\\\n10:45 - Naive Fine-Tuning BERT\\\\n13:25 - Pre-Training on Synthetic Data\\\\n16:50 - Generating the Synthetic Data\\\\n18:30 - Priming via Auxiliary Tasks\\\\n23:35 - Experiments \\\\u0026 Distribution Shifts\\\\n27:00 - Concerns \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2004.04696\\\\nCode: https://github.com/google-research/bleurt\\\\n\\\\nAbstract:\\\\nText generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.\\\\n\\\\nAbstract: Thibault Sellam, Dipanjan Das, Ankur P. Parikh\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1894",
    "uploadDate": "2020-06-07",
    "thumbnail_url": "https://i.ytimg.com/vi/rl4nUngiR2k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4GKCxJQSw-g",
    "title": "Synthetic Petri Dish: A Novel Surrogate Model for Rapid Architecture Search (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nas, nao, uber, openai, architecture search, neural architecture search, inner loop, inner optimization, small, abstract, turing, performance, evolutionary algorithm, outer loop, mlp, sigmoid, ptb, rnn, cell, meta",
    "scraped_at": 1684582652.919605,
    "genre": "Science",
    "views": "4743",
    "desc": "Neural Architecture Search is usually prohibitively expensive in both time and resources to be useful. A search strategy has to keep evaluating new models, training them to convergence in an inner loop to find out if they are any good. This paper proposes to abstract the problem and extract the essential part of the architecture to be optimized into a smaller version and evaluates that version on specifically custom learned data points to predict its performance, which is much faster and cheaper than running the full model.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 High-Level Overview\\\\n1:00 - Neural Architecture Search\\\\n4:30 - Predicting performance via architecture encoding\\\\n7:50 - Synthetic Petri Dish\\\\n12:50 - Motivating MNIST example\\\\n18:15 - Entire Algorithm\\\\n23:00 - Producing the synthetic data\\\\n26:00 - Combination with architecture search\\\\n27:30 - PTB RNN-Cell Experiment\\\\n29:20 - Comments \\\\u0026 Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2005.13092\\\\nCode: https://github.com/uber-research/Synthetic-Petri-Dish\\\\n\\\\nAbstract:\\\\nNeural Architecture Search (NAS) explores a large space of architectural motifs -- a compute-intensive process that often involves ground-truth evaluation of each motif by instantiating it within a large network, and training and evaluating the network with thousands of domain-specific data samples. Inspired by how biological motifs such as cells are sometimes extracted from their natural environment and studied in an artificial Petri dish setting, this paper proposes the Synthetic Petri Dish model for evaluating architectural motifs. In the Synthetic Petri Dish, architectural motifs are instantiated in very small networks and evaluated using very few learned synthetic data samples (to effectively approximate performance in the full problem). The relative performance of motifs in the Synthetic Petri Dish can substitute for their ground-truth performance, thus accelerating the most expensive step of NAS. Unlike other neural network-based prediction models that parse the structure of the motif to estimate its performance, the Synthetic Petri Dish predicts motif performance by training the actual motif in an artificial setting, thus deriving predictions from its true intrinsic properties. Experiments in this paper demonstrate that the Synthetic Petri Dish can therefore predict the performance of new motifs with significantly higher accuracy, especially when insufficient ground truth data is available. Our hope is that this work can inspire a new research direction in studying the performance of extracted components of models in an alternative controlled setting.\\\\n\\\\nAuthors: Aditya Rawal, Joel Lehman, Felipe Petroski Such, Jeff Clune, Kenneth O. Stanley\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2009",
    "uploadDate": "2020-06-06",
    "thumbnail_url": "https://i.ytimg.com/vi/4GKCxJQSw"
  },
  {
    "link": "watch?v=CA8JPbJ75tY",
    "title": "CornerNet: Detecting Objects as Paired Keypoints (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, corner, top left, bottom right, corners, cv, computer vision, vision, object detection, detr, bounding box, center, anchor, pooling, local, cnn, convolutions, convolutional neural network, hourglass, skip connection, heatmap, embedding, push, pull, loss, overlap, filters, channels",
    "scraped_at": 1684582653.0046046,
    "genre": "Science",
    "views": "11357",
    "desc": "Many object detectors focus on locating the center of the object they want to find. However, this leaves them with the secondary problem of determining the specifications of the bounding box, leading to undesirable solutions like anchor boxes. This paper directly detects the top left and the bottom right corners of objects independently, along with descriptors that allows to match the two later and form a complete bounding box. For this, a new pooling method, called corner pooling, is introduced.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 High-Level Overview\\\\n1:40 - Object Detection\\\\n2:40 - Pipeline I - Hourglass\\\\n4:00 - Heatmap \\\\u0026 Embedding Outputs\\\\n8:40 - Heatmap Loss\\\\n10:55 - Embedding Loss\\\\n14:35 - Corner Pooling\\\\n20:40 - Experiments\\\\n\\\\nPaper: https://arxiv.org/abs/1808.01244\\\\nCode: https://github.com/princeton-vl/CornerNet\\\\n\\\\nAbstract:\\\\nWe propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.\\\\n\\\\nAuthors: Hei Law, Jia Deng\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1521",
    "uploadDate": "2020-06-05",
    "thumbnail_url": "https://i.ytimg.com/vi/CA8JPbJ75tY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=nxEr4VNgYOE",
    "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, prune, pruning, transfer learning, weights, magnitude, gradient, moving, small, importance, huggingface, nlp, natural language processing, squad, mnli, bert, transformer, attention, cnn, distillation, teacher, sparse, sparsity, question answering, mobile, edge, tune, fine",
    "scraped_at": 1684582654.7966044,
    "genre": "Science",
    "views": "4516",
    "desc": "Deep neural networks are large models and pruning has become an important part of ML product pipelines, making models small while keeping their performance high. However, the classic pruning method, Magnitude Pruning, is suboptimal in models that are obtained by transfer learning. This paper proposes a solution, called Movement Pruning and shows its superior performance.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 High-Level Overview\\\\n0:55 - Magnitude Pruning\\\\n4:25 - Transfer Learning\\\\n7:25 - The Problem with Magnitude Pruning in Transfer Learning\\\\n9:20 - Movement Pruning\\\\n22:20 - Experiments\\\\n24:20 - Improvements via Distillation\\\\n26:40 - Analysis of the Learned Weights\\\\n\\\\nPaper: https://arxiv.org/abs/2005.07683\\\\nCode: https://github.com/huggingface/transformers/tree/master/examples/movement-pruning\\\\n\\\\nAbstract:\\\\nMagnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.\\\\n\\\\nAuthors: Victor Sanh, Thomas Wolf, Alexander M. Rush\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1810",
    "uploadDate": "2020-06-04",
    "thumbnail_url": "https://i.ytimg.com/vi/nxEr4VNgYOE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hQEnzdLkPj4",
    "title": "Learning To Classify Images Without Labels (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, ethz, clustering, self",
    "scraped_at": 1684582653.0926049,
    "genre": "Science",
    "views": "43470",
    "desc": "How do you learn labels without labels? How do you classify images when you don\\'t know what to classify them into? This paper investigates a new combination of representation learning, clustering, and self-labeling in order to group visually similar images together - and achieves surprisingly high accuracy on benchmark datasets.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 High-level Overview\\\\n2:15 - Problem Statement\\\\n4:50 - Why naive Clustering does not work\\\\n9:25 - Representation Learning\\\\n13:40 - Nearest-neighbor-based Clustering\\\\n28:00 - Self-Labeling\\\\n32:10 - Experiments\\\\n38:20 - ImageNet Experiments\\\\n41:00 - Overclustering\\\\n\\\\nPaper: https://arxiv.org/abs/2005.12320\\\\nCode: https://github.com/wvangansbeke/Unsupervised-Classification\\\\n\\\\nAbstract:\\\\nIs it possible to automatically classify images without the use of ground-truth annotations? Or when even the classes themselves, are not a priori known? These remain important, and open questions in computer vision. Several approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by huge margins, in particular +26.9% on CIFAR10, +21.5% on CIFAR100-20 and +11.7% on STL10 in terms of classification accuracy. Furthermore, results on ImageNet show that our approach is the first to scale well up to 200 randomly selected classes, obtaining 69.3% top-1 and 85.5% top-5 accuracy, and marking a difference of less than 7.5% with fully-supervised methods. Finally, we applied our approach to all 1000 classes on ImageNet, and found the results to be very encouraging. The code will be made publicly available.\\\\n\\\\nAuthors: Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2734",
    "uploadDate": "2020-06-03",
    "thumbnail_url": "https://i.ytimg.com/vi/hQEnzdLkPj4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3_qGrmD6iQY",
    "title": "On the Measure of Intelligence by Fran\u00e7ois Chollet - Part 1: Foundations (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, chollet, keras, google, francois, intelligence, iq, iq test, deep neural networks, prior, skill, performance, measurement, measure, test, number, intelligent, smart, learning, generalization, ability, experience, humans, evolution, nature, nurture, psychometrics, range, adaptability, arc, kaggle, difficulty, entropy, core knowledge, objectness, navigation, contact, agent, goal",
    "scraped_at": 1684582653.1776307,
    "genre": "Science",
    "views": "14010",
    "desc": "How does one measure the Intelligence of an AI? Is AlphaGo intelligent? How about GPT-3? In this landmark paper, Chollet proposes a solid measure of intelligence for AI that revolves around generalization, rather than skill.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:15 - The need for a measure of intelligence\\\\n3:35 - Intelligence as generalization ability\\\\n5:45 - Nature vs nurture\\\\n11:45 - Skill-based evaluation\\\\n18:30 - Generalization based evaluation\\\\n30:25 - Inspiration from psychometrics\\\\n36:30 - Conclusion\\\\n\\\\nhttps://arxiv.org/abs/1911.01547\\\\n\\\\nAbstract:\\\\nTo make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \\\\\"",
    "lengthSeconds": "2219",
    "uploadDate": "2020-06-02",
    "thumbnail_url": "https://i.ytimg.com/vi/3_qGrmD6iQY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=HYEzHX6-fIA",
    "title": "Dynamics-Aware Unsupervised Discovery of Skills (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, deep rl, control, planning, world model, dads, skills, latent, high level, unsupervised, tree search, deep reinforcement learning, mujoco, ant, google",
    "scraped_at": 1684582653.261636,
    "genre": "Science",
    "views": "7129",
    "desc": "This RL framework can discover low-level skills all by itself without any reward. Even better, at test time it can compose its learned skills and reach a specified goal without any additional learning! Warning: Math-heavy!\\\\n\\\\nOUTLINE:\\\\n0:00 - Motivation\\\\n2:15 - High-Level Overview\\\\n3:20 - Model-Based vs Model-Free Reinforcement Learning\\\\n9:00 - Skills\\\\n12:10 - Mutual Information Objective\\\\n18:40 - Decomposition of the Objective\\\\n27:10 - Unsupervised Skill Discovery Algorithm\\\\n42:20 - Planning in Skill Space\\\\n48:10 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/1907.01657\\\\nWebsite: https://sites.google.com/view/dads-skill\\\\nCode: https://github.com/google-research/dads\\\\n\\\\nAbstract:\\\\nConventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.\\\\n\\\\nAuthors: Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "3002",
    "uploadDate": "2020-06-01",
    "thumbnail_url": "https://i.ytimg.com/vi/HYEzHX6"
  },
  {
    "link": "watch?v=q7QP_lfqnQM",
    "title": "Synthesizer: Rethinking Self-Attention in Transformer Models (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nlp, natural language processing, machine translation, google, attention mechanism, attention, transformer, seq2seq, bert, memory, lsh, locality sensitive hashing, reversible, revertible, flow, long sequence",
    "scraped_at": 1684582657.3895254,
    "genre": "Science",
    "views": "15912",
    "desc": "Do we really need dot-product attention? The attention mechanism is a central part of modern Transformers, mainly due to the dot-product attention mechanism. This paper changes the mechanism to remove the quadratic interaction terms and comes up with a new model, the Synthesizer. As it turns out, you can do pretty well like that!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 High Level Overview\\\\n1:00 - Abstract\\\\n2:30 - Attention Mechanism as Information Routing\\\\n5:45 - Dot Product Attention\\\\n8:05 - Dense Synthetic Attention\\\\n15:00 - Random Synthetic Attention\\\\n17:15 - Comparison to Feed-Forward Layers\\\\n22:00 - Factorization \\\\u0026 Mixtures\\\\n23:10 - Number of Parameters\\\\n25:35 - Machine Translation \\\\u0026 Language Modeling Experiments\\\\n36:15 - Summarization \\\\u0026 Dialogue Generation Experiments\\\\n37:15 - GLUE \\\\u0026 SuperGLUE Experiments\\\\n42:00 - Weight Sizes \\\\u0026 Number of Head Ablations\\\\n47:05 - Conclusion\\\\n\\\\nPaper: https://arxiv.org/abs/2005.00743\\\\nMy Video on Transformers (Attention Is All You Need): https://youtu.be/iDulhoQ2pro\\\\nMy Video on BERT: https://youtu.be/-9evrZnBorM\\\\n\\\\nAbstract:\\\\nThe dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is not that important after all. To this end, we propose \\\\\\\\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. Our experimental results show that \\\\\\\\textsc{Synthesizer} is competitive against vanilla Transformer models across a range of tasks, including MT (EnDe, EnFr), language modeling (LM1B), abstractive summarization (CNN/Dailymail), dialogue generation (PersonaChat) and Multi-task language understanding (GLUE, SuperGLUE).\\\\n\\\\nAuthors: Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2901",
    "uploadDate": "2020-05-31",
    "thumbnail_url": "https://i.ytimg.com/vi/q7QP_lfqnQM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=LfUsGv-ESbc",
    "title": "[Code] How to use Facebook's DETR object detection algorithm in Python (Full Tutorial)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, facebook, fair, fb, facebook ai, object detection, coco, bounding boxes, hungarian, matching, bipartite, cnn, transformer, attention, encoder, decoder, images, vision, pixels, segmentation, classes, stuff, things, attention mechanism, squared, unrolled, overlap, threshold, rcnn, code, pytorch, colab, notebook, ipython, python, torch, hub, torchvision, bounding box, image, computer vision",
    "scraped_at": 1684582654.886631,
    "genre": "Science",
    "views": "41461",
    "desc": "Watch my as I struggle my way up the glorious path of using the DETR object detection model in PyTorch.\\\\n\\\\nOriginal Video on DETR: https://youtu.be/T35ba_VXkMY\\\\n\\\\nTheir GitHub repo: https://github.com/facebookresearch/detr\\\\nMy Colab: https://colab.research.google.com/drive/1Exoc3-A141_h8GKk-B6cJxoidJsgOZOZ?usp=sharing\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:45 - TorchHub Model\\\\n2:00 - Getting an Image\\\\n6:00 - Image to PyTorch Tensor\\\\n7:50 - Handling Model Output\\\\n15:00 - Draw Bounding Boxes\\\\n20:10 - The Dress\\\\n22:00 - Rorschach Ink Blots\\\\n23:00 - Forcing More Predictions\\\\n28:30 - Jackson Pollock Images\\\\n32:00 - Elephant Herds\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2010",
    "uploadDate": "2020-05-30",
    "thumbnail_url": "https://i.ytimg.com/vi/LfUsGv"
  },
  {
    "link": "watch?v=SY5PvZrJhLE",
    "title": "GPT-3: Language Models are Few-Shot Learners (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, transformers, attention, nlp, natural language processing, gpt3, gpt",
    "scraped_at": 1684582654.9956045,
    "genre": "Science",
    "views": "202806",
    "desc": "#gpt3 #openai #gpt-3\\\\n\\\\nHow far can you go with ONLY language modeling? Can a large enough language model perform NLP task out of the box? OpenAI take on these and other questions by training a transformer that is an order of magnitude larger than anything that has ever been built before and the results are astounding.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Overview\\\\n1:20 - Language Models\\\\n2:45 - Language Modeling Datasets\\\\n3:20 - Model Size\\\\n5:35 - Transformer Models\\\\n7:25 - Fine Tuning\\\\n10:15 - In-Context Learning\\\\n17:15 - Start of Experimental Results\\\\n19:10 - Question Answering\\\\n23:10 - What I think is happening\\\\n28:50 - Translation\\\\n31:30 - Winograd Schemes\\\\n33:00 - Commonsense Reasoning\\\\n37:00 - Reading Comprehension\\\\n37:30 - SuperGLUE\\\\n40:40 - NLI\\\\n41:40 - Arithmetic Expressions\\\\n48:30 - Word Unscrambling\\\\n50:30 - SAT Analogies\\\\n52:10 - News Article Generation\\\\n58:10 - Made-up Words\\\\n1:01:10 - Training Set Contamination\\\\n1:03:10 - Task Examples\\\\n\\\\nhttps://arxiv.org/abs/2005.14165\\\\nhttps://github.com/openai/gpt-3\\\\n\\\\nAbstract:\\\\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\\'s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\\\\n\\\\nAuthors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "3870",
    "uploadDate": "2020-05-29",
    "thumbnail_url": "https://i.ytimg.com/vi/SY5PvZrJhLE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=T35ba_VXkMY",
    "title": "DETR: End-to-End Object Detection with Transformers (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, facebook, fair, fb, facebook ai, object detection, coco, bounding boxes, hungarian, matching, bipartite, cnn, transformer, attention, encoder, decoder, images, vision, pixels, segmentation, classes, stuff, things, attention mechanism, squared, unrolled, overlap, threshold, rcnn",
    "scraped_at": 1684582653.3526313,
    "genre": "Science",
    "views": "123894",
    "desc": "Object detection in images is a notoriously hard task! Objects can be of a wide variety of classes, can be numerous or absent, they can occlude each other or be out of frame. All of this makes it even more surprising that the architecture in this paper is so simple. Thanks to a clever loss function, a single Transformer stacked on a CNN is enough to handle the entire task!\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 High-Level Overview\\\\n0:50 - Problem Formulation\\\\n2:30 - Architecture Overview\\\\n6:20 - Bipartite Match Loss Function\\\\n15:55 - Architecture in Detail\\\\n25:00 - Object Queries\\\\n31:00 - Transformer Properties\\\\n35:40 - Results\\\\n\\\\nERRATA:\\\\nWhen I introduce bounding boxes, I say they consist of x and y, but you also need the width and height.\\\\n\\\\nMy Video on Transformers: https://youtu.be/iDulhoQ2pro\\\\n\\\\nPaper: https://arxiv.org/abs/2005.12872\\\\nBlog: https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/\\\\nCode: https://github.com/facebookresearch/detr\\\\n\\\\nAbstract:\\\\nWe present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.\\\\n\\\\nAuthors: Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2456",
    "uploadDate": "2020-05-28",
    "thumbnail_url": "https://i.ytimg.com/vi/T35ba_VXkMY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=a-VQfQqIMrE",
    "title": "mixup: Beyond Empirical Risk Minimization (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, classifier, dnn, cnn, high dimensions, class boundaries, mixing, interpolation, latent, beta, regularizer, regularization, generalization, adversarial examples, smooth",
    "scraped_at": 1684582653.434606,
    "genre": "Science",
    "views": "10641",
    "desc": "Neural Networks often draw hard boundaries in high-dimensional space, which makes them very brittle. Mixup is a technique that linearly interpolates between data and labels at training time and achieves much smoother and more regular class boundaries.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - The problem with ERM\\\\n2:50 - Mixup\\\\n6:40 - Code\\\\n9:35 - Results\\\\n\\\\nhttps://arxiv.org/abs/1710.09412\\\\n\\\\nAbstract:\\\\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\\\\n\\\\nAuthors: Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "782",
    "uploadDate": "2020-05-27",
    "thumbnail_url": "https://i.ytimg.com/vi/a"
  },
  {
    "link": "watch?v=l5he9JNJqHA",
    "title": "A critical analysis of self-supervision, or what we can learn from a single image (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, investigation, linear probes, usefulness, representations, intermediate, hidden layers, self",
    "scraped_at": 1684582655.081606,
    "genre": "Science",
    "views": "10110",
    "desc": "Does self-supervision really need a lot of data? How low can you go? This paper shows that a single image is enough to learn the lower layers of a deep neural network. Interestingly, more data does not appear to help as long as enough data augmentation is applied.\\\\n\\\\nOUTLINE:\\\\n0:00 - Overview\\\\n1:40 - What is self-supervision\\\\n4:20 - What does this paper do\\\\n7:00 - Linear probes\\\\n11:15 - Linear probe results\\\\n17:10 - Results\\\\n22:25 - Learned Features\\\\n\\\\nhttps://arxiv.org/abs/1904.13132\\\\n\\\\nAbstract:\\\\nWe look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.\\\\n\\\\nAuthors: Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi\\\\n\\\\nThumbnail Image: https://commons.wikimedia.org/wiki/File:Golden_Gate_Bridge_during_blue_hour_(16_x_10).jpg\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1498",
    "uploadDate": "2020-05-26",
    "thumbnail_url": "https://i.ytimg.com/vi/l5he9JNJqHA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YrO1v7-KcXs",
    "title": "Deep image reconstruction from human brain activity (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, fmri, mind reading, thoughts, visual cortex, vc, v1, v4, vgg, reconstruction, iterative, deep dream, microscope, activity, imagine, visualize, introspection, human, telepathy",
    "scraped_at": 1684582655.166607,
    "genre": "Science",
    "views": "14508",
    "desc": "Can you peek into people\\'s brains? Reading human thoughts is a long-standing dream of the AI field. This paper reads fMRI signals from a person and then reconstructs what that person\\'s eyes currently see. This is achieved by translating the fMRI signal to features of a Deep Neural Network and then iteratively optimizing the input of the network to match those features. The results are impressive.\\\\n\\\\nOUTLINE:\\\\n0:00 - Overview\\\\n1:35 - Pipeline\\\\n4:00 - Training\\\\n5:20 - Image Reconstruction\\\\n7:00 - Deep Generator Network\\\\n8:15 - Results\\\\n\\\\nPaper: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633\\\\nMy Video on OpenAI Microscope (what I called Atlas): https://youtu.be/Ok44otx90D4\\\\n\\\\nAbstract:\\\\nThe mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.\\\\n\\\\nAuthors: Guohua Shen, Tomoyasu Horikawa, Kei Majima, Yukiyasu Kamitani\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1044",
    "uploadDate": "2020-05-25",
    "thumbnail_url": "https://i.ytimg.com/vi/YrO1v7"
  },
  {
    "link": "watch?v=UjJU13GdL94",
    "title": "Regularizing Trajectory Optimization with Denoising Autoencoders (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, reinforcement learning, model predictive control, dae, denoising autoencoders, trajectory, trajectory optimization, planning, adversarial attack, errors, open loop, closed loop, joint, probability, derivative, gaussian, experience, learned model, world model, model predictive, mpc",
    "scraped_at": 1684582655.9735267,
    "genre": "Science",
    "views": "5157",
    "desc": "Can you plan with a learned model of the world? Yes, but there\\'s a catch: The better your planning algorithm is, the more the errors of your world model will hurt you! This paper solves this problem by regularizing the planning algorithm to stay in high probability regions, given its experience.\\\\n\\\\nhttps://arxiv.org/abs/1903.11981\\\\n\\\\nInterview w/ Harri: https://youtu.be/HnZDmxYnpg4\\\\n\\\\nAbstract:\\\\nTrajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency.\\\\n\\\\nAuthors: Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1797",
    "uploadDate": "2020-05-24",
    "thumbnail_url": "https://i.ytimg.com/vi/UjJU13GdL94/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wcHQ3IutSJg",
    "title": "[News] The NeurIPS Broader Impact Statement",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, neurips, conference, ethics, society, impact, statement, submission, authors, accept, reject, flag, review, double blind",
    "scraped_at": 1684582655.2536051,
    "genre": "Science",
    "views": "3867",
    "desc": "For the first time, all authors submitting to the NeurIPS conference are forced to write a statement about the broader impact of their research on society. The messaging around this and how exactly this can influence the paper acceptance process is highly confusing.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:30 - VentureBeat Article\\\\n1:35 - Official Communication\\\\n9:55 - Special Ethics Reviewers\\\\n11:00 - Unofficial Communication\\\\n22:55 - Conclusion\\\\n\\\\nSources:\\\\nhttps://neurips.cc/Conferences/2020/CallForPapers\\\\nhttps://neurips.cc/Conferences/2020/PaperInformation/ReviewerGuidelines\\\\nhttps://neurips.cc/Conferences/2020/PaperInformation/NeurIPS-FAQ\\\\nhttps://medium.com/@NeurIPSConf/getting-started-with-neurips-2020-e350f9b39c28\\\\nhttps://venturebeat.com/2020/02/24/neurips-requires-ai-researchers-to-account-for-societal-impact-and-financial-conflicts-of-interest/\\\\nhttps://medium.com/@NeurIPSConf/a-note-for-submitting-authors-48cebfebae82\\\\nhttps://medium.com/@BrentH/suggestions-for-writing-neurips-2020-broader-impacts-statements-121da1b765bf\\\\nhttps://acm-fca.org/2018/03/29/negativeimpacts/\\\\nhttps://medium.com/@operations_18894/a-guide-to-writing-the-neurips-impact-statement-4293b723f832\\\\nhttps://gdpr-info.eu/\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1448",
    "uploadDate": "2020-05-23",
    "thumbnail_url": "https://i.ytimg.com/vi/wcHQ3IutSJg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=IIebBjbBevs",
    "title": "When BERT Plays the Lottery, All Tickets Are Winning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, bert, nlp, lottery ticket, good, bad, winning, pruning, weights, attention, transformer, heads, multi",
    "scraped_at": 1684582655.340604,
    "genre": "Science",
    "views": "29590",
    "desc": "BERT is a giant model. Turns out you can prune away many of its components and it still works. This paper analyzes BERT pruning in light of the Lottery Ticket Hypothesis and finds that even the \\\\\"",
    "lengthSeconds": "3214",
    "uploadDate": "2020-05-22",
    "thumbnail_url": "https://i.ytimg.com/vi/IIebBjbBevs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=utuz7wBGjKM",
    "title": "[News] OpenAI Model Generates Python Code",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, microsoft, openai, msbuild, build, code, gpt2, language model, completion, intellisense, intellicode, vscode, github, python, code completion, smart, generate, function body, docstring, name, arguments, programmer, stackoverflow, dataset, interpolate",
    "scraped_at": 1684582656.0545564,
    "genre": "Science",
    "views": "91011",
    "desc": "This code completion engine can write an entire function from just the name! OpenAI demonstrates what happens when you learn a language model on thousands of GitHub Python repositories.\\\\n\\\\nSource Clip: https://youtu.be/fZSFNUT6iY8\\\\nFull Video: https://www.pscp.tv/Microsoft/1OyKAYWPRrWKb\\\\nKite: https://kite.com/\\\\nTabNine: https://www.tabnine.com/\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "670",
    "uploadDate": "2020-05-21",
    "thumbnail_url": "https://i.ytimg.com/vi/utuz7wBGjKM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Nfry2b4RFI4",
    "title": "Investigating Human Priors for Playing Video Games (Paper & Demo)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, reinforcement learning, deep rl, human, prior, objects, game, video game, key, visuals, enemy, ladder, gravity, ablation",
    "scraped_at": 1684582655.4246051,
    "genre": "Science",
    "views": "2460",
    "desc": "Why are humans so good at video games? Maybe it\\'s because a lot of games are designed with humans in mind. What happens if we change that? This paper removes the influence of human priors from a game and ends up with a pretty fun experience.\\\\n\\\\nPaper: https://arxiv.org/abs/1802.10217\\\\nWebsite: https://rach0012.github.io/humanRL_website/\\\\nCode: https://github.com/rach0012/humanRL_prior_games\\\\n\\\\nAbstract:\\\\nWhat makes humans so good at solving seemingly complex video games? Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at this https URL\\\\n\\\\nAuthors: Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas L. Griffiths, Alexei A. Efros\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "661",
    "uploadDate": "2020-05-20",
    "thumbnail_url": "https://i.ytimg.com/vi/Nfry2b4RFI4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=u5BkO8XMS2I",
    "title": "iMAML: Meta-Learning with Implicit Gradients (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper",
    "scraped_at": 1684582656.1395266,
    "genre": "Science",
    "views": "19983",
    "desc": "Gradient-based Meta-Learning requires full backpropagation through the inner optimization procedure, which is a computational nightmare. This paper is able to circumvent this and implicitly compute meta-gradients by the clever introduction of a quadratic regularizer.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n0:15 - What is Meta-Learning?\\\\n9:05 - MAML vs iMAML\\\\n16:35 - Problem Formulation\\\\n19:15 - Proximal Regularization\\\\n26:10 - Derivation of the Implicit Gradient\\\\n40:55 - Intuition why this works\\\\n43:20 - Full Algorithm\\\\n47:40 - Experiments\\\\n\\\\nPaper: https://arxiv.org/abs/1909.04630\\\\nBlog Post: https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/\\\\n\\\\nAbstract:\\\\nA core capability of intelligent systems is the ability to quickly learn new tasks by drawing on prior experience. Gradient (or optimization) based meta-learning has recently emerged as an effective approach for few-shot learning. In this formulation, meta-parameters are learned in the outer loop, while task-specific models are learned in the inner-loop, by using only a small amount of data from the current task. A key challenge in scaling these approaches is the need to differentiate through the inner loop learning process, which can impose considerable computational and memory burdens. By drawing upon implicit differentiation, we develop the implicit MAML algorithm, which depends only on the solution to the inner level optimization and not the path taken by the inner loop optimizer. This effectively decouples the meta-gradient computation from the choice of inner loop optimizer. As a result, our approach is agnostic to the choice of inner loop optimizer and can gracefully handle many gradient steps without vanishing gradients or memory constraints. Theoretically, we prove that implicit MAML can compute accurate meta-gradients with a memory footprint that is, up to small constant factors, no more than that which is required to compute a single inner loop gradient and at no overall increase in the total computational cost. Experimentally, we show that these benefits of implicit MAML translate into empirical gains on few-shot image recognition benchmarks.\\\\n\\\\nAuthors: Aravind Rajeswaran, Chelsea Finn, Sham Kakade, Sergey Levine\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "3079",
    "uploadDate": "2020-05-19",
    "thumbnail_url": "https://i.ytimg.com/vi/u5BkO8XMS2I/maxresdefault.jpg"
  },
  {
    "link": "watch?v=G3pOvrKkFuk",
    "title": "[Code] PyTorch sentiment classifier from scratch with Huggingface NLP Library (Full Tutorial)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, code, pytorch, bert, pretrained, lightning, live, tutorial, pip, nlp, transformers, tokenizers, sequence, sentiment, imdb, dataset, full, github",
    "scraped_at": 1684582656.2525258,
    "genre": "Science",
    "views": "38234",
    "desc": "Huggingface released its newest library called NLP, which gives you easy access to almost any NLP dataset and metric in one convenient interface. We will combine this with a BERT model from Huggingface\\'s Transformers library to build a sentiment classifier for IMDB.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:30 - Boilerplate\\\\n3:20 - PyTorch Lightning Module\\\\n9:50 - Load Dataset\\\\n12:15 - Tokenization\\\\n20:50 - Torch Tensors\\\\n25:50 - Data Loader\\\\n28:00 - Create BERT Model\\\\n32:00 - Implement Validation and Train Step\\\\n47:00 - Run \\\\u0026 Recap\\\\n50:20 - Epilogue\\\\n\\\\nMy Code: https://github.com/yk/huggingface-nlp-demo\\\\nNLP Library: https://github.com/huggingface/nlp\\\\nTutorial Colab: https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb\\\\nTransformers Library: https://github.com/huggingface/transformers\\\\nPytorch Lightning: https://github.com/PyTorchLightning/pytorch-lightning\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "3761",
    "uploadDate": "2020-05-18",
    "thumbnail_url": "https://i.ytimg.com/vi/G3pOvrKkFuk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=IiBFqnNu7A8",
    "title": "Planning to Explore via Self-Supervised World Models (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, deep rl, deep reinforcement learning, novelty, curiosity, intrinsic reward, dreamer, planet, control, walker, run forward, imaginary, imagination, planning, google, neural network, actor, critic, uncertainty, information gain, mutual information, model",
    "scraped_at": 1684582655.5156052,
    "genre": "Science",
    "views": "5888",
    "desc": "What can an agent do without any reward? Explore the world! While many formulations of intrinsic rewards exist (Curiosity, Novelty, etc.), they all look back in time to learn. Plan2Explore is the first model that uses planning in a learned imaginary latent world model to seek out states where it is uncertain about what will happen.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro \\\\u0026 Problem Statement\\\\n3:30 - Model\\\\n5:10 - Intrinsic Motivation\\\\n9:05 - Planning in Latent Space\\\\n11:15 - Latent Disagreement\\\\n16:30 - Maximizing Information Gain\\\\n21:00 - More problems with the model\\\\n26:45 - Experiments\\\\n32:10 - Final Comments\\\\n\\\\nPaper: https://arxiv.org/abs/2005.05960\\\\nWebsite: https://ramanans1.github.io/plan2explore/\\\\nCode: https://github.com/ramanans1/plan2explore\\\\n\\\\nAbstract:\\\\nReinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at this https URL\\\\n\\\\nAuthors: Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2121",
    "uploadDate": "2020-05-17",
    "thumbnail_url": "https://i.ytimg.com/vi/IiBFqnNu7A8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XvDzZwoQFcU",
    "title": "[News] Facebook's Real-Time TTS system runs on CPUs only!",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, facebook, fair, tts, text",
    "scraped_at": 1684582655.6026313,
    "genre": "Science",
    "views": "6023",
    "desc": "Facebook AI\\'s new Text-To-Speech system is able to create 1 second of speech in as little as 500ms, making it real-time. What\\'s even more impressive is the fact that this does not require a rack of GPUs, but runs on merely 4 CPUs.\\\\n\\\\nOUTLINE:\\\\n0:00 - Intro\\\\n1:00 - Problem Formulation\\\\n3:20 - System Explanation\\\\n15:00 - Speeding up the computation\\\\n\\\\nhttps://ai.facebook.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1276",
    "uploadDate": "2020-05-16",
    "thumbnail_url": "https://i.ytimg.com/vi/XvDzZwoQFcU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=p-zOeQCoG9c",
    "title": "Weight Standardization (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, normalize, batchnorm, groupnorm, layernorm, mean, center, std, standardize, backpropagation, convergence, gradients, norm, convolution, cnn, convolutional neural networks, filters, kernel, channel, architecture",
    "scraped_at": 1684582656.3345513,
    "genre": "Science",
    "views": "9343",
    "desc": "It\\'s common for neural networks to include data normalization such as BatchNorm or GroupNorm. This paper extends the normalization to also include the weights of the network. This surprisingly simple change leads to a boost in performance and - combined with GroupNorm - new state-of-the-art results.\\\\n\\\\nhttps://arxiv.org/abs/1903.10520\\\\n\\\\nAbstract:\\\\nIn this paper, we propose Weight Standardization (WS) to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The micro-batch training setting is hard because small batch sizes are not enough for training networks with Batch Normalization (BN), while other normalization methods that do not rely on batch knowledge still have difficulty matching the performances of BN in large-batch training. Our WS ends this problem because when used with Group Normalization and trained with 1 image/GPU, WS is able to match or outperform the performances of BN trained with large batch sizes with only 2 more lines of code. In micro-batch training, WS significantly outperforms other normalization methods. WS achieves these superior results by standardizing the weights in the convolutional layers, which we show is able to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients. The effectiveness of WS is verified on many tasks, including image classification, object detection, instance segmentation, video recognition, semantic segmentation, and point cloud recognition. The code is available here: this https URL.\\\\n\\\\nAuthors: Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1156",
    "uploadDate": "2020-05-15",
    "thumbnail_url": "https://i.ytimg.com/vi/p"
  },
  {
    "link": "watch?v=zt_R85Ife_U",
    "title": "[Trash] Automated Inference on Criminality using Face Images",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, trash, wrong, phrenology, physiognomy, face, facial, criminal, violent, features, body, physical, visible, intuition, smile, micro, expression",
    "scraped_at": 1684582656.422527,
    "genre": "Science",
    "views": "5702",
    "desc": "This paper sets out to build a classifier to distinguish criminals from non-criminals using nothing but a face picture. I explore why the research is trash and what lessons we can learn from it.\\\\n\\\\nhttps://arxiv.org/abs/1611.04135\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1963",
    "uploadDate": "2020-05-14",
    "thumbnail_url": "https://i.ytimg.com/vi/zt_R85Ife_U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=bFn2xcGi1TQ",
    "title": "Faster Neural Network Training with Data Echoing (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, brain, pipeline, bottleneck, speed, gpu, tpu, idle, network, distributed, preprocessing, augmentation",
    "scraped_at": 1684582655.6979682,
    "genre": "Science",
    "views": "6966",
    "desc": "CPUs are often bottlenecks in Machine Learning pipelines. Data fetching, loading, preprocessing and augmentation can be slow to a point where the GPUs are mostly idle. Data Echoing is a technique to re-use data that is already in the pipeline to reclaim this idle time and keep the GPUs busy at all times.\\\\n\\\\nhttps://arxiv.org/abs/1907.05550\\\\n\\\\nAbstract:\\\\nIn the twilight of Moore\\'s law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce \\\\\"",
    "lengthSeconds": "2357",
    "uploadDate": "2020-05-13",
    "thumbnail_url": "https://i.ytimg.com/vi/bFn2xcGi1TQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=l_3zj6HeWUE",
    "title": "Group Normalization (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, batchnorm, groupnorm, layer norm, group norm, batch norm, instance norm, fair, normalization, mean, standard deviation, minibatch, batch statistics, kernel, cnn, convolutional neural network",
    "scraped_at": 1684582655.7865567,
    "genre": "Science",
    "views": "24765",
    "desc": "The dirty little secret of Batch Normalization is its intrinsic dependence on the training batch size. Group Normalization attempts to achieve the benefits of normalization without batch statistics and, most importantly, without sacrificing performance compared to Batch Normalization.\\\\n\\\\nhttps://arxiv.org/abs/1803.08494\\\\n\\\\nAbstract:\\\\nBatch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN\\'s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN\\'s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN\\'s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.\\\\n\\\\nAuthors: Yuxin Wu, Kaiming He\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1746",
    "uploadDate": "2020-05-12",
    "thumbnail_url": "https://i.ytimg.com/vi/l_3zj6HeWUE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Cs_j-oNwGgg",
    "title": "Concept Learning with Energy-Based Models (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, openai, ebm, energy function, gradient descent, relational neural network, latent, attention, entities, spatial relation, inference time, reasoning, demonstration",
    "scraped_at": 1684582656.5385268,
    "genre": "Science",
    "views": "24880",
    "desc": "This is a hard paper! Energy-functions are typically a mere afterthought in current machine learning. A core function of the Energy - its smoothness - is usually not exploited at inference time. This paper takes a stab at it. Inferring concepts, world states, and attention masks via gradient descent on a learned energy function leads to an interesting framework with many possibilities.\\\\n\\\\nPaper: https://arxiv.org/abs/1811.02486\\\\nBlog: https://openai.com/blog/learning-concepts-with-energy-functions/\\\\nVideos: https://sites.google.com/site/energyconceptmodels/\\\\n\\\\nAbstract:\\\\nMany hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at this http URL\\\\n\\\\nAuthors: Igor Mordatch\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2369",
    "uploadDate": "2020-05-11",
    "thumbnail_url": "https://i.ytimg.com/vi/Cs_j"
  },
  {
    "link": "watch?v=iZXsWlSdMGY",
    "title": "[News] Google\u2019s medical AI was super accurate in a lab. Real life was a different story.",
    "tags": "deep learning, machine learning, news, google, retina, diabetes, computer vision, neural networks, production, devops, deployment, legal, thailand",
    "scraped_at": 1684582656.9135263,
    "genre": "Science",
    "views": "6278",
    "desc": "A closer look at a story of how the deployment of AI brings its own challenges and what can go wrong.\\\\n\\\\nhttps://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-clinic-covid-diabetes-retina-disease/\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "763",
    "uploadDate": "2020-05-10",
    "thumbnail_url": "https://i.ytimg.com/vi/iZXsWlSdMGY/hqdefault.jpg"
  },
  {
    "link": "watch?v=k1GOF2jmX7c",
    "title": "Big Transfer (BiT): General Visual Representation Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, google, brain, cnn, convolutional neural network, resnet, residual network, pretraining, finetuning, vtab, imagenet, cifar, state of the art, pretrained, computer vision",
    "scraped_at": 1684582656.9955246,
    "genre": "Science",
    "views": "9374",
    "desc": "One CNN to rule them all! BiT is a pre-trained ResNet that can be used as a starting point for any visual task. This paper explains what it takes to pre-train such a large model and details how fine-tuning on downstream tasks is done best.\\\\n\\\\nPaper: https://arxiv.org/abs/1912.11370\\\\nCode \\\\u0026 Models: TBA\\\\n\\\\nAbstract:\\\\nTransfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.\\\\n\\\\nAuthors: Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2050",
    "uploadDate": "2020-05-09",
    "thumbnail_url": "https://i.ytimg.com/vi/k1GOF2jmX7c/hqdefault.jpg"
  },
  {
    "link": "watch?v=tjbEVY5XIk0",
    "title": "Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, reinforcement learning, deep rl, planning, alphago, alphazero, alpha go, alpha zero, mcts, monte carlo, tree search, subdivision, recursive, training data, hindsight experience replay",
    "scraped_at": 1684582657.474552,
    "genre": "Science",
    "views": "4103",
    "desc": "When AI makes a plan it usually does so step by step, forward in time. But often it is beneficial to define intermediate goals to divide a large problem into easier sub-problems. This paper proposes a generalization of MCTS that searches not for the best next actions to take, but for the best way to sub-divide the problem recursively into problems so tiny that they can each be solved in a single step.\\\\n\\\\nPaper: https://arxiv.org/abs/2004.11410\\\\nSite: https://sites.google.com/view/dc-mcts/home\\\\n\\\\nAbstract:\\\\nStandard planners for sequential decision making (including Monte Carlo planning, tree search, dynamic programming, etc.) are constrained by an implicit sequential planning assumption: The order in which a plan is constructed is the same in which it is executed. We consider alternatives to this assumption for the class of goal-directed Reinforcement Learning (RL) problems. Instead of an environment transition model, we assume an imperfect, goal-directed policy. This low-level policy can be improved by a plan, consisting of an appropriate sequence of sub-goals that guide it from the start to the goal state. We propose a planning algorithm, Divide-and-Conquer Monte Carlo Tree Search (DC-MCTS), for approximating the optimal plan by means of proposing intermediate sub-goals which hierarchically partition the initial tasks into simpler ones that are then solved independently and recursively. The algorithm critically makes use of a learned sub-goal proposal for finding appropriate partitions trees of new tasks based on prior experience. Different strategies for learning sub-goal proposals give rise to different planning strategies that strictly generalize sequential planning. We show that this algorithmic flexibility over planning order leads to improved results in navigation tasks in grid-worlds as well as in challenging continuous control environments.\\\\n\\\\nAuthors: Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica B. Hamrick, Nicolas Heess, Alexander Neitz, Theophane Weber\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1533",
    "uploadDate": "2020-05-08",
    "thumbnail_url": "https://i.ytimg.com/vi/tjbEVY5XIk0/hqdefault.jpg"
  },
  {
    "link": "watch?v=eCH0M4wzKJs",
    "title": "WHO ARE YOU? 10k Subscribers Special (w/ Channel Analytics)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, special",
    "scraped_at": 1684582657.0795248,
    "genre": "Science",
    "views": "3477",
    "desc": "An in-depth look at this channel\\'s analytics.\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "599",
    "uploadDate": "2020-05-07",
    "thumbnail_url": "https://i.ytimg.com/vi/eCH0M4wzKJs/hqdefault.jpg"
  },
  {
    "link": "watch?v=to7vCdkLi4s",
    "title": "Reinforcement Learning with Augmented Data (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, rl, reinforcement learning, sac, ppo, deep rl, deep reinforcement learning, dreamer, curl, pixel, pretraining, deepmind, openai, berkeley",
    "scraped_at": 1684582657.5565252,
    "genre": "Science",
    "views": "6579",
    "desc": "This ONE SIMPLE TRICK can take a vanilla RL algorithm to achieve state-of-the-art. What is it? Simply augment your training data before feeding it to the learner! This can be dropped into any RL pipeline and promises big improvements across the board.\\\\n\\\\nPaper: https://arxiv.org/abs/2004.14990\\\\nCode: https://www.github.com/MishaLaskin/rad\\\\n\\\\nAbstract:\\\\nLearning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at this https URL.\\\\n\\\\nAuthors: Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1334",
    "uploadDate": "2020-05-06",
    "thumbnail_url": "https://i.ytimg.com/vi/to7vCdkLi4s/hqdefault.jpg"
  },
  {
    "link": "watch?v=cIUtRNhY6Rw",
    "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, bert, nlp, natural language processing, wikitables, sql, tabular, aggregations, structured, google",
    "scraped_at": 1684582658.0145273,
    "genre": "Science",
    "views": "7648",
    "desc": "Answering complex questions about tabular information is hard. No two tables are alike and sometimes the answer you\\'re looking for is not even in the table and needs to be computed from a subset of the cells. Surprisingly, this model can figure it all out by itself through some clever input encoding and loss engineering.\\\\n\\\\nPaper: https://arxiv.org/abs/2004.02349\\\\nCode: https://github.com/google-research/tapas\\\\n\\\\nAbstract:\\\\nAnswering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT\\'s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.\\\\n\\\\nAuthors: Jonathan Herzig, Pawe\\xc5\\x82 Krzysztof Nowak, Thomas M\\xc3\\xbcller, Francesco Piccinno, Julian Martin Eisenschlos\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1908",
    "uploadDate": "2020-05-05",
    "thumbnail_url": "https://i.ytimg.com/vi/cIUtRNhY6Rw/hqdefault.jpg"
  },
  {
    "link": "watch?v=PDRtyrVskMU",
    "title": "Chip Placement with Deep Reinforcement Learning (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, reinforcement learning, deep reinforcement learning, gans, gan, deconvolution, computer chip, gpu, tpu, fpga, netlist, constrained, google",
    "scraped_at": 1684582657.6415558,
    "genre": "Science",
    "views": "10230",
    "desc": "The AI Singularity is here! Computers designing new computers! It takes human experts multiple weeks to design new computer chips. What looks like a large game of Tetris is actually a very complex optimization problem. This paper uses Deep Reinforcement Learning to solve this optimization both faster and better than humans.\\\\n\\\\nhttps://arxiv.org/abs/2004.10746\\\\n\\\\nAbstract:\\\\nIn this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks.\\\\n\\\\nAuthors: Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, Azade Nazi, Jiwoo Pak, Andy Tong, Kavya Srinivasa, William Hang, Emre Tuncer, Anand Babu, Quoc V. Le, James Laudon, Richard Ho, Roger Carpenter, Jeff Dean\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1647",
    "uploadDate": "2020-05-04",
    "thumbnail_url": "https://i.ytimg.com/vi/PDRtyrVskMU/hqdefault.jpg"
  },
  {
    "link": "watch?v=wTIPGoHLw_8",
    "title": "I talk to the new Facebook Blender Chatbot",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, nlp, chatbot, dialogue, persona, vegan, turing test, natural language processing, transformer, generator, context",
    "scraped_at": 1684582663.4655814,
    "genre": "Science",
    "views": "15076",
    "desc": "This is what a 9 Billion parameter transformer can do. I take a look at FAIR\\'s new paper \\\\\"",
    "lengthSeconds": "680",
    "uploadDate": "2020-05-03",
    "thumbnail_url": "https://i.ytimg.com/vi/wTIPGoHLw_8/hqdefault.jpg"
  },
  {
    "link": "watch?v=1aO-uHXbzmQ",
    "title": "Jukebox: A Generative Model for Music (Paper Explained)",
    "tags": "deep learning, machine learning, arxiv, explained, neural networks, ai, artificial intelligence, paper, music, vae, vq",
    "scraped_at": 1684582658.0995245,
    "genre": "Science",
    "views": "21476",
    "desc": "This generative model for music can make entire songs with remarkable quality and consistency. It can be conditioned on genre, artist, and even lyrics.\\\\n\\\\nBlog: https://openai.com/blog/jukebox/\\\\nPaper: https://cdn.openai.com/papers/jukebox.pdf\\\\nCode: https://github.com/openai/jukebox/\\\\n\\\\nAbstract:\\\\nWe introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.\\\\n\\\\nAuthors: Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2026",
    "uploadDate": "2020-05-02",
    "thumbnail_url": "https://i.ytimg.com/vi/1aO"
  },
  {
    "link": "watch?v=RrBapqCPnmE",
    "title": "[ML Coding Tips] Separate Computation & Plotting using locals",
    "tags": "deep learning, machine learning, coding, research, engineering, ipython, colab, notebook, locals",
    "scraped_at": 1684582657.7275565,
    "genre": "Science",
    "views": "4947",
    "desc": "Here\\'s a lazy way to separate computation and subsequent analysis in a notebook without the overhead of manually saving local variables.\\\\n\\\\nWARNING: Don\\'t do this in a serious project.\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "426",
    "uploadDate": "2020-05-01",
    "thumbnail_url": "https://i.ytimg.com/vi/RrBapqCPnmE/hqdefault.jpg"
  },
  {
    "link": "watch?v=F5aaXrIMWyU",
    "title": "The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies (Paper Explained)",
    "tags": "deep learning, reinforcement learning, society, gini index, welfare, taxes, brackets, progressive, regressive, us, poor, rich, equality, redistribution, outer loop, world, resources, labor, trade, neural networks, ppo",
    "scraped_at": 1684582657.8115246,
    "genre": "Science",
    "views": "21369",
    "desc": "Hail the AI Tax Collector! This very visual framework has RL Agents maximize their coins in a tiny world through collecting, building and trading. But at the same time, the government is also an AI trying to maximize social welfare via taxes. What emerges is very interesting.\\\\n\\\\nPaper: https://arxiv.org/abs/2004.13332\\\\nBlog: https://blog.einstein.ai/the-ai-economist/\\\\n\\\\nAbstract:\\\\nTackling real-world socio-economic challenges requires designing and testing economic policies. However, this is hard in practice, due to a lack of appropriate (micro-level) economic data and limited opportunity to experiment. In this work, we train social planners that discover tax policies in dynamic economies that can effectively trade-off economic equality and productivity. We propose a two-level deep reinforcement learning approach to learn dynamic tax policies, based on economic simulations in which both agents and a government learn and adapt. Our data-driven approach does not make use of economic modeling assumptions, and learns from observational data alone. We make four main contributions. First, we present an economic simulation environment that features competitive pressures and market dynamics. We validate the simulation by showing that baseline tax systems perform in a way that is consistent with economic theory, including in regard to learned agent behaviors and specializations. Second, we show that AI-driven tax policies improve the trade-off between equality and productivity by 16% over baseline policies, including the prominent Saez tax framework. Third, we showcase several emergent features: AI-driven tax policies are qualitatively different from baselines, setting a higher top tax rate and higher net subsidies for low incomes. Moreover, AI-driven tax policies perform strongly in the face of emergent tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are also effective when used in experiments with human participants. In experiments conducted on MTurk, an AI tax policy provides an equality-productivity trade-off that is similar to that provided by the Saez framework along with higher inverse-income weighted social welfare.\\\\n\\\\nAuthors: Stephan Zheng,\\xc2\\xa0Alexander Trott,\\xc2\\xa0Sunil Srinivasa,\\xc2\\xa0Nikhil Naik,\\xc2\\xa0Melvin Gruesbeck,\\xc2\\xa0David C. Parkes,\\xc2\\xa0Richard Socher\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2105",
    "uploadDate": "2020-04-30",
    "thumbnail_url": "https://i.ytimg.com/vi/F5aaXrIMWyU/hqdefault.jpg"
  },
  {
    "link": "watch?v=jhCInVFE2sc",
    "title": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (Paper Explained)",
    "tags": "deep learning, machine learning, initialization, mask, arxiv, uber, training, subnetwork, overparameterization, zero, frozen, weights",
    "scraped_at": 1684582663.5805857,
    "genre": "Science",
    "views": "12809",
    "desc": "This paper dives into the intrinsics of the Lottery Ticket Hypothesis and attempts to shine some light on what\\'s important and what isn\\'t.\\\\n\\\\nhttps://arxiv.org/abs/1905.01067\\\\n\\\\nAbstract:\\\\nThe recent \\\\\"",
    "lengthSeconds": "2139",
    "uploadDate": "2020-04-29",
    "thumbnail_url": "https://i.ytimg.com/vi/jhCInVFE2sc/hqdefault.jpg"
  },
  {
    "link": "watch?v=h9w3KffPPmQ",
    "title": "[Rant] Online Conferences",
    "tags": "machine learning, deep learning, online, conference, iclr, virtual, research",
    "scraped_at": 1684582659.241355,
    "genre": "Science",
    "views": "3548",
    "desc": "Are virtual conferences good or bad? What\\'s missing? How do we go forward? \\\\n\\\\nPictures from here:\\\\nhttps://twitter.com/srush_nlp/status/1253786329575538691\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "414",
    "uploadDate": "2020-04-28",
    "thumbnail_url": "https://i.ytimg.com/vi/h9w3KffPPmQ/hqdefault.jpg"
  },
  {
    "link": "watch?v=fvctpYph8Pc",
    "title": "Do ImageNet Classifiers Generalize to ImageNet? (Paper Explained)",
    "tags": "deep learning, machine learning, imagenet, cifar10, cifar10.1, generalization, overfitting, mturk, arxiv, vision, models, research, hardness, accuracy, classifier, resnet",
    "scraped_at": 1684582658.1875608,
    "genre": "Science",
    "views": "20120",
    "desc": "Has the world overfitted to ImageNet? What if we collect another dataset in exactly the same fashion? This paper gives a surprising answer!\\\\n\\\\nPaper: https://arxiv.org/abs/1902.10811\\\\nData: https://github.com/modestyachts/ImageNetV2\\\\n\\\\nAbstract:\\\\nWe build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models\\' inability to generalize to slightly \\\\\"",
    "lengthSeconds": "1536",
    "uploadDate": "2020-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/fvctpYph8Pc/hqdefault.jpg"
  },
  {
    "link": "watch?v=hDQNCWR3HLQ",
    "title": "[Drama] Schmidhuber: Critique of Honda Prize for Dr. Hinton",
    "tags": "deep learning, machine learning, schmidhuber, hinton, seppo, rummelhardt, hochreiter, lstm, rbm, backpropagation, credit, science",
    "scraped_at": 1684582658.2705247,
    "genre": "Science",
    "views": "19934",
    "desc": "Schmidhuber writes up a critique of Hinton receiving the Honda Price... AND HINTON REPLIES!\\\\n\\\\nSchmidhuber\\'s Blog Entry: http://people.idsia.ch/~juergen/critique-honda-prize-hinton.html\\\\nHinton\\'s Reply: https://www.reddit.com/r/MachineLearning/comments/g5ali0/d_schmidhuber_critique_of_honda_prize_for_dr/\\\\n\\\\nThumbnail Images:\\\\nBy Eviatar Bach -https://de.m.wikipedia.org/wiki/Datei:Geoffrey_Hinton_at_UBC.jpg\\\\nBy ITU/R.Farrell - https://www.flickr.com/photos/itupictures/34343385563, CC BY 2.0, https://commons.wikimedia.org/w/index.php?curid=75018240\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1144",
    "uploadDate": "2020-04-26",
    "thumbnail_url": "https://i.ytimg.com/vi/hDQNCWR3HLQ/hqdefault.jpg"
  },
  {
    "link": "watch?v=gJR28onlqzs",
    "title": "How much memory does Longformer use?",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, google, attention mechanism, attention, transformer, tensor2tensor, rnn, recurrent, seq2seq",
    "scraped_at": 1684582659.3203554,
    "genre": "Science",
    "views": "3849",
    "desc": "A calculation of the memory requirements of the Longformer.\\\\n\\\\nOriginal video: https://youtu.be/_8KNb5iqblE\\\\nPaper: https://arxiv.org/abs/2004.05150\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "558",
    "uploadDate": "2020-04-25",
    "thumbnail_url": "https://i.ytimg.com/vi/gJR28onlqzs/hqdefault.jpg"
  },
  {
    "link": "watch?v=MpdbFLXOOIw",
    "title": "Supervised Contrastive Learning",
    "tags": "deep learning, machine learning, supervised learning, classification, classifier, labels, pretraining, unsupervised, self",
    "scraped_at": 1684582659.401356,
    "genre": "Science",
    "views": "48923",
    "desc": "The cross-entropy loss has been the default in deep learning for the last few years for supervised learning. This paper proposes a new loss, the supervised contrastive loss, and uses it to pre-train the network in a supervised fashion. The resulting model, when fine-tuned to ImageNet, achieves new state-of-the-art.\\\\n\\\\nhttps://arxiv.org/abs/2004.11362\\\\n\\\\nAbstract:\\\\nCross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.\\\\n\\\\nAuthors: Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1808",
    "uploadDate": "2020-04-24",
    "thumbnail_url": "https://i.ytimg.com/vi/MpdbFLXOOIw/hqdefault.jpg"
  },
  {
    "link": "watch?v=pZyxlf6l0N8",
    "title": "Thinking While Moving: Deep Reinforcement Learning with Concurrent Control",
    "tags": "deep learning, machine learning, reinforcement learning, vector to go, vtg, continuous, control, robot, concurrent, deep rl, deep neural networks, berkeley, google, grasping, qlearning",
    "scraped_at": 1684582658.352575,
    "genre": "Science",
    "views": "2841",
    "desc": "Classic RL \\\\\"",
    "lengthSeconds": "1781",
    "uploadDate": "2020-04-23",
    "thumbnail_url": "https://i.ytimg.com/vi/pZyxlf6l0N8/hqdefault.jpg"
  },
  {
    "link": "watch?v=yPjuAo53uNI",
    "title": "[Rant] The Male Only History of Deep Learning",
    "tags": "deep learning, machine learning, neural networks, history, groups, ideology",
    "scraped_at": 1684582658.4305573,
    "genre": "Science",
    "views": "11914",
    "desc": "This casting of our field in terms of ideological narrow-sighted group-think is disgusting. Keep Science about ideas!\\\\n\\\\nhttps://twitter.com/timnitGebru/status/1252752743942328321\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "388",
    "uploadDate": "2020-04-22",
    "thumbnail_url": "https://i.ytimg.com/vi/yPjuAo53uNI/hqdefault.jpg"
  },
  {
    "link": "watch?v=PZypP7PiKi0",
    "title": "Gradient Surgery for Multi-Task Learning",
    "tags": "deep learning, machine learning, neural networks, multi task, conflicting gradients, magnitudes, adam, sgd, momentum, optimization, projection",
    "scraped_at": 1684582659.485387,
    "genre": "Science",
    "views": "7180",
    "desc": "Multi-Task Learning can be very challenging when gradients of different tasks are of severely different magnitudes or point into conflicting directions. PCGrad eliminates this problem by projecting conflicting gradients while still retaining optimality guarantees.\\\\n\\\\nhttps://arxiv.org/abs/2001.06782\\\\n\\\\nAbstract:\\\\nWhile deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task\\'s gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.\\\\n\\\\nAuthors: Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1936",
    "uploadDate": "2020-04-21",
    "thumbnail_url": "https://i.ytimg.com/vi/PZypP7PiKi0/hqdefault.jpg"
  },
  {
    "link": "watch?v=_8KNb5iqblE",
    "title": "Longformer: The Long-Document Transformer",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, attention mechanism, attention, transformer, bert, roberta, mlm, convolution, memory, linear, sliding, dilated, sparse",
    "scraped_at": 1684582659.5683856,
    "genre": "Science",
    "views": "18040",
    "desc": "The Longformer extends the Transformer by introducing sliding window attention and sparse global attention. This allows for the processing of much longer documents than classic models like BERT.\\\\n\\\\nPaper: https://arxiv.org/abs/2004.05150\\\\nCode: https://github.com/allenai/longformer\\\\n\\\\nAbstract:\\\\nTransformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer\\'s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.\\\\n\\\\nAuthors: Iz Beltagy, Matthew E. Peters, Arman Cohan\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1595",
    "uploadDate": "2020-04-20",
    "thumbnail_url": "https://i.ytimg.com/vi/_8KNb5iqblE/hqdefault.jpg"
  },
  {
    "link": "watch?v=a0f07M2uj_A",
    "title": "Backpropagation and the brain",
    "tags": "deep learning, machine learning, biologically plausible, neural networks, spiking, neurons, neuroscience, hinton, google, deepmind, brain, cells, soma, axon, interneurons, action potential, backprop",
    "scraped_at": 1684582658.518552,
    "genre": "Science",
    "views": "13824",
    "desc": "Geoffrey Hinton and his co-authors describe a biologically plausible variant of backpropagation and report evidence that such an algorithm might be responsible for learning in the brain.\\\\n\\\\nhttps://www.nature.com/articles/s41583-020-0277-3\\\\n\\\\nAbstract:\\\\nDuring learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.\\\\n\\\\nAuthors: Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman \\\\u0026 Geoffrey Hinton\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1945",
    "uploadDate": "2020-04-20",
    "thumbnail_url": "https://i.ytimg.com/vi/a0f07M2uj_A/hqdefault.jpg"
  },
  {
    "link": "watch?v=D-eg7k8YSfs",
    "title": "Shortcut Learning in Deep Neural Networks",
    "tags": "deep learning, machine learning, adversarial examples, iid, ood, distribution, bias, discrimination, neural networks, bugs, distortions, data pipeline, causality, intention, grounding",
    "scraped_at": 1684582659.6533544,
    "genre": "Science",
    "views": "9805",
    "desc": "This paper establishes a framework for looking at out-of-distribution generalization failures of modern deep learning as the models learning false shortcuts that are present in the training data. The paper characterizes why and when shortcut learning can happen and gives recommendations for how to counter its effect.\\\\n\\\\nhttps://arxiv.org/abs/2004.07780\\\\n\\\\nAbstract:\\\\nDeep learning has triggered the current rise of artificial intelligence and is the workhorse of today\\'s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning\\'s problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.\\\\n\\\\nAuthors: Robert Geirhos, J\\xc3\\xb6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2951",
    "uploadDate": "2020-04-18",
    "thumbnail_url": "https://i.ytimg.com/vi/D"
  },
  {
    "link": "watch?v=Ok44otx90D4",
    "title": "Feature Visualization & The OpenAI microscope",
    "tags": "deep learning, machine learning, imagenet, visualization, features, intermediate, hidden layers, activations, patterns, openai, google, interactive, explanation",
    "scraped_at": 1684582658.6033838,
    "genre": "Science",
    "views": "9191",
    "desc": "A closer look at the OpenAI microscope, a database of visualizations of the inner workings of ImageNet classifiers, along with an explanation of how to obtain these visualizations.\\\\n\\\\nhttps://distill.pub/2017/feature-visualization/\\\\nhttps://microscope.openai.com/models\\\\nhttps://github.com/tensorflow/lucid\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1250",
    "uploadDate": "2020-04-17",
    "thumbnail_url": "https://i.ytimg.com/vi/Ok44otx90D4/hqdefault.jpg"
  },
  {
    "link": "watch?v=-h1KB8ps11A",
    "title": "Datasets for Data-Driven Reinforcement Learning",
    "tags": "deep learning, machine learning, reinforcement learning, deep rl, off",
    "scraped_at": 1684582659.7303548,
    "genre": "Science",
    "views": "4507",
    "desc": "Offline Reinforcement Learning has come more and more into focus recently in domains where classic on-policy RL algorithms are infeasible to train, such as safety-critical tasks or learning from expert demonstrations. This paper presents an extensive benchmark for evaluating offline RL algorithms in a variety of settings.\\\\n\\\\nPaper: https://arxiv.org/abs/2004.07219\\\\nCode: https://github.com/rail-berkeley/offline_rl\\\\n\\\\nAbstract:\\\\nThe offline reinforcement learning (RL) problem, also referred to as batch RL, refers to the setting where a policy must be learned from a dataset of previously collected data, without additional online data collection. In supervised learning, large datasets and complex deep neural networks have fueled impressive progress, but in contrast, conventional RL algorithms must collect large amounts of on-policy data and have had little success leveraging previously collected datasets. As a result, existing RL benchmarks are not well-suited for the offline setting, making progress in this area difficult to measure. To design a benchmark tailored to offline RL, we start by outlining key properties of datasets relevant to applications of offline RL. Based on these properties, we design a set of benchmark tasks and datasets that evaluate offline RL algorithms under these conditions. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets, where an agent can perform different tasks in the same environment, and datasets consisting of a heterogeneous mix of high-quality and low-quality trajectories. By designing the benchmark tasks and datasets to reflect properties of real-world offline RL problems, our benchmark will focus research effort on methods that drive substantial improvements not just on simulated benchmarks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact.\\\\n\\\\nAuthors: Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1194",
    "uploadDate": "2020-04-16",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=eYgPJ_7BkEw",
    "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
    "tags": "deep learning, machine learning, arxiv, google, semi",
    "scraped_at": 1684582658.6873806,
    "genre": "Science",
    "views": "15422",
    "desc": "FixMatch is a simple, yet surprisingly effective approach to semi-supervised learning. It combines two previous methods in a clever way and achieves state-of-the-art in regimes with few and very few labeled examples.\\\\n\\\\nPaper: https://arxiv.org/abs/2001.07685\\\\nCode: https://github.com/google-research/fixmatch\\\\n\\\\nAbstract:\\\\nSemi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model\\'s performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model\\'s predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch\\'s success. We make our code available at this https URL.\\\\n\\\\nAuthors: Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1212",
    "uploadDate": "2020-04-15",
    "thumbnail_url": "https://i.ytimg.com/vi/eYgPJ_7BkEw/hqdefault.jpg"
  },
  {
    "link": "watch?v=AU30czb4iQA",
    "title": "Imputer: Sequence Modelling via Imputation and Dynamic Programming",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, google, attention mechanism, attention, transformer, seq2seq, autoregressive, independence, decoding",
    "scraped_at": 1684582659.8103883,
    "genre": "Science",
    "views": "1790",
    "desc": "The imputer is a sequence-to-sequence model that strikes a balance between fully autoregressive models with long inference times and fully non-autoregressive models with fast inference. The imputer achieves constant decoding time independent of sequence length by exploiting dynamic programming.\\\\n\\\\nhttps://arxiv.org/abs/2002.08926\\\\n\\\\nAbstract:\\\\nThis paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generative model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.\\\\n\\\\nAuthors: William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1094",
    "uploadDate": "2020-04-14",
    "thumbnail_url": "https://i.ytimg.com/vi/AU30czb4iQA/hqdefault.jpg"
  },
  {
    "link": "watch?v=ZVVnvZdUMUk",
    "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
    "tags": "deep learning, machine learning, neural networks, pruning, distillation, quantization, size, weights, optimization, training, generalization, overparameterization, winning ticket, winning lottery ticket, arxiv",
    "scraped_at": 1684582659.8903544,
    "genre": "Science",
    "views": "17219",
    "desc": "Stunning evidence for the hypothesis that neural networks work so well because their random initialization almost certainly contains a nearly optimal sub-network that is responsible for most of the final performance.\\\\n\\\\nhttps://arxiv.org/abs/1803.03635\\\\n\\\\nAbstract:\\\\nNeural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\\\\nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \\\\\"",
    "lengthSeconds": "1154",
    "uploadDate": "2020-04-13",
    "thumbnail_url": "https://i.ytimg.com/vi/ZVVnvZdUMUk/hqdefault.jpg"
  },
  {
    "link": "watch?v=-0aM99dMu_4",
    "title": "Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery",
    "tags": "deep learning, machine learning, reinforcement learning, deep rl, auxiliary, reward, distance, value function, shortest path, neural networks, maze, unsupervised, discovery, exploration",
    "scraped_at": 1684582658.7703576,
    "genre": "Science",
    "views": "1834",
    "desc": "DDL is an auxiliary task for an agent to learn distances between states in episodes. This can then be used further to improve the agent\\'s policy learning procedure.\\\\n\\\\nPaper: https://arxiv.org/abs/1907.08225\\\\nBlog: https://sites.google.com/view/dynamical-distance-learning/home\\\\n\\\\nAbstract:\\\\nReinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: this https URL.\\\\n\\\\nAuthors: Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1355",
    "uploadDate": "2020-04-12",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=hg2Q_O5b9w4",
    "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
    "tags": "deep learning, machine learning, rl, reinforcement learning, unsupervised, contrast, contrastive, encoder, self",
    "scraped_at": 1684582659.9703548,
    "genre": "Science",
    "views": "11019",
    "desc": "Contrastive Learning has been an established method in NLP and Image classification. The authors show that with relatively minor adjustments, CL can be used to augment and improve RL dramatically.\\\\n\\\\nPaper: https://arxiv.org/abs/2004.04136\\\\nCode: https://github.com/MishaLaskin/curl\\\\n\\\\nAbstract:\\\\nWe present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 2.8x and 1.6x performance gains respectively at the 100K interaction steps benchmark. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency and performance of methods that use state-based features.\\\\n\\\\nAuthors: Aravind Srinivas, Michael Laskin, Pieter Abbeel\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1725",
    "uploadDate": "2020-04-11",
    "thumbnail_url": "https://i.ytimg.com/vi/hg2Q_O5b9w4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gbG1X8Xq-T8",
    "title": "Enhanced POET: Open-Ended RL through Unbounded Invention of Learning Challenges and their Solutions",
    "tags": "deep learning, machine learning, unbounded, open",
    "scraped_at": 1684582658.8483734,
    "genre": "Science",
    "views": "1713",
    "desc": "The enhanced POET makes some substantial and well-crafted improvements over the original POET algorithm and excels at open-ended learning like no system before.\\\\n\\\\nhttps://arxiv.org/abs/2003.08536\\\\nhttps://youtu.be/RX0sKDRq400\\\\n\\\\nAbstract:\\\\nCreating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges, and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential. Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved through other means.\\\\n\\\\nAuthors: Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, Kenneth O. Stanley\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "940",
    "uploadDate": "2020-04-10",
    "thumbnail_url": "https://i.ytimg.com/vi/gbG1X8Xq"
  },
  {
    "link": "watch?v=klPuEHCKG9M",
    "title": "Evolving Normalization-Activation Layers",
    "tags": "deep learning, machine learning, cnn, resnet, residual, efficientnet, mobilenet, cifar10, imagenet, batch normalization, batchnorm, relu, sigmoid, evolution, architecture, transfer, image classification, supervised learning, population, activation, normalization, google, deepmind",
    "scraped_at": 1684582658.9323876,
    "genre": "Science",
    "views": "2583",
    "desc": "Normalization and activation layers have seen a long history of hand-crafted variants with various results. This paper proposes an evolutionary search to determine the ultimate, final and best combined normalization-activation layer... in a very specific setting.\\\\n\\\\nhttps://arxiv.org/abs/2004.02967\\\\n\\\\nAbstract:\\\\nNormalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing them separately, we unify them into a single computation graph, and evolve its structure starting from low-level primitives. Our layer search algorithm leads to the discovery of EvoNorms, a set of new normalization-activation layers that go beyond existing design patterns. Several of these layers enjoy the property of being independent from the batch statistics. Our experiments show that EvoNorms not only excel on a variety of image classification models including ResNets, MobileNets and EfficientNets, but also transfer well to Mask R-CNN for instance segmentation and BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers by a significant margin in many cases.\\\\n\\\\nAuthors: Hanxiao Liu, Andrew Brock, Karen Simonyan, Quoc V. Le\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1837",
    "uploadDate": "2020-04-09",
    "thumbnail_url": "https://i.ytimg.com/vi/klPuEHCKG9M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DRy_Mr732yA",
    "title": "[Drama] Who invented Contrast Sets?",
    "tags": "deep learning, machine learning, nlp, natural language processing, arxiv, twitter, drama, credit, related, lipton, gardner, counterfactual, augmentation, plagiarism",
    "scraped_at": 1684582659.0083704,
    "genre": "Science",
    "views": "2674",
    "desc": "Funny Twitter spat between researchers arguing who was the first to invent an idea that has probably been around since 1990 :D\\\\n\\\\nReferences:\\\\nhttps://arxiv.org/abs/2004.02709\\\\nhttps://twitter.com/nlpmattg/status/1247326213296672768\\\\nhttps://arxiv.org/abs/1909.12434\\\\nhttps://twitter.com/zacharylipton/status/1247357810410762240\\\\nhttps://twitter.com/nlpmattg/status/1247373386839252992\\\\nhttps://twitter.com/zacharylipton/status/1247383141075083267\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "570",
    "uploadDate": "2020-04-08",
    "thumbnail_url": "https://i.ytimg.com/vi/DRy_Mr732yA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qeEO2GECQk0",
    "title": "Evaluating NLP Models via Contrast Sets",
    "tags": "deep learning, machine learning, nlp, natural language processing, arxiv, attention, evaluation, cheat, easy, hard, adversarial, counterfactual, hand",
    "scraped_at": 1684582660.0483549,
    "genre": "Science",
    "views": "2215",
    "desc": "Current NLP models are often \\\\\"",
    "lengthSeconds": "1139",
    "uploadDate": "2020-04-07",
    "thumbnail_url": "https://i.ytimg.com/vi/qeEO2GECQk0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8wkgDnNxiVs",
    "title": "POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Solutions",
    "tags": "deep learning, machine learning, arxiv, evolution, reinforcement learning, neat, open",
    "scraped_at": 1684582660.1343544,
    "genre": "Science",
    "views": "2964",
    "desc": "From the makers of Go-Explore, POET is a mixture of ideas from novelty search, evolutionary methods, open-ended learning and curriculum learning.\\\\n\\\\nhttps://arxiv.org/abs/1901.01753\\\\n\\\\nAbstract:\\\\nWhile the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. Our results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.\\\\n\\\\nAuthors: Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2029",
    "uploadDate": "2020-04-06",
    "thumbnail_url": "https://i.ytimg.com/vi/8wkgDnNxiVs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=awyuuJoHawo",
    "title": "Dream to Control: Learning Behaviors by Latent Imagination",
    "tags": "deep learning, machine learning, arxiv, google, rnn, recurrent, reinforcement learning, deep reinforcement learning, imagination, latent space, world model, control, deepmind, deep mind",
    "scraped_at": 1684582661.6434834,
    "genre": "Science",
    "views": "6251",
    "desc": "Dreamer is a new RL agent by DeepMind that learns a continuous control task through forward-imagination in latent space.\\\\n\\\\nhttps://arxiv.org/abs/1912.01603\\\\nVideos: https://dreamrl.github.io/\\\\n\\\\nAbstract:\\\\nLearned world models summarize an agent\\'s experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.\\\\n\\\\nAuthors: Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1358",
    "uploadDate": "2020-04-03",
    "thumbnail_url": "https://i.ytimg.com/vi/awyuuJoHawo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XdpF9ZixIbI",
    "title": "Can we Contain Covid-19 without Locking-down the Economy?",
    "tags": "machine learning, epidemiology, worst case, statistics, hypothesis test, covid, corona, coronavirus",
    "scraped_at": 1684582661.7274835,
    "genre": "Science",
    "views": "1030",
    "desc": "My thoughts on the let-the-young-get-infected argument.\\\\n\\\\nhttps://medium.com/amnon-shashua/can-we-contain-covid-19-without-locking-down-the-economy-2a134a71873f\\\\n\\\\nAbstract:\\\\nIn this article, we present an analysis of a risk-based selective quarantine model where the population is divided into low and high-risk groups. The high-risk group is quarantined until the low-risk group achieves herd-immunity. We tackle the question of whether this model is safe, in the sense that the health system can contain the number of low-risk people that require severe ICU care (such as life support systems).\\\\n\\\\nAuthors: Shai Shalev-Shwartz, Amnon Shashua\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1336",
    "uploadDate": "2020-04-02",
    "thumbnail_url": "https://i.ytimg.com/vi/XdpF9ZixIbI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lqtlua-Ylts",
    "title": "State-of-Art-Reviewing: A Radical Proposal to Improve Scientific Publication",
    "tags": "deep learning, machine learning, nlp, natural language processing, arxiv, attention, peer review, automate, distributed, scalable, neurips, score, objective",
    "scraped_at": 1684582660.22438,
    "genre": "Science",
    "views": "3038",
    "desc": "Peer Review is outdated and ineffective. SOAR is a new and revolutionary way to distribute scientific reviewing and scale to the new age of faster, better and more significant research.\\\\n\\\\nhttps://arxiv.org/abs/2003.14415\\\\n\\\\nAbstract:\\\\nPeer review forms the backbone of modern scientific manuscript evaluation. But after two hundred and eighty-nine years of egalitarian service to the scientific community, does this protocol remain fit for purpose in 2020? In this work, we answer this question in the negative (strong reject, high confidence) and propose instead State-Of-the-Art Review (SOAR), a neoteric reviewing pipeline that serves as a \\'plug-and-play\\' replacement for peer review. At the heart of our approach is an interpretation of the review process as a multi-objective, massively distributed and extremely-high-latency optimisation, which we scalarise and solve efficiently for PAC and CMT-optimal solutions. We make the following contributions: (1) We propose a highly scalable, fully automatic methodology for review, drawing inspiration from best-practices from premier computer vision and machine learning conferences; (2) We explore several instantiations of our approach and demonstrate that SOAR can be used to both review prints and pre-review pre-prints; (3) We wander listlessly in vain search of catharsis from our latest rounds of savage CVPR rejections.\\\\n\\\\nAuthors: Samuel Albanie, Jaime Thewmore, Robert McCraith, Joao F. Henriques\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "761",
    "uploadDate": "2020-04-01",
    "thumbnail_url": "https://i.ytimg.com/vi/lqtlua"
  },
  {
    "link": "watch?v=U3zmekzQ8WQ",
    "title": "Agent57: Outperforming the Atari Human Benchmark",
    "tags": "deep learning, machine learning, arxiv, google, rnn, recurrent, deepmind, r2d2, ngu, reinforcement learning, deep q learning, replay buffer, exploration, exploitation, tradeoff, policy, lstm, atari",
    "scraped_at": 1684582661.807515,
    "genre": "Science",
    "views": "8332",
    "desc": "DeepMind\\'s Agent57 is the first RL agent to outperform humans in all 57 Atari benchmark games. It extends previous algorithms like Never Give Up and R2D2 by meta-learning the exploration-exploitation tradeoff controls.\\\\n\\\\nhttps://arxiv.org/abs/2003.13350\\\\nhttps://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark\\\\n\\\\nAbstract:\\\\nAtari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.\\\\n\\\\nAuthors: Adri\\xc3\\xa0 Puigdom\\xc3\\xa8nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Charles Blundell\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1928",
    "uploadDate": "2020-03-31",
    "thumbnail_url": "https://i.ytimg.com/vi/U3zmekzQ8WQ/hqdefault.jpg"
  },
  {
    "link": "watch?v=lmAj0SU_bW0",
    "title": "Axial Attention & MetNet: A Neural Weather Model for Precipitation Forecasting",
    "tags": "deep learning, machine learning, arxiv, google, attention mechanism, attention, transformer, rnn, recurrent, weather, long",
    "scraped_at": 1684582660.3073547,
    "genre": "Science",
    "views": "2968",
    "desc": "MetNet is a predictive neural network model for weather prediction. It uses axial attention to capture long-range dependencies. Axial attention decomposes attention layers over images into row-attention and column-attention in order to save memory and computation.\\\\n\\\\nhttps://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html\\\\nhttps://arxiv.org/abs/1912.12180\\\\n\\\\nAbstract:\\\\nWeather forecasting is a long standing scientific challenge with direct social and economic impact. The task is suitable for deep neural networks due to vast amounts of continuously collected data and a rich spatial and temporal structure that presents long range dependencies. We introduce MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km2 and at the temporal resolution of 2 minutes with a latency in the order of seconds. MetNet takes as input radar and satellite data and forecast lead time and produces a probabilistic precipitation map. The architecture uses axial self-attention to aggregate the global context from a large input patch corresponding to a million square kilometers. We evaluate the performance of MetNet at various precipitation thresholds and find that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to 8 hours on the scale of the continental United States.\\\\n\\\\nAuthors: Casper Kaae S\\xc3\\xb8nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver,Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1953",
    "uploadDate": "2020-03-30",
    "thumbnail_url": "https://i.ytimg.com/vi/lmAj0SU_bW0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wAgO2WZzjn4",
    "title": "[Rant] coronavirus",
    "tags": "corona, covid, covid19, lockdown, social distancing",
    "scraped_at": 1684582660.3843548,
    "genre": "Science",
    "views": "5625",
    "desc": "A rant about toilet paper and lockdowns.\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1147",
    "uploadDate": "2020-03-25",
    "thumbnail_url": "https://i.ytimg.com/vi/wAgO2WZzjn4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=H3Bhlan0mE0",
    "title": "Online Education - How I Make My Videos",
    "tags": "deep learning, machine learning, online video, university, online, create, lecture",
    "scraped_at": 1684582660.4673545,
    "genre": "Science",
    "views": "3392",
    "desc": "Just a short overview of tools I use to make my videos.\\\\n\\\\nOneNote - https://www.onenote.com\\\\niSpring Free Cam - https://www.ispringsolutions.com/ispring-cam\\\\nShotcut - https://shotcut.org\\\\nSlack - https://slack.com\\\\nRocketChat - https://rocket.chat\\\\nZoom - https://zoom.us\\\\nJitsi - https://jitsi.org\\\\nGDocs - https://www.google.com/docs/about\\\\nPiazza - https://piazza.com\\\\nCMT - https://cmt3.research.microsoft.com/About\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "451",
    "uploadDate": "2020-03-23",
    "thumbnail_url": "https://i.ytimg.com/vi/H3Bhlan0mE0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=p3sAF3gVMMA",
    "title": "Deep Learning for Symbolic Mathematics",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, attention mechanism, attention, transformer, rnn, recurrent, seq2seq, facebook, fair, research, math, integral, ode",
    "scraped_at": 1684582665.09158,
    "genre": "Science",
    "views": "5242",
    "desc": "This model solves integrals and ODEs by doing seq2seq!\\\\n\\\\nhttps://arxiv.org/abs/1912.01412\\\\nhttps://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/\\\\n\\\\nAbstract:\\\\nNeural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.\\\\n\\\\nAuthors: Guillaume Lample, Fran\\xc3\\xa7ois Charton\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1456",
    "uploadDate": "2020-02-24",
    "thumbnail_url": "https://i.ytimg.com/vi/p3sAF3gVMMA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JPX_jSZtszY",
    "title": "NeurIPS 2020 Changes to Paper Submission Process",
    "tags": "machine learning, deep learning, phd, papers, neurips, nips, conference, submission, society, ethics",
    "scraped_at": 1684582661.8894835,
    "genre": "Science",
    "views": "4182",
    "desc": "My thoughts on the changes to the paper submission process for NeurIPS 2020.\\\\n\\\\nThe main new changes are:\\\\n1. ACs can desk reject papers\\\\n2. All authors have to be able to review if asked\\\\n3. Resubmissions from other conferences must be marked and a summary of changes since the last submission must be provided\\\\n4. Borader societal / ethical impact must be discussed\\\\n5. Upon acceptance, all papers must link to an explanatory video and the PDFs for slides and poster\\\\n\\\\nhttps://neurips.cc/Conferences/2020/CallForPapers\\\\nhttps://youtu.be/361h6lHZGDg\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "609",
    "uploadDate": "2020-02-21",
    "thumbnail_url": "https://i.ytimg.com/vi/JPX_jSZtszY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=9Kec_7WFyp0",
    "title": "Growing Neural Cellular Automata",
    "tags": "machine learning, deep learning, cellular automata, game of life, conway, google, distill, interactive, colab, local, global, update",
    "scraped_at": 1684582660.5533555,
    "genre": "Science",
    "views": "17012",
    "desc": "The Game of Life on steroids! This model learns to grow complex patterns in an entirely local way. Each cell is trained to listen to its neighbors and update itself in a way such that, collectively, an overall goal is reached. Fascinating and interactive!\\\\n\\\\nhttps://distill.pub/2020/growing-ca/\\\\nhttps://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\\\\n\\\\nAbstract:\\\\nMost multicellular organisms begin their life as a single egg cell - a single cell whose progeny reliably self-assemble into highly complex anatomies with many organs and tissues in precisely the same arrangement each time. The ability to build their own bodies is probably the most fundamental skill every living creature possesses. Morphogenesis (the process of an organism\\xe2\\x80\\x99s shape development) is one of the most striking examples of a phenomenon called self-organisation. Cells, the tiny building blocks of bodies, communicate with their neighbors to decide the shape of organs and body plans, where to grow each organ, how to interconnect them, and when to eventually stop. Understanding the interplay of the emergence of complex outcomes from simple rules and homeostatic 1 feedback loops is an active area of research. What is clear is that evolution has learned to exploit the laws of physics and computation to implement the highly robust morphogenetic software that runs on genome-encoded cellular hardware.\\\\n\\\\nThis process is extremely robust to perturbations. Even when the organism is fully developed, some species still have the capability to repair damage - a process known as regeneration. Some creatures, such as salamanders, can fully regenerate vital organs, limbs, eyes, or even parts of the brain! Morphogenesis is a surprisingly adaptive process. Sometimes even a very atypical development process can result in a viable organism - for example, when an early mammalian embryo is cut in two, each half will form a complete individual - monozygotic twins!\\\\n\\\\nThe biggest puzzle in this field is the question of how the cell collective knows what to build and when to stop. The sciences of genomics and stem cell biology are only part of the puzzle, as they explain the distribution of specific components in each cell, and the establishment of different types of cells. While we know of many genes that are required for the process of regeneration, we still do not know the algorithm that is sufficient for cells to know how to build or remodel complex organs to a very specific anatomical end-goal. Thus, one major lynch-pin of future work in biomedicine is the discovery of the process by which large-scale anatomy is specified within cell collectives, and how we can rewrite this information to have rational control of growth and form. It is also becoming clear that the software of life possesses numerous modules or subroutines, such as \\xe2\\x80\\x9cbuild an eye here\\xe2\\x80\\x9d, which can be activated with simple signal triggers. Discovery of such subroutines and a mapping out of the developmental logic is a new field at the intersection of developmental biology and computer science. An important next step is to try to formulate computational models of this process, both to enrich the conceptual toolkit of biologists and to help translate the discoveries of biology into better robotics and computational technology.\\\\n\\\\nImagine if we could design systems of the same plasticity and robustness as biological life: structures and machines that could grow and repair themselves. Such technology would transform the current efforts in regenerative medicine, where scientists and clinicians seek to discover the inputs or stimuli that could cause cells in the body to build structures on demand as needed. To help crack the puzzle of the morphogenetic code, and also exploit the insights of biology to create self-repairing systems in real life, we try to replicate some of the desired properties in an in silico experiment.\\\\n\\\\nAuthors: Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "948",
    "uploadDate": "2020-02-12",
    "thumbnail_url": "https://i.ytimg.com/vi/9Kec_7WFyp0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tC01FRB0M7w",
    "title": "Turing-NLG, DeepSpeed and the ZeRO optimizer",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, attention mechanism, attention, transformer, seq2seq, bert, long sequence, memory, gpt",
    "scraped_at": 1684582661.9705164,
    "genre": "Science",
    "views": "10325",
    "desc": "Microsoft has trained a 17-billion parameter language model that achieves state-of-the-art perplexity. This video takes a look at the ZeRO optimizer that enabled this breakthrough. ZeRO allows you to do model- and data-parallelism without having huge cuts in training speed.\\\\n\\\\nhttps://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\\\\nhttps://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/\\\\nhttps://github.com/microsoft/DeepSpeed\\\\nhttps://arxiv.org/abs/1910.02054\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1278",
    "uploadDate": "2020-02-11",
    "thumbnail_url": "https://i.ytimg.com/vi/tC01FRB0M7w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vB_hQ5NmtPs",
    "title": "[Interview] Mark Ledwich - Algorithmic Extremism: Examining YouTube's Rabbit Hole of Radicalization",
    "tags": "machine learning, youtube, recommendation, algorithm, extremism, alt right, pipeline, pathway, mainstream, radicalization",
    "scraped_at": 1684582659.0873897,
    "genre": "Science",
    "views": "2182",
    "desc": "Interview with one of the authors of a widely reported study on YouTube\\'s recommendation engine and where it leads its users.\\\\n\\\\nhttps://arxiv.org/abs/1912.11211\\\\nhttps://www.recfluence.net/\\\\nhttps://github.com/markledwich2/Recfluence\\\\nhttps://www.patreon.com/ledwich\\\\n\\\\nAbstract:\\\\nThe role that YouTube and its behind-the-scenes recommendation algorithm plays in encouraging online radicalization has been suggested by both journalists and academics alike. This study directly quantifies these claims by examining the role that YouTube\\'s algorithm plays in suggesting radicalized content. After categorizing nearly 800 political channels, we were able to differentiate between political schemas in order to analyze the algorithm traffic flows out and between each group. After conducting a detailed analysis of recommendations received by each channel type, we refute the popular radicalization claims. To the contrary, these data suggest that YouTube\\'s recommendation algorithm actively discourages viewers from visiting radicalizing or extremist content. Instead, the algorithm is shown to favor mainstream media and cable news content over independent YouTube channels with slant towards left-leaning or politically neutral channels. Our study thus suggests that YouTube\\'s recommendation algorithm fails to promote inflammatory or radicalized content, as previously claimed by several outlets.\\\\n\\\\nAuthors: Mark Ledwich, Anna Zaitsev\"",
    "lengthSeconds": "1562",
    "uploadDate": "2020-01-27",
    "thumbnail_url": "https://i.ytimg.com/vi/vB_hQ5NmtPs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=i4H0kjxrias",
    "title": "Reformer: The Efficient Transformer",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, google, attention mechanism, attention, transformer, seq2seq, bert, memory, lsh, locality sensitive hashing, reversible, revertible, flow, long sequence",
    "scraped_at": 1684582660.6363573,
    "genre": "Science",
    "views": "18304",
    "desc": "The Transformer for the masses! Reformer solves the biggest problem with the famous Transformer model: Its huge resource requirements. By cleverly combining Locality Sensitive Hashing and ideas from Reversible Networks, the classically huge footprint of the Transformer is drastically reduced. Not only does that mean the model uses less memory, but it can process much longer input sequences, up to 16K tokens with just 16gb of memory!\\\\n\\\\nhttps://arxiv.org/abs/2001.04451\\\\nhttps://ai.googleblog.com/2020/01/reformer-efficient-transformer.html\\\\n\\\\nAbstract:\\\\nLarge Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(LlogL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\\\\n\\\\nAuthors: Nikita Kitaev, \\xc5\\x81ukasz Kaiser, Anselm Levskaya\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1752",
    "uploadDate": "2020-01-22",
    "thumbnail_url": "https://i.ytimg.com/vi/i4H0kjxrias/maxresdefault.jpg"
  },
  {
    "link": "watch?v=EbFosdOi5SY",
    "title": "Go-Explore: a New Approach for Hard-Exploration Problems",
    "tags": "machine learning, ml, reinforcement learning, rl, ai, artificial intelligence, uber, exploration, hard exploration, research, novelty, graph, robustify, explore, montezuma, montezuma",
    "scraped_at": 1684582660.7173557,
    "genre": "Science",
    "views": "3139",
    "desc": "This algorithm solves the hardest games in the Atari suite and makes it look so easy! This modern version of Dijkstra\\'s shortest path algorithm is outperforming everything else by orders of magnitude, and all based on random exploration.\\\\n\\\\nhttps://arxiv.org/abs/1901.10995\\\\nhttps://eng.uber.com/go-explore/\\\\nhttps://github.com/uber-research/go-explore\\\\n\\\\nAbstract:\\\\nA grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma\\'s Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma\\'s Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma\\'s Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of \\\\\"",
    "lengthSeconds": "1142",
    "uploadDate": "2020-01-10",
    "thumbnail_url": "https://i.ytimg.com/vi/EbFosdOi5SY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=waK7AD-AEyc",
    "title": "NeurIPS 19 Poster Session",
    "tags": "machine learning, conference, posters, research, bubble",
    "scraped_at": 1684582660.8103576,
    "genre": "Science",
    "views": "4954",
    "desc": "I\\'m at the poster session and the amount of people here is just crazy\"",
    "lengthSeconds": "111",
    "uploadDate": "2019-12-11",
    "thumbnail_url": "https://i.ytimg.com/vi/waK7AD"
  },
  {
    "link": "watch?v=RrvC8YW0pT0",
    "title": "Reinforcement Learning Upside Down: Don't Predict Rewards -- Just Map Them to Actions",
    "tags": "rl, reinforcement learning, ai, artificial intelligence, udrl, schmidhuber, policy, value, reward",
    "scraped_at": 1684582662.0474834,
    "genre": "Science",
    "views": "9055",
    "desc": "Schmidhuber thinking outside the box! Upside-Down RL turns RL on its head and constructs a behavior function that uses the desired reward as an input. The new paradigm shows surprising performance compared to classic RL algorithms.\\\\n\\\\nAbstract:\\\\nWe transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this Upside Down RL (UDRL). Standard RL predicts rewards, while UDRL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. UDRL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. UDRL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [61] on first experiments with UDRL shows that even a pilot version of UDRL can outperform traditional baseline algorithms on certain challenging RL problems. We also introduce a related simple but general approach for teaching a robot to imitate humans. First videotape humans imitating the robot\\'s current behaviors, then let the robot learn through SL to map the videos (as input commands) to these behaviors, then let it generalize and imitate videos of humans executing previously unknown behavior. This Imitate-Imitator concept may actually explain why biological evolution has resulted in parents who imitate the babbling of their babies.\\\\n\\\\nAuthor: Juergen Schmidhuber\\\\n\\\\nhttps://arxiv.org/abs/1912.02875\\\\nhttps://arxiv.org/abs/1912.02877\"",
    "lengthSeconds": "1562",
    "uploadDate": "2019-12-10",
    "thumbnail_url": "https://i.ytimg.com/vi/RrvC8YW0pT0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Z6ea_AbnnCc",
    "title": "NeurIPS 2019",
    "tags": "machine learning, conference, ai, neurips, neurips2019, canada, research",
    "scraped_at": 1684582665.2016056,
    "genre": "Science",
    "views": "6613",
    "desc": "I\\'m at the 2019 conference on Neural Information Processing Systems in Vancouver, trying to register, but the line was just so long that I decided to bail :D\"",
    "lengthSeconds": "141",
    "uploadDate": "2019-12-08",
    "thumbnail_url": "https://i.ytimg.com/vi/Z6ea_AbnnCc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=We20YSAJZSE",
    "title": "MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
    "tags": "ml, ai, machine learning, reinforcement learning, deep rl, deepmind, google, alphago, alphazero, value function, policy, artificial intelligence, rl, deep reinforcement learning, model",
    "scraped_at": 1684582660.9012535,
    "genre": "Science",
    "views": "22098",
    "desc": "MuZero harnesses the power of AlphaZero, but without relying on an accurate environment model. This opens up planning-based reinforcement learning to entirely new domains, where such environment models aren\\'t available. The difference to previous work is that, instead of learning a model predicting future observations, MuZero predicts the future observations\\' latent representations, and thus learns to only represent things that matter to the task!\\\\n\\\\nAbstract:\\\\nConstructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.\\\\n\\\\nAuthors: Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver\\\\n\\\\nhttps://arxiv.org/abs/1911.08265\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1138",
    "uploadDate": "2019-11-21",
    "thumbnail_url": "https://i.ytimg.com/vi/We20YSAJZSE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=KXEEqcwXn8w",
    "title": "A neurally plausible model learns successor representations in partially observable environments",
    "tags": "ml, ai, machine learning, artificial ingelligence, deep learning, reinforcement learning, model",
    "scraped_at": 1684582662.134521,
    "genre": "Science",
    "views": "2865",
    "desc": "Successor representations are a mid-point between model-based and model-free reinforcement learning. This paper learns successor representation in environments where only incomplete information is available.\\\\n\\\\nAbstract:\\\\nAnimals need to devise strategies to maximize returns while interacting with their environment based on incoming noisy sensory observations. Task-relevant states, such as the agent\\'s location within an environment or the presence of a predator, are often not directly observable but must be inferred using available sensory information. Successor representations (SR) have been proposed as a middle-ground between model-based and model-free reinforcement learning strategies, allowing for fast value computation and rapid adaptation to changes in the reward function or goal locations. Indeed, recent studies suggest that features of neural responses are consistent with the SR framework. However, it is not clear how such representations might be learned and computed in partially observed, noisy environments. Here, we introduce a neurally plausible model using distributional successor features, which builds on the distributed distributional code for the representation and computation of uncertainty, and which allows for efficient value function computation in partially observed environments via the successor representation. We show that distributional successor features can support reinforcement learning in noisy environments in which direct learning of successful policies is infeasible.\\\\n\\\\nAuthors: Eszter Vertes, Maneesh Sahani\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2225",
    "uploadDate": "2019-11-07",
    "thumbnail_url": "https://i.ytimg.com/vi/KXEEqcwXn8w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Xc9Rkbg6IZA",
    "title": "SinGAN: Learning a Generative Model from a Single Natural Image",
    "tags": "ml, ai, machine learning, artificial ingelligence, gan, generative, image processing, deep learning, image editing, deep dream, style transfer, convolutional neural networks, generative adversarial networks, photoshop",
    "scraped_at": 1684582660.9894853,
    "genre": "Science",
    "views": "7929",
    "desc": "With just a single image as an input, this algorithm learns a generative model that matches the input image\\'s patch distribution at multiple scales and resolutions. This enables sampling of extremely realistic looking variations on the original image and much more.\\\\n\\\\nAbstract:\\\\nWe introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.\\\\n\\\\nAuthors: Tamar Rott Shaham, Tali Dekel, Tomer Michaeli\\\\n\\\\nhttps://arxiv.org/abs/1905.01164\\\\nhttps://github.com/tamarott/SinGAN\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "1099",
    "uploadDate": "2019-11-03",
    "thumbnail_url": "https://i.ytimg.com/vi/Xc9Rkbg6IZA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BTLCdge7uSQ",
    "title": "AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning",
    "tags": "ml, ai, machine learning, reinforcement learning, deep rl, deepmind, google, starcraft, alphastar, alphago, alphazero, value function, policy, vtrace, upgo, terran, protoss, zerg, build order, strategy, pointer network, transformer, league training, league, battlenet, artificial intelligence, bot, rl, deep reinforcement learning, model",
    "scraped_at": 1684582661.0714855,
    "genre": "Science",
    "views": "17386",
    "desc": "DeepMind\\'s new agent to tackle yet another Esport: Starcraft II. This agent uses deep reinforcement learning with a new technique, called League Training, to catapult itself to Grandmaster-level skill at playing this game.\\\\n\\\\nAbstract:\\\\nMany real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.\\\\n\\\\nAuthors: Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha\\xc3\\xabl Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, R\\xc3\\xa9mi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W\\xc3\\xbcnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver\\\\n\\\\nhttps://www.deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\\\\n\\\\nLinks:\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/yannic-kilcher\\\\nMinds: https://www.minds.com/ykilcher\"",
    "lengthSeconds": "2236",
    "uploadDate": "2019-11-02",
    "thumbnail_url": "https://i.ytimg.com/vi/BTLCdge7uSQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kOy49NqZeqI",
    "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
    "tags": "machine learning, ml, ai, artificial intellgence, deepmind, reinforcement learning, deep rl, a2c, a3c, actor, critic, distributed, scale, bias, off",
    "scraped_at": 1684582662.2124853,
    "genre": "Science",
    "views": "5545",
    "desc": "Policy Gradient RL on a massively distributed scale with theoretical guarantees!\\\\n\\\\nAbstract:\\\\nIn this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.\\\\n\\\\nAuthors: Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu\\\\n\\\\nhttps://arxiv.org/abs/1802.01561\\\\nhttps://github.com/deepmind/scalable_agent\"",
    "lengthSeconds": "1246",
    "uploadDate": "2019-11-01",
    "thumbnail_url": "https://i.ytimg.com/vi/kOy49NqZeqI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ctCv_NRpqvM",
    "title": "The Visual Task Adaptation Benchmark",
    "tags": "ml, machine learning, cnn, imagenet, pretraining, finetuning, fine",
    "scraped_at": 1684582661.1554835,
    "genre": "Science",
    "views": "1302",
    "desc": "This paper presents a new benchmark for Visual Task Adaptation (i.e. BERT for images) and investigates several baseline methods for doing so.\\\\n\\\\nAbstract:\\\\nRepresentation learning promises to unlock deep learning for the long tail of vision tasks without expansive labelled datasets. Yet, the absence of a unified yardstick to evaluate general visual representations hinders progress. Many sub-fields promise representations, but each has different evaluation protocols that are either too constrained (linear classification), limited in scope (ImageNet, CIFAR, Pascal-VOC), or only loosely related to representation quality (generation). We present the Visual Task Adaptation Benchmark (VTAB): a diverse, realistic, and challenging benchmark to evaluate representations. VTAB embodies one principle: good representations adapt to unseen tasks with few examples. We run a large VTAB study of popular algorithms, answering questions like: How effective are ImageNet representation on non-standard datasets? Are generative models competitive? Is self-supervision useful if one already has labels?\\\\n\\\\nAuthors: Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, Neil Houlsby\\\\n\\\\nhttps://arxiv.org/abs/1910.04867\\\\nhttps://github.com/google-research/task_adaptation\"",
    "lengthSeconds": "1297",
    "uploadDate": "2019-10-31",
    "thumbnail_url": "https://i.ytimg.com/vi/ctCv_NRpqvM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=69IjNZaoeao",
    "title": "LeDeepChef \ud83d\udc68\u200d\ud83c\udf73 Deep Reinforcement Learning Agent for Families of Text-Based Games",
    "tags": "ml, machine learning, reinforcement learning, recipe, text",
    "scraped_at": 1684582662.2934837,
    "genre": "Science",
    "views": "2181",
    "desc": "The AI cook is here! This agent learns to play a text-based game where the goal is to prepare a meal according to a recipe. Challenges? Many! The number of possible actions is huge, ingredients change and can include ones never seen before, you need to navigate rooms, use tools, manage an inventory and sequence everything correctly and all of this from a noisy textual description that the game engine throws at you. This paper mixes supervised explicit training with reinforcement learning in order to solve this task.\\\\n\\\\nAbstract:\\\\nWhile Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent--LeDeepChef--that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research\\'s \\\\\"",
    "lengthSeconds": "1822",
    "uploadDate": "2019-10-15",
    "thumbnail_url": "https://i.ytimg.com/vi/69IjNZaoeao/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BK3rv0MQMwY",
    "title": "[News] The Siraj Raval Controversy",
    "tags": "machine learning, siraj, controversy, scam, scammer, fraud, plagiarism, plagiarized, course, refund, policy, ai, online, hype, credit, attribution, paper, scandal, news, twitter, neural qubit, intellectual property",
    "scraped_at": 1684582661.2355173,
    "genre": "Science",
    "views": "27897",
    "desc": "Popular ML YouTuber Siraj Raval is in the middle of not just one, but two controversies: First, a lot of students of his 200$ online-course have accused him of breaking major promises he made when advertising the course and denying them refunds. Second, his paper on \\\\\"",
    "lengthSeconds": "563",
    "uploadDate": "2019-10-14",
    "thumbnail_url": "https://i.ytimg.com/vi/BK3rv0MQMwY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rvr143crpuU",
    "title": "Accelerating Deep Learning by Focusing on the Biggest Losers",
    "tags": "machine learning, deep learning, dl, neural network, training, convergence, loss, importance, speed",
    "scraped_at": 1684582661.313515,
    "genre": "Science",
    "views": "2461",
    "desc": "What if you could reduce the time your network trains by only training on the hard examples? This paper proposes to select samples with high loss and only train on those in order to speed up training.\\\\n\\\\nAbstract:\\\\nThis paper introduces Selective-Backprop, a technique that accelerates the training of deep neural networks (DNNs) by prioritizing examples with high loss at each iteration. Selective-Backprop uses the output of a training example\\'s forward pass to decide whether to use that example to compute gradients and update parameters, or to skip immediately to the next example. By reducing the number of computationally-expensive backpropagation steps performed, Selective-Backprop accelerates training. Evaluation on CIFAR10, CIFAR100, and SVHN, across a variety of modern image models, shows that Selective-Backprop converges to target error rates up to 3.5x faster than with standard SGD and between 1.02--1.8x faster than a state-of-the-art importance sampling approach. Further acceleration of 26% can be achieved by using stale forward pass results for selection, thus also skipping forward passes of low priority examples.\\\\n\\\\nAuthors: Angela H. Jiang, Daniel L.-K. Wong, Giulio Zhou, David G. Andersen, Jeffrey Dean, Gregory R. Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C. Lipton, Padmanabhan Pillai\\\\n\\\\nhttps://arxiv.org/abs/1910.00762\"",
    "lengthSeconds": "1509",
    "uploadDate": "2019-10-07",
    "thumbnail_url": "https://i.ytimg.com/vi/rvr143crpuU/hqdefault.jpg"
  },
  {
    "link": "watch?v=MIEA8azwu1k",
    "title": "DEEP LEARNING MEME REVIEW - Episode 1",
    "tags": "deep learning, memes, meme review, artificial intelligence, review, discussion, reaction, ai, machine learning, ml, dnn, gpu, deep neural network, ml memes, deep learning memes, machine learning memes, funny, gpus, classifier, hinton, turing award, bert, xlnet, optimization, error rate, culture, community, research",
    "scraped_at": 1684582661.3924832,
    "genre": "Comedy",
    "views": "31611",
    "desc": "The wait is finally over! Antonio and I discuss the best, funniest and dankest memes of the machine learning world. Join us for a laugh!\"",
    "lengthSeconds": "697",
    "uploadDate": "2019-09-05",
    "thumbnail_url": "https://i.ytimg.com/vi/MIEA8azwu1k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=nXGHJTtFYRU",
    "title": "Dynamic Routing Between Capsules",
    "tags": "machine learning, deep learning, capsules, capsule networks, google brain, hinton, jeff hinton, geoff hinton, routing, neural networks, convolution, convolutional neural networks, deep neural networks, cnns, mnist, multimnist, disentanglement, architecture, reconstruction, alternative, dnn, ml, ai, artificial intelligence, brain, visual system, classifier, image, nonlinearity, entities, objects, capsule, network",
    "scraped_at": 1684582662.3714833,
    "genre": "Science",
    "views": "8748",
    "desc": "Geoff Hinton\\'s next big idea! Capsule Networks are an alternative way of implementing neural networks by dividing each layer into capsules. Each capsule is responsible for detecting the presence and properties of one particular entity in the input sample. This information is then allocated dynamically to higher-level capsules in a novel and unconventional routing scheme. While Capsule Networks are still in their infancy, they are an exciting and promising new direction.\\\\n\\\\nAbstract:\\\\nA capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.\\\\n\\\\nAuthors: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton\\\\n\\\\nhttps://arxiv.org/abs/1710.09829\\\\n\\\\n\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/10a5ui845DOJ/\"",
    "lengthSeconds": "2526",
    "uploadDate": "2019-09-04",
    "thumbnail_url": "https://i.ytimg.com/vi/nXGHJTtFYRU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-MCYbmU9kfg",
    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, google, attention mechanism, attention, transformer, tensor2tensor, rnn, recurrent, seq2seq, bert, unsupervised, squad, wordpiece, embeddings, language, language modeling, attention layers, bidirectional, elmo, word vectors, pretrained, fine tuning",
    "scraped_at": 1684582661.4734836,
    "genre": "Science",
    "views": "20784",
    "desc": "This paper shows that the original BERT model, if trained correctly, can outperform all of the improvements that have been proposed lately, raising questions about the necessity and reasoning behind these.\\\\n\\\\nAbstract:\\\\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\\\\n\\\\nAuthors: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov\\\\n\\\\nhttps://arxiv.org/abs/1907.11692\\\\n\\\\n\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/10a5ui845DOJ/\"",
    "lengthSeconds": "1154",
    "uploadDate": "2019-09-03",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=AR3W-nfcDe4",
    "title": "Auditing Radicalization Pathways on YouTube",
    "tags": "machine learning, data science, empirical, study, youtube, radicalization, alt",
    "scraped_at": 1684582663.6775815,
    "genre": "Science",
    "views": "1172",
    "desc": "This paper claims that there is a radicalization pipeline on YouTube pushing people towards the Alt-Right, backing up their claims with empirical analysis of channel recommendations and commenting behavior. I suggest that there is a much simpler explanation of this data: A basic diffusion process.\\\\n\\\\nAbstract:\\\\nNon-profits and the media claim there is a radicalization pipeline on YouTube. Its content creators would sponsor fringe ideas, and its recommender system would steer users towards edgier content. Yet, the supporting evidence for this claim is mostly anecdotal, and there are no proper measurements of the influence of YouTube\\'s recommender system. In this work, we conduct a large scale audit of user radicalization on YouTube. We analyze 331,849 videos of\\xc2\\xa0360 channels which we broadly classify into: control, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right ---channels in the I.D.W. and the Alt-lite would be gateways to fringe far-right ideology, here represented by Alt-right channels. Processing more than\\xc2\\xa079M\\xc2\\xa0comments, we show that the three communities increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube\\'s recommendation algorithm, looking at more than\\xc2\\xa02M\\xc2\\xa0million recommendations for videos and channels between May and July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels via recommendations and that Alt-right channels may be reached from both I.D.W. and Alt-lite channels. Overall, we paint a comprehensive picture of user radicalization on YouTube and provide methods to transparently audit the platform and its recommender system.\\\\n\\\\nAuthors: Manoel Horta Ribeiro,\\xc2\\xa0Raphael Ottoni,\\xc2\\xa0Robert West,\\xc2\\xa0Virg\\xc3\\xadlio A. F. Almeida,\\xc2\\xa0Wagner Meira\\\\n\\\\nhttps://arxiv.org/abs/1908.08313\\\\n\\\\nYouTube: https://www.youtube.com/c/yannickilcher\\\\nTwitter: https://twitter.com/ykilcher\\\\nMinds: https://www.minds.com/ykilcher\\\\nBitChute: https://www.bitchute.com/channel/10a5ui845DOJ/\"",
    "lengthSeconds": "2593",
    "uploadDate": "2019-08-28",
    "thumbnail_url": "https://i.ytimg.com/vi/AR3W"
  },
  {
    "link": "watch?v=wZWn7Hm8osA",
    "title": "Gauge Equivariant Convolutional Networks and the Icosahedral CNN",
    "tags": "machine learning, deep learning, artificial intelligence, ai, data science, convolution, convolutional neural networks, cnn, manifolds, curvature, parallel transport, gauge, gauge transformation, icosahedron, weight sharing, coordinate frame, invariant, coordinate system, equivariance, sphere, spherical",
    "scraped_at": 1684582662.4484832,
    "genre": "Science",
    "views": "6055",
    "desc": "Ever wanted to do a convolution on a Klein Bottle? This paper defines CNNs over manifolds such that they are independent of which coordinate frame you choose. Amazingly, this then results in an efficient practical method to achieve state-of-the-art in several tasks!\\\\n\\\\nhttps://arxiv.org/abs/1902.04615\\\\n\\\\nAbstract:\\\\nThe principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.\\\\n\\\\nAuthors: Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling\"",
    "lengthSeconds": "1310",
    "uploadDate": "2019-08-13",
    "thumbnail_url": "https://i.ytimg.com/vi/wZWn7Hm8osA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=H6Qiegq_36c",
    "title": "Processing Megapixel Images with Deep Attention-Sampling Models",
    "tags": "machine learning, deep learning, research, attention, attention sampling, attention model, attention distribution, megapixel images, large images, artificial intelligence, megapixel mnist, street sign dataset, monte carlo, speed, memory, cnn, convolutional neural networks, limited resources, ai, image recognition, image classifier",
    "scraped_at": 1684582663.7585824,
    "genre": "Science",
    "views": "3292",
    "desc": "Current CNNs have to downsample large images before processing them, which can lose a lot of detail information. This paper proposes attention sampling, which learns to selectively process parts of any large image in full resolution, while discarding uninteresting bits. This leads to enormous gains in speed and memory consumption.\\\\n\\\\nhttps://arxiv.org/abs/1905.03711\\\\n\\\\nAbstract:\\\\nExisting deep architectures cannot operate on very large signals such as megapixel images due to computational and memory constraints. To tackle this limitation, we propose a fully differentiable end-to-end trainable model that samples and processes only a fraction of the full resolution input image. The locations to process are sampled from an attention distribution computed from a low resolution view of the input. We refer to our method as attention sampling and it can process images of several megapixels with a standard single GPU setup. We show that sampling from the attention distribution results in an unbiased estimator of the full model with minimal variance, and we derive an unbiased estimator of the gradient that we use to train our model end-to-end with a normal SGD procedure. This new method is evaluated on three classification tasks, where we show that it allows to reduce computation and memory footprint by an order of magnitude for the same accuracy as classical architectures. We also show the consistency of the sampling that indeed focuses on informative parts of the input images.\\\\n\\\\nAuthors: Angelos Katharopoulos, Fran\\xc3\\xa7ois Fleuret\"",
    "lengthSeconds": "1032",
    "uploadDate": "2019-08-12",
    "thumbnail_url": "https://i.ytimg.com/vi/H6Qiegq_36c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1L83tM8nwHU",
    "title": "Manifold Mixup: Better Representations by Interpolating Hidden States",
    "tags": "deep learning, neural networks, adversarial examples, machine learning, bengio, classification, smooth, flat representations, ai, artificial intelligence, supervised learning, regluarization, regularizer, hidden representations, overconfidence",
    "scraped_at": 1684582662.5264835,
    "genre": "Science",
    "views": "13184",
    "desc": "Standard neural networks suffer from problems such as un-smooth classification boundaries and overconfidence. Manifold Mixup is an easy regularization technique that rectifies these problems. It works by interpolating hidden representations of different data points and then train them to predict equally interpolated labels.\\\\n\\\\nhttps://arxiv.org/abs/1806.05236\\\\n\\\\nAbstract:\\\\nDeep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.\\\\n\\\\nAuthors:\\\\nVikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio\"",
    "lengthSeconds": "1268",
    "uploadDate": "2019-08-09",
    "thumbnail_url": "https://i.ytimg.com/vi/1L83tM8nwHU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Qk4lJdp7ZAs",
    "title": "Learning World Graphs to Accelerate Hierarchical Reinforcement Learning",
    "tags": "deep learning, reinforcement learning, deep reinforcement learning, world model, hierarchical reinforcement learning, planning, salesforce, research, machine learning, navigation, pivot states, ai, artificial intelligence",
    "scraped_at": 1684582663.838607,
    "genre": "Science",
    "views": "3112",
    "desc": "The goal of hierarchical reinforcement learning is to divide a task into different levels of coarseness with the top-level agent planning only over a high-level view of the world and each subsequent layer having a more detailed view. This paper proposes to learn a set of important states as well as their connections to each other as a high-level abstraction.\\\\n\\\\nhttps://arxiv.org/abs/1907.00664\\\\n\\\\nAbstract:\\\\nIn many real-world scenarios, an autonomous agent often encounters various tasks within a single complex environment. We propose to build a graph abstraction over the environment structure to accelerate the learning of these tasks. Here, nodes are important points of interest (pivotal states) and edges represent feasible traversals between them. Our approach has two stages. First, we jointly train a latent pivotal state model and a curiosity-driven goal-conditioned policy in a task-agnostic manner. Second, provided with the information from the world graph, a high-level Manager quickly finds solution to new tasks and expresses subgoals in reference to pivotal states to a low-level Worker. The Worker can then also leverage the graph to easily traverse to the pivotal states of interest, even across long distance, and explore non-locally. We perform a thorough ablation study to evaluate our approach on a suite of challenging maze tasks, demonstrating significant advantages from the proposed framework over baselines that lack world graph knowledge in terms of performance and efficiency.\\\\n\\\\nAuthors: Wenling Shang, Alex Trott, Stephan Zheng, Caiming Xiong, Richard Socher\"",
    "lengthSeconds": "1119",
    "uploadDate": "2019-08-08",
    "thumbnail_url": "https://i.ytimg.com/vi/Qk4lJdp7ZAs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZAW9EyNo2fw",
    "title": "Reconciling modern machine learning and the bias-variance trade-off",
    "tags": "machine learning, bias, variance, tradeoff, generalization, overfitting, interpolation, parameters, model class, complexity, deep learning, neural networks, overparameterization, erm, random fourier features",
    "scraped_at": 1684582662.6074834,
    "genre": "Science",
    "views": "10105",
    "desc": "It turns out that the classic view of generalization and overfitting is incomplete! If you add parameters beyond the number of points in your dataset, generalization performance might increase again due to the increased smoothness of overparameterized functions.\\\\n\\\\nAbstract:\\\\nThe question of generalization in machine learning---how algorithms are able to learn predictors from a training sample to make accurate predictions out-of-sample---is revisited in light of the recent breakthroughs in modern machine learning technology. \\\\nThe classical approach to understanding generalization is based on bias-variance trade-offs, where model complexity is carefully calibrated so that the fit on the training sample reflects performance out-of-sample. \\\\nHowever, it is now common practice to fit highly complex models like deep neural networks to data with (nearly) zero training error, and yet these interpolating predictors are observed to have good out-of-sample accuracy even for noisy data. \\\\nHow can the classical understanding of generalization be reconciled with these observations from modern machine learning practice? \\\\nIn this paper, we bridge the two regimes by exhibiting a new \\\\\"",
    "lengthSeconds": "1133",
    "uploadDate": "2019-08-05",
    "thumbnail_url": "https://i.ytimg.com/vi/ZAW9EyNo2fw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=l8JeokY5NsU",
    "title": "Conversation about Population-Based Methods (Re-upload)",
    "tags": "machine learning, ai, artificial intelligence, open ended learning, quality diversity, conference, icml, icml2019, tutorial, population",
    "scraped_at": 1684582663.9165826,
    "genre": "Science",
    "views": "1161",
    "desc": "Being interviewed by Connor Shorten of Henry AI Labs (https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw) on the topic of population-based methods and open-ended learning.\\\\n\\\\nTutorial: https://www.facebook.com/icml.imls/videos/481758745967365/\\\\nBook: https://www.amazon.com/dp/B00X57B4JG/\"",
    "lengthSeconds": "2674",
    "uploadDate": "2019-07-05",
    "thumbnail_url": "https://i.ytimg.com/vi/l8JeokY5NsU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=H5vpBCLo74U",
    "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
    "tags": "deep learning, machine learning, artificial intelligence, ai, nlp, natural language processing, bert, xlnet, transformer, transformer xl, attention, attention layer, language model, language modeling, pretraining, autoregressive, autoencoder, permutation, google, carnegie mellon, cmu, state of the art, masked language model",
    "scraped_at": 1684582663.9985826,
    "genre": "Science",
    "views": "20869",
    "desc": "Abstract:\\\\nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.\\\\n\\\\nAuthors: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le\\\\n\\\\nhttps://arxiv.org/abs/1906.08237\"",
    "lengthSeconds": "1805",
    "uploadDate": "2019-07-03",
    "thumbnail_url": "https://i.ytimg.com/vi/H5vpBCLo74U/hqdefault.jpg"
  },
  {
    "link": "watch?v=hkw-WDBipgo",
    "title": "Talking to companies at ICML19",
    "tags": "machine learning, conference, ai, artificial intelligence, industry, academia, deep learning, hardware, lidar, graphcore",
    "scraped_at": 1684582662.6874833,
    "genre": "Science",
    "views": "2095",
    "desc": "A short rant on sponsor companies at ICML and how to talk to them.\"",
    "lengthSeconds": "446",
    "uploadDate": "2019-06-13",
    "thumbnail_url": "https://i.ytimg.com/vi/hkw"
  },
  {
    "link": "watch?v=TFiZYA_JfJs",
    "title": "Population-Based Search and Open-Ended Algorithms",
    "tags": "machine learning, ai, artificial intelligence, open ended learning, quality diversity, conference, icml, icml2019, tutorial, population",
    "scraped_at": 1684582664.0806065,
    "genre": "Science",
    "views": "4243",
    "desc": "Comments on the ICML2019 tutorial on population-based search and open-ended learning.\\\\n\\\\nTalk: https://www.facebook.com/icml.imls/videos/481758745967365/\\\\nSlides: http://www.cs.uwyo.edu/~jeffclune/share/2019_06_10_ICML_Tutorial.pdf\\\\nBook: https://www.amazon.com/dp/B00X57B4JG/\\\\nEvent: https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4336\"",
    "lengthSeconds": "1004",
    "uploadDate": "2019-06-12",
    "thumbnail_url": "https://i.ytimg.com/vi/TFiZYA_JfJs/hqdefault.jpg"
  },
  {
    "link": "watch?v=EA96xh9qog0",
    "title": "I'm at ICML19 :)",
    "tags": "machine learning, conference, long beach, california, icml19, icml, artificial intelligence, ai, deep learning",
    "scraped_at": 1684582664.1575801,
    "genre": "Science",
    "views": "1365",
    "desc": "Short intro to the International Conference on Machine Learning in Long Beach, CA.\\\\nI\\'ll be making some updates from the conference.\"",
    "lengthSeconds": "60",
    "uploadDate": "2019-06-10",
    "thumbnail_url": "https://i.ytimg.com/vi/EA96xh9qog0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hMO6rbMAPew",
    "title": "Adversarial Examples Are Not Bugs, They Are Features",
    "tags": "machine learning, deep learning, adversarial examples, adversarial samples, pgd, projected gradient descent, vulnerabiliby, security, artificial intelligence, MIT, geometry, classifier, deep neural network, attack, convolutional neural networks, research, robust features, robust classifier, robust network, neural network",
    "scraped_at": 1684582664.2396064,
    "genre": "Science",
    "views": "10448",
    "desc": "Abstract:\\\\nAdversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.\\\\n\\\\nAuthors: Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry\\\\n\\\\nhttps://arxiv.org/abs/1905.02175\"",
    "lengthSeconds": "2420",
    "uploadDate": "2019-05-14",
    "thumbnail_url": "https://i.ytimg.com/vi/hMO6rbMAPew/hqdefault.jpg"
  },
  {
    "link": "watch?v=_N_nFzMtWkA",
    "title": "Reinforcement Learning, Fast and Slow",
    "tags": "machine learning, reinforcement learning, meta",
    "scraped_at": 1684582664.321612,
    "genre": "Science",
    "views": "3418",
    "desc": "Abstract:\\\\nDeep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient \\xe2\\x80\\x93 that is, it may simply be too slow \\xe2\\x80\\x93 to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.\\\\n\\\\nAuthors: Matthew Botvinick, Sam Ritter, Jane X. Wang, Zeb Kurth-Nelson, Charles Blundell, Demis Hassabis\\\\n\\\\nhttps://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30061-0\"",
    "lengthSeconds": "1445",
    "uploadDate": "2019-05-10",
    "thumbnail_url": "https://i.ytimg.com/vi/_N_nFzMtWkA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=F5mxzvgl_oU",
    "title": "S.H.E. - Search. Human. Equalizer.",
    "tags": "pantene, search, google, bias, machine learning, artificial intelligence, search engine, ranking, equality, diversity",
    "scraped_at": 1684582664.3986137,
    "genre": "Science",
    "views": "1186",
    "desc": "Short opinion on Pantene\\'s tool to de-bias Google search results.\\\\n\\\\nhttps://www.apnews.com/Business%20Wire/c53a0e8f5fe04bf68e8311f214c806cf\\\\nhttps://shetransforms.us/\"",
    "lengthSeconds": "283",
    "uploadDate": "2019-05-09",
    "thumbnail_url": "https://i.ytimg.com/vi/F5mxzvgl_oU/hqdefault.jpg"
  },
  {
    "link": "watch?v=3Tqp_B2G6u0",
    "title": "Blockwise Parallel Decoding for Deep Autoregressive Models",
    "tags": "machine learning, deep learning, transformers, nlp, natural language processing, ai, artificial intelligence, google brain, autoregressive, greedy decoding, inference, language model, speedup",
    "scraped_at": 1684582661.5494835,
    "genre": "Science",
    "views": "693",
    "desc": "https://arxiv.org/abs/1811.03115\\\\n\\\\nAbstract:\\\\nDeep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While common architecture classes such as recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, generation still remains an inherently sequential process. To overcome this limitation, we propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model. This allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel. We verify our approach empirically through a series of experiments using state-of-the-art self-attention models for machine translation and image super-resolution, achieving iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. In terms of wall-clock time, our fastest models exhibit real-time speedups of up to 4x over standard greedy decoding.\\\\n\\\\nAuthors: Mitchell Stern, Noam Shazeer, Jakob Uszkoreit\"",
    "lengthSeconds": "1432",
    "uploadDate": "2019-05-06",
    "thumbnail_url": "https://i.ytimg.com/vi/3Tqp_B2G6u0/hqdefault.jpg"
  },
  {
    "link": "watch?v=pPBqM4CKjUU",
    "title": "Discriminating Systems - Gender, Race, and Power in AI",
    "tags": "ai, machine learning, bias, fairness, ml fairness, algorithmic bias, algorithmic discrimination, ai and society, ainow, google, microsoft, race, gender, stem, pipeline, gender gap, diversity, inclusion, equity, power",
    "scraped_at": 1684582662.7775738,
    "genre": "Science",
    "views": "986",
    "desc": "TL;DR:\\\\n- There exists both an unequal representation of people in the AI workforce as well as examples of societal bias in AI systems.\\\\n- The authors claim that the former causally leads to the latter and vice versa.\\\\n- To me, the report does not manage to make a strong enough argument for that claim.\\\\n- I find the statements made quite dishonest at times.\\\\n\\\\nhttps://ainowinstitute.org/discriminatingsystems.pdf\\\\n\\\\nAuthors:\\\\nSarah Myers West, Meredith Whittaker, Kate Crawford\"",
    "lengthSeconds": "7021",
    "uploadDate": "2019-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/pPBqM4CKjUU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=sbKaUc0tPaY",
    "title": "The Odds are Odd: A Statistical Test for Detecting Adversarial Examples",
    "tags": "film, udost",
    "scraped_at": 1684582662.85358,
    "genre": "Science",
    "views": "1538",
    "desc": "https://arxiv.org/abs/1902.04818\\\\n\\\\nAbstract:\\\\nWe investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.\\\\n\\\\nAuthors:\\\\nKevin Roth, Yannic Kilcher, Thomas Hofmann\"",
    "lengthSeconds": "1826",
    "uploadDate": "2019-02-19",
    "thumbnail_url": "https://i.ytimg.com/vi/sbKaUc0tPaY/hqdefault.jpg"
  },
  {
    "link": "watch?v=jltgNGt8Lpg",
    "title": "Neural Ordinary Differential Equations",
    "tags": "film, udost",
    "scraped_at": 1684582664.4806104,
    "genre": "Science",
    "views": "39971",
    "desc": "https://arxiv.org/abs/1806.07366\\\\n\\\\nAbstract:\\\\nWe introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.\\\\n\\\\nAuthors:\\\\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud\"",
    "lengthSeconds": "1339",
    "uploadDate": "2019-02-18",
    "thumbnail_url": "https://i.ytimg.com/vi/jltgNGt8Lpg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=u1_qMdb0kYU",
    "title": "GPT-2: Language Models are Unsupervised Multitask Learners",
    "tags": "gpt2, transformer, language model, deep learning, nlp, openai, security, translation, neural network, attention, attention mechanism, unsupervised learning, controversy",
    "scraped_at": 1684582662.9345806,
    "genre": "Science",
    "views": "25022",
    "desc": "A look at OpenAI\\'s new GPT-2 model and the surrounding controversy.\\\\n\\\\nhttps://blog.openai.com/better-language-models/\\\\n\\\\nAbstract:\\\\nNatural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\\\\n\\\\nAuthors:\\\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever\"",
    "lengthSeconds": "1652",
    "uploadDate": "2019-02-18",
    "thumbnail_url": "https://i.ytimg.com/vi/u1_qMdb0kYU/hqdefault.jpg"
  },
  {
    "link": "watch?v=OioFONrSETc",
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "tags": "machine learning, deep learning, neural networks, batch normalization, batchnorm, whitening, data, internal covariate shift, deep neural networks, deep nets, mini",
    "scraped_at": 1684582664.572582,
    "genre": "Science",
    "views": "22032",
    "desc": "https://arxiv.org/abs/1502.03167\\\\n\\\\nAbstract:\\\\nTraining Deep Neural Networks is complicated by the fact that the distribution of each layer\\'s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.\\\\n\\\\nAuthors:\\\\nSergey Ioffe, Christian Szegedy\"",
    "lengthSeconds": "1544",
    "uploadDate": "2019-02-02",
    "thumbnail_url": "https://i.ytimg.com/vi/OioFONrSETc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-9evrZnBorM",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "tags": "bert, deep learning, attention, unsupervised, nlp, transformer, squad, wordpiece, embeddings, language, language modeling, attention layers, bidirectional, elmo, natural language processing, machine learning, word vectors, pretrained, fine tuning",
    "scraped_at": 1684582663.01958,
    "genre": "Science",
    "views": "90839",
    "desc": "https://arxiv.org/abs/1810.04805\\\\n\\\\nAbstract:\\\\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \\\\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\\\\n\\\\nAuthors:\\\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\"",
    "lengthSeconds": "2413",
    "uploadDate": "2019-01-30",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=nPB0ppcnzZA",
    "title": "What\u2019s in a name? The need to nip NIPS",
    "tags": "NIPS, NeurIPS, nips 2018, neurips 2018, nips name change, machine learning, deep learning, community, sexism, diversity, inclusion, bias, gender, women, tech, women in tech, women in stem, majority vote, minorities, statistics, computer science, harassment",
    "scraped_at": 1684582663.0995798,
    "genre": "Science",
    "views": "2578",
    "desc": "http://tensorlab.cms.caltech.edu/users/anima/pubs/NIPS_Name_Debate.pdf\\\\n\\\\nAbstract:\\\\nThere has been substantial recent controversy surrounding the use of the acronym \\\\\"",
    "lengthSeconds": "3371",
    "uploadDate": "2019-01-09",
    "thumbnail_url": "https://i.ytimg.com/vi/nPB0ppcnzZA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_PyusGsbBPY",
    "title": "Stochastic RNNs without Teacher-Forcing",
    "tags": "NeurIPS2018, NIPS2018, NLP, deep learning, RNN",
    "scraped_at": 1684582663.1805801,
    "genre": "Science",
    "views": "3046",
    "desc": "We present a stochastic non-autoregressive RNN that does not require teacher-forcing for training. The content is based on our 2018 NeurIPS paper:\\\\n\\\\nDeep State Space Models for Unconditional Word Generation\\\\nhttps://arxiv.org/abs/1806.04550\"",
    "lengthSeconds": "1098",
    "uploadDate": "2018-12-21",
    "thumbnail_url": "https://i.ytimg.com/vi/_PyusGsbBPY/hqdefault.jpg"
  },
  {
    "link": "watch?v=WYrvh50yu6s",
    "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
    "tags": "ai, deep learning, variational, autoencoders, vae, disentanglement, representation learning, machine learning, unsupervised, arxiv, google, google ai, mpi, eth, eth zurich, ethz",
    "scraped_at": 1684582663.2606099,
    "genre": "Science",
    "views": "8310",
    "desc": "https://arxiv.org/abs/1811.12359\\\\n\\\\nAbstract:\\\\nIn recent years, the interest in unsupervised learning of disentangled representations has significantly increased. The key assumption is that real-world data is generated by a few explanatory factors of variation and that these factors can be recovered by unsupervised learning algorithms. A large number of unsupervised learning approaches based on auto-encoding and quantitative evaluation metrics of disentanglement have been proposed; yet, the efficacy of the proposed approaches and utility of proposed notions of disentanglement has not been challenged in prior work. In this paper, we provide a sober look on recent progress in the field and challenge some common assumptions. \\\\nWe first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering the six most prominent methods, and evaluate them across six disentanglement metrics in a reproducible large-scale experimental study on seven different data sets. On the positive side, we observe that different methods successfully enforce properties \\\\\"",
    "lengthSeconds": "1665",
    "uploadDate": "2018-12-18",
    "thumbnail_url": "https://i.ytimg.com/vi/WYrvh50yu6s/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dPsXxLyqpfs",
    "title": "World Models",
    "tags": "deep learning, reinforcement learning, deep reinforcement learning, deep rl, schmidhuber, environment model, imagination, vae, rnn, lstm",
    "scraped_at": 1684582664.6576114,
    "genre": "Science",
    "views": "10926",
    "desc": "Authors: David Ha, J\\xc3\\xbcrgen Schmidhuber\\\\n\\\\nAbstract:\\\\nWe explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.\\\\n\\\\nhttps://arxiv.org/abs/1803.10122\"",
    "lengthSeconds": "1122",
    "uploadDate": "2018-04-07",
    "thumbnail_url": "https://i.ytimg.com/vi/dPsXxLyqpfs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_Z9ZP1eiKsI",
    "title": "Curiosity-driven Exploration by Self-supervised Prediction",
    "tags": "film, udost",
    "scraped_at": 1684582664.736612,
    "genre": "Science",
    "views": "7346",
    "desc": "https://arxiv.org/abs/1705.05363\\\\n\\\\nAuthors: Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell\\\\n\\\\nAbstract:\\\\nIn many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent\\'s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.\"",
    "lengthSeconds": "1054",
    "uploadDate": "2018-03-18",
    "thumbnail_url": "https://i.ytimg.com/vi/_Z9ZP1eiKsI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BBp0tHcirtQ",
    "title": "git for research basics: fundamentals, commits, branches, merging",
    "tags": "git, research, commit, merge, conflict",
    "scraped_at": 1684582663.34458,
    "genre": "Science",
    "views": "1907",
    "desc": "Don\\'t watch this if you already know how to solve a merge conflict :)\"",
    "lengthSeconds": "1642",
    "uploadDate": "2017-12-12",
    "thumbnail_url": "https://i.ytimg.com/vi/BBp0tHcirtQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=iDulhoQ2pro",
    "title": "Attention Is All You Need",
    "tags": "deep learning, machine learning, nlp, natural language processing, machine translation, arxiv, google, attention mechanism, attention, transformer, tensor2tensor, rnn, recurrent, seq2seq",
    "scraped_at": 1684582664.82458,
    "genre": "Science",
    "views": "492579",
    "desc": "https://arxiv.org/abs/1706.03762\\\\n\\\\nAbstract:\\\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\\\n\\\\nAuthors:\\\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\"",
    "lengthSeconds": "1626",
    "uploadDate": "2017-11-28",
    "thumbnail_url": "https://i.ytimg.com/vi/iDulhoQ2pro/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-YiMVR3HEuY",
    "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
    "tags": "machine learning, artificial intelligence, ai, deep learning, unsupervised learning, research, academia, paper, review, agents, tasks",
    "scraped_at": 1684582664.89958,
    "genre": "Science",
    "views": "3661",
    "desc": "https://arxiv.org/abs/1611.05397\\\\n\\\\nAbstract:\\\\nDeep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\\\\\\\% expert human performance, and a challenging suite of first-person, three-dimensional \\\\\\\\emph{Labyrinth} tasks leading to a mean speedup in learning of 10\\xc3\\x97 and averaging 87\\\\\\\\% expert human performance on Labyrinth.\\\\n\\\\nAuthors:\\\\nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu\"",
    "lengthSeconds": "660",
    "uploadDate": "2017-08-27",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=56GW1IlWgMg",
    "title": "Learning model-based planning from scratch",
    "tags": "machine learning, artificial intelligence, ai, deep learning, reinforcement learning, deep mind, research, academia, paper, review, imagination, planning, agents",
    "scraped_at": 1684582664.9795802,
    "genre": "Science",
    "views": "4294",
    "desc": "https://arxiv.org/abs/1707.06170\\\\n\\\\nAbstract:\\\\nConventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the \\\\\"",
    "lengthSeconds": "662",
    "uploadDate": "2017-08-08",
    "thumbnail_url": "https://i.ytimg.com/vi/56GW1IlWgMg/maxresdefault.jpg"
  }
]