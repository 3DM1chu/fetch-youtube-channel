[
  {
    "link": "watch?v=DXpk9K7DgMo",
    "title": "Using NEW MPT-7B in Hugging Face and LangChain",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, chatgpt, langchain agent, langchain, llm, llm course, chatbot python, james briggs, ai, openai api, openai chatgpt python, auto gpt, hugging face agents, hugging face, hugging face agents python, chatgpt hugging face, hugging gpt, open source ai, mpt",
    "scraped_at": 1684585774.737623,
    "genre": "Science",
    "views": "14838",
    "desc": "Let\\'s take a look at Mosaic ML\\'s new MPT-7B LLM. We\\'ll see how to use any MPT-7B model (instruct, chat, and storywriter-65k) in both Hugging Face transformers and LangChain. By using MPT-7B in LangChain we give it access to all of the tooling available via the library, like AI agents, chatbot functionality, and more.\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Notebook link:\\\\nhttps://github.com/pinecone-io/examples/blob/master/generation/llm-field-guide/mpt-7b/mpt-7b-huggingface-langchain.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x91\\x8b\\xf0\\x9f\\x8f\\xbc Socials:\\\\nTwitter: https://twitter.com/jamescalam\\\\nLinkedIn: https://www.linkedin.com/in/jamescalam/\\\\nInstagram: https://www.instagram.com/jamescalam/\\\\n\\\\n00:00 Open Source LLMs like MPT-7B\\\\n00:50 MPT-7B Models in Hugging Face\\\\n02:29 Python setup\\\\n04:16 Initializing MPT-7B-Instruct\\\\n06:28 Initializing the MPT-7B tokenizer\\\\n07:10 Stopping Criteria and HF Pipeline\\\\n09:52 Hugging Face Pipeline\\\\n14:18 Generating Text with Hugging Face\\\\n16:01 Implementing MPT-7B in LangChain\\\\n17:08 Final Thoughts on Open Source LLMs\\\\n\\\\n#artificialintelligence #nlp #langchain #deeplearning #huggingface\"",
    "lengthSeconds": "1134",
    "uploadDate": "2023-05-18",
    "thumbnail_url": "https://i.ytimg.com/vi/DXpk9K7DgMo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-xAeqi_2cis",
    "title": "NEW Hugging Face Agents \u2014 First Look",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, chatgpt, text",
    "scraped_at": 1684585775.1136212,
    "genre": "Science",
    "views": "40424",
    "desc": "Hugging Face has announced its take on Large Language Model (LLM) Agents. Similar to what we see in LangChain agents, Haystack agents, and ChatGPT plugins \\xe2\\x80\\x94 but incredibly easy to get started with and with access to Hugging Face\\'s huge hub of NLP models (taking inspiration from HuggingGPT).\\\\n\\\\nThese agents look great, with the integration into the HF hub, multi-modality, and an easy-to-use implementation, and I\\'m looking forward to doing more on HF agents in the near future.\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Link to notebook:\\\\nhttps://github.com/aurelio-labs/cookbook/blob/main/gen-ai/agents/hf-agents/hf-agents-intro.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x91\\x8b\\xf0\\x9f\\x8f\\xbc Socials:\\\\nTwitter: https://twitter.com/jamescalam\\\\nLinkedIn: https://www.linkedin.com/in/jamescalam/\\\\nInstagram: https://www.instagram.com/jamescalam/\\\\n\\\\n00:00 Hugging Face Agents\\\\n00:38 Agents and Tools Explained\\\\n02:15 Current Agents Landscape\\\\n04:35 Taking Inspiration from HuggingGPT\\\\n05:45 Getting Started with Hugging Face Agents\\\\n08:28 Querying the Agents\\\\n10:58 Agent Prompt Template\\\\n11:46 Conversational Chatbot Agent in Hugging Face\\\\n16:44 Community Tools in Hugging Face\\\\n\\\\n#artificialintelligence #nlp #openai #huggingface #generativeai\"",
    "lengthSeconds": "1111",
    "uploadDate": "2023-05-13",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=H6bCqqw9xyI",
    "title": "Build Conversational Agents with Vector DBs - LangChain #9",
    "tags": "artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, pinecone database, pinecone vector database, vector database, chatgpt, langchain vector database, chroma db langchain, llm, langchain tutorial, langchain, james briggs, gpt 3.5, gpt 4, langchain 101, langchain search, langchain memory, ai, openai api, langchain python, langchain ai, retrieval augmentation, pinecone langchain, langchain agents",
    "scraped_at": 1684585775.337622,
    "genre": "Science",
    "views": "7791",
    "desc": "We\\'ve seen in previous chapters how powerful retrieval augmentation (vector databases) and conversational agents (chatbots) can be. They become even more impressive when we begin using them together.\\\\n\\\\nConversational agents can struggle with data freshness, knowledge about specific domains, or accessing internal documentation. By coupling agents with retrieval augmentation tools, we no longer have these problems.\\\\n\\\\nOn the other side, using \\\\\"",
    "lengthSeconds": "1125",
    "uploadDate": "2023-05-11",
    "thumbnail_url": "https://i.ytimg.com/vi/H6bCqqw9xyI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kvdVduIJsc8",
    "title": "Fixing LLM Hallucinations with Retrieval Augmentation in LangChain #6",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, pinecone database, pinecone vector database, vector database, chatgpt, langchain vector database, chroma db, chroma db langchain, llm, langchain tutorial, langchain, james briggs, gpt 3.5, gpt 4, langchain 101, langchain search, langchain memory, ai, openai api, langchain python, langchain ai, retrieval augmentation",
    "scraped_at": 1684585772.9966218,
    "genre": "Science",
    "views": "17821",
    "desc": "Large Language Models (LLMs) have a data freshness problem. Even some of the most powerful models, like ChatGPT\\'s gpt-3.5-turbo and GPT-4, have no idea about recent events.\\\\n\\\\nThe world, according to LLMs, is frozen in time. They only know the world as it appeared through their training data.\\\\n\\\\nSo, how do we handle this problem? We can use retrieval augmentation. This technique allows us to retrieve relevant information from an external knowledge base and give that information to our LLM.\\\\n\\\\nThe external knowledge base is our \\\\\"",
    "lengthSeconds": "1860",
    "uploadDate": "2023-05-04",
    "thumbnail_url": "https://i.ytimg.com/vi/kvdVduIJsc8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=q-HNphrWsDE",
    "title": "Create Custom Tools for Chatbots in LangChain \u2014 LangChain #8",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, james briggs, chatgpt langchain, chatgpt, langchain chatbot, chatbot langchain, langchain tutorial, chatbot tutorial, langchain agent, langchain agent example, langchain tools, llm agents, custom llm agent, ai chat, gpt",
    "scraped_at": 1684585774.3706272,
    "genre": "Science",
    "views": "14886",
    "desc": "Agents are one of the most powerful and fascinating approaches to using Large Language Models (LLMs). The explosion of interest in LLMs has led to agents becoming incredibly prevalent in AI-powered use cases.\\\\n\\\\nUsing agents allows us to give LLMs access to tools. These tools present an essentially infinite number of possibilities. With tools, LLMs can search the web, do math, run code, and much more.\\\\n\\\\nThe LangChain library provides a substantial selection of prebuilt tools. However, in many real-world projects, we\\'ll often find that there are no tools that quite fit our requirements. Meaning we must modify existing tools or build entirely new ones.\\\\n\\\\nIn this video, we will explore how to build custom tools for agents in LangChain. \\\\n\\\\n\\xf0\\x9f\\x93\\x8c Code Notebook:\\\\nhttps://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/07-langchain-tools.ipynb\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/langchain-tools/\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 LangChain agents and tools\\\\n01:46 What are LLM tools\\\\n03:12 Code notebook setup and prerequisites\\\\n05:58 Building a simple LangChain calculator tool\\\\n05:50 Initialize the conversational agent\\\\n10:08 Updating agent prompts\\\\n12:14 Building tools with multiple parameters\\\\n15:40 Helping ChatGPT understand images\\\\n23:05 What else can LangChain agents do\\\\n\\\\n#artificialintelligence #langchain #openai #nlp\"",
    "lengthSeconds": "1498",
    "uploadDate": "2023-04-26",
    "thumbnail_url": "https://i.ytimg.com/vi/q"
  },
  {
    "link": "watch?v=CeZroxbdLXY",
    "title": "Chatting with ArXiv Research Papers \u2014 AI Assistant #3",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, langchain agent example, langchain, langchain agent, james briggs, gpt 4, gpt",
    "scraped_at": 1684585774.0706222,
    "genre": "Science",
    "views": "6508",
    "desc": "In this video, we take a look at the first version of the chatbot that can interact with ArXiv research papers \\xe2\\x80\\x94 the beginning of our own \\\\\"",
    "lengthSeconds": "1105",
    "uploadDate": "2023-04-20",
    "thumbnail_url": "https://i.ytimg.com/vi/CeZroxbdLXY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jSP-gSEyVeI",
    "title": "LangChain Agents Deep Dive with GPT 3.5 \u2014 LangChain #7",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, chatgpt, text",
    "scraped_at": 1684585774.9436214,
    "genre": "Science",
    "views": "26327",
    "desc": "Large Language Models (LLMs) are incredibly powerful, yet they lack particular abilities that the \\\\\"",
    "lengthSeconds": "1937",
    "uploadDate": "2023-04-18",
    "thumbnail_url": "https://i.ytimg.com/vi/jSP"
  },
  {
    "link": "watch?v=bAQ6VRewf0w",
    "title": "Building a ChatGPT Plugin for Lex Fridman Podcasts",
    "tags": "machine learning, artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, james briggs, chatgpt, chatgpt plugin, chatgpt retrieval plugin, chatgpt python, chatgpt plus, lex fridman podcast, openai, openai chatgpt, openai chatgpt tutorial, openai chatbot tutorial, massive upgrade to chatgpt, ai chat, chat gpt ai, gpt 3 chatbot python, gpt 4 chatbot, gpt 4, openai plugins, chatgpt",
    "scraped_at": 1684585773.2206209,
    "genre": "Science",
    "views": "6876",
    "desc": "I built a Lex Fridman podcast search plugin for ChatGPT. Here\\'s how I did it. Using the plugin, we can ask OpenAI\\'s ChatGPT any question about the content of Lex Fridman podcasts and get accurate answers than even cite the podcast episodes so that we can go in and check the original source of information.\\\\n\\\\nThe Ask Lex plugin is based on the chatgpt-retrieval-plugin template from OpenAI.\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Links:\\\\nAsk Lex Plugin:\\\\nhttps://github.com/jamescalam/ask-lex-plugin\\\\nUpserts notebook:\\\\nhttps://github.com/pinecone-io/examples/blob/master/generation/chatgpt/plugins/ask-lex/ask-lex-indexer.ipynb\\\\n26:32 ChatGPT Plugins video:\\\\nhttps://youtu.be/hpePPqKxNq8\\\\n27:21 ChatGPT Plugins waitlist:\\\\nhttps://openai.com/waitlist/plugins\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 ChatGPT plugins for YouTube videos\\\\n00:19 Ask Lex ChatGPT Plugin so far\\\\n03:56 How ChatGPT is searching the podcast\\\\n06:58 How the Ask Lex ChatGPT plugin works\\\\n15:58 Finding the plugin code\\\\n16:40 Instructions for ChatGPT plugin\\\\n19:15 Creating and indexing the podcast transcripts\\\\n21:32 Hosting the API on DigitalOcean\\\\n22:33 Installing the plugin in ChatGPT\\\\n23:41 Having a conversation with \\\\\"",
    "lengthSeconds": "1680",
    "uploadDate": "2023-04-11",
    "thumbnail_url": "https://i.ytimg.com/vi/bAQ6VRewf0w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=CfuhRVM1ntQ",
    "title": "Lex Fridman Podcast Chatbot with LangChain Agents + GPT 3.5",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, chatgpt",
    "scraped_at": 1684585774.8436222,
    "genre": "Science",
    "views": "21332",
    "desc": "In this video, we will build a conversational (chatbot) agent using LangChain agents. The agent uses vector search retrieval to find relevant snippets from Lex\\'s podcast and serve them as context to the LLM (we use the OpenAI ChatGPT model called gpt-3.5-turbo).\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code notebook:\\\\nhttps://github.com/pinecone-io/examples/blob/master/generation/chatbots/conversational-agents/langchain-lex-agent.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Building conversational agents in LangChain\\\\n00:14 Tools and Agents in LangChain\\\\n03:57 Notebook setup and prerequisites\\\\n05:23 Data preparation\\\\n11:00 Initialize LangChain vector store\\\\n13:12 Initializing everything needed by agent\\\\n13:41 Using RetrievalQA chain in LangChain\\\\n15:59 Creating Lex Fridman DB tool\\\\n17:37 Initializing a LangChain conversational agent\\\\n21:49 Conversational memory prompt\\\\n27:41 Testing a conversation with the Lex agent\\\\n\\\\n#langchain #artificialintelligence #nlp #openai\"",
    "lengthSeconds": "1954",
    "uploadDate": "2023-04-06",
    "thumbnail_url": "https://i.ytimg.com/vi/CfuhRVM1ntQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=e9TSnAZAXOo",
    "title": "Scraping ArXiv Papers with OpenAI's GPT 3.5 \u2014 AI Assistant #2",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, ai assistant python, james briggs, ai assistant, ai assistant project, langchain, openai, pinecone, pinecone ai, gpt 3, gpt 3.5, gpt 4, gpt 3 tutorial, gpt 3.5 tutorial, gpt 3 python api, chat gpt, llm, llm course, chatbot tutorial, chatbot using python, python chatbot, langchain in python, chatgpt 4",
    "scraped_at": 1684585773.9746227,
    "genre": "Science",
    "views": "7804",
    "desc": "I\\'m building an AI assistant that will *actually* be useful. This video covers the ArXiv paper scraping component, using a graph approach where each paper\\'s references become \\\\\"",
    "lengthSeconds": "925",
    "uploadDate": "2023-03-29",
    "thumbnail_url": "https://i.ytimg.com/vi/e9TSnAZAXOo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hpePPqKxNq8",
    "title": "ChatGPT Plugins: Build Your Own in Python!",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, chatgpt",
    "scraped_at": 1684585780.598648,
    "genre": "Science",
    "views": "115459",
    "desc": "OpenAI\\'s ChatGPT now has plugins! Creatively named \\\\\"",
    "lengthSeconds": "2466",
    "uploadDate": "2023-03-24",
    "thumbnail_url": "https://i.ytimg.com/vi/hpePPqKxNq8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=eqOfr4AGLk8",
    "title": "LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, langchain, openai, llm, chatgpt, gpt 4, gpt",
    "scraped_at": 1684585777.9006226,
    "genre": "Science",
    "views": "16926",
    "desc": "In this video, we\\'re going to focus on preparing our text using LangChain data loaders, tokenization using the tiktoken tokenizers, chunking with LangChain text splitters, and storing data with Hugging Face datasets. Naturally, the focus here is on OpenAI embedding and completion models, but we can apply the same logic to other LLMs like those available via Hugging Face, Cohere, and so on.\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Notebook link:\\\\nhttps://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/xx-langchain-chunking.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Data preparation for LLMs\\\\n00:45 Downloading the LangChain docs\\\\n03:29 Using LangChain document loaders\\\\n05:54 How much text can we fit in LLMs?\\\\n11:57 Using tiktoken tokenizer to find length of text\\\\n16:02 Initializing the recursive text splitter in Langchain\\\\n17:25 Why we use chunk overlap\\\\n20:23 Chunking with RecursiveCharacterTextSplitter\\\\n21:37 Creating the dataset\\\\n24:50 Saving and loading with JSONL file\\\\n28:40 Data prep is important\"",
    "lengthSeconds": "1788",
    "uploadDate": "2023-03-23",
    "thumbnail_url": "https://i.ytimg.com/vi/eqOfr4AGLk8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tBJ-CTKG2dM",
    "title": "GPT 4: Superpower results with search",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, gpt4, gpt 4 python, gpt 4, gpt 4 openai, gpt 4 launch, gpt 4 chat, chatgpt 4, james briggs, llm, retrieval augmentation, openai api, llm gpt, gpt 4 code, gpt",
    "scraped_at": 1684585774.5576224,
    "genre": "Science",
    "views": "26296",
    "desc": "GPT-4 is pretty incredible, but like its predecessors, it has problems. Hallucinations and the lack of up-to-date information are typical of Large Language Models (LLMs), from PaLM and LLaMa, to ChatGPT (gpt-3.5-turbo) and GPT 4.\\\\n\\\\nIn this video, we\\'ll demonstrate how to supercharge GPT4 (and other language models) using OpenAI\\'s ChatCompletion endpoint with the latest gpt-4 model and the Pinecone vector database. Augmenting the knowledge and abilities of GPT4 with up-to-date information about the LangChain Python library.\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code notebook:\\\\nhttps://github.com/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs.ipynb\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n00:00 Why GPT-4 can fail - hallucinations\\\\n01:50 What we can do with retrieval augmentation\\\\n02:16 How retrieval augmentation works\\\\n07:41 Scraping docs for LLMs\\\\n10:01 Preprocessing and chunking text for GPT4\\\\n13:24 Creating embeddings with text-embedding-ada-002\\\\n14:58 Creating the Pinecone vector database\\\\n19:24 Retrieving relevant docs with semantic search\\\\n20:23 GPT-4 generated answers\\\\n23:44 GPT-4 with augmentation vs. GPT-4 without\\\\n25:34 Building powerful tools is almost too easy\\\\n\\\\n#artificialintelligence #gpt4 #openai #nlp #vectorsearch\"",
    "lengthSeconds": "1630",
    "uploadDate": "2023-03-16",
    "thumbnail_url": "https://i.ytimg.com/vi/tBJ"
  },
  {
    "link": "watch?v=OafUcJ2Eeo8",
    "title": "GPT 4: Hands on with the API",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, gpt4, gpt 4, gpt 4 openai, gpt 4 api, gpt 4 python, gpt 4 launch, gpt 4 vs gpt 3.5, gpt 4 live, gpt 3, gpt 3.5, gpt",
    "scraped_at": 1684585774.4666228,
    "genre": "Science",
    "views": "42860",
    "desc": "OpenAI\\'s GPT 4 is here. Here we\\'ll take a first look at GPT 4, how it compares with the original ChatGPT model gpt-3.5-turbo, and how to use gpt-4 via the OpenAI Python API.\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code notebook:\\\\nhttps://gist.github.com/jamescalam/95b4c042ca77734ede64cdb575294232\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n00:00 GPT-4 has been released\\\\n00:31 Hands-on with GPT-4\\\\n02:39 Max token limits for GPT-4\\\\n05:23 Coding with GPT-4\\\\n09:56 Using GPT-4 API in Python\\\\n12:32 GPT-4 vs gpt-3.5-turbo\\\\n15:59 Why GPT-4 is a big step forward\"",
    "lengthSeconds": "1055",
    "uploadDate": "2023-03-15",
    "thumbnail_url": "https://i.ytimg.com/vi/OafUcJ2Eeo8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=CnAgB3A5OlU",
    "title": "Chat with OpenAI in LangChain - #5",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, chatgpt, chat gpt, chatgpt api, chat gpt to make money, chatgpt api python, text",
    "scraped_at": 1684585773.7396228,
    "genre": "Science",
    "views": "6795",
    "desc": "With the advent of OpenAI\\'s Chat-GPT API endpoint (ChatCompletion), LangChain quickly added support for the new endpoint. Unlike previous LLM endpoints, the chat endpoint takes multiple inputs and so has its own unique set of objects and methods.\\\\n\\\\nOpenAI\\'s `ChatCompletion` endpoint consumes three types of input:\\\\n\\\\n- System message \\xe2\\x80\\x94 this acts as an initial prompt to \\\\\"",
    "lengthSeconds": "1253",
    "uploadDate": "2023-03-14",
    "thumbnail_url": "https://i.ytimg.com/vi/CnAgB3A5OlU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=X05uK0TZozM",
    "title": "Chatbot Memory for Chat-GPT, Davinci + other LLMs - LangChain #4",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, chatgpt, chat gpt, chatgpt api, chatgpt api python, text",
    "scraped_at": 1684585773.380622,
    "genre": "Science",
    "views": "19803",
    "desc": "Conversational memory is how a chatbot can respond to multiple queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\\\\n\\\\nThe memory allows a Large Language Model (LLM) to remember previous interactions with the user. By default, LLMs are *stateless* \\xe2\\x80\\x94 meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\\\\n\\\\nThere are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\\\\n\\\\nThere are several ways that we can implement conversational memory. In the context of LangChain, they are all built on top of the `ConversationChain`.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://pinecone.io/learn/langchain-conversational-memory/\\\\n\\\\n\\xf0\\x9f\\x93\\x8c LangChain Handbook Code:\\\\nhttps://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook\\\\n\\\\n\\xf0\\x9f\\x99\\x8b\\xf0\\x9f\\x8f\\xbd\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f Francisco:\\\\nhttps://twitter.com/fpingham\\\\n\\\\nPart 1 (Intro): https://youtu.be/BP9fi_0XTlw\\\\nPart 2 (PromptTemplate): https://youtu.be/RflBcK0oDH0\\\\nPart 3 (Chains): https://youtu.be/S8j9Tk0lZHU\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Conversational memory for chatbots\\\\n00:28 Why we need conversational memory for chatbots\\\\n01:45 Implementation of conversational memory\\\\n04:05 LangChain\\'s Conversation Chain\\\\n12:00 Conversation Summary Memory in LangChain\\\\n19:06 Conversation Buffer Window Memory in LangChain\\\\n21:35 Conversation Summary Buffer Memory in LangChain\\\\n24:33 Other LangChain Memory Types\\\\n25:25 Final thoughts on conversational memory\\\\n\\\\n#artificialintelligence #nlp #openai #deeplearning #langchain\"",
    "lengthSeconds": "1589",
    "uploadDate": "2023-03-09",
    "thumbnail_url": "https://i.ytimg.com/vi/X05uK0TZozM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=a3-RM_u5YoU",
    "title": "Medical Search Engine with SPLADE + Sentence Transformers in Python",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, medical search, medical search engine, medical search engine project, pinecone vector database, ai development, ai development roadmap, data retrieval, data retrieval system, information retrieval, information retrieval nlp, search engine, search engine project, machine learning medical, james briggs",
    "scraped_at": 1684585774.6436229,
    "genre": "Science",
    "views": "4929",
    "desc": "In this video we\\'ll build a search engine for the medical field using hybrid search with NLP information retrieval models.\\\\n\\\\nWe use hybrid search with sentence transformers and SPLADE for medical quesiton-answering. By using hybrid search we\\'re able to search using both dense and sparse vectors. This allows us to cover semantics with the dense vectors, and features like exact matching and keyword search with the sparse vectors.\\\\n\\\\nFor the sparse vectors we use SPLADE. SPLADE is the first sparse embedding method to outperform BM25 across a variety of tasks. It\\'s an incredibly powerful technique that enables the typical sparse search advantages while also enabling learning term expansion to help minimize the vocabulary mismatch problem.\\\\n\\\\nThe demo we work through here uses SPLADE and a sentence transformer model trained on MS-MARCO. These are all implemented via Hugging Face transformers.\\\\n\\\\nFinally, for the search component we use the Pinecone vector database. The only vector DB at the time of writing that natively supports SPLADE vectors.\\\\n\\\\n\\xc2\\xa0\\xf0\\x9f\\x94\\x97 Code notebook:\\\\nhttps://github.com/pinecone-io/examples/blob/master/search/hybrid-search/medical-qa/pubmed-splade.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Hybrid search for medical field\\\\n00:18 Hybrid search process\\\\n02:42 Prerequisites and Installs\\\\n03:26 Pubmed QA data preprocessing step\\\\n08:25 Creating dense vectors with sentence-transformers\\\\n10:30 Creating sparse vector embeddings with SPLADE\\\\n18:12 Preparing sparse-dense format for Pinecone\\\\n21:02 Creating the Pinecone sparse-dense index\\\\n24:25 Making hybrid search queries\\\\n29:59 Final thoughts on sparse-dense with SPLADE\\\\n\\\\n#artificialintelligence #nlp #naturallanguageprocessing #machinelearning #searchengine\"",
    "lengthSeconds": "1863",
    "uploadDate": "2023-03-07",
    "thumbnail_url": "https://i.ytimg.com/vi/a3"
  },
  {
    "link": "watch?v=rxE7xBzYU_o",
    "title": "OpenAI's ChatGPT API First Look",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, chatgpt, chatgpt api, chatgpt api python, gpt",
    "scraped_at": 1684585776.4456217,
    "genre": "Science",
    "views": "26487",
    "desc": "First look at OpenAI\\'s new API endpoint for the new GPT 3.5 model powering ChatGPT \\xe2\\x80\\x94 gpt-3.5-turbo. We take a look at a few demos of the new endpoint in action, how we can use the new openai.ChatCompletion.create endpoint, and the parameters that come with it.\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n00:00 OpenAI ChatGPT API is here\\\\n03:02 Testing the new ChatCompletion endpoint\\\\n03:31 Comparing this to ChatGPT app\\\\n06:54 More testing of ChatGPT API\\\\n08:07 Formatting ChatGPT responses\\\\n09:58 Feeding responses back into ChatGPT\\\\n11:21 Final thoughts on ChatGPT API\"",
    "lengthSeconds": "723",
    "uploadDate": "2023-03-02",
    "thumbnail_url": "https://i.ytimg.com/vi/rxE7xBzYU_o/maxresdefault.jpg"
  },
  {
    "link": "watch?v=AELtGhiAqio",
    "title": "Supercharge eCommerce Search: OpenAI's CLIP, BM25, and Python",
    "tags": "openai, openai clip, python, search engine, ecommerce search, hybrid search, artificial intelligence, machine learning, product search, image search, pinecone vector database, ai development, data retrieval, natural language processing, computer vision, text",
    "scraped_at": 1684585773.652622,
    "genre": "Science",
    "views": "5575",
    "desc": "We build a multi-modal hybrid search engine for ecommerce using OpenAI\\'s CLIP, BM25, Pinecone vector database, and Python. The search engine processes text and image-based queries and can produce better results than traditional methods.\\\\n\\\\nThe search engine allows users to search and retrieve data using both text and visual queries, which is especially useful in e-commerce domains where users have a range of search queries, from specific product searches to image-based searches for related items.\\\\n\\\\nBy using CLIP and BM25, the search engine can process both text and image-based queries, providing users with a comprehensive search experience. Additionally, Pinecone vector database and Python allow for easy indexing, storage, and retrieval of data, making it possible to handle large volumes of data in real time.\\\\n\\\\n\\xf0\\x9f\\x93\\x8c Example notebooks:\\\\nhttps://github.com/pinecone-io/examples/blob/master/search/hybrid-search/ecommerce-search/ecommerce-search.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n00:00 Multi-modal hybrid search\\\\n01:05 Multi-modal hybrid search in e-commerce\\\\n05:14 How do we construct multi-modal embeddings\\\\n07:05 Difference between sparse and dense vectors\\\\n09:43 E-commerce search in Python\\\\n11:11 Connect to Pinecone vector db\\\\n12:04 Creating a Pinecone index\\\\n13:45 Data preparation\\\\n16:32 Creating BM25 sparse vectors\\\\n19:33 Creating dense vectors with sentence transformers\\\\n20:26 Indexing everything in Pinecone\\\\n24:41 Making hybrid queries\\\\n26:01 Mixing dense vs sparse with alpha\\\\n32:11 Adding product metadata filtering\\\\n34:13 Final thoughts on search\"",
    "lengthSeconds": "2119",
    "uploadDate": "2023-03-01",
    "thumbnail_url": "https://i.ytimg.com/vi/AELtGhiAqio/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1aequYq5yTo",
    "title": "Cohere vs. OpenAI embeddings \u2014 multilingual search",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, cohere, cohere ai, cohere ai hackathon, cohere api, pinecone, pinecone ai, pinecone vector database, vector database, openai, openai gpt 3, openai gpt 3.5, openai ada model, cohere embed, large language model, large language models explained, openai tutorial, cohere tutorial, llm, llms, llm tutorial",
    "scraped_at": 1684585773.1356225,
    "genre": "Science",
    "views": "4704",
    "desc": "In this video, we\\'re going to work through a multilingual semantic search example using Cohere\\'s new multilingual model. I also expect many of you will be curious about how it stacks up against OpenAI\\'s GPT 3.5 text-embedding-ada-002 model, so we cover that too.\\\\n\\\\nBig thanks to @NilsReimersTalks \\xe2\\x80\\x94 he basically wrote all of the code here and explained a ton of things to me, you should go look at his channel, he has a ton of useful content on semantic search.\\\\n\\\\n\\xf0\\x9f\\x93\\x8c Notebook:\\\\nhttps://github.com/pinecone-io/examples/blob/master/search/multilingual/cohere-multilingual/cohere-multilingual-search.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n00:00 What are Cohere embeddings\\\\n00:46 Cohere v OpenAI on cost\\\\n04:37 Cohere v OpenAI on performance\\\\n06:37 Implementing Cohere multilingual model\\\\n07:55 Data prep and embedding\\\\n10:45 Creating a vector index with Pinecone\\\\n14:07 Embedding and indexing everything\\\\n17:24 Making multilingual queries\\\\n21:55 Final throughts on Cohere and OpenAI\"",
    "lengthSeconds": "1436",
    "uploadDate": "2023-02-23",
    "thumbnail_url": "https://i.ytimg.com/vi/1aequYq5yTo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=0FQ2WmM0t3w",
    "title": "SPLADE: the first search model to beat BM25",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, ai search, ai search algorithms, bm25, bm25 information retrieval, bm25 model, tf idf, tf idf implementation, tf idf example, splade, splade model, sparse neural networks, sparse embedding, future of search engines, future of search, retrieval augmentation, james briggs, pinecone, sentence transformers",
    "scraped_at": 1684585775.9106224,
    "genre": "Science",
    "views": "8533",
    "desc": "AI-powered search is heating up, but it\\'s nothing new. Google, Netflix, Amazon, and many more big tech companies have all powered their search and recommendation systems with \\xe2\\x80\\x9cvector search\\xe2\\x80\\x9d.\\\\n\\\\nIn this video, we\\'ll talk about sparse and dense vector search, their pros and cons, and how the latest sparse embedding model called SPLADE can help us eliminate many of the downsides of traditional sparse embedding methods like TF-IDF and BM25.\\\\n\\\\nSPLADE can be used as an alternative to dense embedding models like OpenAI\\'s text-embedding-ada-002, Cohere\\'s embed endpoint, or sentence transformers. But more interestingly it can be used alongside these dense embedding models to give us the best of both worlds.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://pinecone.io/learn/splade\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n00:00 Sparse and dense vector search\\\\n00:44 Comparing sparse vs. dense vectors\\\\n03:59 Using sparse and dense together\\\\n06:46 What is SPLADE?\\\\n09:06 Vocabulary mismatch problem\\\\n09:51 How SPLADE works (transformers 101)\\\\n14:28 Masked language modeling (MLM)\\\\n15:57 How SPLADE builds embeddings with MLM\\\\n17:35 Where SPLADE doesn\\'t work so well\\\\n20:14 Implementing SPLADE in Python\\\\n20:38 SPLADE with PyTorch and Hugging Face\\\\n24:08 Using the Naver SPLADE library\\\\n27:11 What\\'s next for vector search?\"",
    "lengthSeconds": "1732",
    "uploadDate": "2023-02-21",
    "thumbnail_url": "https://i.ytimg.com/vi/0FQ2WmM0t3w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=15TDwVSpwKc",
    "title": "OpenAI GPT 3.5 AI assistant with Langchain + Pinecone #1",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, semantic search, similarity search, vector similarity search, vector search, ai assistant python, james briggs, ai assistant, ai assistant project, langchain, openai, pinecone, pinecone ai, gpt 3, gpt 3.5, gpt 4, gpt 3 tutorial, gpt 3.5 tutorial, gpt 3 python api, chat gpt, chatgpt plus free, llm, llm course, chatbot tutorial, chatbot using python, python chatbot, langchain in python, gpt 2",
    "scraped_at": 1684585774.1706214,
    "genre": "Science",
    "views": "24939",
    "desc": "I\\'m building an AI assistant that will *actually* be useful. Here\\'s the outline of the idea for this AI assistant. The idea is to rely heavily on the Langchain library and use tools and services like OpenAI and Pinecone (OP stack).\\\\n\\\\nWe\\'re currently focusing on the AI assistant\\'s ArXiv \\\\\"",
    "lengthSeconds": "1027",
    "uploadDate": "2023-02-16",
    "thumbnail_url": "https://i.ytimg.com/vi/15TDwVSpwKc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=S8j9Tk0lZHU",
    "title": "LLM Chains using GPT 3.5 and other LLMs \u2014 LangChain #3",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, langchain, langchain ai, langchain in python, james briggs, gpt 3.5, gpt 3, gpt 4, gpt 3 open source, openai tutorial, llm course, large language model, llm, gpt index, gpt 3 chatbot, gpt 3 tutorial, langchain tutorial, langchain course, gpt 3.5 explained, gpt 3.5 python, langchain chains, llm chain",
    "scraped_at": 1684585773.892622,
    "genre": "Science",
    "views": "9770",
    "desc": "In the third part of our LangChain series, we\\'ll explore chains, focusing on generic and utility chains like LLMChain. These are key features in LangChain that act as the foundation behind more advanced langchain uses like conversational AI (chatbots), retrieval augmented ML, and more.\\\\n\\\\nLangChain is a popular framework that allows users to quickly build apps and pipelines around Large Language Models. It integrates directly with OpenAI\\'s GPT-3 and GPT-3.5 models and Hugging Face\\'s open-source alternatives like Google\\'s flan-t5 models.\\\\n\\\\nIt can be used for chatbots, Generative Question-Anwering (GQA), summarization, and much more.\\\\n\\\\nThe core idea of the library is that we can \\\\\"",
    "lengthSeconds": "998",
    "uploadDate": "2023-02-14",
    "thumbnail_url": "https://i.ytimg.com/vi/S8j9Tk0lZHU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=RflBcK0oDH0",
    "title": "Prompt Templates for GPT 3.5 and other LLMs - LangChain #2",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, langchain, langchain ai, langchain in python, gpt 3 open source, gpt 3.5, gpt 3, gpt 4, openai tutorial, prompt engineering, prompt engineering gpt 3, llm course, large language model, llm, gpt index, gpt 3 chatbot, langchain prompt, gpt 3 tutorial, gpt 3 tutorial python, gpt 3.5 python, gpt 3 explained",
    "scraped_at": 1684585773.8156207,
    "genre": "Science",
    "views": "21718",
    "desc": "In the second part of our LangChain series, we\\'ll explore PromptTemplates, FewShotPromptTemplates, and example selectors. These are key features in LangChain that support prompt engineering for LLMs like OpenAI\\'s GPT 3, Cohere, and Hugging Face\\'s OS alternatives.\\\\n\\\\nLangChain is a popular framework that allows users to quickly build apps and pipelines around Large Language Models. It integrates directly with OpenAI\\'s GPT-3 and GPT-3.5 models and Hugging Face\\'s open-source alternatives like Google\\'s flan-t5 models.\\\\n\\\\nIt can be used for chatbots, Generative Question-Anwering (GQA), summarization, and much more.\\\\n\\\\nThe core idea of the library is that we can \\\\\"",
    "lengthSeconds": "1377",
    "uploadDate": "2023-02-08",
    "thumbnail_url": "https://i.ytimg.com/vi/RflBcK0oDH0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BP9fi_0XTlw",
    "title": "Prompt Engineering with OpenAI's GPT-3 and other LLMs",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, gpt 3, gpt 4, gpt chat, chat gpt, prompt engineering, prompt engineering tutorial, prompt engineering gpt 3, prompt engineering 101, prompt engineering course, prompt engineering ai, prompt engineering openai, openai, openai tutorial, openai gpt 3 python, llms, large language model, large language models explained, generative ai, openai tiktoken",
    "scraped_at": 1684585775.190622,
    "genre": "Science",
    "views": "18426",
    "desc": "In this video, we\\'ll talk about how to build better prompts for OpenAI\\'s GPT-3, Cohere LLMs, and open-source LLMs (like those on Hugging Face). We\\'ll treat prompt engineering as a mix of engineering and artistry, using rules of thumb from OpenAI, Cohere, and others.\\\\n\\\\nAll of these models are large language models (LLMs) capable of doing a huge range of tasks. Prompt engineering is the key behind applying these models to different use cases.\\\\n\\\\n\\xf0\\x9f\\x93\\x8c Colab Notebook:\\\\nhttps://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/prompt-engineering.ipynb\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone Gen AI Examples:\\\\nhttps://github.com/pinecone-io/examples/tree/master/generation\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/nlp-transformers\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 What is Prompt Engineering?\\\\n02:15 Anatomy of a Prompt\\\\n07:03 Building prompts with OpenAI GPT 3\\\\n08:35 Generation / completion temperature\\\\n13:50 Few-shot training with examples\\\\n16:08 Adding external information\\\\n22:55 Max context window\\\\n27:18 Final thoughts on Gen AI and prompts\\\\n\\\\n#artificialintelligence #openai #gpt3 #deeplearning #nlp\"",
    "lengthSeconds": "1762",
    "uploadDate": "2023-02-01",
    "thumbnail_url": "https://i.ytimg.com/vi/BP9fi_0XTlw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=nE2skSRWTTs",
    "title": "Getting Started with GPT-3 vs. Open Source LLMs - LangChain #1",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, langchain, langchain ai, openai, gpt 3, gpt 3 ai, gpt 3.5, gpt 4, openai tutorial, gpt 3 open source, llm, llm course, large language model, large language models explained, gpt 3 chatbot, gpt 3 question answering, huggingface chatbot, huggingface llm, gpt index, gpt 3 tutorial, gpt 3 tutorial python",
    "scraped_at": 1684585773.3056214,
    "genre": "Science",
    "views": "44612",
    "desc": "LangChain is a popular framework that allows users to quickly build apps and pipelines around Large Language Models. It integrates directly with OpenAI\\'s GPT-3 and GPT-3.5 models and Hugging Face\\'s open-source alternatives like Google\\'s flan-t5 models.\\\\n\\\\nIt can be used to for chatbots, Generative Question-Anwering (GQA), summarization, and much more.\\\\n\\\\nThe core idea of the library is that we can \\\\\"",
    "lengthSeconds": "1215",
    "uploadDate": "2023-01-25",
    "thumbnail_url": "https://i.ytimg.com/vi/nE2skSRWTTs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rrAChpbwygE",
    "title": "Generative AI and Long-Term Memory for LLMs (OpenAI, Cohere, OS, Pinecone)",
    "tags": "python, machine learning, artificial intelligence, natural language processing, Huggingface, semantic search, similarity search, vector similarity search, vector search, generative ai, generative ai tutorial, gen ai, generative question answering, openai, cohere ai, google palm, large language model, large language models explained, ai memory, hugging face, hugging face nlp, gpt chat, gpt 3 tutorial, gpt 4, gpt 3.5, gpt 3.5 chat, gpt 3 explained, gpt 3 model explained, gqa",
    "scraped_at": 1684585777.1766415,
    "genre": "Science",
    "views": "11173",
    "desc": "Generative AI is what many expect to be the next big technology boom, and being what it is \\xe2\\x80\\x94 AI \\xe2\\x80\\x94 could have far-reaching implications far beyond what we\\'d expect.\\\\n\\\\nOne of the most thought-provoking use cases of generative AI belongs to Generative Question-Answering (GQA). \\\\n\\\\nNow, the most straightforward GQA system requires nothing more than a user text query and a large language model (LLM).\\\\n\\\\nWe can test this out with OpenAI\\'s GPT-3, Cohere, or open-source Hugging Face models.\\\\n\\\\nHowever, sometimes LLMs need help. For this, we can use retrieval augmentation. When applied to LLMs can be thought of as a form of \\\\\"",
    "lengthSeconds": "951",
    "uploadDate": "2023-01-19",
    "thumbnail_url": "https://i.ytimg.com/vi/rrAChpbwygE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dRUIGgNBvVk",
    "title": "Generative Question-Answering with OpenAI's GPT-3.5 and Davinci",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, gpt3, gpt4, text",
    "scraped_at": 1684585774.2766225,
    "genre": "Science",
    "views": "17619",
    "desc": "Generative Question Answering (GQA) is the application of generative AI to create human-like interactions and information retrieval with machines.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/openai-gen-qa/\\\\n\\\\nGitHub:\\\\nhttps://github.com/pinecone-io/examples/tree/master/generation/generative-qa/openai/gen-qa-openai\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/nlp-transformers\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Generative Question Answering with OpenAI\\\\n00:56 Example App for Generative QA\\\\n05:02 OpenAI Pinecone Stack Architecture\\\\n07:18 Dealing with LLM Hallucination\\\\n09:28 Indexing all data\\\\n13:47 Querying\\\\n16:37 Generation of answers\\\\n21:25 Testing some generative question answers\\\\n22:55 Final notes\\\\n\\\\n#openai #artificialintelligence #machinelearning #deeplearning\"",
    "lengthSeconds": "1445",
    "uploadDate": "2023-01-11",
    "thumbnail_url": "https://i.ytimg.com/vi/dRUIGgNBvVk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ejpc-nbKY2Y",
    "title": "Cohere AI's LLM for Semantic Search in Python",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, cohere, cohere ai, cohere ai hackathon, cohere api, pinecone, pinecone ai, pinecone vector database, vector database, openai, openai gpt 3, openai gpt 3.5, openai ada model, cohere embed, large language model, large language models explained, large language models tutorial, openai tutorial, cohere tutorial",
    "scraped_at": 1684585778.5826304,
    "genre": "Science",
    "views": "8813",
    "desc": "In this video, we will learn how to use the Cohere Embed API endpoint to generate language embeddings using a large language model (LLM) and then index those embeddings in the Pinecone vector database for fast and scalable vector search.\\\\n\\\\nCohere is an AI company that allows us to use state-of-the-art large language models (LLMs) in NLP. The Cohere Embed endpoint we use in this video gives us access to models similar to other popular LLMs like OpenAI\\'s GPT 3, particularly their recent offerings via OpenAI Embeddings like the text-embedding-ada-002 model.\\\\n\\\\nPinecone is a vector database company allowing us to use state-of-the-art vector search through millions or even billions of data points.\\\\n\\\\nBoth services together are a powerful and common combination for building semantic search, question-answering, advanced sentiment analysis, and other applications that rely on NLP and search over a large corpus of text data.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone docs:\\\\nhttps://docs.pinecone.io/docs/cohere\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/nlp-transformers\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Semantic search with Cohere LLM and Pinecone\\\\n00:45 Architecture overview\\\\n04:06 Getting code and prerequisites install\\\\n04:50 Cohere and Pinecone API keys\\\\n06:12 Initialize Cohere, get data, create embeddings\\\\n07:43 Creating Pinecone vector index\\\\n10:37 Querying with Cohere and Pinecone\\\\n12:56 Testing a few queries\\\\n14:35 Final notes\"",
    "lengthSeconds": "923",
    "uploadDate": "2023-01-04",
    "thumbnail_url": "https://i.ytimg.com/vi/ejpc"
  },
  {
    "link": "watch?v=ocxq84ocYi0",
    "title": "OpenAI's New GPT 3.5 Embedding Model for Semantic Search",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, gpt",
    "scraped_at": 1684585776.3686223,
    "genre": "Science",
    "views": "52852",
    "desc": "In this video we\\'ll learn how to use OpenAI\\'s new embedding model text-embedding-ada-002.\\\\n\\\\nWe will learn how to use the OpenAI Embedding API to generate language embeddings, and then index those embeddings in the Pinecone vector database for fast and scalable vector search.\\\\n\\\\nThis is a powerful and common combination for building semantic search, question-answering, threat-detection, and other applications that rely on NLP and search over a large corpus of text data.\\\\n\\\\nEverything will be implemented with OpenAI\\'s new GPT 3.5 class embedding model called text-embedding-ada-002; their latest embedding model that is 10x cheaper than earlier embedding models, more performant, and capable of indexing ~10 pages into a single vector embedding.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone docs:\\\\nhttps://docs.pinecone.io/docs/openai\\\\nColab notebook:\\\\nhttps://colab.research.google.com/github/pinecone-io/examples/blob/master/integrations/openai/semantic_search_openai.ipynb\\\\n\\\\n\\xf0\\x9f\\x8e\\x99\\xef\\xb8\\x8f Support me on Patreon:\\\\nhttps://patreon.com/JamesBriggs\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/nlp-transformers\\\\n\\\\n\\xf0\\x9f\\x8e\\xa8 AI Art:\\\\nhttps://www.etsy.com/uk/shop/IntelligentArtEU\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n00:30 Semantic search with OpenAI GPT architecture\\\\n03:43 Getting started with OpenAI embeddings in Python\\\\n04:12 Initializing connection to OpenAI API\\\\n05:49 Creating OpenAI embeddings with ada\\\\n07:24 Initializing the Pinecone vector index\\\\n09:04 Getting dataset from Hugging Face to embed and index\\\\n10:03 Populating vector index with embeddings\\\\n12:01 Semantic search querying\\\\n15:09 Deleting the environment\\\\n15:23 Final notes\"",
    "lengthSeconds": "974",
    "uploadDate": "2022-12-28",
    "thumbnail_url": "https://i.ytimg.com/vi/ocxq84ocYi0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZBfpkepdZlw",
    "title": "Convolutional Neural Nets Explained and Implemented in Python (PyTorch)",
    "tags": "python, machine learning, artificial intelligence, convolutional neural network, cnn, cnn machine learning, cnn deep learning, cnn explained, cnn visualized, cnn visualisation, cnn visualization github, james briggs, deep learning tutorial, ml explained, deep learning krish naik, deep learning explained, computer vision python, computer vision, computer vision course, image classification, image classification python, object detection python, image classification cnn, pytorch",
    "scraped_at": 1684585773.5696204,
    "genre": "Science",
    "views": "2417",
    "desc": "Convolutional Neural Networks (CNNs) have been the undisputed champions of Computer Vision (CV) for almost a decade. Their widespread adoption kickstarted the world of deep learning; without them, the field of AI would look very different today.\\\\n\\\\nRather than manual feature extraction, deep learning CNNs are capable of doing image classification, object detection, and much more automatically for a vast number of datasets and use cases. All they need is training data.\\\\n\\\\nDeep CNNs are the de-facto standard in computer vision. New models using vision transformers (ViT) and multi-modality may change this in the future, but for now, CNNs still dominate state-of-the-art benchmarks in vision.\\\\n\\\\nIn this hands-on video, we will learn why this is, how to implement deep learning CNNs for computer vision tasks like image classification using Python and PyTorch, and everything you could need to know about well known CNNs like LeNet, AlexNet, VGGNet, and ResNet.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://pinecone.io/learn/cnn\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/nlp-transformers\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:59 What Makes a Convolutional Neural Network\\\\n03:24 Image preprocessing for CNNs\\\\n09:15 Common components of a CNN\\\\n11:01 Components: pooling layers\\\\n12:31 Building the CNN with PyTorch\\\\n14:14 Notable CNNs\\\\n17:52 Implementation of CNNs\\\\n18:52 Image Preprocessing for CNNs\\\\n22:46 How to normalize images for CNN input\\\\n23:53 Image preprocessing pipeline with pytorch\\\\n24:59 Pytorch data loading pipeline for CNNs\\\\n25:32 Building the CNN with PyTorch\\\\n28:08 CNN training parameters\\\\n28:49 CNN training loop\\\\n30:27 Using PyTorch CNN for inference\"",
    "lengthSeconds": "2087",
    "uploadDate": "2022-12-21",
    "thumbnail_url": "https://i.ytimg.com/vi/ZBfpkepdZlw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=L8U-pm-vZ4c",
    "title": "Open Source Generative AI in Question-Answering (NLP) using Python",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, generative ai, generative models, generative models in machine learning, generative models with memory, generative models deep learning, gpt",
    "scraped_at": 1684585775.6656206,
    "genre": "Science",
    "views": "12008",
    "desc": "Generative question-answering focuses on the generation of multi-sentence answers to open-ended questions. It usually works by searching massive document stores for relevant information and then using it to generate answers synthetically. This tutorial demonstrates how to build a question-answering system using generative AI.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone example:\\\\nhttps://docs.pinecone.io/docs/abstractive-question-answering\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/nlp-transformers\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 What is generative AI and Q\\\\u0026A?\\\\n01:02 Generative question-answering architecture\\\\n04:36 Getting code and prerequisites\\\\n05:06 Data preprocessing\\\\n07:41 Embedding and indexing text\\\\n13:50 BART text generation model\\\\n14:52 Querying with generative question-answering\\\\n17:45 Asking questions and getting results\\\\n21:29 Final notes\"",
    "lengthSeconds": "1327",
    "uploadDate": "2022-12-14",
    "thumbnail_url": "https://i.ytimg.com/vi/L8U"
  },
  {
    "link": "watch?v=Fb-6K00SUtc",
    "title": "Table Question-Answering with TAPAS in Python",
    "tags": "machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, table question",
    "scraped_at": 1684585779.3606222,
    "genre": "Science",
    "views": "3890",
    "desc": "Table question-answering (QA) is like asking Excel a natural language question and getting a truly intelligent, human-like response. We can ask something like \\\\\"",
    "lengthSeconds": "1221",
    "uploadDate": "2022-12-07",
    "thumbnail_url": "https://i.ytimg.com/vi/Fb"
  },
  {
    "link": "watch?v=iIGlAsN1nEs",
    "title": "Advanced Sentiment Analysis with NLP Transformers + Vector Search",
    "tags": "natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, hugging face, hugging face tutorial, transformers pytorch, huggingface pytorch, huggingface tensorflow, huggingface pipeline, james briggs, natural language processing tutorial, natural language processing in artificial intelligence, natural language processing python, sentiment mining, opinion mining, sentiment analysis, sentiment analysis project, python",
    "scraped_at": 1684585776.1456468,
    "genre": "Science",
    "views": "2337",
    "desc": "Sentiment analysis, often known as opinion mining, is a technique used in natural language processing (NLP) to determine the emotional undertone of a text. Organizations use this to identify and group opinions about their product, service, and ideas.\\\\n\\\\nIn this video, we will learn how to apply sentiment analysis to huge datasets that can be turned into meaningful query databases rich with insights. We will apply this technique to the hotel industry and understand customer perception and potential improvement areas. To do this, we will:\\\\n\\\\n1. Generate Sentiment labels and scores based on customer reviews.\\\\n2. Store them in a Pinecone index as metadata (alongside respective text vectors).\\\\n3. Query Pinecone index on selected areas and understand customer opinions.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone doc page:\\\\nhttps://www.pinecone.io/docs/\\\\n\\\\nCode:\\\\nhttps://colab.research.google.com/github/pinecone-io/examples/blob/master/analytics-and-ml/data-mining/sentiment-mining/sentiment-mining.ipynb\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:31 What we will build\\\\n03:01 Code links and prerequisites\\\\n04:16 Dataset download and preprocessing\\\\n05:49 Using RoBERTa sentiment analysis model\\\\n08:15 Retriever model for building dense vectors\\\\n09:39 Create Pinecone vector index\\\\n11:40 Sentiment scores, vectors, and indexing\\\\n17:35 Sentiment analysis / opinion mining\\\\n20:43 Sentiment analysis with specific date range\\\\n21:44 Sentiment analysis on specific info\\\\n23:58 Final notes\\\\n\\\\n#machinelearning #deeplearning #ai #python\"",
    "lengthSeconds": "1508",
    "uploadDate": "2022-11-30",
    "thumbnail_url": "https://i.ytimg.com/vi/iIGlAsN1nEs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qU7wO02urYU",
    "title": "Vision Transformers (ViT) Explained + Fine-tuning in Python",
    "tags": "natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, vision transformer, vision transformer pytorch, vision transformer explained, vision transformer code, vision transformers huggingface, computer vision transformers, vit huggingface, hugging face, hugging face tutorial, transformers pytorch, huggingface pytorch, huggingface tensorflow, vision transformer tensorflow, huggingface pipeline, james briggs",
    "scraped_at": 1684585778.061621,
    "genre": "Science",
    "views": "12719",
    "desc": "Vision and language are the two big domains in machine learning. Two distinct disciplines with their own problems, best practices, and model architectures. At least, that was the case.\\\\n\\\\nThe Vision Transformer (ViT) marks the first step towards the merger of these two fields into a single unified discipline. For the first time in the history of ML, a single model architecture has come to dominate both language and vision.\\\\n\\\\nBefore ViT, transformers were \\\\\"",
    "lengthSeconds": "1827",
    "uploadDate": "2022-11-23",
    "thumbnail_url": "https://i.ytimg.com/vi/qU7wO02urYU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3K94GRjDG2Q",
    "title": "NER Powered Semantic Search in Python",
    "tags": "python, machine learning, artificial intelligence, natural language processing, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, named entity recognition, elasticsearch, elasticsearch alternative, opensearch, ner powered search, hybrid search, search engine, python semantic search, ner python, named entity recognition bert, named entity recognition ",
    "scraped_at": 1684585775.0356228,
    "genre": "Science",
    "views": "3461",
    "desc": "Semantic search is a compelling technology allowing us to search using abstract concepts and meaning rather than relying on specific words. However, sometimes a simple keyword search can be just as valuable \\xe2\\x80\\x94 especially if we know the exact wording of what we\\'re searching for.\\\\n\\\\nPinecone allows you to pair semantic search with a basic keyword filter. If you know that the document you\\'re looking for contains a specific word or set of words, you simply tell Pinecone to restrict the search to only include documents with those keywords.\\\\n\\\\nWe even support functionality for keyword search using sets of words with AND, OR, NOT logic.\\\\n\\\\nIn this video, we will explore these features through a start-to-finish example of basic keyword search in Pinecone.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone Docs Page:\\\\nhttps://www.pinecone.io/docs/examples/metadata-filtered-search/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 NER Powered Semantic Search\\\\n01:19 Dependencies and Hugging Face Datasets Prep\\\\n04:18 Creating NER Entities with Transformers\\\\n07:00 Creating Embeddings with Sentence Transformers\\\\n07:48 Using Pinecone Vector Database\\\\n11:33 Indexing the Full Medium Articles Dataset\\\\n15:09 Making Queries to Pinecone\\\\n17:01 Final Thoughts\"",
    "lengthSeconds": "1064",
    "uploadDate": "2022-11-16",
    "thumbnail_url": "https://i.ytimg.com/vi/3K94GRjDG2Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lqK4ocAKveE",
    "title": "Hugging Face Datasets #3 | Adding Images",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, nlp, Huggingface, Tensorflow, pytorch, tutorials, tutorial, learning, code, semantic search, similarity search, vector similarity search, vector search, python for beginners, python tutorial, vscode python, python speed tutorial, speed course python, hugging face tutorial, huggingface api, hugging face course, huggingface hub, huggingface datasets, huggingface sentence similarity",
    "scraped_at": 1684585775.515622,
    "genre": "Science",
    "views": "1160",
    "desc": "How to work with the Hugging Face datasets library in Python. Here we focus on adding images, using dataset builder scripts, the download manager and iter_archive function. Everything we do is using the best-practice methods for Hugging Face (huggingface) Datasets, all in Python. \\\\n\\\\nCan be used for datasets in image search, similarity search/semantic search/vector similarity search, classification, question-answering. Makes training/fine-tuning models with pytorch and tensorflow easy.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n02:05 Creating Tar Files for Images\\\\n05:11 Compressing Images in Tar Files\\\\n06:26 Adding Dataset Builder Script\\\\n09:07 Iterable Download Manager with iter_archive\\\\n09:56 _generate_examples Function Definition\\\\n12:52 Adding to Hugging Face Datasets Hub\\\\n13:34 Fixing Errors\\\\n14:23 Using Your New Dataset\\\\n14:53 Dealing with Larger Image Datasets\"",
    "lengthSeconds": "931",
    "uploadDate": "2022-11-09",
    "thumbnail_url": "https://i.ytimg.com/vi/lqK4ocAKveE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=0cKtkaR883c",
    "title": "Pinecone's New *Hybrid* Search - the future of search?",
    "tags": "python, machine learning, artificial intelligence, natural language processing, bert, nlp, Huggingface, semantic search, similarity search, vector similarity search, vector search, sparse index, sparse indexing vs dense indexing, elasticsearch vs algolia, elasticsearch, elasticsearch bm25 similarity, bm25 information retrieval, tf idf, tf idf implementation, bm25, keyword search, python tutorial, software development, data science, data structures, computer science, james briggs",
    "scraped_at": 1684585777.9726212,
    "genre": "Science",
    "views": "4426",
    "desc": "Vector search has unlocked the door to another level of relevance and efficiency in semantic search and information retrieval. In the past year, the number of vector search use cases has exploded, showing no signs of slowing down.\\\\n\\\\nThe capabilities of vector search are impressive, but it isn\\xe2\\x80\\x99t a perfect technology. In fact, without big domain-specific datasets to fine-tune models on, a traditional search still has some advantages.\\\\n\\\\nWe repeatedly see that vector search unlocks incredible and intelligent retrieval but struggles to adapt to new domains. Whereas traditional search can cope with new domains but is fundamentally limited to a set performance level.\\\\n\\\\nBoth approaches have pros and cons, but what if we merge them somehow to eliminate a few of those cons? Could we create a hybrid search with the heightened performance potential of vector search and the zero-shot adaptability of traditional search?\\\\n\\\\nThis video will show us how to take our search to a new level. Taking both vector and traditional search and merging them via Pinecone\\xe2\\x80\\x99s new hybrid search.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/hybrid-search-intro/\\\\n\\\\n\\xf0\\x9f\\x90\\xba Hybrid Search Early Access!\\\\nhttps://www.pinecone.io/hybrid-search-early-access/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "1228",
    "uploadDate": "2022-11-01",
    "thumbnail_url": "https://i.ytimg.com/vi/0cKtkaR883c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=i3OYlaoj-BM",
    "title": "Fast Zero Shot Object Detection with OpenAI CLIP",
    "tags": "artificial intelligence, natural language processing, Huggingface, pytorch, vscode python, openai whisper, python, machine learning, explained, openai, deep learning, code walk",
    "scraped_at": 1684585778.436641,
    "genre": "Science",
    "views": "4384",
    "desc": "Zero shot object detection is made easy with OpenAI CLIP. A state-of-the-art multi-modal deep learning model. Here we will learn about zero shot object detection (and object localization) and how to implement it in practice with OpenAI\\'s CLIP.\\\\n\\\\nILSVRC was a world-changing competition hosted annually from 2010 until 2017. It was the catalyst for the Renaissance of deep learning and was the place to find state-of-the-art image classification, object localization, and object detection.\\\\n\\\\nResearchers fine-tuned better-performance computer vision (CV) models to achieve ever more impressive results year-after-year. But there was an unquestioned assumption causing problems.\\\\n\\\\nWe assumed that every new task required model fine-tuning; this required *a lot* of data. and this needed both time and capital.\\\\n\\\\nIt wasn\\'t until very recently that this assumption was questioned and proven wrong.\\\\n\\\\nThe astonishing rise of multi-modal models has made the impossible possible across various domains and tasks. One of those is zero-shot object detection and localization.\\\\n\\\\nZero shot means applying a model without the need for fine-tuning. Meaning we take a multi-modal model and use it to detect images in one domain, then switch to another entirely different domain *without* the model seeing a single training example from the new domain. \\\\n\\\\nNot needing a single training example means we completely skip the hard part of data annotation and model training. We can focus solely on application of our models.\\\\n\\\\nIn this chapter, we will explore how to apply OpenAI\\'s CLIP to this task\\xe2\\x80\\x94using CLIP for localization and detection across domains with *zero* fine-tuning.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://pinecone.io/learn/zero-shot-object-detection-clip/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Early Progress in Computer Vision\\\\n02:03 Classification vs. Localization and Detection\\\\n03:55 Zero Shot with OpenAI CLIP\\\\n05:23 Zero Shot Object Localization with OpenAI CLIP\\\\n06:40 Localization with Occlusion Algorithm\\\\n07:44 Zero Shot Object Detection with OpenAI CLIP\\\\n08:34 Data Preprocessing for CLIP\\\\n13:55 Initializing OpenAI CLIP in Python\\\\n17:05 Clipping the Localization Visual\\\\n18:32 Applying Scores for Visual\\\\n20:25 Object Localization with New Prompt\\\\n20:52 Zero Shot Object Detection in Python\\\\n21:20 Creating Bounding Boxes with Matplotlib\\\\n25:15 Object Detection Code\\\\n27:11 Object Detection Results\\\\n28:29 Trends in Multi-Modal ML\\\\n\\\\n#machinelearning #python #openai\"",
    "lengthSeconds": "1771",
    "uploadDate": "2022-10-26",
    "thumbnail_url": "https://i.ytimg.com/vi/i3OYlaoj"
  },
  {
    "link": "watch?v=vpU_6x3jowg",
    "title": "How to Use OpenAI Whisper to Fix YouTube Search",
    "tags": "artificial intelligence, natural language processing, Huggingface, pytorch, vscode python, openai whisper, open ai whisper, openai whisper speech recognition system, convert audio to text in python with openai whisper, speech",
    "scraped_at": 1684585778.354642,
    "genre": "Science",
    "views": "9044",
    "desc": "OpenAI\\'s Whisper is a new open-source, state-of-the-art speech-to-text. This project will use Open AI Whisper to make YouTube search amazing. \\\\n\\\\nSearch on YouTube is good but has its limitations. With trillions of hours of content, there should be an answer to almost every question. Yet, if we have a specific question like \\\\\"",
    "lengthSeconds": "2394",
    "uploadDate": "2022-10-18",
    "thumbnail_url": "https://i.ytimg.com/vi/vpU_6x3jowg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=QGdbs2S2YHk",
    "title": "How to Learn (a lot)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding, vector search, How to Learn ",
    "scraped_at": 1684585778.6526535,
    "genre": "Science",
    "views": "6966",
    "desc": "The modern world rewards those that can learn and adapt quickly. Both financially and in your own personal fulfillment. Learning new skills keeps life interesting and fresh, but it isn\\'t easy to do.\\\\n\\\\nContinuous learning can fry your brain very quickly. Yet, there\\'s nothing better than expanding the scope of your world in the way that learning can do.\\\\n\\\\nHow do we learn (a lot) optimally without hitting roadblocks like burnout?\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Medium article:\\\\nhttps://jamescalam.medium.com/how-to-learn-a-lot-696f7f626acc\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link (free access):\\\\nhttps://jamescalam.medium.com/how-to-learn-a-lot-696f7f626acc?sk=2f241465ee0c3be7fb609a846afe3480\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "985",
    "uploadDate": "2022-10-12",
    "thumbnail_url": "https://i.ytimg.com/vi/QGdbs2S2YHk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=98POYg2HZqQ",
    "title": "OpenAI's CLIP for Zero Shot Image Classification",
    "tags": "Huggingface, Tensorflow, artificial intelligence, coding, convolutional neural network, data science, machine learning, natural language processing, nlp, openai clip, programming, python, pytorch, semantic search, similarity search, vector search, vector similarity search, openai multimodal, openai clip tutorial, huggingface pytorch, huggingface fine tune, how to use huggingface models, deep learning projects, zero shot, few shot learning, zero shot learning, vision transformer",
    "scraped_at": 1684585780.7556646,
    "genre": "Science",
    "views": "3304",
    "desc": "State-of-the-art (SotA) computer vision (CV) models are characterized by a *restricted* understanding of the visual world specific to their training data [1].\\\\n\\\\nThese models can perform *very well* on specific tasks and datasets, but they do not generalize well. They cannot handle new classes or images beyond the domain they have been trained with.\\\\n\\\\nIdeally, a CV model should learn the contents of images without excessive focus on the specific labels it is initially trained to understand.\\\\n\\\\nFortunately, OpenAI\\'s CLIP has proved itself as an incredibly flexible CV classification model that often requires *zero* retraining. In this chapter, we will explore CLIP in zero-shot image classification.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://pinecone.io/learn/zero-shot-image-classification-clip/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "1302",
    "uploadDate": "2022-10-05",
    "thumbnail_url": "https://i.ytimg.com/vi/98POYg2HZqQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ODdKC30dT8c",
    "title": "Hugging Face Datasets #2 | Dataset Builder Scripts (for Beginners)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding, semantic search, similarity search, vector similarity search, vector search, python for beginners, learn python, python tutorial, edureka python, vscode python, python speed tutorial, speed course python",
    "scraped_at": 1684585777.4706457,
    "genre": "Science",
    "views": "1694",
    "desc": "How to work with dataset builder scripts, intro to the download manager, and Apache Arrow datatypes used in Hugging Face (huggingface) Datasets - all in Python. Can be used for datasets in similarity search/semantic search/vector similarity search, classification, question-answering. Makes training/fine-tuning models with pytorch and tensorflow easy.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:49 Creating Compressed Files\\\\n02:41 Creating Dataset Build Script\\\\n04:49 Download Manager\\\\n08:59 Finishing Split Generator\\\\n10:13 Generate Examples Method\\\\n14:47 Add Dataset to Hugging Face\\\\n17:49 Apache Arrow Features\\\\n22:52 What\\'s Next?\"",
    "lengthSeconds": "1404",
    "uploadDate": "2022-09-23",
    "thumbnail_url": "https://i.ytimg.com/vi/ODdKC30dT8c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=fGwH2YoQkDM",
    "title": "OpenAI CLIP Explained | Multi-modal ML",
    "tags": "CLIP Explained ",
    "scraped_at": 1684585779.8626204,
    "genre": "Science",
    "views": "9229",
    "desc": "OpenAI\\'s CLIP explained simply and intuitively with visuals and code. Language models (LMs) can not rely on language alone. That is the idea behind the \\\\\"",
    "lengthSeconds": "2012",
    "uploadDate": "2022-09-15",
    "thumbnail_url": "https://i.ytimg.com/vi/fGwH2YoQkDM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-S20nblUuNw",
    "title": "Hugging Face Datasets #1 | Hosting Your Datasets (for Beginners)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, programming, coding, semantic search, similarity search, vector similarity search, vector search, hugging face tutorial, hugging face course, hugging face, huggingface nlp",
    "scraped_at": 1684585775.7396464,
    "genre": "Science",
    "views": "2451",
    "desc": "Introduction to Hugging Face datasets, how it works, and how to host your own simple datasets (JSONL, TSV, CSV, etc) for free via Hugging Face Datasets Hub\\\\n\\\\nWarp download:\\\\nhttps://app.warp.dev/referral/7G3N39\\\\n\\\\nGit LFS Install:\\\\nMac:\\\\n$ brew install git-lfs\\\\nDebian/Ubuntu:\\\\n$ curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\\\\n$ sudo apt-get install git-lfs\\\\nWindows:\\\\nGet install from https://github.com/git-lfs/git-lfs/releases\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n04:36 Creating our own Datasets\\\\n08:29 Creating JSONL for Hugging Face\\\\n15:15 Uploading Datasets for Git\\\\n19:10 LFS for Large Files\\\\n21:56 Closing Notes\"",
    "lengthSeconds": "1381",
    "uploadDate": "2022-09-09",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=pfwBut7E60Q",
    "title": "GUI-based Few Shot Classification Model Trainer | Demo",
    "tags": "Huggingface, Tensorflow, ai text to image, artificial intelligence, code, data science, james briggs, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, vector search, vector similarity search, few shot, zero shot, few shot learning, zero shot learning, gui python, image classification, image classification pytorch, image classification python",
    "scraped_at": 1684585775.4406214,
    "genre": "Science",
    "views": "1337",
    "desc": "Learn how to use vector search to create highly targeted training for any classification model using a final linear classification layer. Easily fine-tune models in 10 minutes with less than 100 labeled examples.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://pinecone.io/learn/classifier-train-vector-search/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:14 Classification\\\\n02:49 Better Classifier Training\\\\n06:33 Classification as Vector Search\\\\n08:47 How Fine-tuning Works\\\\n10:50 Identifying Important Samples\\\\n12:39 CODE IMPLEMENTATION\\\\n13:13 Indexing\\\\n18:59 Fine-tuning the Classifier\\\\n27:37 Classifier Predictions\\\\n30:43 Closing Notes\\\\n\\\\n#machinelearning #python #codingchallenge\"",
    "lengthSeconds": "1931",
    "uploadDate": "2022-08-31",
    "thumbnail_url": "https://i.ytimg.com/vi/pfwBut7E60Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=c_u4AHNjOpk",
    "title": "AlexNet and ImageNet Explained",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, convolutional neural network, data science, education, imagenet, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.7206225,
    "genre": "Science",
    "views": "4301",
    "desc": "Today\\'s deep learning revolution traces back to the 30th of September, 2012. On this day, a Convolutional Neural Network (CNN) called AlexNet won the ImageNet 2012 challenge. AlexNet didn\\'t just win; it dominated.\\\\n\\\\nAlexNet was unlike the other competitors. This new model demonstrated unparalleled performance on the largest image dataset of the time, ImageNet. This event made AlexNet the first widely acknowledged, successful application of deep learning. It caught people\\'s attention with a 9.8 percentage point advantage over the nearest competitor.\\\\n\\\\nUntil this point, deep learning was a nice idea that most deemed as impractical. AlexNet showed that deep learning was more than a pipedream, and the authors showed the world how to make it practical. Yet, the surge of deep learning that followed was not fueled solely by AlexNet. Indeed, without the huge ImageNet dataset, there would have been no AlexNet.\\\\n\\\\nThe future of AI was to be built on the foundations set by the ImageNet challenge and the novel solutions that enabled the synergy between ImageNet and AlexNet.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://pinecone.io/learn/imagenet\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:06 Birth of Deep Learning\\\\n02:52 ImageNet\\\\n07:56 Lack of Readiness for Big Datasets\\\\n09:57 ImageNet Challenge (ILSVRC)\\\\n11:47 AlexNet\\\\n19:30 PYTORCH IMPLEMENTATION\\\\n19:55 Data Preprocessing\\\\n27:06 Class Prediction with AlexNet\\\\n31:50 Goldfish Results\\\\n34:27 Closing Notes\"",
    "lengthSeconds": "2180",
    "uploadDate": "2022-08-24",
    "thumbnail_url": "https://i.ytimg.com/vi/c_u4AHNjOpk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=szfG55juoJE",
    "title": "How I work from anywhere with the best tools for remote work",
    "tags": "work from anywhere in the world, work from anywhere in the world jobs, remote work, how to work remotely effectively, how to work remotely and travel, work from anywhere in the world online, how to work remotely, make money online, data science, work from anywhere, how to work remotely as a software developer, machine learning, artificial intelligence, natural language processing, nlp, coding, python, vector similarity search, Huggingface, how to make money online, work from home",
    "scraped_at": 1684585778.1306465,
    "genre": "Science",
    "views": "1168",
    "desc": "Overview of how I deal with travel and work. Showing how to work while you travel if you make money online. With work and travel a good remote desk setup is important for staying as ergonomic and productive as possible, enjoy!\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Links to products (mostly affiliate):\\\\nLaptop stand: https://amzn.to/3bZqMHM\\\\nSecond screen: https://amzn.to/3w6IT5B\\\\nCable bag (international): https://amzn.to/3QBH7S7\\\\n                              ... or UK: https://amzn.to/3ps5lT2\\\\nPeak Design backpacks: https://www.peakdesign.com/products/everyday-backpack\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "767",
    "uploadDate": "2022-08-16",
    "thumbnail_url": "https://i.ytimg.com/vi/szfG55juoJE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=989aKUVBfbk",
    "title": "Fast intro to multi-modal ML with OpenAI's CLIP",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, clip openai, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, openai clip, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585775.8166206,
    "genre": "Science",
    "views": "3108",
    "desc": "OpenAI\\'s CLIP is \\\\\"",
    "lengthSeconds": "1374",
    "uploadDate": "2022-08-11",
    "thumbnail_url": "https://i.ytimg.com/vi/989aKUVBfbk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jjQetJtQDS4",
    "title": "Bag of *Visual* Words for Image Classification and Retrieval",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bag, bag of visual words, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585780.000647,
    "genre": "Science",
    "views": "2398",
    "desc": "In computer vision, bag of visual words (BoVW) is one of the pre-deep learning models used for building image embeddings. Allowing us to retrieve images from a database that are similar to another \\\\\"",
    "lengthSeconds": "3367",
    "uploadDate": "2022-08-03",
    "thumbnail_url": "https://i.ytimg.com/vi/jjQetJtQDS4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=UzkdOg7wWmI",
    "title": "\ud83e\udd17 Hugging Face just released *Diffusers* - for models like DALL-E 2 and Imagen!",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, dall e, dall e 2, dall e",
    "scraped_at": 1684585777.6816227,
    "genre": "Science",
    "views": "3710",
    "desc": "Hugging Face of transformer fame have created a whole new Python library for diffusion models! Diffusion models are a key component of models like OpenAI\\'s DALL-E-2, Google\\'s Imagen, and Midjourney\\'s image generation service. HuggingFace Diffusers brings these models to a new level of accessibility (and open source!).\\\\n\\\\n\\xf0\\x9f\\x93\\x95  Article:\\\\nhttps://towardsdatascience.com/hugging-face-just-released-the-diffusers-library-846f32845e65\\\\n\\\\n\\xf0\\x9f\\x93\\x96  Friend Link (free access):\\\\nhttps://towardsdatascience.com/hugging-face-just-released-the-diffusers-library-846f32845e65?sk=9ec4027460defa1fd25178af9a55da13\\\\n\\\\n\\xf0\\x9f\\xa7\\xa8 Diffusers:\\\\nhttps://github.com/huggingface/diffusers\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n00:00 What are Diffusers?\\\\n01:55 Getting started\\\\n04:20 Prompt engineering\\\\n09:34 Testing other diffusers\"",
    "lengthSeconds": "933",
    "uploadDate": "2022-07-26",
    "thumbnail_url": "https://i.ytimg.com/vi/UzkdOg7wWmI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=I3na13AESjw",
    "title": "How to use Color Histograms for Image Retrieval",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search, wav2vec 2.0",
    "scraped_at": 1684585780.290622,
    "genre": "Science",
    "views": "1634",
    "desc": "Browsing, searching, and retrieving images has never been easy. Traditionally, many technologies relied on manually appending metadata to images and searching via this metadata. This approach works for datasets with high-quality annotation, but most datasets are too large for manual annotation.\\\\n\\\\nThat means any large image dataset must rely on Content-Based Image Retrieval (CBIR). Search with CBIR focuses on comparing the *content* of an image rather than its metadata. Content can be color, shapes, textures \\xe2\\x80\\x93 or with some of the latest advances in ML - the \\\\\"",
    "lengthSeconds": "1864",
    "uploadDate": "2022-07-13",
    "thumbnail_url": "https://i.ytimg.com/vi/I3na13AESjw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=coaaSxys5so",
    "title": "How to build next-level Q&A with OpenAI",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, open ai api, openai, openai playground, pinecone, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.2176454,
    "genre": "Science",
    "views": "4720",
    "desc": "Walkthrough of the OpenAI x Pinecone Q\\\\u0026A app I built for a webinar with OpenAI. This is the coolest Q\\\\u0026A app I\\'ve ever built thanks to Pinecone vector search and OpenAI\\'s incredible embeddings and generation endpoints.\\\\n\\\\nLINKS:\\\\n\\xf0\\x9f\\x95\\xb9 App:\\\\nhttps://pinecone-io-playground-beyond-search-openaisrcserver-h65vzl.streamlitapp.com\\\\n\\xf0\\x9f\\x91\\xa8\\xe2\\x80\\x8d\\xf0\\x9f\\x92\\xbb Code and Data:\\\\nhttps://github.com/pinecone-io/examples/tree/master/integrations/openai/beyond_search_webinar\\\\nOpenAI x Pinecone Webinar:\\\\n\\xe2\\x96\\xb6\\xef\\xb8\\x8f https://www.youtube.com/watch?v=HtI9easWtAA\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "1168",
    "uploadDate": "2022-07-07",
    "thumbnail_url": "https://i.ytimg.com/vi/coaaSxys5so/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BD9TkvEsKwM",
    "title": "Evaluation Measures for Search and Recommender Systems",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, haystack, information retrieval, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, recommender system, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585777.2536204,
    "genre": "Science",
    "views": "3349",
    "desc": "In this video you will learn about popular offline metrics (evaluation measures) like Recall@K, Mean Reciprocal Rank (MRR), Mean Average Precision@K (MAP@K), and Normalized Discounted Cumulative Gain (NDCG@K). We will also demonstrate how each of these metrics can be replicated in Python.\\\\n\\\\nEvaluation of information retrieval (IR) systems is critical to making well-informed design decisions. From search to recommendations, evaluation measures are paramount to understanding what does and does not work in retrieval.\\\\n\\\\nMany big tech companies contribute much of their success to well-built IR systems. One of Amazon\\'s earliest iterations of the technology was reportedly driving more than 35% of their sales. Google attributes 70% of YouTube views to their IR recommender systems.\\\\n\\\\nIR systems power some of the greatest companies in the world, and behind every successful IR system is a set of evaluation measures.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/offline-evaluation\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code notebooks:\\\\nhttps://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/offline-evaluation\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:51 Offline Metrics\\\\n02:38 Dataset and Retrieval 101\\\\n06:08 Recall@K\\\\n07:57 Recall@K in Python\\\\n09:03 Disadvantages of Recall@K\\\\n10:21 MRR\\\\n13:32 MRR in Python\\\\n14:18 MAP@K\\\\n18:17 MAP@K in Python\\\\n19:27 NDCG@K\\\\n29:26 Pros and Cons of NDCG@K\\\\n29:48 Final Thoughts\"",
    "lengthSeconds": "1885",
    "uploadDate": "2022-06-28",
    "thumbnail_url": "https://i.ytimg.com/vi/BD9TkvEsKwM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_OAU1kQdmgE",
    "title": "How to Learn Data Science | ML | Programming",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, learn data science, learn data science for free, how to learn data science for free, learn machine learning, learn machine learning for free, how to become data scientist, freelance data scientist, data scientist, data science jobs, how to learn machine learning, learn python, machine learning tutorial, data science for beginners, deep learning, learn to code, data science projects",
    "scraped_at": 1684585778.2766464,
    "genre": "Education",
    "views": "852",
    "desc": "In this video I share five of the approaches/thoughts I have regarding learning, in particular for learning data science, machine learning, or programming.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:33 Scale of Theory vs. Applied\\\\n02:55 Shape of Learning\\\\n05:52 Courses vs. Projects\\\\n08:37 Open Source\\\\n10:44 Writing\\\\n12:44 Following Interests\\\\n15:42 Final Notes\"",
    "lengthSeconds": "992",
    "uploadDate": "2022-06-15",
    "thumbnail_url": "https://i.ytimg.com/vi/_OAU1kQdmgE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=xXsDIK9z_fg",
    "title": "Using Semantic Search to Find GIFs",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding, semantic search, similarity search, vector similarity search, vector search",
    "scraped_at": 1684585780.4366205,
    "genre": "Science",
    "views": "802",
    "desc": "Vector search powers some of the most popular services in the world. It serves your Google results, delivers the best podcasts on Spotify, and accounts for at least 35% of consumer purchases on Amazon.\\\\n\\\\nIn this article, we will use vector search applied to language, called semantic search, to build a GIF search engine. Unlike more traditional search where we rely on keyword matching, semantic search enables search based on the human meaning behind text and images. That means we can find highly relevant GIFs with natural language prompts.\\\\n\\\\nThe pipeline for a project like this is simple, yet powerful. It can easily be adapted to tasks as diverse as video search or answering Super Bowl questions, or as we\\xe2\\x80\\x99ll see, finding GIFs.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/gif-search\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code:\\\\nhttps://github.com/pinecone-io/examples/tree/master/search/semantic-search/gif-search\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:17 GIF Search Demo\\\\n01:56 Pipeline Overview\\\\n05:33 Data Preparation\\\\n08:17 Vector Database and Retriever\\\\n12:37 Querying\\\\n15:42 Streamlit App Code\"",
    "lengthSeconds": "1050",
    "uploadDate": "2022-06-07",
    "thumbnail_url": "https://i.ytimg.com/vi/xXsDIK9z_fg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=FzLIIwiaXSU",
    "title": "How to Build an AI-Powered Video Search App",
    "tags": "Huggingface, Tensorflow, ai app, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585775.5896237,
    "genre": "Science",
    "views": "3358",
    "desc": "Technology and culture have advanced and become ever more entangled. Some of the most significant technological breakthroughs are integrated so tightly into our culture that we never even notice they\\'re there.\\\\n\\\\nOne of those is AI-powered search. It powers your Google results, Netflix recommendations, and ads you see everywhere. It is being rapidly weaved throughout all aspects of our lives. Further, this is a new technology; its full potential is unknown.\\\\n\\\\nThis technology weaves directly into the cultural phenomenon of YouTube. Imagine a search engine like Google that allows you to rapidly access the billions of hours of YouTube content. There is no comparison to that level of highly engaging video content in the world.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/youtube-search\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code:\\\\nhttps://github.com/pinecone-io/examples/tree/master/search/semantic-search/yt-search\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n02:56 YouTube Search App\\\\n04:43 Getting Data\\\\n07:58 Enhancing the Data\\\\n12:45 Scraping Other Metadata\\\\n14:52 Loading Data from Hugging Face\\\\n15:42 Index and Query the Data\\\\n20:43 Streamlit App Code\"",
    "lengthSeconds": "1342",
    "uploadDate": "2022-06-01",
    "thumbnail_url": "https://i.ytimg.com/vi/FzLIIwiaXSU/hqdefault.jpg"
  },
  {
    "link": "watch?v=uYas6ysyjgY",
    "title": "New GPU-Acceleration for PyTorch on M1 Macs! + using with BERT",
    "tags": "Huggingface, Tensorflow, andrej karpathy, artificial intelligence, bert, code, coding, data science, education, gpu acceleration, ipados 16 leaks, learning, m1 max, m1 ultra, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search, pytorch tutorial, natural language processing tutorial",
    "scraped_at": 1684585779.5726476,
    "genre": "Science",
    "views": "12761",
    "desc": "GPU-acceleration on Mac is finally here!\\\\n\\\\nToday\\'s deep learning models owe a great deal of their exponential performance gains to ever increasing model sizes. Those larger models require more computations to train and run.\\\\n\\\\nThese models are simply too big to be run on CPU hardware, which performs large step-by-step computations. Instead, they need massively parallel computations. That leaves us with either GPU or TPU hardware.\\\\n\\\\nOur home PCs aren\\'t coming with TPUs anytime soon, so we\\'re left with the GPU option. GPUs use a highly parallel structure, originally designed to process images for visual heavy processes. They became essential components in gaming for rendering real-time 3D images.\\\\n\\\\nGPUs are essential for the scale of today\\'s models. Using CPUs makes many of these models too slow to be useful, which can make deep learning on M1 machines rather disappointing.\\\\n\\\\nFortunately, this is changing with the support of GPU on M1 machines beginning with PyTorch v1.12. In this video we will explain the new integration and how to implement it yourself.\\\\n\\\\n\\xf0\\x9f\\x93\\x95  Article:\\\\nhttps://towardsdatascience.com/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1\\\\n\\\\n\\xf0\\x9f\\x93\\x96  Friend Link (free access):\\\\nhttps://towardsdatascience.com/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1?sk=a88acd35f600858093c177b97d690b03\\\\n\\\\n\\xf0\\x9f\\x94\\x97  Code notebooks:\\\\nhttps://github.com/jamescalam/pytorch-mps\\\\n\\\\n\\xf0\\x9f\\xa4\\x96  70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89  Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe  Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:34 PyTorch MPS\\\\n04:57 Installing ARM Python\\\\n09:09 Using PyTorch with GPU\\\\n12:14 BERT on PyTorch GPU\\\\n13:51 Best way to train LLMs on Mac\\\\n16:01 Buffer Size Bug\\\\n17:24 When we would use Mac M1 GPU\"",
    "lengthSeconds": "1139",
    "uploadDate": "2022-05-24",
    "thumbnail_url": "https://i.ytimg.com/vi/uYas6ysyjgY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=O9lrWt15wH8",
    "title": "Long Form Question Answering (LFQA) in Haystack",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, haystack, learning, machine learning, natural language processing, nlp, nlproc, programming, publicis sapient, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585780.6896465,
    "genre": "Science",
    "views": "5195",
    "desc": "Question-Answering (QA) has exploded as a subdomain of Natural Language Processing (NLP) in the last few years. QA is a widely applicable use case in NLP yet was out of reach until the introduction of [transformer models](/learn/transformers/) in 2017.\\\\n\\\\nWithout transformer models, the level of language comprehension required to make something as complex as QA work simply was not possible.\\\\n\\\\nAlthough QA is a complex topic, it comes from a simple idea. The automatic retrieval of information via a more human-like interaction. The task of information retrieval (IR) is performed by almost every organization in the world. Without other options, organizations rely on person-to-person IR and rigid keyword search tools. This haphazard approach to IR generates a lot of friction, particularly for larger organizations.\\\\n\\\\nQA offers a solution to this problem. Rather than these documents being lost in an abyss, they can be stored within a space where an intelligent QA agent can access them. Unlike humans, our QA agent can scan millions of documents in seconds and return answers from these documents almost instantly.\\\\n\\\\nWith QA tools, employees can stop wasting time searching for snippets of information and focus on their *real*, value-adding tasks.\\\\n\\\\nA small investment in QA is, for most organizations, a no-brainer.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/haystack-lfqa\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code notebooks:\\\\nhttps://github.com/pinecone-io/examples/blob/master/integrations/haystack/haystack_lfqa.ipynb\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n04:20 Approaches to Question Answering\\\\n05:43 Components of QA Pipeline\\\\n08:58 LFQA Generator\\\\n09:40 Haystack Setup\\\\n10:32 Initialize Document Store\\\\n13:02 Getting Data\\\\n17:53 Indexing Embeddings\\\\n21:51 Initialize Generator\\\\n24:10 Asking Questions\\\\n26:12 Common Problems\\\\n29:32 Generator Memory\\\\n31:30 Few More Questions\\\\n34:54 Outro\"",
    "lengthSeconds": "2158",
    "uploadDate": "2022-05-17",
    "thumbnail_url": "https://i.ytimg.com/vi/O9lrWt15wH8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=fb7LENb9eag",
    "title": "BERTopic Explained",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, bert model explained, bert sentiment analysis, freecodecamp, hdbscan, james briggs, lda topic modelling explained, machine learning, natural language processing, nlp, nlproc, python, top2vec, topic modeling, topic modeling python, topic modelling, topic modelling nlp, umap",
    "scraped_at": 1684585780.3676472,
    "genre": "Science",
    "views": "13775",
    "desc": "90% of the world\\'s data is unstructured. It is built by humans, for humans. That\\'s great for human consumption, but it is *very* hard to organize when we begin dealing with the massive amounts of data abundant in today\\'s information age.\\\\n\\\\nOrganization is complicated because unstructured text data is not intended to be understood by machines, and having humans process this abundance of data is wildly expensive and *very slow*.\\\\n\\\\nFortunately, there is light at the end of the tunnel. More and more of this unstructured text is becoming accessible and understood by machines. We can now search text based on *meaning*, identify the sentiment of text, extract entities, and much more.\\\\n\\\\nTransformers are behind much of this. These transformers are (unfortunately) not Michael Bay\\'s Autobots and Decepticons and (fortunately) not buzzing electrical boxes. Our NLP transformers lie somewhere in the middle, they\\'re not sentient Autobots (yet), but they can understand language in a way that existed only in sci-fi until a short few years ago.\\\\n\\\\nMachines with a human-like comprehension of language are pretty helpful for organizing masses of unstructured text data. In machine learning, we refer to this task as *topic modeling*, the automatic clustering of data into particular topics.\\\\n\\\\nBERTopic takes advantage of the superior language capabilities of these (not yet sentient) transformer models and uses some other ML magic like UMAP and HDBSCAN (more on these later) to produce what is one of the most advanced techniques in language topic modeling today.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/bertopic\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code notebooks:\\\\nhttps://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:40 In this video\\\\n02:58 BERTopic Getting Started\\\\n08:48 BERTopic Components\\\\n15:21 Transformer Embedding\\\\n18:33 Dimensionality Reduction\\\\n25:07 UMAP\\\\n31:48 Clustering\\\\n37:22 c-TF-IDF\\\\n40:49 Custom BERTopic\\\\n44:04 Final Thoughts\"",
    "lengthSeconds": "2714",
    "uploadDate": "2022-05-11",
    "thumbnail_url": "https://i.ytimg.com/vi/fb7LENb9eag/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gVAJ_l_S7uQ",
    "title": "How to learn NLP for free for beginners",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, coding live, coursera, data science, learn natural language processing, learning, machine learning, natural language processing, nlp, nlp for beginners, nlp transformer, nlproc, programming, python, pytorch, recurrent neural network, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585777.5476465,
    "genre": "Science",
    "views": "8919",
    "desc": "Knowing what to learn is one of the hardest parts about self-learning. Imagine being thrown into the wilderness and being told to find a specific landmark. Without a map you will end up wandering to wilderness with no better option than taking one step after another.\\\\n\\\\nI spent a long time wandering step-by-step and eventually found my way into working with deep learning and NLP full-time.\\\\n\\\\nHere I will share many of the resources I used or wish I had used in the past. You can this \\\\\"",
    "lengthSeconds": "1402",
    "uploadDate": "2022-04-26",
    "thumbnail_url": "https://i.ytimg.com/vi/gVAJ_l_S7uQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ok0SDdXdat8",
    "title": "Spotify's Podcast Search Explained",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding, semantic search, similarity search, vector similarity search, vector search",
    "scraped_at": 1684585776.811647,
    "genre": "Science",
    "views": "1712",
    "desc": "The market for podcasts has grown tremendously in recent years.\\\\n\\\\nDriving the charge in podcast adoption is Spotify. In a few short years, they have become the undisputed leaders in podcasting. Despite only entering the game in 2018, by late 2021, Spotify had already usurped Apple, the long-reigning leader in podcasts, with more than 28M monthly podcast listeners.\\\\n\\\\nTo back their podcast investments, Spotify has worked on making the podcast experience as seamless and accessible as possible. From their all-in-one podcast creation app (Anchor) to podcast APIs and their latest natural language enabled podcast search.\\\\n\\\\nSpotify\\xe2\\x80\\x99s natural language search for podcasts is a fascinating use case. In the past, users had to rely on keyword/term matching to find the podcast episodes they wanted. Now, they can search in natural language, in much the same way we might ask a real person where to find something.\\\\n\\\\nIn this video, we will take a look under the hood of Spotify\\'s podcast search, and learn how to implement a similar system ourselves.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/spotify-podcast-search\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code and tests:\\\\nhttps://github.com/pinecone-io/examples/tree/master/search/semantic-search/spotify-podcast-search\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n04:16 NLP in Semantic Search\\\\n08:35 Why Now?\\\\n09:29 Transformer Models\\\\n11:52 Sentence Transformers\\\\n13:12 Vector Search\\\\n15:56 How Spotify Built Podcast Search\\\\n17:35 Data Source, Fine-tuning, and Eval\\\\n22:58 Code Implementation, Dataset\\\\n24:44 Data Preparation\\\\n26:39 Query Generation\\\\n29:54 Fine-tuning a Podcast Model\\\\n41:40 Evaluation\\\\n48:05 Does it Scale?\\\\n49:00 Sharing Your Work\"",
    "lengthSeconds": "2997",
    "uploadDate": "2022-04-14",
    "thumbnail_url": "https://i.ytimg.com/vi/ok0SDdXdat8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=j3psNM5y-eA",
    "title": "Implementing Filters in the New Haystack Doc Store",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, haystack, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585776.2916234,
    "genre": "Science",
    "views": "306",
    "desc": "\\xf0\\x9f\\xa5\\xb3  Released with Haystack v1.3! Install direct from PyPI with:\\\\n\\\\npip install \\'farm-haystack[pinecone]\\'\\\\n\\\\nJoin me as I work through the final few PR issues on the latest Haystack document store, and figure out how Haystack\\'s filter_utils work.\\\\n\\\\nPR:\\\\nhttps://github.com/deepset-ai/haystack/pull/2254\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n02:41 Filtering\\\\n05:36 Testing Existing Filter Utils\\\\n07:57 Making Sense of Filter Utils\\\\n10:35 Writing the First Filter\\\\n16:26 First Working Filter\\\\n18:24 Testing New Filters\\\\n21:27 Implementing in the Doc Store\\\\n24:02 Testing Pipeline Filters\\\\n27:11 Final Issue and Outro\"",
    "lengthSeconds": "1694",
    "uploadDate": "2022-04-06",
    "thumbnail_url": "https://i.ytimg.com/vi/j3psNM5y"
  },
  {
    "link": "watch?v=uEbCXwInnPs",
    "title": "Is GPL the Future of Sentence Transformers? | Generative Pseudo-Labeling Deep Dive",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, gpl, james briggs, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, transformer model, transformer nlp, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585776.0016212,
    "genre": "Science",
    "views": "7482",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nTraining sentence transformers is hard; they need vast amounts of labeled data. On one hand, the internet is full of data, and, on the other, this data is *not* in the format we need. We usually need to use a supervised training method to train a high-performance bi-encoder (sentence transformer) model.\\\\n\\\\nThere is research producing techniques placing us ever closer to fine-tuning high-perfomance bi-encoder models with unlabeled text data. One of the most promising is GPL. At its core, GPL allows us to take unstructured text data and use it to build models that can understand this text. These models can then intelligently respond to natural language queries regarding this same text data.\\\\n\\\\nIt is a fascinating approach, with massive potential across innumerous use cases spanning all industries and borders. With that in mind, let\\'s dive into the details of GPL and how we can implement it to build high-performance LMs with nothing more than plain text.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/gpl/\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Notebooks:\\\\nhttps://github.com/pinecone-io/examples/tree/master/analytics-and-ml/model-training/gpl\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:08 Semantic Web and Other Uses\\\\n04:36 Why GPL?\\\\n07:31 How GPL Works\\\\n10:37 Query Generation\\\\n12:08 CORD-19 Dataset and Download\\\\n13:27 Query Generation Code\\\\n21:53 Query Generation is Not Perfect\\\\n22:39 Negative Mining\\\\n26:28 Negative Mining Implementation\\\\n27:21 Negative Mining Code\\\\n35:19 Pseudo-Labeling\\\\n35:55 Pseudo-Labeling Code\\\\n37:01 Importance of Pseudo-Labeling\\\\n41:20 Margin MSE Loss\\\\n43:40 MarginMSE Fine-tune Code\\\\n46:30 Choosing Number of Steps\\\\n48:54 Fast Evaluation\\\\n51:43 What\\'s Next for Sentence Transformers?\"",
    "lengthSeconds": "3174",
    "uploadDate": "2022-03-30",
    "thumbnail_url": "https://i.ytimg.com/vi/uEbCXwInnPs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Dn8OYkatiU0",
    "title": "Testing the New Haystack Doc Store",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, faiss, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585777.328623,
    "genre": "Science",
    "views": "522",
    "desc": "\\xf0\\x9f\\xa5\\xb3  Released with Haystack v1.3! Install direct from PyPI with:\\\\n\\\\npip install \\'farm-haystack[pinecone]\\'\\\\n\\\\nPR:\\\\nhttps://github.com/deepset-ai/haystack/pull/2254\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:19 Demo Start and Install\\\\n03:25 Initialization\\\\n06:30 Download and Write Documents\\\\n10:55 Extractive QA Pipeline\\\\n11:23 Fetch by ID\\\\n19:01 Metadata Filtering\\\\n22:24 Get All Documents\"",
    "lengthSeconds": "1399",
    "uploadDate": "2022-03-22",
    "thumbnail_url": "https://i.ytimg.com/vi/Dn8OYkatiU0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JydpRavoJqI",
    "title": "Adding New Doc Stores to Haystack",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding, semantic search, similarity search, vector similarity search, vector search",
    "scraped_at": 1684585777.6156216,
    "genre": "Science",
    "views": "1453",
    "desc": "\\xf0\\x9f\\xa5\\xb3  Released with Haystack v1.3! Install direct from PyPI with:\\\\n\\\\npip install \\'farm-haystack[pinecone]\\'\\\\n\\\\nPR:\\\\nhttps://github.com/deepset-ai/haystack/pull/2254\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n02:15 Contributing or Testing\\\\n03:31 ODQA\\\\n06:20 What is Haystack?\\\\n08:13 Haystack QA Workflow\\\\n14:52 Contributing to Open Source\\\\n22:54 Haystack Doc Stores\\\\n26:09 Doc Store Core Methods\\\\n29:31 Final Notes, Contribute/Test\"",
    "lengthSeconds": "1824",
    "uploadDate": "2022-03-15",
    "thumbnail_url": "https://i.ytimg.com/vi/JydpRavoJqI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=J0cntjLKpmU",
    "title": "Train Sentence Transformers by Generating Queries (GenQ)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, james, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, sentence transformers, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585776.8866475,
    "genre": "Science",
    "views": "1498",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nFine-tuning effective dense retrieval models is challenging. Bi-encoders (sentence transformers) are the current best models for dense retrieval in semantic search. Unfortunately, they\\'re also notoriously data-hungry models that typically require a particular type of labeled training data.\\\\n\\\\nHard problems like this attract attention. As expected, there is plenty of attention on building ever better techniques for training retrievers.\\\\n\\\\nOne of the most impressive is GenQ. This approach to building bi-encoder retrievers uses the latest text generation techniques to synthetically generate training data. In short, all we need are passages of text. The generation model then augments these passages with synthetic queries, giving us the exact format we need to train an effective bi-encoder model.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/genq/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:32 Why GenQ?\\\\n02:23 GenQ Overview\\\\n04:28 Training Data\\\\n06:48 Asymmetric Semantic Search\\\\n07:54 T5 Query Generation\\\\n13:52 Finetuning Bi-encoders\\\\n16:02 GenQ Code Walkthrough\\\\n21:40 Finetuning Bi-encoder Walkthrough\\\\n26:48 Final Points\"",
    "lengthSeconds": "1634",
    "uploadDate": "2022-03-08",
    "thumbnail_url": "https://i.ytimg.com/vi/J0cntjLKpmU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lZ2EaPUnV7k",
    "title": "Streamlit for ML #5.3 - Publishing Components to Pip",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, streamlit, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585777.3996217,
    "genre": "Science",
    "views": "729",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nThere are plenty of prebuilt components designed by Streamlit themselves, and if you can\\'t find what you need, there are even community-built components.\\\\n\\\\nIf you\\'re still stuck, and there is just no component that covers what you need, we can build our own custom components.\\\\n\\\\nTo do this we do need to start playing with the lower-level web technologies that Streamlit itself is built upon. So it isn\\'t as simple as using a prebuilt component. However, thanks to pre-made templates, it isn\\'t too hard to create a new component.\\\\n\\\\nIn this sub-series, we\\'ll learn exactly how to create custom components. We\\'ll focus on designing an interactive card component using Material UI design elements.\\\\n\\\\n\\xe2\\x9d\\x97 Python Packaging Video:\\\\nhttps://youtu.be/JkeNVaiUq_c\\\\n\\\\n\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Playlist:\\\\nhttps://www.youtube.com/watch?v=JLKUV-LiXjk\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=1\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nComing soon\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link to article:\\\\nComing soon\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:09 PyPI\\\\n02:41 Preparing for Distribution\\\\n05:43 Build React Component\\\\n06:39 Create Python Package\\\\n11:57 Pip Install\\\\n13:58 Ending\"",
    "lengthSeconds": "857",
    "uploadDate": "2022-02-28",
    "thumbnail_url": "https://i.ytimg.com/vi/lZ2EaPUnV7k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=mxm8ihWoVbk",
    "title": "Streamlit for ML #5.2 - MUI Card Component Build",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, streamlit, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.430647,
    "genre": "Science",
    "views": "2360",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Part 5.3:\\\\nhttps://www.youtube.com/watch?v=lZ2EaPUnV7k\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=7\\\\n\\\\nThere are plenty of prebuilt components designed by Streamlit themselves, and if you can\\'t find what you need, there are even community-built components.\\\\n\\\\nIf you\\'re still stuck, and there is just no component that covers what you need, we can build our own custom components.\\\\n\\\\nTo do this we do need to start playing with the lower-level web technologies that Streamlit itself is built upon. So it isn\\'t as simple as using a prebuilt component. However, thanks to pre-made templates, it isn\\'t too hard to create a new component.\\\\n\\\\nIn this sub-series, we\\'ll learn exactly how to create custom components. We\\'ll focus on designing an interactive card component using Material UI design elements.\\\\n\\\\n\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Playlist:\\\\nhttps://www.youtube.com/watch?v=JLKUV-LiXjk\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=1\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nComing soon\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link to article:\\\\nComing soon\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:59 Clearing Card Component\\\\n04:59 Building the Component\\\\n14:22 Pulling in MUI Code\\\\n24:08 Adding Roboto Font\\\\n26:05 Final Points\"",
    "lengthSeconds": "1619",
    "uploadDate": "2022-02-21",
    "thumbnail_url": "https://i.ytimg.com/vi/mxm8ihWoVbk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=SGazDb8o-to",
    "title": "Streamlit for ML #5.1 - Custom React Components in Streamlit Setup",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, data structures and algorithms in python, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, react js, react js with typescript, semantic search, sharepoint syntex, similarity search, streamlit, streamlit components, streamlit web app, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.0816455,
    "genre": "Science",
    "views": "3186",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Part 5.2:\\\\nhttps://www.youtube.com/watch?v=mxm8ihWoVbk\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=6\\\\n\\\\nThere are plenty of prebuilt components designed by Streamlit themselves, and if you can\\'t find what you need, there are even community-built components.\\\\n\\\\nIf you\\'re still stuck, and there is just no component that covers what you need, we can build our own custom components.\\\\n\\\\nTo do this we do need to start playing with the lower-level web technologies that Streamlit itself is built upon. So it isn\\'t as simple as using a prebuilt component. However, thanks to pre-made templates, it isn\\'t too hard to create a new component.\\\\n\\\\nIn this sub-series, we\\'ll learn exactly how to create custom components. We\\'ll focus on designing an interactive card component using Material UI design elements.\\\\n\\\\n\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Playlist:\\\\nhttps://www.youtube.com/watch?v=JLKUV-LiXjk\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=1\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nComing soon\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link to article:\\\\nComing soon\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n02:19 Environment Setup\\\\n03:42 Starting with a Template\\\\n07:41 Naming for Card Component\\\\n11:31 Installing Node Packages\\\\n15:12 Running the Component\"",
    "lengthSeconds": "1157",
    "uploadDate": "2022-02-17",
    "thumbnail_url": "https://i.ytimg.com/vi/SGazDb8o"
  },
  {
    "link": "watch?v=XdxeKiY2UXg",
    "title": "Streamlit for ML #4 - Adding Bootstrap Components",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, bootstrap, bootstrap components, code, coding, comment utiliser bootstrap, data science, decision tree, education, flutter, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, rplidar, semantic search, similarity search, streamlit, streamlit cache, streamlit dashboard, torch, tutorial, tutorials, vector search, vector similarity search, what is streamlit",
    "scraped_at": 1684585777.7476223,
    "genre": "Science",
    "views": "3562",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Part 5.1:\\\\nhttps://www.youtube.com/watch?v=SGazDb8o-to\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=5\\\\n\\\\nStreamlit has proven itself as an incredibly popular tool for quickly putting together high-quality ML-oriented web apps. More recently, it has seen wider adoption in production environments by ever-larger organizations.\\\\n\\\\nAll of this means that there is no better time to pick up some experience with Streamlit. Fortunately, the basics of Streamlit are incredibly easy to learn, and for most tools, this will be more than you need!\\\\n\\\\nIn this series, we will introduce Streamlit by building a general knowledge Q\\\\u0026A interface. We will learn about key Streamlit components like write, text_input, container. How to use external libraries like Bootstrap to quickly create new app components. And use caching to speed up our app.\\\\n\\\\n\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Playlist:\\\\nhttps://www.youtube.com/watch?v=JLKUV-LiXjk\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=1\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link to article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec?sk=ac5e0b7c39938f52162862411a66a58b\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n02:35 Streamlit Caching\\\\n06:56 Experimental Caching Primitives\"",
    "lengthSeconds": "589",
    "uploadDate": "2022-01-28",
    "thumbnail_url": "https://i.ytimg.com/vi/XdxeKiY2UXg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lYDiSCDcxmc",
    "title": "Streamlit for ML #3 - Make Apps Fast with Caching",
    "tags": "Huggingface, Tensorflow, angela 1995, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, streamlit, streamlit cache, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585776.515622,
    "genre": "Science",
    "views": "2048",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Part 4:\\\\nhttps://www.youtube.com/watch?v=XdxeKiY2UXg\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=4\\\\n\\\\nStreamlit has proven itself as an incredibly popular tool for quickly putting together high-quality ML-oriented web apps. More recently, it has seen wider adoption in production environments by ever-larger organizations.\\\\n\\\\nAll of this means that there is no better time to pick up some experience with Streamlit. Fortunately, the basics of Streamlit are incredibly easy to learn, and for most tools, this will be more than you need!\\\\n\\\\nIn this series, we will introduce Streamlit by building a general knowledge Q\\\\u0026A interface. We will learn about key Streamlit components like write, text_input, container. How to use external libraries like Bootstrap to quickly create new app components. And use caching to speed up our app.\\\\n\\\\n\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Playlist:\\\\nhttps://www.youtube.com/watch?v=JLKUV-LiXjk\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=1\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link to article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec?sk=ac5e0b7c39938f52162862411a66a58b\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n02:35 Streamlit Caching\\\\n06:56 Experimental Caching Primitives\"",
    "lengthSeconds": "583",
    "uploadDate": "2022-01-27",
    "thumbnail_url": "https://i.ytimg.com/vi/lYDiSCDcxmc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=U0EoaFFGyTg",
    "title": "Streamlit for ML #2 - ML Models and APIs",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, streamlit, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585778.1986215,
    "genre": "Science",
    "views": "1538",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Part 3:\\\\nhttps://www.youtube.com/watch?v=lYDiSCDcxmc\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=3\\\\n\\\\nStreamlit has proven itself as an incredibly popular tool for quickly putting together high-quality ML-oriented web apps. More recently, it has seen wider adoption in production environments by ever-larger organizations.\\\\n\\\\nAll of this means that there is no better time to pick up some experience with Streamlit. Fortunately, the basics of Streamlit are incredibly easy to learn, and for most tools, this will be more than you need!\\\\n\\\\nIn this series, we will introduce Streamlit by building a general knowledge Q\\\\u0026A interface. We will learn about key Streamlit components like write, text_input, container. How to use external libraries like Bootstrap to quickly create new app components. And use caching to speed up our app.\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Code to Create Index:\\\\nhttps://gist.github.com/jamescalam/2123ce0bb8a871f48a151a023a7ece67\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link to article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec?sk=ac5e0b7c39938f52162862411a66a58b\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:47 Creating the Vector DB\\\\n08:56 Implementing Retrieval\"",
    "lengthSeconds": "910",
    "uploadDate": "2022-01-26",
    "thumbnail_url": "https://i.ytimg.com/vi/U0EoaFFGyTg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JLKUV-LiXjk",
    "title": "Streamlit for ML #1 - Installation and API",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, streamlit, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585777.030642,
    "genre": "Science",
    "views": "2493",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Streamlit for ML Part 2:\\\\nhttps://www.youtube.com/watch?v=U0EoaFFGyTg\\\\u0026list=PLIUOU7oqGTLg5ssYxPGWaci6695wtosGw\\\\u0026index=2\\\\n\\\\nStreamlit has proven itself as an incredibly popular tool for quickly putting together high-quality ML-oriented web apps. More recently, it has seen wider adoption in production environments by ever-larger organizations.\\\\n\\\\nAll of this means that there is no better time to pick up some experience with Streamlit. Fortunately, the basics of Streamlit are incredibly easy to learn, and for most tools, this will be more than you need!\\\\n\\\\nIn this series, we will introduce Streamlit by building a general knowledge Q\\\\u0026A interface. We will learn about key Streamlit components like write, text_input, container. How to use external libraries like Bootstrap to quickly create new app components. And use caching to speed up our app.\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Friend link to article:\\\\nhttps://towardsdatascience.com/getting-started-with-streamlit-for-nlp-75fe463821ec?sk=ac5e0b7c39938f52162862411a66a58b\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:39 App Outline\\\\n03:36 Streamlit Installation\\\\n06:15 Streamlit API Basics\"",
    "lengthSeconds": "734",
    "uploadDate": "2022-01-25",
    "thumbnail_url": "https://i.ytimg.com/vi/JLKUV"
  },
  {
    "link": "watch?v=-fzCSPsfMic",
    "title": "How to build a Q&A Reader Model in Python (Open-domain QA)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, james, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, question answering, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search, weights and biases",
    "scraped_at": 1684585775.2636216,
    "genre": "Science",
    "views": "1999",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nOpen-domain question-answering (ODQA) is a wildly popular *pipeline* of databases and language models that allow us to ask a machine human-like questions and return comprehensible and even intelligent answers.\\\\n\\\\nDespite the outward guise of simplicity, ODQA requires a reasonably advanced set of components placed together to enable the *extractive* Q\\\\u0026A functionality.\\\\n\\\\nWe call this *extractive* Q\\\\u0026A because the models are not generating an answer. Instead, the answer already exists but is hidden somewhere within potentially thousands, millions, or even more data sources.\\\\n\\\\nBy enabling extractive Q\\\\u0026A, we enable a more *intelligent* and *efficient* way to retrieve information from what can be massive stores of data.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/reader-models/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:13 ODQA Components\\\\n03:09 Data Preprocessing\\\\n22:35 Fine-tuning\"",
    "lengthSeconds": "1503",
    "uploadDate": "2022-01-18",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=w1dMEWm7jBc",
    "title": "How to build a Q&A AI in Python (Open-domain Question-Answering)",
    "tags": "Huggingface, ai answer questions, artificial intelligence, bert, bert question answering, data science, krish naik, learning, machine learning, natural language processing, nlp, nlproc, python, python questions and answers, python virtual assistant, quastion answring streamlit, question and answering by, question answering, question answering nlp, semantic search, similarity search, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585778.7246473,
    "genre": "Science",
    "views": "8191",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nHow can we design these natural, human-like Q\\\\u0026A interfaces? The answer is open-domain question-answering (ODQA). ODQA allows us to use natural language to query a database.\\\\n\\\\nThat means that, given a dataset like a set of internal company documents, online documentation, or as is the case with Google, everything on the world\\'s internet, we can retrieve relevant information in a natural, more human way.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/retriever-models/\\\\n\\\\n\\xf0\\x9f\\x94\\x97 Nils YT Talk: https://youtu.be/XNJThigyvos?t=118\\\\n\\xf0\\x9f\\x94\\x97 MNR Loss Article: \\\\n\\xf0\\x9f\\x94\\x97 Free Pinecone API Key: https://app.pinecone.io/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Why QA\\\\n04:05 Open Domain QA\\\\n08:24 Do we need to fine-tune?\\\\n11:44 How Retriever Training Works\\\\n12:59 SQuAD Training Data\\\\n16:29 Retriever Fine-tuning\\\\n19:32 IR Evaluation\\\\n25:58 Vector Database Setup\\\\n33:42 Querying\\\\n37:41 Final Notes\"",
    "lengthSeconds": "2364",
    "uploadDate": "2022-01-11",
    "thumbnail_url": "https://i.ytimg.com/vi/w1dMEWm7jBc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=a8jyue22SJM",
    "title": "AugSBERT: Domain Transfer for Sentence Transformers",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, sentence transformers, similarity search, torch, transformers, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585776.6606476,
    "genre": "Science",
    "views": "1024",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nWhen building language models, we can spend months optimizing training and model parameters, but it\\'s useless if we don\\'t have the correct data.\\\\n\\\\nThe success of our language models relies first and foremost on data. The augmented SBERT training strategy can help us.\\\\n\\\\nGiven this scenario, we can transfer information from an out-of-domain (or *source*) dataset to our target domain. We will learn how to do this here. First, we will learn to assess which source datasets align best with our target domain quickly. Then we will explain and work through the AugSBERT domain-transfer training strategy.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/augsbert-domain-transfer/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x94\\x97 n-gram Similarity Script: https://gist.github.com/jamescalam/b73f37017ae32bd6094747c4b0fca94a\\\\n\\xf0\\x9f\\x94\\x97 AugSBERT In-Domain Article: https://www.pinecone.io/learn/data-augmentation/\\\\n\\\\n00:00 Why Use Domain Transfer\\\\n04:08 Strategy Outline\\\\n06:05 Train Source Cross-Encoder\\\\n12:44 Cross-Encoder Outcome\\\\n15:12 Labeling Target Data\\\\n20:31 Training Bi-encoder\\\\n23:58 Evaluator Bi-encoder Performance\\\\n28:08 Final Points\"",
    "lengthSeconds": "1749",
    "uploadDate": "2022-01-04",
    "thumbnail_url": "https://i.ytimg.com/vi/a8jyue22SJM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=mjKqP3kRxbQ",
    "title": "Building Transformer Tokenizers (Dhivehi NLP #1)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, dhivehi, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, similarity search, torch, transformers nlp, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585780.5146472,
    "genre": "Science",
    "views": "1243",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nGet in touch with Ashraq:\\\\nhttps://www.linkedin.com/in/ismailashraq/\\\\n\\\\nThe language of Dhivehi (or Maldivian) is fascinating. It uses a complex writing system known as Thaana, and I absolutely cannot comprehend any of it. It is so wildly different from anything I know\\xe2\\x80\\x8a-\\xe2\\x80\\x8abut, like the archipelago, it looks wonderful.\\\\n\\\\nAshraq described the difficulty of applying NLP to his native tongue of Dhivehi. There are several reasons for this, which we will explore in this video, and learn how to build an effective Dhivehi WordPiece tokenizer.\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/designing-tokenizers-for-low-resource-languages-7faa4ab30ef4\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Article Friend Link (Free Access):\\\\nhttps://towardsdatascience.com/designing-tokenizers-for-low-resource-languages-7faa4ab30ef4?sk=c0c16de9eea7dbe1d2a9c106abf38e1a\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:06 Dhivehi Project\\\\n02:28 Hurdles for Low Resource Domains\\\\n04:21 Dhivehi Dataset\\\\n04:52 Download Dhivehi Corpus\\\\n08:25 Tokenizer Components\\\\n08:44 Normalizer Component\\\\n11:55 Pre-tokenization Component\\\\n14:59 Post-tokenization Component\\\\n16:26 Decoder Component\\\\n17:41 Tokenizer Implementation\\\\n21:04 Tokenizer Training\\\\n24:22 Post-processing Implementation\\\\n27:12 Decoder Implementation\\\\n28:07 Saving for Transformers\\\\n30:33 Tokenizer Test and Usage\\\\n31:36 Download Dhivehi Models\\\\n32:21 First Steps\"",
    "lengthSeconds": "1982",
    "uploadDate": "2021-12-28",
    "thumbnail_url": "https://i.ytimg.com/vi/mjKqP3kRxbQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3IPCEeh4xTg",
    "title": "Making The Most of Data: Augmented SBERT",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, procedural generation, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.2846417,
    "genre": "Science",
    "views": "1800",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nML models are data-hungry. They consume massive amounts of data to identify generalized patterns and apply those learned patterns to new data.\\\\n\\\\nAs models get bigger, so do datasets. And although we have seen an explosion of data in the past decade, it is often not accessible or in an ML-friendly format, especially in niche domains.\\\\n\\\\nFor many niche, low-resource domains, finding or annotating a substantial dataset manually is practically impossible.\\\\n\\\\nFortunately, we don\\'t need to label (or even find) this new data. Instead, we can automatically generate or label data using one or more *data augmentation* techniques.\\\\n\\\\nIn this video, we will introduce data augmentation and its application to the field of NLP. We will focus on the \\'in-domain\\' flavor of a particular data-augmentation strategy named augmented SBERT (AugSBERT).\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/data-augmentation/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "3309",
    "uploadDate": "2021-12-17",
    "thumbnail_url": "https://i.ytimg.com/vi/3IPCEeh4xTg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=pNvujJ1XyeQ",
    "title": "Today Unsupervised Sentence Transformers, Tomorrow Skynet (how TSDAE works)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, sentence transformers, similarity search, skynet, torch, transformer nlp, transformers, tsdae, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585780.145647,
    "genre": "Science",
    "views": "3474",
    "desc": "To adapt a pretrained transformer to produce meaningful sentence vectors, we typically need a more supervised fine-tuning approach. We can use datasets like natural language inference (NLI) pairs, labeled semantic textual similarity (STS) data, or parallel data (pairs of translations).\\\\n\\\\nFor some domains and languages, such as finance and English, this data is fairly easy to find or gather. But many domains and many languages have very little labeled data. If you can find semantic similarity pairs for the agriculture industry, please let me know. There are many languages, such as Dhivehi, where unlabelled data is hard to find and labelled data practically non-existent.\\\\n\\\\nThis means you either spend a very long time gathering tens of thousands of labeled samples or you can try an unsupervised fine-tuning approach.\\\\n\\\\nUnsupervised training methods for sentence transformers are not as effective as their supervised counterparts, but they do work. And if you have no other choice, why not?\\\\n\\\\nIn this video, we will introduce the concept of unsupervised fine-tuning for sentence transformers. We will learn to train these models using the unsupervised Transformer-based Sequential Denoising Auto-Encoder (TSDAE) approach.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/unsupervised-training-sentence-transformers/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Why Language Embedding Matters\\\\n05:12 Supervised Methods\\\\n05:29 Natural Language Inference\\\\n07:15 Semantic Textual Similarity\\\\n07:43 Multilingual Training\\\\n10:00 TSDAE (Unsupervised)\\\\n18:50 Data Preparation\\\\n29:05 Initialize Model\\\\n32:39 Model Training\\\\n36:25 NLTK Error\\\\n37:15 Evaluation\\\\n41:01 TSDAE vs Supervised Methods\\\\n42:42 Why TSDAE is Cool\"",
    "lengthSeconds": "2661",
    "uploadDate": "2021-11-24",
    "thumbnail_url": "https://i.ytimg.com/vi/pNvujJ1XyeQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-td57YvJdHc",
    "title": "Question-Answering in NLP (Extractive QA and Abstractive QA)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlp question answering, nlproc, programming, python, pytorch, question and answering by, question answering, question answering nlp, question answering system, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585778.509637,
    "genre": "Science",
    "views": "5437",
    "desc": "Search is a crucial functionality in many applications and companies globally. Whether in manufacturing, finance, healthcare, or *almost* any other industry, organizations have vast internal information and document repositories.\\\\n\\\\nUnfortunately, the scale of many companies\\' data means that the organization and accessibility of information can become incredibly inefficient. The problem is exacerbated for language-based information. Language is a tool for people to communicate often abstract ideas and concepts. Naturally, ideas and concepts are harder for a computer to comprehend and store in a meaningful way.\\\\n\\\\nHow do we minimize this problem? The answer lies with *semantic search*, specifically with the question-answering (QA) flavor of semantic search.\\\\n\\\\nThis article will introduce the different forms of QA, the components of these \\'QA stacks\\', and where we might use them.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/question-answering/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Meaningful Search\\\\n01:23 Use-case\\\\n02:22 Open Domain QA (ODQA)\\\\n06:41 SQuAD Format\\\\n10:45 Quick Preprocessing\\\\n15:18 Creating Context Vectors Database\\\\n23:24 Open-book Extractive QA\\\\n32:50 Open-book Abstractive QA\\\\n41:53 Closed-book Abstractive QA\\\\n47:27 Final Thoughts\"",
    "lengthSeconds": "2886",
    "uploadDate": "2021-11-16",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=NNS5pOpjvAQ",
    "title": "All You Need to Know on Multilingual Sentence Vectors (1 Model, 50+ Languages)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, multilingual, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, sentence transformers, similarity search, torch, tutorial, tutorials, universal sentence encoder, vector search, vector similarity search, vectorspace ai",
    "scraped_at": 1684585773.480621,
    "genre": "Science",
    "views": "1499",
    "desc": "We\\'ve learned about how sentence transformers can be used to create high-quality vector representations of text. We can then use these vectors to find similar vectors, which can be used for many applications such as semantic search or topic modeling.\\\\n\\\\nThese models are very good at producing meaningful, information-dense vectors. But they don\\'t allow us to compare sentences across different languages.\\\\n\\\\nOften this may not be a problem. However, the world is becoming increasingly interconnected, and many companies span across multiple borders and languages. Naturally, there is a need for sentence vectors that are language agnostic.\\\\n\\\\nUnfortunately, very few textual similarity datasets span multiple languages, particularly for less common languages. And the standard training methods used for sentence transformers would require these types of datasets.\\\\n\\\\nDifferent approaches need to be used. Fortunately, some techniques allow us to extend models to other languages using more easily obtained language translations.\\\\n\\\\nIn this video, we will cover how multilingual models work and are built. We\\'ll learn how to develop our own multilingual sentence transformers, the datasets to look for, and how to use high-performing pretrained multilingual models.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/multilingual-transformers/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:19 Multilingual Vectors\\\\n05:55 Multi-task Training (mUSE)\\\\n09:36 Multilingual Knowledge Distillation\\\\n11:13 Knowledge Distillation Training\\\\n13:43 Visual Walkthrough\\\\n14:53 Parallel Data Prep\\\\n20:23 Choosing a Student Model\\\\n24:55 Initializing the Models\\\\n30:05 ParallelSentencesDataset\\\\n33:54 Loss and Fine-tuning\\\\n36:59 Model Evaluation\\\\n39:23 Outro\"",
    "lengthSeconds": "2392",
    "uploadDate": "2021-11-04",
    "thumbnail_url": "https://i.ytimg.com/vi/NNS5pOpjvAQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3fsIcMgUOY8",
    "title": "API Series #3 - How to Deploy Flask APIs to the Cloud (GCP)",
    "tags": "Huggingface, Tensorflow, api gateway, artificial intelligence, bert, code, coding, data science, deploy api to google cloud, education, flask, flask api, gcp, gcp api gateway, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, rest api flask, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search, webhooks",
    "scraped_at": 1684585780.217642,
    "genre": "Science",
    "views": "7437",
    "desc": "Building that first API is for many of us, a significant step towards creating impactful tools that may one day be used by many developers. But often those APIs don\\'t make it out of our local machines.\\\\n\\\\nFortunately, it\\'s incredibly easy to deploy APIs. Assuming you have no idea what you\\'re doing right now\\xe2\\x80\\x8a-\\xe2\\x80\\x8ayou will probably be deploying your first API in around ten minutes.\\\\n\\\\nI\\'m not joking, it\\'s super easy. Let\\'s get started.\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/how-to-deploy-a-flask-api-8d54dd8d8b8a\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Free article link:\\\\nTO ADD\"",
    "lengthSeconds": "805",
    "uploadDate": "2021-11-02",
    "thumbnail_url": "https://i.ytimg.com/vi/3fsIcMgUOY8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=iCkftKsnQgg",
    "title": "Hybrid Search Walkthrough in Pinecone",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, hybrid search algorithm, learning, machine learning, natural language processing, nlp, nlproc, pinecone, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585776.588623,
    "genre": "Science",
    "views": "966",
    "desc": "Pinecone offers a production-ready vector database for high performance and reliable *semantic search* at scale. But did you know Pinecone\\'s semantic search can be paired with the more traditional keyword search?\\\\n\\\\nSemantic search is a compelling technology allowing us to search using abstract concepts and *meaning* rather than relying on specific words. However, sometimes a simple keyword search can be just as valuable - especially if we know the exact wording of what we\\'re searching for.\\\\n\\\\nIn this video, we will explore these features through a start-to-finish example of basic keyword search in Pinecone.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Check the docs:\\\\nhttps://www.pinecone.io/docs/examples/basic-hybrid-search/\\\\n\\\\n\\xf0\\x9f\\x94\\x91 Free API key:\\\\nhttps://app.pinecone.io\\\\n\\\\n00:52 How Hybrid Search Works\\\\n01:25 Preprocessing\\\\n03:01 Creating Keywords\\\\n05:34 Creating an Index\\\\n06:50 Data Upsert\\\\n08:33 Query Setup\\\\n10:52 Keyword Search\\\\n12:31 OR Logic\\\\n14:49 AND Logic\\\\n15:10 Negation\\\\n17:04 Outro\"",
    "lengthSeconds": "1040",
    "uploadDate": "2021-10-29",
    "thumbnail_url": "https://i.ytimg.com/vi/iCkftKsnQgg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=or5ew7dqA-c",
    "title": "Fine-tune High Performance Sentence Transformers (with Multiple Negatives Ranking)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, fine tuning, gpt 2, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, semantic search, sentence transformers, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.791622,
    "genre": "Science",
    "views": "4986",
    "desc": "Transformer-produced sentence embeddings have come a long way in a very short time. Starting with the slow but accurate similarity prediction of BERT cross-encoders, the world of sentence embeddings was ignited with the introduction of SBERT in 2019. Since then, many more sentence transformers have been introduced. These models quickly made the original SBERT obsolete.\\\\n\\\\nHow did these newer sentence transformers manage to outperform SBERT so quickly? The answer is multiple negatives ranking (MNR) loss.\\\\n\\\\nThis video will cover what MNR loss is, the data it requires, and how to implement it to fine-tune our own high-quality sentence transformers.\\\\n\\\\nImplementation will cover two approaches. The first is more involved, and outlines the exact steps to fine-tune the model (we\\'ll just run over it quickly). The second approach makes use of the sentence-transformers library\\'s excellent utilities for fine-tuning.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/\\\\n\\\\nCheck out the Sentence Transformers library:\\\\nhttps://github.com/UKPLab/sentence-transformers\\\\n\\\\nTalk by Nils Reimers (one of the SBERT creators) on training:\\\\nhttps://www.youtube.com/watch?v=RHXZKUr8qOY\\\\n\\\\nHe does more NLP vids too:\\\\nhttps://www.youtube.com/channel/UC1zCuTrfpjT6Sv2kJk-JkvA\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:02 NLI Training Data\\\\n02:56 Preprocessing\\\\n10:11 SBERT Finetuning Visuals\\\\n14:14 MNR Loss Visual\\\\n16:37 MNR in PyTorch\\\\n23:04 MNR in Sentence Transformers\\\\n34:20 Results\\\\n36:14 Outro\"",
    "lengthSeconds": "2212",
    "uploadDate": "2021-10-26",
    "thumbnail_url": "https://i.ytimg.com/vi/or5ew7dqA"
  },
  {
    "link": "watch?v=aSx0jg9ZILo",
    "title": "Fine-tune Sentence Transformers the OG Way (with NLI Softmax loss)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, ir concepts and construction, james briggs, learning, machine learning, natural language processing, nlp, nlp transformer, nlproc, programming, python, pytorch, semantic search, sentence bert, sentence transformers, similarity search, softmax, torch, transformers nlp, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.0116227,
    "genre": "Science",
    "views": "4685",
    "desc": "Sentence embeddings with transformers can be used across a range of applications, such as semantic textual similarity (STS), semantic clustering, or information retrieval (IR) using concepts rather than words.\\\\n\\\\nThis video dives deeper into the training process of the first sentence transformer, sentence-BERT, or more commonly known as SBERT. We will explore the Natural Language Inference (NLI) training approach of softmax loss to fine-tune models for producing sentence embeddings.\\\\n\\\\nBe aware that softmax loss is no longer the preferred approach to training sentence transformers and has been superseded by other methods such as MSE margin and multiple negatives ranking loss. But we\\'re covering this training method as an important milestone in the development of ever-improving sentence embeddings.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/train-sentence-transformers-softmax/\\\\n\\\\nCheck out the Sentence Transformers library:\\\\nhttps://github.com/UKPLab/sentence-transformers\\\\n\\\\nTalk by Nils Reimers (one of the SBERT creators) on training:\\\\nhttps://www.youtube.com/watch?v=RHXZKUr8qOY\\\\n\\\\nHe does more NLP vids too:\\\\nhttps://www.youtube.com/channel/UC1zCuTrfpjT6Sv2kJk-JkvA\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:42 NLI Fine-tuning\\\\n01:44 Softmax Loss Training Overview\\\\n05:47 Preprocessing NLI Data\\\\n12:48 PyTorch Process\\\\n19:48 Using Sentence-Transformers\\\\n30:45 Results\\\\n35:49 Outro\"",
    "lengthSeconds": "2222",
    "uploadDate": "2021-10-22",
    "thumbnail_url": "https://i.ytimg.com/vi/aSx0jg9ZILo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=WS1uVMGhlWQ",
    "title": "Intro to Sentence Embeddings with Transformers",
    "tags": "Huggingface, artificial intelligence, bert, bert embedding, bert explained, bert transformer, embedding, natural language processing, nlp, sentence bert, sentence embedding, sentence transformers, similarity search, tf idf, transformer neural network, transformer nlp, transformers, transformers explained, transformers nlp, universal sentence encoder, vector search, vector similarity search, word embedding, word embeddings",
    "scraped_at": 1684585777.1056225,
    "genre": "Science",
    "views": "13699",
    "desc": "Transformers have wholly rebuilt the landscape of natural language processing (NLP). Before transformers, we had okay translation and language classification thanks to recurrent neural nets (RNNs) - their language comprehension was limited and led to many minor mistakes, and coherence over larger chunks of text was practically impossible.\\\\n\\\\nSince the introduction of the first transformer model in the 2017 paper \\'Attention is all you need\\', NLP has moved from RNNs to models like BERT and GPT. These new models can answer questions, write articles (maybe GPT-3 wrote this), enable incredibly intuitive semantic search - and much more.\\\\n\\\\nIn this video, we will explore how these embeddings have been adapted and applied to a range of semantic similarity applications by using a new breed of transformers called \\'sentence transformers\\'.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/sentence-embeddings/\\\\n\\\\nVectors in ML:\\\\nhttps://www.youtube.com/playlist?list=PLIUOU7oqGTLgz-BI8bNMVGwQxIMuQddJO\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "1865",
    "uploadDate": "2021-10-20",
    "thumbnail_url": "https://i.ytimg.com/vi/WS1uVMGhlWQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=bVZJ_O_-0RE",
    "title": "Intro to Dense Vectors for NLP and Vision",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, learning, machine learning, natural language processing, nlp, nlp course, nlproc, programming, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585776.9586225,
    "genre": "Science",
    "views": "7427",
    "desc": "There is perhaps no greater component to the success of modern Natural Language Processing (NLP) technology than vector representations of language. The meteoric early 2010s rise of NLP was ignited with the introduction of word2vec by a team lead by Tom\\xc3\\xa1\\xc5\\xa1 Mikolov in 2013.\\\\n\\\\nWord2vec is one of the most iconic and earliest examples of dense vectors representing text. But since the days of word2vec, developments in representing language have advanced at ludicrous speeds.\\\\n\\\\nThis video will explore *why* we use dense vectors - and some of the best approaches to building dense vectors available today.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/dense-vector-embeddings-nlp/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:50 Why Dense Vectors?\\\\n03:55 Word2vec and Representing Meaning\\\\n08:40 Sentence Transformers\\\\n09:58 Sentence Transformers in Python\\\\n15:08 Question-Answering\\\\n18:18 DPR in Python\\\\n29:55 Vision Transformers\\\\n33:22 OpenAI\\'s CLIP in Python\\\\n42:49 Review and What\\'s Next\"",
    "lengthSeconds": "2628",
    "uploadDate": "2021-10-12",
    "thumbnail_url": "https://i.ytimg.com/vi/bVZJ_O_"
  },
  {
    "link": "watch?v=MF75aNH3Gjs",
    "title": "API Series #2 - Building an API with Flask in Python",
    "tags": "Huggingface, Tensorflow, api, api flask, artificial intelligence, bert, build api, code, coding, data science, education, flask, flask api, flask rest api, learning, machine learning, natural language processing, nlp, nlproc, programming, python, pytorch, rest api, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585779.1496217,
    "genre": "Science",
    "views": "16737",
    "desc": "Next video - how to deploy to the cloud: https://youtu.be/3fsIcMgUOY8\\\\n\\\\nHow can we set up a way to communicate from one software instance to another? It sounds simple, and - to be completely honest - it is.\\\\n\\\\nAll we need is an API.\\\\n\\\\nAn API (Application Programming Interface) is a simple interface that defines the types of requests (demands/questions, etc.) that can be made, how they are made, and how they are processed.\\\\n\\\\nIn our case, we will be building an API that allows us to send a range of GET/POST/PUT/PATCH/DELETE requests (more on this later), to different endpoints, and return or modify data connected to our API.\\\\n\\\\nWe will be using the Flask framework to create our API and Insomnia to test it.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Medium article:\\\\nhttps://towardsdatascience.com/the-right-way-to-build-an-api-with-python-cd08ab285f8f\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\nFree article link: \\\\nhttps://towardsdatascience.com/the-right-way-to-build-an-api-with-python-cd08ab285f8f?sk=6e2dda4c8b6012767114e12ff34b1464\\\\n\\\\nDownload Insomnia:\\\\nhttps://insomnia.rest/download\"",
    "lengthSeconds": "1902",
    "uploadDate": "2021-10-07",
    "thumbnail_url": "https://i.ytimg.com/vi/MF75aNH3Gjs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=QvKMwLjdK-s",
    "title": "HNSW for Vector Search Explained and Implemented with Faiss (Python)",
    "tags": "Huggingface, Tensorflow, artificial intelligence, bert, code, coding, data science, education, faiss, hnsw, learning, lsh, machine learning, natural language processing, nlp, nlproc, programming, pyaudio, python, pytorch, semantic search, similarity search, torch, tutorial, tutorials, vector search, vector similarity search",
    "scraped_at": 1684585780.0716212,
    "genre": "Science",
    "views": "10298",
    "desc": "Hierarchical Navigable Small World (HNSW) graphs are among the top-performing indexes for vector similarity search. HNSW is a hugely popular technology that time and time again produces state-of-the-art performance with super-fast search speeds and flawless recall - HNSW is not to be missed.\\\\n\\\\nDespite being a popular and robust algorithm for approximate nearest neighbors (ANN) searches, understanding how it works is far from easy.\\\\n\\\\nThis video helps demystify HNSW and explains this intelligent algorithm in an easy-to-understand way. Towards the end of the video, we\\'ll look at how to implement HNSW using Faiss and which parameter settings give us the performance we need.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/hnsw/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://jamescalam.medium.com/subscribe (it\\'s free!)\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:41 Foundations of HNSW\\\\n08:41 How HNSW Works\\\\n16:38 The Basics of HNSW in Faiss\\\\n21:40 How Faiss Builds an HNSW Graph\\\\n26.49 Building the Best HNSW Index\\\\n33:33 Fine-tuning HNSW\\\\n34:30 Outro\"",
    "lengthSeconds": "2075",
    "uploadDate": "2021-10-05",
    "thumbnail_url": "https://i.ytimg.com/vi/QvKMwLjdK"
  },
  {
    "link": "watch?v=g_yMowQikOE",
    "title": "Intro to APIs in Python - API Series #1",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding, semantic search, similarity search, vector similarity search, vector search",
    "scraped_at": 1684585777.8196213,
    "genre": "Science",
    "views": "5549",
    "desc": "Taking those first steps into interacting with the web using Python can seem daunting\\xe2\\x80\\x8a-\\xe2\\x80\\x8abut it need not be. It is a surprisingly simple process, with well established rules and guidelines.\\\\n\\\\nWe\\'ll cover the absolute essentials for getting started, including:\\\\n\\\\n- Application Program Interfaces (APIs)\\\\n- Javascript Object Notation (JSON)\\\\n- Requests with Python\\\\n- Real world use-cases\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/quick-fire-guide-to-apis-in-python-891dd98c8877\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://jamescalam.medium.com/subscribe (it\\'s free!)\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Free Access Link (if you don\\'t have Medium membership): \\\\nhttps://towardsdatascience.com/quick-fire-guide-to-apis-in-python-891dd98c8877?sk=7c159ba45154db23abcc6a7f9de4f910\\\\n\\\\nGeocoding Docs:\\\\nhttps://developers.google.com/maps/documentation/geocoding/cloud-setup\\\\n\\\\nGitHub Docs:\\\\nhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\\\\n\\\\n00:00 Intro\\\\n00:20 What is an API?\\\\n01:47 RESTful APIs\\\\n05:26 API Methods\\\\n07:20 HTTP Codes (200s)\\\\n08:14 HTTP Codes (400s)\\\\n10:00 JSON Format\\\\n11:21 Talking to APIs in Python\\\\n14:30 Google Geocoding API\\\\n22:08 GitHub API\\\\n27:48 Outro\"",
    "lengthSeconds": "1704",
    "uploadDate": "2021-09-29",
    "thumbnail_url": "https://i.ytimg.com/vi/g_yMowQikOE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3Wqh4iUupbM",
    "title": "Best Indexes for Similarity Search in Faiss",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585776.7296207,
    "genre": "Science",
    "views": "1987",
    "desc": "In the world of vector search, there are many indexing methods and vector processing techniques that allow us to prioritize between recall, latency, and memory usage.\\\\n\\\\nUsing specific methods such as IVF, PQ, or HNSW, we can often return good results. But for best performance we will usually want to use composite indexes.\\\\n\\\\nWe can view a composite index as a step-by-step process of vector transformations and one or more indexing methods. Allowing us to place multiple indexes and/or processing steps together to create our \\xe2\\x80\\x98ideal\\xe2\\x80\\x99 index.\\\\n\\\\nFor example, we can use an inverted file (IVF) index to reduce the scope of our search (increasing search speed), and then add a compression technique such as product quantization (PQ) to keep larger indexes within a reasonable size limit.\\\\n\\\\nWhere there is the ability to customize indexes, there is the risk of producing indexes with unnecessarily poor recall, latency, or memory usage.\\\\n\\\\nWe must know how composite indexes work if we want to build robust and high-performance vector similarity search applications. It is essential to understand where different indexes or vector transformations can be used \\xe2\\x80\\x94 and when they are not needed.\\\\n\\\\nPart 1: https://youtu.be/GEhmmcx1lvM\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/composite-indexes/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://jamescalam.medium.com/subscribe (it\\'s free!)\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:30 IVFADC\\\\n03:30 IVFADC in Faiss\\\\n07:29 Multi-D-ADC\\\\n09:17 Multi-D-ADC in Faiss\\\\n14:43 IVF-HNSW\\\\n21:39 IVF-HNSW in Faiss\\\\n25:58 Outro\"",
    "lengthSeconds": "1582",
    "uploadDate": "2021-09-24",
    "thumbnail_url": "https://i.ytimg.com/vi/3Wqh4iUupbM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GEhmmcx1lvM",
    "title": "Composite Indexes and the Faiss Index Factory",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585779.4996474,
    "genre": "Science",
    "views": "1239",
    "desc": "In the world of vector search, there are many indexing methods and vector processing techniques that allow us to prioritize between recall, latency, and memory usage.\\\\n\\\\nUsing specific methods such as IVF, PQ, or HNSW, we can often return good results. But for best performance we will usually want to use composite indexes.\\\\n\\\\nWe can view a composite index as a step-by-step process of vector transformations and one or more indexing methods. Allowing us to place multiple indexes and/or processing steps together to create our \\xe2\\x80\\x98ideal\\xe2\\x80\\x99 index.\\\\n\\\\nFor example, we can use an inverted file (IVF) index to reduce the scope of our search (increasing search speed), and then add a compression technique such as product quantization (PQ) to keep larger indexes within a reasonable size limit.\\\\n\\\\nWhere there is the ability to customize indexes, there is the risk of producing indexes with unnecessarily poor recall, latency, or memory usage.\\\\n\\\\nWe must know how composite indexes work if we want to build robust and high-performance vector similarity search applications. It is essential to understand where different indexes or vector transformations can be used \\xe2\\x80\\x94 and when they are not needed.\\\\n\\\\nPart 2: https://youtu.be/3Wqh4iUupbM\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/composite-indexes/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://jamescalam.medium.com/subscribe (it\\'s free!)\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n01:54 Composite Indexes\\\\n06:43 Faiss Index Factory\\\\n11:34 Why we use Index Factory\\\\n17:11 Outro\"",
    "lengthSeconds": "1063",
    "uploadDate": "2021-09-24",
    "thumbnail_url": "https://i.ytimg.com/vi/GEhmmcx1lvM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=r-zQQ16wTCA",
    "title": "Build NLP Pipelines with HuggingFace Datasets",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585779.646622,
    "genre": "Science",
    "views": "2942",
    "desc": "HF Datasets is an essential tool for NLP practitioners\\xe2\\x80\\x8a-\\xe2\\x80\\x8ahosting over 1.4K (mostly) high-quality language-focused datasets, and an easy-to-use treasure trove of functions for building efficient pre-processing pipelines.\\\\n\\\\nIn this article, we will take a look at the massive repository of datasets available, and explore some of the library\\'s brilliant data processing capabilities.\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Medium article:\\\\nhttps://towardsdatascience.com/build-nlp-pipelines-with-huggingface-datasets-d597ff5f68ad\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Free Article Access (if you don\\'t have Medium membership!): \\\\nhttps://towardsdatascience.com/build-nlp-pipelines-with-huggingface-datasets-d597ff5f68ad?sk=948106e47e64bc3e9e8a1358b0568d48\"",
    "lengthSeconds": "2030",
    "uploadDate": "2021-09-23",
    "thumbnail_url": "https://i.ytimg.com/vi/r"
  },
  {
    "link": "watch?v=H_kJDHvu-v8",
    "title": "Metadata Filtering for Vector Search + Latest Filter Tech",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585778.7986214,
    "genre": "Science",
    "views": "1649",
    "desc": "Vector similarity search makes massive datasets searchable in fractions of a second. Yet despite the brilliance and utility of this technology, often what seem to be the most straightforward problems are the most difficult to solve. Such as filtering.\\\\n\\\\nFiltering takes the top place in being seemingly simple \\xe2\\x80\\x94 but actually incredibly complex. Applying fast-but-accurate filters when performing a vector search (ie, nearest-neighbor search) on massive datasets is a surprisingly stubborn problem.\\\\n\\\\nThis article explains the two common methods for adding filters to vector search, and their serious limitations. Then we will explore Pinecone\\xe2\\x80\\x99s solution to filtering in vector search.\\\\n\\\\n\\xf0\\x9f\\x93\\xa3 Get the API key!\\\\nhttps://www.pinecone.io/start/\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/vector-search-filtering/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Subscribe for Article and Video Updates!\\\\nhttps://jamescalam.medium.com/subscribe\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n00:00 Intro\\\\n00:24 Vector Search Recap\\\\n02:03 Why Filter?\\\\n02:56 Metadata Filtering 101\\\\n07:48 Pre-filtering\\\\n09:37 Post-filtering\\\\n11:30 Single-Stage Filtering\\\\n12:22 Vectors and Metadata Code\\\\n13:58 Connecting to Pinecone\\\\n14:55 Building Query Vector\\\\n16:47 Querying\\\\n21:37 First Filter\\\\n24:40 Adding More Conditions\\\\n27:03 Filtering with Numbers\\\\n30:55 Search Speed and Filtering\\\\n33:44 Outro\"",
    "lengthSeconds": "2053",
    "uploadDate": "2021-09-20",
    "thumbnail_url": "https://i.ytimg.com/vi/H_kJDHvu"
  },
  {
    "link": "watch?v=cR4qMSIvX28",
    "title": "How to Build a Bert WordPiece Tokenizer in Python and HuggingFace",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585778.8696518,
    "genre": "Science",
    "views": "5381",
    "desc": "Building a transformer model from scratch can often be the only option for many more specific use cases. Although BERT and other transformer models have been pre-trained for a vast number of languages and domains, they do not cover everything.\\\\n\\\\nOften, it is these less common use cases that stand to gain the most from having someone come along and build a specific transformer model. It could be for an uncommon language or less tech-savvy domain.\\\\n\\\\nBERT is the most popular transformer for a wide range of language-based machine learning\\xe2\\x80\\x8a-\\xe2\\x80\\x8afrom sentiment analysis to question and answering, BERT has enabled a diverse range of innovation across many borders and industries.\\\\n\\\\nThe first step for many in designing a new BERT model is the tokenizer. In this article, we\\'ll take a look at the WordPiece tokenizer used by BERT\\xe2\\x80\\x8a-\\xe2\\x80\\x8aand see how we can build our own from scratch.\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Medium article:\\\\nhttps://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free Article link (if you don\\'t have Medium membership): \\\\nhttps://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb?sk=eea06e01c9faecd939e10589e9de1291\"",
    "lengthSeconds": "1880",
    "uploadDate": "2021-09-14",
    "thumbnail_url": "https://i.ytimg.com/vi/cR4qMSIvX28/maxresdefault.jpg"
  },
  {
    "link": "watch?v=t9mRf2S5vDI",
    "title": "Product Quantization for Vector Similarity Search (+ Python)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585778.937647,
    "genre": "Science",
    "views": "5129",
    "desc": "Vector similarity search can require huge amounts of memory. Indexes containing 1M dense vectors (a small dataset in today\\xe2\\x80\\x99s world) will often require several GBs of memory to store. When building recommendation systems or semantic search engines, this is not acceptable.\\\\n\\\\nThe problem of excessive memory usage is exasperated by high-dimensional data, and with ever-increasing dataset sizes, this can very quickly become unmanageable.\\\\n\\\\nProduct quantization (PQ) is a popular method for dramatically compressing high-dimensional vectors to use 97% less memory, and for making nearest-neighbor search speeds 5.5x faster in our tests.\\\\n\\\\nA composite IVF+PQ index speeds up the search by another 16.5x without affecting accuracy, for a whopping total speed increase of 92x compared to non-quantized indexes.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/product-quantization/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1776",
    "uploadDate": "2021-08-30",
    "thumbnail_url": "https://i.ytimg.com/vi/t9mRf2S5vDI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BMYBwbkbVec",
    "title": "Faiss - Vector Compression with PQ and IVFPQ (in Python)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585776.217647,
    "genre": "Science",
    "views": "2741",
    "desc": "So far we\\xe2\\x80\\x99ve worked through the logic behind a simple, readable implementation of product quantization (PQ) in Python for semantic search. Realistically we wouldn\\xe2\\x80\\x99t use this because it is not optimized and we already have excellent implementations elsewhere. Instead, we would use a library like Faiss (Facebook AI Similarity Search) \\xe2\\x80\\x94 or a production-ready service like Pinecone.\\\\n\\\\nWe\\xe2\\x80\\x99ll take a look at how we can build a PQ index in Faiss, and we\\xe2\\x80\\x99ll even take a look at combining PQ with an Inverted File (IVF) step to improve search speed.\\\\n\\\\nBefore we start, we need to get data. We will be using the Sift1M dataset. It can be downloaded and opened using this script:\\\\nhttps://gist.github.com/jamescalam/928a374b85daffa49a565f3dc18d059c#file-get_sift1m-ipynb\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/product-quantization/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1160",
    "uploadDate": "2021-08-30",
    "thumbnail_url": "https://i.ytimg.com/vi/BMYBwbkbVec/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZLfdQq_u7Eo",
    "title": "IndexLSH for Fast Similarity Search in Faiss",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585776.0666463,
    "genre": "Science",
    "views": "1970",
    "desc": "Faiss \\xe2\\x80\\x8a- \\xe2\\x80\\x8aor Facebook AI Similarity Search\\xe2\\x80\\x8a - \\xe2\\x80\\x8ais an open-source framework built for enabling similarity search.\\\\n\\\\nFaiss has many super-efficient implementations of different indexes that we can use in similarity search. That long list of indexes includes IndexLSH\\xe2\\x80\\x8a-\\xe2\\x80\\x8aan easy-to-use implementation of everything we have covered so far in LSH.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/locality-sensitive-hashing-random-projection/\\\\n\\\\nDownload Sift1M:\\\\nhttps://gist.github.com/jamescalam/a09a16c17b677f2cf9c019114711f3bf\\\\n\\\\nHow LSH Random Projection works in search (+Python):\\\\nhttps://youtu.be/8bOrMqEdfiQ\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1119",
    "uploadDate": "2021-08-24",
    "thumbnail_url": "https://i.ytimg.com/vi/ZLfdQq_u7Eo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8bOrMqEdfiQ",
    "title": "How LSH Random Projection works in search (+Python)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585779.9296212,
    "genre": "Science",
    "views": "3037",
    "desc": "Locality sensitive hashing (LSH) is a widely popular technique used in approximate similarity search. The solution to efficient similarity search is a profitable one\\xe2\\x80\\x8a-\\xe2\\x80\\x8ait is at the core of several billion (and even trillion) dollar companies.\\\\n\\\\nThe problem with similarity search is scale. Many companies deal with millions-to-billions of data points every single day. Given a billion data points, is it feasible to compare all of them with every search?\\\\n\\\\nFurther, many companies are not performing single searches\\xe2\\x80\\x8a-\\xe2\\x80\\x8aGoogle deals with more than 3.8 million searches every minute.\\\\n\\\\nBillions of data points combined with high-frequency searches are problematic\\xe2\\x80\\x8a-\\xe2\\x80\\x8aand we haven\\'t considered the dimensionality nor the similarity function itself. Clearly, an exhaustive search across all data points is unrealistic for larger datasets.\\\\n\\\\nThe solution to searching impossibly huge datasets? Approximate search. Rather than exhaustively comparing every pair, we approximate\\xe2\\x80\\x8a-\\xe2\\x80\\x8arestricting the search scope only to high probability matches.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/locality-sensitive-hashing-random-projection/\\\\n\\\\nDownload Sift1M:\\\\nhttps://gist.github.com/jamescalam/a09a16c17b677f2cf9c019114711f3bf\\\\n\\\\nIndexLSH for Fast Similarity Search in Faiss:\\\\nhttps://youtu.be/ZLfdQq_u7Eo\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1148",
    "uploadDate": "2021-08-24",
    "thumbnail_url": "https://i.ytimg.com/vi/8bOrMqEdfiQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=e_SBq3s20M8",
    "title": "Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585780.8266468,
    "genre": "Science",
    "views": "12758",
    "desc": "Locality sensitive hashing (LSH) is a widely popular technique used in approximate nearest neighbor (ANN) search. The solution to efficient similarity search is a profitable one\\xe2\\x80\\x8a-\\xe2\\x80\\x8ait is at the core of several billion (and even trillion) dollar companies.\\\\n\\\\nLSH consists of a variety of different methods. In this video, we\\'ll be covering the traditional approach\\xe2\\x80\\x8a-\\xe2\\x80\\x8awhich consists of multiple steps\\xe2\\x80\\x8a-\\xe2\\x80\\x8ashingling, MinHashing, and the final banded LSH function.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone article:\\\\nhttps://www.pinecone.io/learn/locality-sensitive-hashing/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord:\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n01:21 Overview\\\\n05:58 Shingling\\\\n08:45 Vocab\\\\n09:27 One-hot Encoding\\\\n11:10 MinHash\\\\n15:51 Signature Info\\\\n18:08 LSH\\\\n22:20 Tuning LSH\"",
    "lengthSeconds": "1626",
    "uploadDate": "2021-08-20",
    "thumbnail_url": "https://i.ytimg.com/vi/e_SBq3s20M8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=B7wmo_NImgM",
    "title": "Choosing Indexes for Similarity Search (Faiss in Python)",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585784.1336205,
    "genre": "Science",
    "views": "8241",
    "desc": "Facebook AI Similarity Search (Faiss) is a game-changer in the world of search. It allows us to efficiently search a huge range of media, from GIFs to articles\\xe2\\x80\\x8a-\\xe2\\x80\\x8awith incredible accuracy in sub-second timescales for billion+ size datasets.\\\\n\\\\nThe success in Faiss is due to many reasons. One of those, in particular, is its flexibility. Faiss recognizes that there is no \\'one-size-fits-all\\' in similarity search.\\\\n\\\\nInstead, Faiss comes with a wide range of search indexes\\xe2\\x80\\x8a-\\xe2\\x80\\x8awhich we can mix and match to our choosing.\\\\n\\\\nHowever, this great flexibility produces a question\\xe2\\x80\\x8a-\\xe2\\x80\\x8ahow do we know which size fits our use case?\\\\n\\\\nWhich index do we choose? Should we use multiple indexes, or is one enough?\\\\n\\\\nThis video will explore the pros and cons of some of the most important indexes\\xe2\\x80\\x8a-\\xe2\\x80\\x8aFlat, LSH, HNSW, and IVF. We will learn how we decide which to use and the impact of parameters in each index to build some of the best indexes for semantic search.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone Article:\\\\nhttps://www.pinecone.io/learn/vector-indexes/\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\nDownload script for Sift1M dataset:\\\\nhttps://gist.github.com/jamescalam/a09a16c17b677f2cf9c019114711f3bf\\\\n\\\\nSimilarity Search Series:\\\\nhttps://www.youtube.com/playlist?list=PLIUOU7oqGTLhlWpTz4NnuT3FekouIVlqc\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\nMining Massive Datasets Book (Similarity Search):\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3CC0zrc (3rd ed)\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3AtHSnV (1st ed, cheaper)\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1893",
    "uploadDate": "2021-08-09",
    "thumbnail_url": "https://i.ytimg.com/vi/B7wmo_NImgM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=bWLvGGJLzF8",
    "title": "Why are there so many Tokenization methods in HF Transformers?",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585781.1136205,
    "genre": "Science",
    "views": "2166",
    "desc": "HuggingFace\\'s transformers library is the de-facto standard for NLP\\xe2\\x80\\x8a-\\xe2\\x80\\x8aused by practitioners worldwide, it\\'s powerful, flexible, and easy to use. It achieves this through a fairly large (and complex) code-base, which has resulted in the question:\\\\n\\\\n\\\\\"",
    "lengthSeconds": "1080",
    "uploadDate": "2021-07-27",
    "thumbnail_url": "https://i.ytimg.com/vi/bWLvGGJLzF8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ee71R4Cqb5o",
    "title": "Angular App Setup With Material - Stoic Q&A #5",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585781.0236206,
    "genre": "Science",
    "views": "871",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Stoic Q\\\\u0026A App Playlist: https://www.youtube.com/playlist?list=PLIUOU7oqGTLixb-CatMxNCO-mJioMmZEB\\\\n\\\\nThe fifth video in our Stoic Q\\\\u0026A series - setting up our Angular app with Angular Material.\\\\n\\\\nPrerequisites:\\\\nInstallation of Node.js and NPM - https://nodejs.org/en/\\\\nAngular - https://angular.io/guide/setup-local\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\"",
    "lengthSeconds": "813",
    "uploadDate": "2021-07-20",
    "thumbnail_url": "https://i.ytimg.com/vi/ee71R4Cqb5o/maxresdefault.jpg"
  },
  {
    "link": "watch?v=sKyvsdEv6rk",
    "title": "Faiss - Introduction to Similarity Search",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585783.7256224,
    "genre": "Science",
    "views": "22787",
    "desc": "Full Similarity Search Playlist:\\\\nhttps://www.youtube.com/watch?v=AY62z7HrghY\\\\u0026list=PLIUOU7oqGTLhlWpTz4NnuT3FekouIVlqc\\\\u0026index=1\\\\n\\\\nFacebook AI Similarity Search (FAISS) is one of the most popular implementations of efficient similarity search, but what is it\\xe2\\x80\\x8a-\\xe2\\x80\\x8aand how can we use it?\\\\n\\\\nWhat is it that makes FAISS special? How do we make the best use of this incredible tool?\\\\n\\\\nFortunately, it\\'s a brilliantly simple process to get started with. And in this video, we\\'ll explore some of the options FAISS provides, how they work, and\\xe2\\x80\\x8a-\\xe2\\x80\\x8amost importantly\\xe2\\x80\\x8a-\\xe2\\x80\\x8ahow FAISS can make our semantic search faster.\\\\n\\\\n\\xf0\\x9f\\x8c\\xb2 Pinecone Article:\\\\nhttps://www.pinecone.io/learn/faiss-tutorial/\\\\n\\\\n\\xf0\\x9f\\x93\\x8a Data:\\\\nhttps://github.com/jamescalam/data/tree/main/sentence_embeddings_15K\\\\n\\\\nNotebook:\\\\nhttps://gist.github.com/jamescalam/7117aa92235a7f52141ad0654795aa48\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\nMining Massive Datasets Book (Similarity Search):\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3CC0zrc (3rd ed)\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3AtHSnV (1st ed, cheaper)\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1896",
    "uploadDate": "2021-07-13",
    "thumbnail_url": "https://i.ytimg.com/vi/sKyvsdEv6rk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=35Pdoyi6ZoQ",
    "title": "Training and Testing an Italian BERT - Transformers From Scratch #4",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585784.2056215,
    "genre": "Science",
    "views": "6399",
    "desc": "We need two things for training, our DataLoader and a model. The DataLoader we have \\xe2\\x80\\x94 but no model.\\\\n\\\\nFor training, we need a raw (not pre-trained) RobertaForMaskedLM. To create that, we first need to create a RoBERTa config object to describe the parameters we\\xe2\\x80\\x99d like to initialize FiliBERTo with.\\\\n\\\\nOnce we have our model, we set up our training loop and train!\\\\n\\\\nPost-training, we\\'ll test the model with Laura, who is Italian - and hope for the best.\\\\n\\\\nPart 1: https://youtu.be/GhGUZrcB-WM\\\\nPart 2: https://youtu.be/JIeAB8vvBQo\\\\nPart 3: https://youtu.be/heTYbpr9mD8\\\\n---\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6?sk=9db6224efbd4ec6fd407a80b528e69b0\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n00:35 Review of Code\\\\n02:02 Config Object\\\\n06:28 Setup For Training\\\\n10:30 Training Loop\\\\n14:57 Dealing With CUDA Errors\\\\n16:17 Training Results\\\\n19:52 Loss\\\\n21:18 Fill-mask Pipeline For Testing\\\\n21:54 Testing With Laura\"",
    "lengthSeconds": "1838",
    "uploadDate": "2021-07-06",
    "thumbnail_url": "https://i.ytimg.com/vi/35Pdoyi6ZoQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=heTYbpr9mD8",
    "title": "Building MLM Training Input Pipeline - Transformers From Scratch #3",
    "tags": "python, machine learning, data science, artificial intelligence, natural language processing, bert, nlp, nlproc, Huggingface, Tensorflow, pytorch, torch, programming, tutorials, tutorial, education, learning, code, coding",
    "scraped_at": 1684585781.2816205,
    "genre": "Education",
    "views": "5381",
    "desc": "The input pipeline of our training process is the more complex part of the entire transformer build. It consists of us taking our raw OSCAR training data, transforming it, and preparing it for Masked-Language Modeling (MLM). Finally, we load our data into a DataLoader ready for training!\\\\n\\\\nPart 1: https://youtu.be/GhGUZrcB-WM\\\\nPart 2: https://youtu.be/JIeAB8vvBQo\\\\n---\\\\nPart 4: https://youtu.be/35Pdoyi6ZoQ\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Free link:\\\\nhttps://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6?sk=9db6224efbd4ec6fd407a80b528e69b0\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1391",
    "uploadDate": "2021-07-05",
    "thumbnail_url": "https://i.ytimg.com/vi/heTYbpr9mD8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ziiF1eFM3_4",
    "title": "3 Vector-based Methods for Similarity Search (TF-IDF, BM25, SBERT)",
    "tags": "film, udost",
    "scraped_at": 1684585781.3486207,
    "genre": "Education",
    "views": "20550",
    "desc": "Vector similarity search is one of the fastest-growing domains in AI and machine learning. At its core, it is the process of matching relevant pieces of information together.\\\\n\\\\nSimilarity search is a complex topic and there are countless techniques for building effective search engines.\\\\n\\\\nIn this video, we\\'ll cover three vector-based approaches for comparing languages and identifying similar \\'documents\\', covering both vector similarity search and semantic search:\\\\n\\\\n- TF-IDF\\\\n- BM25\\\\n- Sentence-BERT\\\\n\\\\n\\xf0\\x9f\\x93\\xb0 Original article:\\\\nhttps://www.pinecone.io/learn/semantic-search/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\nMining Massive Datasets Book (Similarity Search):\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3CC0zrc (3rd ed)\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3AtHSnV (1st ed, cheaper)\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n01:37 TF-IDF\\\\n11:44 BM25\\\\n20:30 SBERT\"",
    "lengthSeconds": "1764",
    "uploadDate": "2021-06-29",
    "thumbnail_url": "https://i.ytimg.com/vi/ziiF1eFM3_4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=AY62z7HrghY",
    "title": "3 Traditional Methods for Similarity Search (Jaccard, w-shingling, Levenshtein)",
    "tags": "film, udost",
    "scraped_at": 1684585782.2046232,
    "genre": "Education",
    "views": "8270",
    "desc": "Similarity search is one of the fastest-growing domains in AI and machine learning. At its core, it is the process of matching relevant pieces of information together.\\\\n\\\\nSimilarity search is a complex topic and there are countless techniques for building effective search engines.\\\\n\\\\nIn this video, we\\'ll cover three traditional approaches for comparing languages and identifying similar \\'documents\\':\\\\n\\\\n- Jaccard Similarity\\\\n- w-shingling\\\\n- Levenshtein distance\\\\n\\\\n\\xf0\\x9f\\x93\\xb0 Original article:\\\\nhttps://www.pinecone.io/learn/semantic-search/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\nMining Massive Datasets Book (Similarity Search):\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3CC0zrc (3rd ed)\\\\n\\xf0\\x9f\\x93\\x9a https://amzn.to/3AtHSnV (1st ed, cheaper)\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n00:23 Jaccard Similarity\\\\n02:39 w-shingling\\\\n07:17 Levenshtein Distance\"",
    "lengthSeconds": "1520",
    "uploadDate": "2021-06-29",
    "thumbnail_url": "https://i.ytimg.com/vi/AY62z7HrghY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JIeAB8vvBQo",
    "title": "Build a Custom Transformer Tokenizer - Transformers From Scratch #2",
    "tags": "film, udost",
    "scraped_at": 1684585782.1346228,
    "genre": "Education",
    "views": "8287",
    "desc": "How can we build our own custom transformer models?\\\\n\\\\nMaybe we\\'d like our model to understand a less common language, how many transformer models out there have been trained on Piemontese or the Nahuatl languages?\\\\n\\\\nIn that case, we need to do something different. We need to build our own model\\xe2\\x80\\x8a-\\xe2\\x80\\x8afrom scratch.\\\\n\\\\nIn this video, we\\'ll learn how to use HuggingFace\\'s tokenizers library to build our own custom transformer tokenizer.\\\\n\\\\nPart 1: https://youtu.be/GhGUZrcB-WM\\\\n---\\\\nPart 3: https://youtu.be/heTYbpr9mD8\\\\nPart 4: https://youtu.be/35Pdoyi6ZoQ\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/transformers-from-scratch-creating-a-tokenizer-7d7418adb403\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/transformers-from-scratch-creating-a-tokenizer-7d7418adb403?sk=aea909609f41be43bdb2dbbd75a801f2\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "857",
    "uploadDate": "2021-06-24",
    "thumbnail_url": "https://i.ytimg.com/vi/JIeAB8vvBQo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GhGUZrcB-WM",
    "title": "How-to Use HuggingFace's Datasets - Transformers From Scratch #1",
    "tags": "film, udost",
    "scraped_at": 1684585782.2806237,
    "genre": "Education",
    "views": "11638",
    "desc": "How can we build our own custom transformer models?\\\\n\\\\nMaybe we\\'d like our model to understand a less common language, how many transformer models out there have been trained on Piemontese or the Nahuatl languages?\\\\n\\\\nIn that case, we need to do something different. We need to build our own model\\xe2\\x80\\x8a-\\xe2\\x80\\x8afrom scratch.\\\\n\\\\nIn this video, we\\'ll learn how to use HuggingFace\\'s datasets library to download multilingual data and prepare it for training our custom transformer tokenizer and model.\\\\n\\\\n---\\\\nPart 2: https://youtu.be/JIeAB8vvBQo\\\\nPart 3: https://youtu.be/heTYbpr9mD8\\\\nPart 4: https://youtu.be/35Pdoyi6ZoQ\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/transformers-from-scratch-creating-a-tokenizer-7d7418adb403\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/transformers-from-scratch-creating-a-tokenizer-7d7418adb403?sk=aea909609f41be43bdb2dbbd75a801f2\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "861",
    "uploadDate": "2021-06-22",
    "thumbnail_url": "https://i.ytimg.com/vi/GhGUZrcB"
  },
  {
    "link": "watch?v=IC9FaVPKlYc",
    "title": "Training BERT #5 - Training With BertForPretraining",
    "tags": "film, udost",
    "scraped_at": 1684585781.6216228,
    "genre": "Education",
    "views": "9789",
    "desc": "NSP Logic\\\\nhttps://youtu.be/1gN1snKBLP0\\\\n\\\\nMLM Logic\\\\nhttps://youtu.be/q9NS5WpfkrU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/how-to-train-bert-aaad00533168\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Here\\'s a free link:\\\\nhttps://towardsdatascience.com/how-to-train-bert-aaad00533168?sk=5ad4e5e44a6c573b3be1967c9abdcc35\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1305",
    "uploadDate": "2021-06-15",
    "thumbnail_url": "https://i.ytimg.com/vi/IC9FaVPKlYc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5-A435hIYio",
    "title": "New Features in Python 3.10",
    "tags": "film, udost",
    "scraped_at": 1684585783.0966208,
    "genre": "Education",
    "views": "20847",
    "desc": "The Python 3.10 release has several new features like structural pattern matching, a new typing Union operator, and parenthesized context managers!\\\\n\\\\nPython 3.10 has now been released, here we test all of the best new features introduced.\\\\n\\\\nWe\\'ll cover some of the most interesting additions to Python\\xe2\\x80\\x8a-\\xe2\\x80\\x8astructural pattern matching, parenthesized context managers, more typing, and the new and improved error messages.\\\\n\\\\nDownload the latest release:\\\\nhttps://www.python.org/downloads/release/python-3100/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/whats-new-in-python-3-10-a757c6c69342\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/whats-new-in-python-3-10-a757c6c69342?sk=648ae12c1025a83affba4eecec0d46c6\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n00:45 Type Annotations in Python\\\\n01:10  Typing Union Operator\\\\n02:07 Parenthesized Context Managers\\\\n05:07 Structural Pattern Matching\\\\n09:31 Better Error Messages\"",
    "lengthSeconds": "800",
    "uploadDate": "2021-06-08",
    "thumbnail_url": "https://i.ytimg.com/vi/5"
  },
  {
    "link": "watch?v=fA0dFQacmic",
    "title": "FREE 11 Hour NLP Transformers Course (Next 3 Days Only)",
    "tags": "film, udost",
    "scraped_at": 1684585782.7886202,
    "genre": "Education",
    "views": "1713",
    "desc": "The offer has now expired! You can find the final 70% discount here:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nIn total, 10823 people redeemed the code - which is incredible, I\\'m very happy so many of you were interested in the course and I hope it will help many of you in learning about transformers and NLP where it may have been too expensive to otherwise - so thank you all!\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery:\\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "266",
    "uploadDate": "2021-06-04",
    "thumbnail_url": "https://i.ytimg.com/vi/fA0dFQacmic/maxresdefault.jpg"
  },
  {
    "link": "watch?v=x1lAcT3xl5M",
    "title": "Training BERT #4 - Train With Next Sentence Prediction (NSP)",
    "tags": "film, udost",
    "scraped_at": 1684585781.6936476,
    "genre": "Education",
    "views": "6493",
    "desc": "Next sentence prediction (NSP) is one-half of the training process behind the BERT model (the other being masked-language modeling\\xe2\\x80\\x8a-\\xe2\\x80\\x8aMLM).\\\\n\\\\nAlthough NSP (and MLM) are used to pre-train BERT models, we can use these exact methods to further pre-train our models to better understand the specific style of language in our own use cases.\\\\n\\\\nSo, in this video, we\\'ll cover exactly how we take an unstructured body of text, and use it to pre-train a BERT model using NSP.\\\\n\\\\nMeditations data:\\\\nhttps://github.com/jamescalam/transformers/blob/main/data/text/meditations/clean.txt\\\\n\\\\nJupyter Notebook\\\\nhttps://github.com/jamescalam/transformers/blob/main/course/training/06_nsp_training.ipynb\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f?sk=3595968413abde1c5833e1a96e449673\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "2205",
    "uploadDate": "2021-05-27",
    "thumbnail_url": "https://i.ytimg.com/vi/x1lAcT3xl5M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1gN1snKBLP0",
    "title": "Training BERT #3 - Next Sentence Prediction (NSP)",
    "tags": "film, udost",
    "scraped_at": 1684585783.0026217,
    "genre": "Education",
    "views": "7720",
    "desc": "Next sentence prediction (NSP) is one-half of the training process behind the BERT model (the other being masked-language modeling\\xe2\\x80\\x8a-\\xe2\\x80\\x8aMLM).\\\\n\\\\nWhere MLM teaches BERT to understand relationships between words\\xe2\\x80\\x8a-\\xe2\\x80\\x8aNSP teaches BERT to understand relationships between sentences.\\\\n\\\\nIn the original BERT paper, it was found that without NSP, BERT performed worse on every single metric - \\xe2\\x80\\x8aso it\\'s important.\\\\n\\\\nNow, when we use a pre-trained BERT model, training with NSP and MLM has already been done, so why do we need to know about it?\\\\n\\\\nWell, we can actually further pre-train these pre-trained BERT models so that they better understand the language used in our specific use-cases. To do that, we can use both MLM and NSP.\\\\n\\\\nSo, in this video, we\\'ll go into depth on what NSP is, how it works, and how we can implement it in code.\\\\n\\\\nTraining with NSP:\\\\nhttps://youtu.be/x1lAcT3xl5M\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f?sk=3595968413abde1c5833e1a96e449673\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "822",
    "uploadDate": "2021-05-25",
    "thumbnail_url": "https://i.ytimg.com/vi/1gN1snKBLP0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=R6hcxMMOrPE",
    "title": "Training BERT #2 - Train With Masked-Language Modeling (MLM)",
    "tags": "film, udost",
    "scraped_at": 1684585784.5756466,
    "genre": "Education",
    "views": "14488",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nBERT has enjoyed unparalleled success in NLP thanks to two unique training approaches, masked-language modeling (MLM), and next sentence prediction (NSP).\\\\n\\\\nIn many cases, we might be able to take the pre-trained BERT model out-of-the-box and apply it successfully to our own language tasks.\\\\n\\\\nBut often, we might need to pre-train the model for a specific use case even further.\\\\n\\\\nFurther training with MLM allows us to tune BERT to better understand the particular use of language in a more specific domain.\\\\n\\\\nOut-of-the-box BERT\\xe2\\x80\\x8a-\\xe2\\x80\\x8agreat for general purpose use. Fine-tuned with MLM BERT\\xe2\\x80\\x8a-\\xe2\\x80\\x8agreat for domain-specific use.\\\\n\\\\nIn this video, we\\'ll cover exactly how to fine-tune BERT models using MLM in PyTorch.\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Code:\\\\nhttps://github.com/jamescalam/transformers/blob/main/course/training/03_mlm_training.ipynb\\\\n\\\\nMeditations data:\\\\nhttps://github.com/jamescalam/transformers/blob/main/data/text/meditations/clean.txt\\\\n\\\\nUnderstanding MLM:\\\\nhttps://youtu.be/q9NS5WpfkrU\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c?sk=17a19eca8dc8280bea4138802580ffe0\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1666",
    "uploadDate": "2021-05-19",
    "thumbnail_url": "https://i.ytimg.com/vi/R6hcxMMOrPE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=q9NS5WpfkrU",
    "title": "Training BERT #1 - Masked-Language Modeling (MLM)",
    "tags": "film, udost",
    "scraped_at": 1684585781.5516207,
    "genre": "Education",
    "views": "21506",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nBERT, everyone\\'s favorite transformer costs Google ~$7K to train (and who knows how much in R\\\\u0026D costs). From there, we write a couple of lines of code to use the same model\\xe2\\x80\\x8a-\\xe2\\x80\\x8aall for free.\\\\n\\\\nBERT has enjoyed unparalleled success in NLP thanks to two unique training approaches, masked-language modeling (MLM), and next sentence prediction (NSP).\\\\n\\\\nMLM consists of giving BERT a sentence and optimizing the weights inside BERT to output the same sentence on the other side.\\\\n\\\\nSo we input a sentence and ask that BERT outputs the same sentence.\\\\n\\\\nHowever, before we actually give BERT that input sentence\\xe2\\x80\\x8a-\\xe2\\x80\\x8awe mask a few tokens.\\\\n\\\\nSo we\\'re actually inputting an incomplete sentence and asking BERT to complete it for us.\\\\n\\\\nHow to train BERT with MLM:\\\\nhttps://youtu.be/R6hcxMMOrPE\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium article:\\\\nhttps://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c?sk=17a19eca8dc8280bea4138802580ffe0\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM3\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "983",
    "uploadDate": "2021-05-19",
    "thumbnail_url": "https://i.ytimg.com/vi/q9NS5WpfkrU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=W8ZPQOcHnlE",
    "title": "NER With Transformers and spaCy (Python)",
    "tags": "film, udost",
    "scraped_at": 1684585782.5666227,
    "genre": "Education",
    "views": "9456",
    "desc": "Named entity recognition (NER) consists of extracting \\'entities\\' from text\\xe2\\x80\\x8a-\\xe2\\x80\\x8awhat we mean by that is given the sentence:\\\\n\\\\n\\\\\"",
    "lengthSeconds": "567",
    "uploadDate": "2021-05-11",
    "thumbnail_url": "https://i.ytimg.com/vi/W8ZPQOcHnlE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jVPd7lEvjtg",
    "title": "Sentence Similarity With Transformers and PyTorch (Python)",
    "tags": "film, udost",
    "scraped_at": 1684585784.6446218,
    "genre": "Education",
    "views": "13779",
    "desc": "Easy mode: https://youtu.be/Ey81KfQ3PQU\\\\n\\\\nAll we ever seem to talk about nowadays are BERT this, BERT that. I want to talk about something else, but BERT is just too good \\xe2\\x80\\x8a- \\xe2\\x80\\x8aso this video will be about BERT for sentence similarity.\\\\n\\\\nA big part of NLP relies on similarity in highly-dimensional spaces. Typically an NLP solution will take some text, process it to create a big vector/array representing said text\\xe2\\x80\\x8a-\\xe2\\x80\\x8athen perform several transformations.\\\\n\\\\nIt\\'s highly-dimensional magic.\\\\n\\\\nSentence similarity is one of the clearest examples of how powerful highly-dimensional magic can be.\\\\n\\\\nThe logic is this:\\\\n- Take a sentence, convert it into a vector.\\\\n- Take many other sentences, and convert them into vectors.\\\\n- Find sentences that have the smallest distance (Euclidean) or smallest angle (cosine similarity) between them\\xe2\\x80\\x8a-\\xe2\\x80\\x8amore on that here.\\\\n- We now have a measure of semantic similarity between sentences\\xe2\\x80\\x8a-\\xe2\\x80\\x8aeasy!\\\\n\\\\nAt a high level, there\\'s not much else to it. But of course, we want to understand what is happening in a little more detail and implement this in Python too.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium article:\\\\nhttps://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?sk=c0f2990b4660210b447e52d55bd0f4e5\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n00:16 BERT Base Network\\\\n1:11 Sentence Vectors and Similarity\\\\n1:47 The Data and Model\\\\n3:01 Two Approaches\\\\n3:16 Tokenizing Sentences\\\\n9:11 Creating last_hidden_state Tensor\\\\n11:08 Creating Sentence Vectors\\\\n17:53 Cosine Similarity\"",
    "lengthSeconds": "1270",
    "uploadDate": "2021-05-05",
    "thumbnail_url": "https://i.ytimg.com/vi/jVPd7lEvjtg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Ey81KfQ3PQU",
    "title": "Sentence Similarity With Sentence-Transformers in Python",
    "tags": "film, udost",
    "scraped_at": 1684585783.792647,
    "genre": "Education",
    "views": "24833",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nHard mode: https://youtu.be/jVPd7lEvjtg\\\\n\\\\nAll we ever seem to talk about nowadays are BERT this, BERT that. I want to talk about something else, but BERT is just too good \\xe2\\x80\\x8a- \\xe2\\x80\\x8aso this video will be about BERT for sentence similarity.\\\\n\\\\nA big part of NLP relies on similarity in highly-dimensional spaces. Typically an NLP solution will take some text, process it to create a big vector/array representing said text\\xe2\\x80\\x8a-\\xe2\\x80\\x8athen perform several transformations.\\\\n\\\\nIt\\'s highly-dimensional magic.\\\\n\\\\nSentence similarity is one of the clearest examples of how powerful highly-dimensional magic can be.\\\\n\\\\nThe logic is this:\\\\n- Take a sentence, convert it into a vector.\\\\n- Take many other sentences, and convert them into vectors.\\\\n- Find sentences that have the smallest distance (Euclidean) or smallest angle (cosine similarity) between them\\xe2\\x80\\x8a-\\xe2\\x80\\x8amore on that here.\\\\n- We now have a measure of semantic similarity between sentences\\xe2\\x80\\x8a-\\xe2\\x80\\x8aeasy!\\\\n\\\\nAt a high level, there\\'s not much else to it. But of course, we want to understand what is happening in a little more detail and implement this in Python too.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium article:\\\\nhttps://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1\\\\n\\\\n\\xf0\\x9f\\x8e\\x89 Sign-up For New Articles Every Week on Medium!\\\\nhttps://medium.com/@jamescalam/membership\\\\n\\\\n\\xf0\\x9f\\x93\\x96 If membership is too expensive - here\\'s a free link:\\\\nhttps://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?sk=c0f2990b4660210b447e52d55bd0f4e5\\\\n\\\\n\\xf0\\x9f\\x91\\xbe Discord\\\\nhttps://discord.gg/c5QtDB9RAP\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "369",
    "uploadDate": "2021-05-05",
    "thumbnail_url": "https://i.ytimg.com/vi/Ey81KfQ3PQU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2tdLYIKPafc",
    "title": "Extractive Q&A With Haystack and FastAPI in Python",
    "tags": "film, udost",
    "scraped_at": 1684585781.4826226,
    "genre": "Education",
    "views": "6135",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Stoic Q\\\\u0026A App Playlist: https://www.youtube.com/playlist?list=PLIUOU7oqGTLixb-CatMxNCO-mJioMmZEB\\\\n\\\\nIn this video we work through building an extractive Q\\\\u0026A stack using Haystack, and embedding it within a FastAPI instance in Python.\\\\n\\\\nWe use the BERT transformer for our reader model, alongside Elasticsearch and the BM25 retriever algorithm.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "1058",
    "uploadDate": "2021-04-29",
    "thumbnail_url": "https://i.ytimg.com/vi/2tdLYIKPafc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=QrzHImDEq_w",
    "title": "How to Use Type Annotations in Python",
    "tags": "film, udost",
    "scraped_at": 1684585784.4366467,
    "genre": "Education",
    "views": "6573",
    "desc": "Type annotations\\xe2\\x80\\x8a-\\xe2\\x80\\x8aalso known as type signatures\\xe2\\x80\\x8a-\\xe2\\x80\\x8aare used to indicate the datatypes of variables and input/outputs of functions and methods.\\\\n\\\\nIn many languages, datatypes are explicitly stated. In these languages, if you don\\'t declare your datatype\\xe2\\x80\\x8a-\\xe2\\x80\\x8athe code will not run.\\\\n\\\\nType annotations have a long and convoluted history with Python, going all the way back to the first release of Python 3 with the initial implementation of function annotations.\\\\n\\\\nType annotations in Python are not make-or-break like in other languages (like C). They\\'re optional chunks of syntax that we can add to make our code more explicit.\\\\n\\\\nErroneous type annotations will do nothing more than highlight the incorrect annotation in our code editor\\xe2\\x80\\x8a-\\xe2\\x80\\x8ano errors are ever raised due to annotations.\\\\n\\\\nSo, if type annotations are not enforced, why use them?\\\\n\\\\nWell, as we touched upon already\\xe2\\x80\\x8a-\\xe2\\x80\\x8adeclaring types makes our code more explicit, and if done well, easier to read\\xe2\\x80\\x8a-\\xe2\\x80\\x8aboth for ourselves and others.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nRead the Medium article here:\\\\nhttps://towardsdatascience.com/type-annotations-in-python-d90990b172dc\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Here\\'s a free link:\\\\nhttps://towardsdatascience.com/type-annotations-in-python-d90990b172dc?sk=29bc29ab5478a842363963b421781b47\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n00:55 Datatypes Example in C\\\\n2:53 Static and Dynamic Typed Languages\\\\n3:47 Type Annotations in Python\\\\n4:25 How to Define Simple Types\\\\n6:04 IDE Warnings\\\\n8:20 More Complex Types\\\\n9:53 dict[str, int]\\\\n11.07 Multiple Types\\\\n11:38 Union Operator (Py 3.9)\\\\n12:34 Union Operator (Py 3.10)\\\\n13:21 Optional Operator\"",
    "lengthSeconds": "906",
    "uploadDate": "2021-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/QrzHImDEq_w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DBsxUSUhfRg",
    "title": "Q&A Document Retrieval With DPR",
    "tags": "film, udost",
    "scraped_at": 1684585782.6466222,
    "genre": "Education",
    "views": "5672",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Stoic Q\\\\u0026A App Playlist: https://www.youtube.com/playlist?list=PLIUOU7oqGTLixb-CatMxNCO-mJioMmZEB\\\\n\\\\nThe third video in building our Stoic Q\\\\u0026A app.\\\\n\\\\nIn open-domain question answering, we typically design a model architecture that contains a data source, retriever, and reader/generator.\\\\n\\\\nThe first of these components is typically a document store. The two most popular stores we use here are Elasticsearch and FAISS.\\\\n\\\\nNext up is our retriever \\xe2\\x80\\x94 the topic of this video. The job of the retriever is to filter through our document store for relevant chunks of information (the documents) and pass them to the reader/generator model.\\\\n\\\\nDPR (dense passage retriever) is a dense vector retriever that is trained on question-context pairs. Encoding both accordingly - enabling super accurate similarity indexing.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nIf you\\'re interested in learning more about DPR, I wrote about it on Medium here:\\\\nhttps://towardsdatascience.com/how-to-create-an-answer-from-a-question-with-dpr-d76e29cc5d60\\\\n\\\\n(Free link):\\\\nhttps://towardsdatascience.com/how-to-create-an-answer-from-a-question-with-dpr-d76e29cc5d60?sk=1bdd7c1bff80bf51410962691c690c69\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery: \\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\"",
    "lengthSeconds": "889",
    "uploadDate": "2021-04-15",
    "thumbnail_url": "https://i.ytimg.com/vi/DBsxUSUhfRg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Vwq7Ucp9UCw",
    "title": "How to Index Q&A Data With Haystack and Elasticsearch",
    "tags": "film, udost",
    "scraped_at": 1684585781.8976471,
    "genre": "Education",
    "views": "8280",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Stoic Q\\\\u0026A App Playlist: https://www.youtube.com/playlist?list=PLIUOU7oqGTLixb-CatMxNCO-mJioMmZEB\\\\n\\\\nThe second video in \\'Building a Stoic Q\\\\u0026A App\\' - here we\\'re setting up Elasticsearch and Haystack to store the data (Meditations) ready for retrieval when we ask our app questions.\\\\n\\\\nFind the code here:\\\\nhttps://github.com/jamescalam/aurelius/tree/main/code/labs\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\"",
    "lengthSeconds": "806",
    "uploadDate": "2021-04-12",
    "thumbnail_url": "https://i.ytimg.com/vi/Vwq7Ucp9UCw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4Jmq28RQ3hU",
    "title": "How-to Structure a Q&A ML App",
    "tags": "film, udost",
    "scraped_at": 1684585783.9266455,
    "genre": "Education",
    "views": "2390",
    "desc": "\\xe2\\x96\\xb6\\xef\\xb8\\x8f Stoic Q\\\\u0026A App Playlist: https://www.youtube.com/playlist?list=PLIUOU7oqGTLixb-CatMxNCO-mJioMmZEB\\\\n\\\\nI\\'m planning on doing something different, a series of videos where we work through the steps - from start-to-finish - of (attempting) to build a Q\\\\u0026A web app that answers our questions with Stoic answers.\\\\n\\\\nIn this video, I\\'m outlining the idea and describing the high-level setup that I think we\\'ll need to put together. It should be cool!\\\\n\\\\nWe\\'ll be using the Haystack framework for \\'Q\\\\u0026A at scale\\', which using HuggingFace transformers under-the-hood, and the Elasticsearch document store.\\\\n\\\\nFind the repo here:\\\\nhttps://github.com/jamescalam/aurelius\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\"",
    "lengthSeconds": "585",
    "uploadDate": "2021-04-09",
    "thumbnail_url": "https://i.ytimg.com/vi/4Jmq28RQ3hU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JkeNVaiUq_c",
    "title": "How to Build Python Packages for Pip",
    "tags": "film, udost",
    "scraped_at": 1684585781.4166467,
    "genre": "Education",
    "views": "33549",
    "desc": "The most powerful feature of Python is its community. Almost every use-case out there has a package built specifically for it.\\\\n\\\\nNeed to send mobile/email alerts? pip install knockknock \\xe2\\x80\\x8a- \\xe2\\x80\\x8aBuild ML apps? pip install streamlit \\xe2\\x80\\x8a- \\xe2\\x80\\x8aBored of your terminal? pip install colorama\\xe2\\x80\\x8a - \\xe2\\x80\\x8aIt\\'s too easy!\\\\n\\\\nI know this is obvious, but those libraries didn\\'t magically appear. For each package, there is a person, or many persons\\xe2\\x80\\x8a-\\xe2\\x80\\x8athat actively developed and deployed that package.\\\\n\\\\nEvery single one.\\\\n\\\\nAll 300K+ of them.\\\\n\\\\nThat is why Python is Python, the level of support is phenomenal\\xe2\\x80\\x8a-\\xe2\\x80\\x8amindblowing.\\\\n\\\\nIn this video, we will learn how to build our own packages. And add them to the Python Package Index (PyPI). Afterward, we will be able to install our packages using pip install!\\\\n\\\\nGitHub Repo:\\\\nhttps://github.com/jamescalam/aesthetic_ascii\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium Article:\\\\nhttps://towardsdatascience.com/how-to-package-your-python-code-df5a7739ab2e\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Here\\'s a free link:\\\\nhttps://towardsdatascience.com/how-to-package-your-python-code-df5a7739ab2e?sk=04d9f67c0654445bbcbbf6825f535900\"",
    "lengthSeconds": "1267",
    "uploadDate": "2021-04-02",
    "thumbnail_url": "https://i.ytimg.com/vi/JkeNVaiUq_c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=pjtnkCGElcE",
    "title": "Multi-Class Language Classification With BERT in TensorFlow",
    "tags": "film, udost",
    "scraped_at": 1684585782.7216213,
    "genre": "Education",
    "views": "15393",
    "desc": "Chapters for each section of the video (preprocessing, model build, prediction) are in the video timeline.\\\\n\\\\nTransformers have been described as the fourth pillar of deep learning [1], alongside the three big neural net architectures of CNNs, RNNs, and MLPs.\\\\n\\\\nHowever, from the perspective of natural language processing\\xe2\\x80\\x8a-\\xe2\\x80\\x8atransformers are much more than that. Since their introduction in 2017, they\\'ve come to dominate a majority of NLP benchmarks\\xe2\\x80\\x8a-\\xe2\\x80\\x8aand continue to impress daily.\\\\n\\\\nWhat I\\'m saying is, transformers are damn cool. And with libraries like HuggingFace\\'s transformers\\xe2\\x80\\x8a-\\xe2\\x80\\x8ait has become too easy to build incredible solutions with them.\\\\n\\\\nSo, what\\'s not to love? Incredible performance paired with the ultimate ease-of-use.\\\\n\\\\nIn this video, we\\'ll work through building a multi-class classification model using transformers\\xe2\\x80\\x8a-\\xe2\\x80\\x8afrom start-to-finish.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium article:\\\\nhttps://towardsdatascience.com/multi-class-classification-with-transformers-6cf7b59a033a\\\\n\\\\nFree access:\\\\nhttps://towardsdatascience.com/multi-class-classification-with-transformers-6cf7b59a033a?sk=544872025c2283c54cf4294814b8cae3\\\\n\\\\nLink to Kaggle video:\\\\nhttps://youtu.be/DgGFhQmfxHo\\\\n\\\\n[1] Fourth Pillar of AI:\\\\nhttps://ark-invest.com/articles/analyst-research/transformers-comprise-the-fourth-pillar-of-deep-learning/\\\\n\\\\n00:00 Intro\\\\n01:21 Pulling Data\\\\n01:47 Preprocessing\\\\n14:33 Data Input Pipeline\\\\n24:14 Defining Model\\\\n33:29 Model Training\\\\n35:36 Saving and Loading Models\\\\n37:37 Making Predictions\"",
    "lengthSeconds": "2603",
    "uploadDate": "2021-03-25",
    "thumbnail_url": "https://i.ytimg.com/vi/pjtnkCGElcE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2qJavL-VX9Y",
    "title": "The NEW Match-Case Statement in Python 3.10",
    "tags": "film, udost",
    "scraped_at": 1684585782.0666223,
    "genre": "Education",
    "views": "25688",
    "desc": "Python 3.10 is beginning to fill-out with plenty of fascinating new features. One of those, in particular, caught my attention\\xe2\\x80\\x8a-\\xe2\\x80\\x8astructural pattern matching\\xe2\\x80\\x8a-\\xe2\\x80\\x8aor as most of us will know it, switch/case statements.\\\\n\\\\nSwitch-statements have been absent from Python despite being a common feature of most languages. Python is leapfrogging ahead of those languages by introducing the match-case statement as a switch-case v2.0.\\\\n\\\\nBack in 2006, PEP 3103 was raised, recommending the implementation of a switch-case statement. However, after a poll at PyCon 2007 received no support for the feature, the Python devs dropped it.\\\\n\\\\nFast-forward to 2020, and Guido van Rossum, the creator of Python, committed the first documentation showing the new match-statements, which have been named Structural Pattern Matching, as found in PEP 634.\\\\n\\\\nLet\\'s take a look at how this new logic works.\\\\n\\\\nMedium Article:\\\\nhttps://towardsdatascience.com/switch-case-statements-are-coming-to-python-d0caf7b2bfd3\\\\n\\\\nFriend Link (free access):\\\\nhttps://towardsdatascience.com/switch-case-statements-are-coming-to-python-d0caf7b2bfd3?sk=363e0f7696502647e007f91910b4c817\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x95\\xb9\\xef\\xb8\\x8f Free AI-Powered Code Refactoring with Sourcery:\\\\nhttps://sourcery.ai/?utm_source=YouTub\\\\u0026utm_campaign=JBriggs\\\\u0026utm_medium=aff\\\\n\\\\n00:00 Intro\\\\n00:58 Switch-Case\\\\n02:37 Flow of Logic\\\\n03:21 Second Example (Tuples)\\\\n05:00 Final Example Setup\\\\n11:30 Final Example If-Else Version\\\\n15:22 Final Example Match-Case Version\"",
    "lengthSeconds": "1088",
    "uploadDate": "2021-03-19",
    "thumbnail_url": "https://i.ytimg.com/vi/2qJavL"
  },
  {
    "link": "watch?v=9Od9-DV9kd8",
    "title": "Unicode Normalization for NLP in Python",
    "tags": "natural language processing, unicode normalization, unicode normalisation, natural language, nlp, tensorflow, pytorch, artificial intelligence, python, learn data science, data science, machine learning, learn machine learning, machine learning engineer, data scientist, ml, ml engineering, mlops, ai, ai engineer, programming, tech, software development, software engineering",
    "scraped_at": 1684585783.3066223,
    "genre": "Education",
    "views": "1575",
    "desc": "\\xe2\\x84\\x95\\xf0\\x9d\\x95\\xa0-\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\x96 \\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x9f \\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\xa3 \\xf0\\x9d\\x95\\xa3\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x98\\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\xa5 \\xf0\\x9d\\x95\\x9e\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\x95 \\xf0\\x9d\\x95\\xa8\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xa6\\xf0\\x9d\\x95\\x9d\\xf0\\x9d\\x95\\x95 \\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\xa7\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\xa3 \\xf0\\x9d\\x95\\xa6\\xf0\\x9d\\x95\\xa4\\xf0\\x9d\\x95\\x96 \\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\xa4\\xf0\\x9d\\x95\\x96 \\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xaa\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\x98 \\xf0\\x9d\\x95\\x97\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\xa5 \\xf0\\x9d\\x95\\xa7\\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\xa3\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\xa4. \\xf0\\x9d\\x95\\x8b\\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\x96 \\xf0\\x9d\\x95\\xa8\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xa3\\xf0\\x9d\\x95\\xa4\\xf0\\x9d\\x95\\xa5 \\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\x98, \\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\xa4 \\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x97 \\xf0\\x9d\\x95\\xaa\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xa6 \\xf0\\x9d\\x95\\x95\\xf0\\x9d\\x95\\xa0 \\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\xaa \\xf0\\x9d\\x95\\x97\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xa3\\xf0\\x9d\\x95\\x9e \\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\x97 \\xe2\\x84\\x95\\xf0\\x9d\\x95\\x83\\xe2\\x84\\x99 \\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\x95 \\xf0\\x9d\\x95\\xaa\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xa6 \\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\xa7\\xf0\\x9d\\x95\\x96 \\xf0\\x9d\\x95\\x94\\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\xa3\\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\x94\\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\xa3\\xf0\\x9d\\x95\\xa4 \\xf0\\x9d\\x95\\x9d\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x9c\\xf0\\x9d\\x95\\x96 \\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\x99\\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\xa4 \\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x9f \\xf0\\x9d\\x95\\xaa\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xa6\\xf0\\x9d\\x95\\xa3 \\xf0\\x9d\\x95\\x9a\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\xa1\\xf0\\x9d\\x95\\xa6\\xf0\\x9d\\x95\\xa5, \\xf0\\x9d\\x95\\xaa\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\xa6\\xf0\\x9d\\x95\\xa3 \\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\xa9\\xf0\\x9d\\x95\\xa5 \\xf0\\x9d\\x95\\x93\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\x94\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\x9e\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\xa4 \\xf0\\x9d\\x95\\x94\\xf0\\x9d\\x95\\xa0\\xf0\\x9d\\x95\\x9e\\xf0\\x9d\\x95\\xa1\\xf0\\x9d\\x95\\x9d\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\xa5\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\x9d\\xf0\\x9d\\x95\\xaa \\xf0\\x9d\\x95\\xa6\\xf0\\x9d\\x95\\x9f\\xf0\\x9d\\x95\\xa3\\xf0\\x9d\\x95\\x96\\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\x95\\xf0\\x9d\\x95\\x92\\xf0\\x9d\\x95\\x93\\xf0\\x9d\\x95\\x9d\\xf0\\x9d\\x95\\x96.\\\\n\\\\nWe also find that text like this is incredibly common\\xe2\\x80\\x8a-\\xe2\\x80\\x8aparticularly on social media.\\\\n\\\\nAnother pain-point comes from diacritics (the little glyphs in \\xc3\\x87, \\xc3\\xa9, \\xc3\\x85) that you\\'ll find in almost every European language.\\\\n\\\\nThese characters have a hidden property that can trip up any NLP model\\xe2\\x80\\x8a-\\xe2\\x80\\x8atake a look at the Unicode for two versions of \\xc3\\x87:\\\\n\\\\nLatin capital letter C with cedilla: \\\\\\\\u00C7\\\\n\\\\nLatin capital letter C + combining cedilla: \\\\\\\\u0043\\\\\\\\u0327\\\\n\\\\nBoth are completely different, despite rendering as the same character.\\\\n\\\\nTo deal with all of these text variants we need to use Unicode normalization - which we will cover in this video.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium article:\\\\nhttps://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0\\\\n\\\\nFriend link (free access):\\\\nhttps://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0?sk=0cd19a9ad9f5d948b33179bab3c3b7cd\"",
    "lengthSeconds": "927",
    "uploadDate": "2021-03-17",
    "thumbnail_url": "https://i.ytimg.com/vi/9Od9"
  },
  {
    "link": "watch?v=yDGo9z_RlnE",
    "title": "Sentiment Analysis on ANY Length of Text With Transformers (Python)",
    "tags": "sentiment analysis, transformers, deep learning, python transformers, sentiment analysis with transformers, machine learning, tensorflow, pytorch, natural language processing, nlp, language, artificial intelligence, artifical intelligence, sentiment classification, huggingface, huggingface transformers",
    "scraped_at": 1684585784.2966452,
    "genre": "Education",
    "views": "5777",
    "desc": "The de-facto standard in many natural language processing (NLP) tasks nowadays is to use a transformer. Text generation? Transformer. Question-and-answering? Transformer. Language classification? Transformer!\\\\n\\\\nHowever, one of the problems with many of these models (a problem that is not just restricted to transformer models)  is that we cannot process long pieces of text.\\\\n\\\\nAlmost every article I write on Medium contains 1000+ words, which, when tokenized for a transformer model like BERT, will produce 1000+ tokens. BERT (and many other transformer models) will consume 512 tokens max\\xe2\\x80\\x8a-\\xe2\\x80\\x8atruncating anything beyond this length.\\\\n\\\\nAlthough I think you may struggle to find value in processing my Medium articles, the same applies to many useful data sources\\xe2\\x80\\x8a-\\xe2\\x80\\x8alike news articles or Reddit posts.\\\\n\\\\nWe will take a look at how we can work around this limitation. In this article, we will find the sentiment for long posts from the /r/investing subreddit. This video will cover:\\\\n\\\\nHigh-Level Approach\\\\nGetting Started\\\\n- Data\\\\n- Initialization\\\\nTokenization\\\\nPreparing The Chunks\\\\n- Split\\\\n- CLS and SEP\\\\n- Padding\\\\n- Reshaping For BERT\\\\nMaking Predictions\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nHere\\'s a link to the Medium article:\\\\nhttps://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f\\\\n\\\\nAnd a free access link if you don\\'t have Medium membership:\\\\nhttps://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f?sk=d4e717eb2ff31fb27ea67019bbb63ad6\"",
    "lengthSeconds": "1630",
    "uploadDate": "2021-03-10",
    "thumbnail_url": "https://i.ytimg.com/vi/yDGo9z_RlnE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=TCZgXFPNnbc",
    "title": "Identify Stocks on Reddit with SpaCy (NER in Python)",
    "tags": "film, udost",
    "scraped_at": 1684585782.9296217,
    "genre": "Education",
    "views": "1852",
    "desc": "We will learn how to process unstructured text data from Reddit and extract organization names so that any further analysis is automatically classified and results assigned to the correct stocks.\\\\n\\\\nOrganizations are mentioned in each subreddit in a variety of formats. Typically we will find two formats:\\\\n\\\\n- Organization name, eg Tesla/Tesla Motors\\\\n- Ticker symbol, eg TSLA, tsla, or $TSLA\\\\n\\\\nWe also need to be able to differentiate between tickers and other abbreviations/slang -some of these are unclear like AI (AI can mean both artificial intelligence and refer to the ticker symbol for C3.ai).\\\\n\\\\nSo, we need a reasonable competent NER process to accurately classify our data.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nReddit API video: https://youtu.be/FdjVoOf9HN4\\\\n/r/investing data: https://github.com/jamescalam/transformers/blob/main/course/named_entity_recognition/data/reddit_investing.csv\\\\nMedium article: https://towardsdatascience.com/ner-for-extracting-stock-mentions-on-reddit-aa604e577be\\\\n(Free version if you don\\'t have Medium membership): https://towardsdatascience.com/ner-for-extracting-stock-mentions-on-reddit-aa604e577be?sk=d16305d40b18e7955a0665633182d2b4\\\\n\\\\nThanks for watching!\"",
    "lengthSeconds": "1307",
    "uploadDate": "2021-03-03",
    "thumbnail_url": "https://i.ytimg.com/vi/TCZgXFPNnbc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=QJq9RTp_OVE",
    "title": "How-to Decode Outputs From NLP Models (Python)",
    "tags": "natural language processing, python, transformers, text generation, gpt",
    "scraped_at": 1684585782.3536224,
    "genre": "Education",
    "views": "1175",
    "desc": "In this video, we will cover three ways to decode the output probabilities from NLP models - greedy search, random sampling, and beam search.\\\\n\\\\nLearning how to decode outputs can make a huge difference in diagnosing model issues and improving text output quality - and as an added bonus it\\'s super easy.\\\\n\\\\nOne of the often-overlooked parts of sequence generation in natural language processing (NLP) is how we select our output tokens \\xe2\\x80\\x94 otherwise known as decoding.\\\\n\\\\nYou may be thinking \\xe2\\x80\\x94 we select a token/word/character based on the probability of each token assigned by our model.\\\\n\\\\nThis is half-true \\xe2\\x80\\x94 in language-based tasks, we typically build a model which outputs a set of probabilities to an array where each value in that array represents the probability of a specific word/token.\\\\n\\\\nAt this point, it might seem logical to select the token with the highest probability? Well, not really \\xe2\\x80\\x94 this can create some unforeseen consequences \\xe2\\x80\\x94 as we will see soon.\\\\n\\\\nWhen we are selecting a token in machine-generated text, we have a few alternative methods for performing this decode \\xe2\\x80\\x94 and options for modifying the exact behavior too.\\\\n\\\\nIn this video we will explore three different methods for selecting our output token, these are:\\\\n\\\\n- Greedy Decoding\\\\n- Random Sampling\\\\n- Beam Search\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nLink to the article version on Medium:\\\\nhttps://towardsdatascience.com/the-three-decoding-methods-for-nlp-23ca59cb1e9d\\\\n\\\\nFree link (if you don\\'t have membership):\\\\nhttps://towardsdatascience.com/the-three-decoding-methods-for-nlp-23ca59cb1e9d?sk=64fbb0204c174dc520af027a69f88030\"",
    "lengthSeconds": "577",
    "uploadDate": "2021-02-24",
    "thumbnail_url": "https://i.ytimg.com/vi/QJq9RTp_OVE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=scJsty_DR3o",
    "title": "How to Build Q&A Models in Python (Transformers)",
    "tags": "python, tensorflow, huggingface, transformers, nlp, natural language processing, pytorch, bert, electra, q",
    "scraped_at": 1684585784.7146208,
    "genre": "Education",
    "views": "14415",
    "desc": "In this video we\\'ll cover how to build a question-answering model in Python using HuggingFace\\'s Transformers.\\\\n\\\\nYou will need to install the transformers library with:\\\\npip install transformers\\\\n\\\\nAlongside either TensorFlow or PyTorch (to follow this video exactly you will need PyTorch). To install TensorFlow just type:\\\\npip install tensorflow\\\\nOR\\\\nconda install tensorflow\\\\n\\\\nAnd for PyTorch follow the instructions under \\'Install PyTorch\\' here:\\\\nhttps://pytorch.org/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nLink to Q\\\\u0026A fine-tuning video:\\\\nhttps://youtu.be/ZIRmXkHp0-c\\\\n\\\\nYou can find the Medium article link below here:\\\\nhttps://towardsdatascience.com/question-and-answering-with-bert-6ef89a78dac\"",
    "lengthSeconds": "1189",
    "uploadDate": "2021-02-19",
    "thumbnail_url": "https://i.ytimg.com/vi/scJsty_DR3o/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZIRmXkHp0-c",
    "title": "How to Build Custom Q&A Transformer Models in Python",
    "tags": "python, nlp, machine learning, data science, natural language processing, transformers, natural language, learn to code, learn data science, data science tutorial, data scientist, python tutorial, machine learning tutorial, ML, DS, ML tutorial, programming, software development, software dev, artificial intelligence, artifical intelligence, huggingface, huggingface transformers, qna transformer, q",
    "scraped_at": 1684585784.365623,
    "genre": "Education",
    "views": "13324",
    "desc": "In this video, we will learn how to take a pre-trained transformer model and train it for question-and-answering. We will be using the HuggingFace transformers library with the PyTorch implementation of models in Python.\\\\n\\\\nTransformers are one of the biggest developments in Natural Language Processing (NLP) and learning how to use them properly is basically a data science superpower - they\\'re genuinely amazing I promise!\\\\n\\\\nI hope you enjoy the video :)\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium article:\\\\nhttps://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460\\\\n\\\\n(Free link):\\\\nhttps://towardsdatascience.com/how-to-fine-tune-a-q-a-transformer-86f91ec92997?sk=9344fd51afe71a0905db833d0183d436\\\\n\\\\nCode:\\\\nhttps://gist.github.com/jamescalam/55daf50c8da9eb3a7c18de058bc139a3\\\\n\\\\nPhoto in thumbnail by Lorenzo Herrera on Unsplash\\\\nhttps://unsplash.com/@lorenzoherrera\"",
    "lengthSeconds": "4215",
    "uploadDate": "2021-02-12",
    "thumbnail_url": "https://i.ytimg.com/vi/ZIRmXkHp0"
  },
  {
    "link": "watch?v=FdjVoOf9HN4",
    "title": "How-to Use The Reddit API in Python",
    "tags": "reddit, python, api, data science, data analytics, tutorial, learn to code, programming, reddit api, pandas, requests, pandas dataframes, dataframe",
    "scraped_at": 1684585784.5066473,
    "genre": "Education",
    "views": "38683",
    "desc": "Learn how to use the Reddit API in Python, including setup, authorization, and pulling data from subreddits.\\\\n\\\\nReddit API docs:\\\\nhttps://www.reddit.com/dev/api/\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\n\\xf0\\x9f\\x93\\x99 Medium article:\\\\nhttps://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c\\\\n\\\\n\\xf0\\x9f\\x93\\x96 Free link:\\\\nhttps://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c?sk=0295f297c1365bee7cc7a32bdff21b61\\\\n\\\\nExtract from article:\\\\n\\\\n\\\\\"",
    "lengthSeconds": "1400",
    "uploadDate": "2021-02-12",
    "thumbnail_url": "https://i.ytimg.com/vi/FdjVoOf9HN4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1ZcXmjZtJJ8",
    "title": "Building a PlotLy $GME Chart in Python",
    "tags": "film, udost",
    "scraped_at": 1684585782.4926224,
    "genre": "Education",
    "views": "251",
    "desc": "A code-along video covering the coding process from imagination to Python.\\\\nSomething a little different, I\\'m not overly keen on this format - it\\'s pretty long - but I\\'ve recorded it and I think maybe this can be useful for a few of you.\\\\nI haven\\'t prepared anything beforehand, this is just going into the coding process with a rough outline of wanting to build a stock chart for GME (GameStop) and adding a few technical indicators - to get more familiar with PlotLy and the AlphaVantage API.\\\\nSo, it\\'s a weird one, but I hope a few of you enjoy it - thanks :)\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\"",
    "lengthSeconds": "4491",
    "uploadDate": "2021-02-07",
    "thumbnail_url": "https://i.ytimg.com/vi/1ZcXmjZtJJ8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GyJtxd14DTc",
    "title": "Novice to Advanced RegEx in Less-than 30 Minutes + Python",
    "tags": "python, programming, tutorial, regular expressions, learn to code, code, coding, data science, data analytics, machine learning, deep learning, artificial intelligence, web development, developer, regex, natural language, natural language processing, artifical intelligence, natural langage, nlp, regular espressions, regex modifiers, global modifiers, regex conditionals, conditional logic, re.match, re.findall, re.search, lookahead assertion, negative lookahead, capture groups",
    "scraped_at": 1684585783.5206466,
    "genre": "Education",
    "views": "10728",
    "desc": "A full tutorial covering everything you need to know about Regular Expressions - an essential for anyone learning to code - and even more so for anyone interested in Natural Language Processing.\\\\n\\\\nThis video includes:\\\\n\\\\n- metacharacters\\\\n- quantifiers\\\\n- capture groups\\\\n- using capture groups in Python\\\\n- character sets\\\\n- look-ahead and look-behind assertions\\\\n- negative look-ahead and look-behind assertions\\\\n- inline modifiers\\\\n- passing modifiers as function parameters in Python\\\\n- conditionals (if-else statements for RegEx)\\\\n- re.match\\\\n- re.search\\\\n- re.findall\\\\n\\\\nWe cover all of this in-depth in this tutorial, incl. examples all the way through on RegEx101 (an interactive debugging/regex building tool) and also in Python.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\"",
    "lengthSeconds": "1769",
    "uploadDate": "2021-01-27",
    "thumbnail_url": "https://i.ytimg.com/vi/GyJtxd14DTc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=KTFWNI0qL28",
    "title": "6 of Python's Newest and Best Features (3.7-3.9)",
    "tags": "python, programming, tutorial, learn to code, coding, code, tech, technology, software, software development, computer science, languages, software engineering, data science, data scientist, machine learning, artificial intelligence, AI, deep learning, web development, computer programming, internet of things, IoT, cloud, data analytics",
    "scraped_at": 1684585782.4246213,
    "genre": "Education",
    "views": "912",
    "desc": "A rundown of the six most recent, and coolest features added to Python in the past few years!\\\\n\\\\n2018 brought us a plethora of new features with the release of Python 3.7, followed by 3.8 in 2019, and 3.9 in 2020.\\\\n\\\\nMany of those changes were behind the scenes. Optimizations and upgrades that the vast majority of us will never notice, despite their benefits.\\\\n\\\\nOthers are more obvious, additions to syntax or functionality that can change how we write our code. But even these visible changes can be hard to keep up with.\\\\n\\\\nIn this video, we will run through the more apparent upgrades to provide a brief but hopefully invaluable refresher on everything new to Python from the past few years.\\\\n\\\\n- Python 3.7\\\\n  - Breakpoints\\\\n- Python 3.8\\\\n  - Walrus Operator\\\\n  - F-string \\'=\\' Specifier\\\\n  - Positional-only Parameters\\\\n- Python 3.9\\\\n  - More Type Hinting\\\\n  - Dictionary Unions\\\\n\\\\nMedium Article:\\\\nhttps://towardsdatascience.com/amazing-features-added-to-python-from-3-7-to-now-4f35f0bb1ea6\\\\n\\\\n(Free access link):\\\\nhttps://towardsdatascience.com/amazing-features-added-to-python-from-3-7-to-now-4f35f0bb1ea6?sk=bda3cb7717caa969b81619f85191f241\\\\n\\\\nThumbnail background by Martin Sanchez on Unsplash:\\\\nhttps://unsplash.com/photos/4PDPLw1flgE\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\"",
    "lengthSeconds": "1084",
    "uploadDate": "2021-01-12",
    "thumbnail_url": "https://i.ytimg.com/vi/KTFWNI0qL28/maxresdefault.jpg"
  },
  {
    "link": "watch?v=f6XVfgJTbp4",
    "title": "Input Data Pipelines - TensorFlow Essentials #4",
    "tags": "python, programming, python tutorial, data science, machine learning, tensorflow, programming tutorial, learn code, code, coding, software engineering, software development, computer science, technology, tutorial, how",
    "scraped_at": 1684585783.8586214,
    "genre": "Education",
    "views": "2361",
    "desc": "Learn how to set-up efficient and clean input data pipelines using tf.data.Dataset\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nThumbnail background by Daria Nepriakhina on Unsplash\\\\nhttps://unsplash.com/@epicantus?utm_source=unsplash\\\\u0026amp;utm_medium=referral\\\\u0026amp;utm_content=creditCopyText\"",
    "lengthSeconds": "751",
    "uploadDate": "2020-12-30",
    "thumbnail_url": "https://i.ytimg.com/vi/f6XVfgJTbp4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_8Bydxud1XU",
    "title": "Training Parameters - TensorFlow Essentials #3",
    "tags": "python, tutorial, tensorflow, machine learning, data science, coding, learn code, python tutorial, programming tutorial, how",
    "scraped_at": 1684585781.7666469,
    "genre": "Education",
    "views": "535",
    "desc": "Learn how to set up model training parameters and compile the model before training.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nThumbnail background by Alex McCarthy on Unsplash\\\\nhttps://unsplash.com/@4lexmccarthy?utm_source=unsplash\\\\u0026amp;utm_medium=referral\\\\u0026amp;utm_content=creditCopyText\"",
    "lengthSeconds": "449",
    "uploadDate": "2020-12-29",
    "thumbnail_url": "https://i.ytimg.com/vi/_8Bydxud1XU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BYbJ_HH788U",
    "title": "Functional API - TensorFlow Essentials #2",
    "tags": "python, tensorflow, data science, machine learning, programming, software development, code, coding, code tutorial",
    "scraped_at": 1684585783.1946225,
    "genre": "Education",
    "views": "827",
    "desc": "A look at the functional API method for building models in TensorFlow 2 for Python.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nThumbnail background by Darius Bashar on Unsplash\\\\nhttps://unsplash.com/@dariusbashar?utm_source=unsplash\\\\u0026amp;utm_medium=referral\\\\u0026amp;utm_content=creditCopyText\"",
    "lengthSeconds": "341",
    "uploadDate": "2020-12-29",
    "thumbnail_url": "https://i.ytimg.com/vi/BYbJ_HH788U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=MQD1yMnZ_jk",
    "title": "Sequential Model - TensorFlow Essentials #1",
    "tags": "python, programming, python tutorial, tutorial, programming tutorial, tensorflow, technology, data science, machine learning",
    "scraped_at": 1684585782.8556218,
    "genre": "Education",
    "views": "6057",
    "desc": "Learn how to use the sequential model building approach in TensorFlow 2.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nBackground thumbnail by Aryan Dhiman on Unsplash\\\\nhttps://unsplash.com/@mylifeasaryan_?utm_source=unsplash\\\\u0026amp;utm_medium=referral\\\\u0026amp;utm_content=creditCopyText\"",
    "lengthSeconds": "391",
    "uploadDate": "2020-12-29",
    "thumbnail_url": "https://i.ytimg.com/vi/MQD1yMnZ_jk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8o3jvkK2GGU",
    "title": "Python Environment Setup for Machine Learning",
    "tags": "film, udost",
    "scraped_at": 1684585784.057653,
    "genre": "People",
    "views": "2789",
    "desc": "Everything you need for a Python environment set up for Machine Learning and Data Science!\\\\n\\\\n\\xf0\\x9f\\x93\\x95 Article:\\\\nhttps://towardsdatascience.com/how-to-setup-python-for-machine-learning-173cb25f0206\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nThumbnail background by Christian Wiediger on Unsplash\\\\nhttps://unsplash.com/@christianw\"",
    "lengthSeconds": "753",
    "uploadDate": "2020-12-23",
    "thumbnail_url": "https://i.ytimg.com/vi/8o3jvkK2GGU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DFtP1THE8fE",
    "title": "How-to do Sentiment Analysis with Flair in Python",
    "tags": "film, udost",
    "scraped_at": 1684585783.5846243,
    "genre": "People",
    "views": "6514",
    "desc": "Learn how to perform powerful sentiment analysis with no fine-tuning or pre-training required using the Flair NLP library in Python.\\\\n\\\\nWith the real-time information available to us on massive social media platforms like Twitter, we have all the data we could ever need to create these accurate and up-to-date sentiment metrics for different companies.\\\\n\\\\nBut then comes the question, how can our computer understand what this unstructured text data means?\\\\n\\\\nThat is where sentiment analysis comes in. Sentiment analysis is a particularly interesting branch of Natural Language Processing (NLP), which is used to rate the language used in a body of text.\\\\n\\\\nThrough sentiment analysis, we can take thousands of tweets about a company and judge whether they are generally positive or negative (the sentiment) in real-time!\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium article:\\\\nhttps://towardsdatascience.com/sentiment-analysis-for-stock-price-prediction-in-python-bed40c65d178\\\\n\\\\n(Free link if you don\\'t have Medium membership):\\\\nhttps://towardsdatascience.com/sentiment-analysis-for-stock-price-prediction-in-python-bed40c65d178?sk=1cbf33a5d1fd2ed841f9487972c1cbed\\\\n\\\\nThumbnail photo by Alexander London on Unsplash\\\\nhttps://unsplash.com/@alxndr_london\"",
    "lengthSeconds": "847",
    "uploadDate": "2020-12-04",
    "thumbnail_url": "https://i.ytimg.com/vi/DFtP1THE8fE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=egDIqQIjDCI",
    "title": "Text Summarization with Google AI's T5 in Python",
    "tags": "film, udost",
    "scraped_at": 1684585783.4536219,
    "genre": "People",
    "views": "7631",
    "desc": "Easy text summarization using Google AI\\'s T5 model using HuggingFace transformers and PyTorch in Python.\\\\n\\\\nAutomatic text summarization allows us to shorten long pieces of text into easy-to-read, short snippets that still convey the most important and relevant information of the original text.\\\\n\\\\nIn this video, we\\xe2\\x80\\x99ll build a simple but incredibly powerful text summarizer using Google\\xe2\\x80\\x99s T5. We\\xe2\\x80\\x99ll be using the PyTorch and HuggingFace\\xe2\\x80\\x99s Transformers frameworks.\\\\n\\\\nThis is split into three parts:\\\\n1. Import and Initialization\\\\n2. Data and Tokenization\\\\n3. Summary Generation\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nYou can read the article version of this on Medium here:\\\\nhttps://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6\\\\n\\\\n(And for those of you without Medium membership, here\\'s a free link):\\\\nhttps://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6?sk=740d3009282cb2c4f7478a0c073dedb3\\\\n\\\\nThumbnail background by gustavo centurion on Unsplash\\\\nhttps://unsplash.com/photos/O6fs4ablxw8\"",
    "lengthSeconds": "418",
    "uploadDate": "2020-11-26",
    "thumbnail_url": "https://i.ytimg.com/vi/egDIqQIjDCI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YvVQgvAz9dY",
    "title": "Language Generation with OpenAI's GPT-2 in Python",
    "tags": "film, udost",
    "scraped_at": 1684585781.960621,
    "genre": "People",
    "views": "9754",
    "desc": "Easy natural language generation with Transformers and PyTorch. We apply OpenAI\\'s GPT-2 model to generate text in just a few lines of Python code.\\\\n\\\\nLanguage generation is one of those natural language tasks that can really produce an incredible feeling of awe at how far the fields of machine learning and artificial intelligence have come.\\\\n\\\\nGPT-1, 2, and 3 are OpenAI\\xe2\\x80\\x99s top language models \\xe2\\x80\\x94 well known for their ability to produce incredibly natural, coherent, and genuinely interesting language.\\\\n\\\\nIn this article, we will take a small snippet of text and learn how to feed that into a pre-trained GPT-2 model using PyTorch and Transformers to produce high-quality language generation in just eight lines of code. We cover:\\\\n\\\\nPyTorch and Transformers\\\\n- Data\\\\nBuilding the Model\\\\n- Initialization\\\\n- Tokenization\\\\n- Generation\\\\n- Decoding\\\\nResults\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nMedium Article:\\\\nhttps://towardsdatascience.com/text-generation-with-python-and-gpt-2-1fecbff1635b\\\\n\\\\nFriend Link (free access):\\\\nhttps://towardsdatascience.com/text-generation-with-python-and-gpt-2-1fecbff1635b?sk=930367d835f15abb4ef3164f7791e1b1\\\\n\\\\nThumbnail background by gustavo centurion on Unsplash\\\\nhttps://unsplash.com/photos/O6fs4ablxw8\"",
    "lengthSeconds": "498",
    "uploadDate": "2020-11-24",
    "thumbnail_url": "https://i.ytimg.com/vi/YvVQgvAz9dY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DgGFhQmfxHo",
    "title": "How-to use the Kaggle API in Python",
    "tags": "film, udost",
    "scraped_at": 1684585781.8326225,
    "genre": "People",
    "views": "10839",
    "desc": "Simple step-by-step tutorial covering the setup and use of the Kaggle API for downloading datasets using the Kaggle library in Python.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\"",
    "lengthSeconds": "461",
    "uploadDate": "2020-11-22",
    "thumbnail_url": "https://i.ytimg.com/vi/DgGFhQmfxHo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GYDFBfx8Ts8",
    "title": "How-to Build a Transformer for Language Classification in TensorFlow",
    "tags": "film, udost",
    "scraped_at": 1684585783.3926218,
    "genre": "People",
    "views": "21380",
    "desc": "\\xf0\\x9f\\x8e\\x81 Free NLP for Semantic Search Course:\\\\nhttps://www.pinecone.io/learn/nlp\\\\n\\\\nHow to build a transformer model for sentiment analysis (language classification) using HuggingFace\\'s Transformers library in TensorFlow 2 with Python.\\\\n\\\\nWe cover the full process from downloading data all the way through to building and training the transformer model.\\\\n\\\\nThis is a multi-class classification problem using both TensorFlow and Transformers to build a multiclass sentiment classifier.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nArticle version is here:\\\\nhttps://betterprogramming.pub/build-a-natural-language-classifier-with-bert-and-tensorflow-4770d4442d41\\\\n\\\\nOr here (free link if you don\\'t have Medium membership):\\\\nhttps://betterprogramming.pub/build-a-natural-language-classifier-with-bert-and-tensorflow-4770d4442d41?sk=346cd4ce5ee019c400835588b56d8574\\\\n\\\\nArticle extract:\\\\n\\\\\"",
    "lengthSeconds": "2299",
    "uploadDate": "2020-11-19",
    "thumbnail_url": "https://i.ytimg.com/vi/GYDFBfx8Ts8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yYEPNla4tlQ",
    "title": "Every New Feature in Python 3.10.0a2",
    "tags": "film, udost",
    "scraped_at": 1684585783.9936213,
    "genre": "People",
    "views": "4302",
    "desc": "Every new feature in the early release alpha 2 preview of Python 3.10\\\\n\\\\nThere is video lag 5:00 - 9:55 covering the Type Alias section (sorry!) - the audio is okay though\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\"",
    "lengthSeconds": "883",
    "uploadDate": "2020-11-10",
    "thumbnail_url": "https://i.ytimg.com/vi/yYEPNla4tlQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gXqHd6-NKBo",
    "title": "How to Build TensorFlow Pipelines with tf.data.Dataset",
    "tags": "film, udost",
    "scraped_at": 1684585783.652647,
    "genre": "People",
    "views": "4783",
    "desc": "Link to updated version (without video freeze): https://youtu.be/f6XVfgJTbp4\\\\n\\\\nAn introduction to building better input pipelines for Machine Learning in TF2.\\\\n\\\\n\\xf0\\x9f\\xa4\\x96 70% Discount on the NLP With Transformers in Python course:\\\\nhttps://bit.ly/3DFvvY5\\\\n\\\\nLink to tf.data API docs: https://www.tensorflow.org/guide/data\"",
    "lengthSeconds": "1853",
    "uploadDate": "2020-11-02",
    "thumbnail_url": "https://i.ytimg.com/vi/gXqHd6"
  }
]