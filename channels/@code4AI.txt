[
  {
    "link": "watch?v=Wed-ceioprw",
    "title": "PyTrees: Optimal Data Structure for JAX Parallelization",
    "tags": "film, udost",
    "scraped_at": 1685113815.5484018,
    "genre": "Science",
    "views": "484",
    "desc": "Why are pytrees the optimal data structure for JAX and FLAX parallelizations on GPUs and TPUs? \\\\n\\\\nPytrees explained in simple terms. W/ a little help from  ChatGPT. The free ChatGPT version from Mar 14.\\\\n\\\\nPytrees is a data structure in JAX that is used to represent collections of nested Python containers, such as lists, tuples, and dictionaries. Pytrees allow for efficient processing and manipulation of structured data, especially when dealing with batched inputs to machine learning models.\\\\n\\\\nPytrees are represented as nested tuples, where each leaf node corresponds to a scalar value, and each non-leaf node corresponds to a collection of values. The structure of the pytree is determined by the structure of the nested Python containers.\\\\n\\\\nOne of the main benefits of pytrees in JAX is that they can be easily processed using JAX\\'s automatic differentiation functionality. For example, if you have a pytree of tensors that represent the parameters of a neural network, you can compute the gradients of the loss with respect to those parameters using JAX\\'s grad function.\\\\n\\\\nPytrees can also be used to represent the inputs and outputs of JAX functions, allowing for more flexibility in how data is passed between different parts of a computation. This is particularly useful in deep learning applications where the same function may be applied to multiple inputs at once, such as in a batched training scenario.\\\\n\\\\nOverall, pytrees are an important tool for working with structured data in JAX, and understanding how they work is essential for using JAX effectively in machine learning applications.\\\\n\\\\n#chatgpt \\\\n#naturallanguageprocessing \\\\n#chatgpttutorial \\\\n#jax \\\\n#gpu\"",
    "lengthSeconds": "1824",
    "uploadDate": "2023-05-25",
    "thumbnail_url": "https://i.ytimg.com/vi/Wed"
  },
  {
    "link": "watch?v=I--Z2KwH-Hs",
    "title": "JAX compared to PyTorch 2: Get a feeling for JAX!",
    "tags": "film, udost",
    "scraped_at": 1685113816.1804013,
    "genre": "Science",
    "views": "884",
    "desc": "A simple torch.nn.Module for neural network model definition and training with gradient descent in PyTorch2 compared to a similar code implementation in JAX, in functional programming. \\\\n\\\\nHow to convert a stateful to a stateless operation in JAX, in functional programming? A simple coding example in JAX: regression via gradient descent, where there is one kind of state: the model parameters.\\\\n\\\\nLink to documentation and free Colab NB:\\\\nhttps://jax.readthedocs.io/en/latest/jax-101/07-state.html\\\\n\\\\nhttps://colab.research.google.com/github/google/jax/blob/main/docs/jax-101/07-state.ipynb\\\\n\\\\n#jax \\\\n#ai \\\\n#parallel \\\\n#computerscience \\\\n#computertipsandtricks\"",
    "lengthSeconds": "1251",
    "uploadDate": "2023-05-23",
    "thumbnail_url": "https://i.ytimg.com/vi/I"
  },
  {
    "link": "watch?v=8PbpFxPibJM",
    "title": "Connect multiple ChatGPT sessions w/ dynamic ChatGPT prompts",
    "tags": "film, udost",
    "scraped_at": 1685113814.6673992,
    "genre": "Science",
    "views": "2029",
    "desc": "Connect multiple ChatGPT sessions. Do not loose your insights about advanced prompt design from one free ChatGPT session when starting the next day another session. Dynamic Instruction Prompt Templates (DIPT) will help you to design the most effective prompts for your specific tasks, spanning multiple sessions and continuously evolving, the best fit for your needs. \\\\n\\\\nConnect ChatGPT sessions w/ Instruction Prompt Templates\\\\n\\\\nPlus: I show you how to create the best free ChatGPT prompts for complex queries. \\\\n\\\\nWe start with linear Chain-of-Thoughts, evolve to Mash-CoT and finally create a Dynamical Instruction Prompt Template (DIPT) either yourself or let ChatGPT do it for you. \\\\n\\\\nI show you how to get the best ChatGPT prompts for your task. \\\\n\\\\nSave your ChatGPT result from one session for the next ChatGPT session. Without LangChain Networks or external Databases. \\\\n\\\\nTree-of-Thoughts (ToT) frameworks are an interesting evolution from the area of linear instruction fine-tuning of LLMs (like Vicuna, Alpaca, ..) and build upon the semantic complexities current LLMs, like our free ChatGPT, are able to process and understand within their learned linguistic pattern recognition abilities.\\\\n\\\\nIf you are interested in the DIPT I created in this video, here is it\\\\n----------------------------------------------------------------------------------------------------:\\\\nTemplate Instruction:\\\\n\\\\\"",
    "lengthSeconds": "1872",
    "uploadDate": "2023-05-21",
    "thumbnail_url": "https://i.ytimg.com/vi/8PbpFxPibJM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lGH4b-rHqwQ",
    "title": "New AI cascade of LLMs - FrugalGPT (Stanford)",
    "tags": "film, udost",
    "scraped_at": 1685113816.1134012,
    "genre": "Science",
    "views": "3592",
    "desc": "ChatGPT and GPT-4 can be expensive for a small enterprise, like $21K per month to support customer service. How to reduce the price (costs) for ChatGPT and GPT-4 services with PROMPT adaptation, LLM approximation (caching solutions and fine-tuning Transformer sub-systems) and/or LLM cascades?\\\\n\\\\nData-adaptive LLM selection!\\\\n\\\\nStanford University published new insights how to save costs and increase AI system performance, for you as a business owner or a AI developer. The optimal way to use LLM systems like ChatGPT or GPT-4 (also valid for GPT-5) to reduce the price you have to pay (to OpenAI, Microsoft, ...).\\\\n\\\\nAll rights (data, model, charts and tables) are with the authors of the scientific preprint:\\\\nFrugalGPT: How to Use Large Language Models\\\\nWhile Reducing Cost and Improving Performance\\\\nby Lingjiao Chen, Matei Zaharia, James Zou\\\\nhttps://arxiv.org/pdf/2305.05176.pdf\\\\n\\\\nArticle by Markettechpost:\\\\nhttps://www.marktechpost.com/2023/05/17/stanford-researchers-introduce-frugalgpt-a-new-ai-framework-for-llm-apis-to-handle-natural-language-queries/\\\\n\\\\n#performance \\\\n#price \\\\n#ai \\\\n#gpt4\\\\n#chatgpt \\\\n#explained \\\\n#insights\"",
    "lengthSeconds": "1759",
    "uploadDate": "2023-05-19",
    "thumbnail_url": "https://i.ytimg.com/vi/lGH4b"
  },
  {
    "link": "watch?v=-SeGfpu3hn0",
    "title": "Instruction Fine-Tuning and In-Context Learning of LLM (w/ Symbols)",
    "tags": "film, udost",
    "scraped_at": 1685113815.0994008,
    "genre": "Science",
    "views": "2598",
    "desc": "New insights in \\\\\"",
    "lengthSeconds": "3833",
    "uploadDate": "2023-05-18",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=OJaDrd-d4T8",
    "title": "AI will  be tangible",
    "tags": "film, udost",
    "scraped_at": 1685113814.9604,
    "genre": "Science",
    "views": "1532",
    "desc": "Future of AI - no code.\\\\n\\\\n#ai \\\\n#future \\\\n#tangible\"",
    "lengthSeconds": "673",
    "uploadDate": "2023-05-17",
    "thumbnail_url": "https://i.ytimg.com/vi/OJaDrd"
  },
  {
    "link": "watch?v=CDE6M-SHsik",
    "title": "Multi-Modal Transformer AGENTS, controlled by StarCoder  (W/o LangChain)",
    "tags": "film, udost",
    "scraped_at": 1685113814.5933993,
    "genre": "Science",
    "views": "2840",
    "desc": "New Transformer Agents, controlled by a central intelligence: StarCoder,  now connect the transformer applications on HuggingFace Hub. The combinatorial set of transformer actions is amazing: From audio to visual, to written and back to audio or anything else. \\\\n\\\\nA coding LLM (like StarCoder: A State-of-the-Art LLM for Code) is the main switching intelligence for coding task, or we choose a) OpenAssistant  or b) OpenAI\\'s \\\\\"",
    "lengthSeconds": "1275",
    "uploadDate": "2023-05-16",
    "thumbnail_url": "https://i.ytimg.com/vi/CDE6M"
  },
  {
    "link": "watch?v=dF8UPjjcYe0",
    "title": "The best LLM Embeddings you can buy (plus multilingual)",
    "tags": "film, udost",
    "scraped_at": 1685113814.509401,
    "genre": "Science",
    "views": "2117",
    "desc": "The incredible capabilities of large language model (LLM) behemoths (like ChatGPT or GPT-4) have attracted a handful of companies to offer access to their proprietary LLMs via APIs for Information Retrieval (IR).\\\\n\\\\nA new study evaluated, qualitatively and quantitatively, the best semantic embedding APIs, that can be used for information retrieval (IR). \\\\n\\\\nOur primary focus is to evaluate Cohere and OpenAI APIs for two tasks: \\\\n1. domain generalization and\\\\n 2. multilingual retrieval.\\\\n\\\\nOpenAI states, that the new 2nd generation model, text-embedding-ada-002, replaces five separate models for text search, text similarity, and code search, and outperforms our previous most capable model, Davinci, at most tasks, while being priced 99.8% lower. See https://openai.com/blog/new-and-improved-embedding-model\\\\n\\\\nUse cases of LLM Embeddings:\\\\n1. Embedding as a text feature encoder for ML algorithms\\\\n2. Classification using the embedding features\\\\n3. Zero-shot classification\\\\n4. Clustering\\\\n5. Text search using embeddings\\\\n6. Code search using embeddings\\\\n7. Recommender systems\\\\n8. Anomaly detection\\\\n9. Diversity measurement\\\\n\\\\n(more info and how to use the code: https://platform.openai.com/docs/guides/embeddings)\\\\n\\\\nPlease note that Google did not release BARD in Europe (it had to pay about $8 billion, as a fine, given current EU law on data privacy and a lot of other topics), therefore I have no data on BARD for multilingual performance. Although - to my current knowledge - Google doesn\\'t sell any BARD Embeddings at all! Please correct me if they do in your country.\\\\n\\\\nPaLM2 API: to build generative AI applications with Google\\'s PaLM 2 model, developers can sign in to a waitlist: https://developers.generativeai.google/\\\\n\\\\nYou can use BARD in the following languages: US English, Japanese and Korean.\\\\n\\\\nHowever in the current PALM 2 technical notes, Google noted the multi-linguality of PALM2. Although only compared to Palm. Check PALM 2 mutlilingual data here:\\\\nhttps://ai.google/static/documents/palm2techreport.pdf\\\\n\\\\n\\\\nAll rights with the authors of the current study about:\\\\n\\\\\"",
    "lengthSeconds": "1176",
    "uploadDate": "2023-05-15",
    "thumbnail_url": "https://i.ytimg.com/vi/dF8UPjjcYe0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5i9X7fWlmjg",
    "title": "Introduction to JAX 2023",
    "tags": "film, udost",
    "scraped_at": 1685113817.7464252,
    "genre": "Science",
    "views": "1562",
    "desc": "New, fresh introduction to JAX for applying Flax.linen to a speed optimization of a T5 - Large Language Models (LLM): T5X. Intro to Jaxpr, XLA w/ tf.function (jit_compile), vmap, pmap,  block_until_ready() to account for JAX\\xe2\\x80\\x99s asynchronous dispatch, jit(grad(fn))(), ...\\\\n\\\\nThe learning curve for the 3rd framework for Neural Networks (after TensorFlow2 and PyTorch2) is not really steep, but I really had to search for excellent introduction and example notebooks for JAX, FLAX and in the end for T5X. Learning examples and documentation is not as massive as for TF2 or PyTorch2. \\\\n\\\\nInstead of going with you through code examples, I explain why JAX can increase our computation speed 10x -100x, compared with other frameworks. XLA memory bandwidth optimization, fusion in XLA, and other optimization methods are described with links to original literature. \\\\n\\\\n\\\\nI recommend :\\\\nhttps://jax.readthedocs.io/en/latest/jaxpr.html\\\\nhttps://jax.readthedocs.io/en/latest/notebooks/quickstart.html\\\\nhttps://www.kaggle.com/code/aakashnain/tf-jax-tutorials-part-4-jax-and-devicearray/notebook\\\\nhttps://github.com/AakashKumarNain/TF_JAX_tutorials\\\\n\\\\n#jax \\\\n#datascience \\\\n#machinelearningwithpython \\\\n#gpucomputing \\\\n#tpu\"",
    "lengthSeconds": "1140",
    "uploadDate": "2023-05-14",
    "thumbnail_url": "https://i.ytimg.com/vi/5i9X7fWlmjg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=fscnJ9Qfu3E",
    "title": "The inner workings of LLMs explained - VISUALIZE the self-attention mechanism",
    "tags": "film, udost",
    "scraped_at": 1685113816.6063993,
    "genre": "Science",
    "views": "3163",
    "desc": "HOW do LLMs (Large Language Models) work, and WHY do they work? \\\\nModels like ChatGPT or GPT-4. Can we understand them?\\\\n\\\\nEasy introduction to:\\\\n1. How does the self-attention mechanism work inside of LLMs?\\\\n2. What makes all those LLM different, their weights, their pre-trained datasets or their architectural design structure?\\\\n3. What makes LLM perform better (hardware /software), and how to tune for optimal layers and attention heads in the LLM architecture?\\\\n\\\\nSimple explanations on how Large Language Models (LLM) or Decoder-based Transformers in general work. Plus LangChain and Vector stores, with their corresponding vector embeddings, explained. Also for beginners to AI. \\\\n\\\\nWe only focus on the decoder stack of the transformer for LLMs and ignore for the moment the RLHF (human feedback forms).\\\\n\\\\nIntroducing Claude\\\\nhttps://www.anthropic.com/index/introducing-claude\\\\n\\\\nGreat new pre-print (all rights with authors):\\\\n\\\\\"",
    "lengthSeconds": "2099",
    "uploadDate": "2023-05-13",
    "thumbnail_url": "https://i.ytimg.com/vi/fscnJ9Qfu3E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qtFBhx7av30",
    "title": "LLM w/ Commercial License: for AI Start-ups",
    "tags": "film, udost",
    "scraped_at": 1685113816.3413992,
    "genre": "Science",
    "views": "2874",
    "desc": "If you want to start a business based on AI or LLMs: What LLMs are free and what LLMs have a free commercial license? What have an open source license? Why some companies do not open-source? Why are they \\\\\"",
    "lengthSeconds": "590",
    "uploadDate": "2023-05-10",
    "thumbnail_url": "https://i.ytimg.com/vi/qtFBhx7av30/maxresdefault.jpg"
  },
  {
    "link": "watch?v=aeDr0duR_jo",
    "title": "OpenAI Playground: Optimize Instruction tuned Conversational AI /LLM",
    "tags": "film, udost",
    "scraped_at": 1685113816.0373993,
    "genre": "Science",
    "views": "1235",
    "desc": "OpenAI Playground: To optimize Instruction tuned Conversational AI /LLM,\\\\nlike gpt-3.5-turbo, on several model parameters, OpenAI playground let\\'s you explore the possibilities of OpenAI API.\\\\n\\\\nApplying OpenAI API for a simple Python code.\\\\n\\\\n#ai \\\\n#pythonprogramming \\\\n#openai \\\\n#chatgpt\"",
    "lengthSeconds": "501",
    "uploadDate": "2023-05-08",
    "thumbnail_url": "https://i.ytimg.com/vi/aeDr0duR_jo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jgzOmJWFbUk",
    "title": "ChatGPT Prompt Engineering w/ ICL (free COLAB, OpenAI API)",
    "tags": "film, udost",
    "scraped_at": 1685113814.4224005,
    "genre": "Science",
    "views": "1659",
    "desc": "ChatGPT Prompt Engineering, focus on In-Context Learning ( ICL) for Developers (with a free COLAB) applying OpenAI API for a simple Python code.\\\\n\\\\n#ai \\\\n#pythonprogramming \\\\n#openai \\\\n#chatgpt\"",
    "lengthSeconds": "725",
    "uploadDate": "2023-05-07",
    "thumbnail_url": "https://i.ytimg.com/vi/jgzOmJWFbUk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=sFrEHb4ivsc",
    "title": "The best AI LLMs compared on Conversation",
    "tags": "film, udost",
    "scraped_at": 1685113817.3544247,
    "genre": "Science",
    "views": "2651",
    "desc": "How powerful are conversational AI LLMs today, for real-world tasks? No synthetic benchmarks,  I will build my new homepage and use the maximum available power and intelligence of AI for my job. But which LLM to choose?\\\\n\\\\nI want to create a new homepage and use the biggest, most advanced CONVERSATIONAL AI for my IDEATION process, applying instruction tuned ICL prompts. Brainstorming, mindmaps, you know the IDEO design thinking process. Data as of Week 18, 2023.\\\\n\\\\nBest CONVERSATIONAL AI: Biggest 3 LLMs compete for Intelligence: OpenAI vs Microsoft\\\\n\\\\nAn advanced instructional ICL prompt, with nested loops, allows me to iteratively communicate with multiple conversational AI systems, and not use any Vector store or Vector DB outside of the primary artificial intelligence. Save your money.\\\\n\\\\nI hope you can improve on my prompt.\\\\n\\\\n#conversationalai \\\\n#ai \\\\n#chatgpt \\\\n#gpt4 \\\\n#huggingface \\\\n#homepage \\\\n#ai_competition\\\\n#languagemodel \\\\n#evaluation \\\\n#the_best_language_models\"",
    "lengthSeconds": "1151",
    "uploadDate": "2023-05-06",
    "thumbnail_url": "https://i.ytimg.com/vi/sFrEHb4ivsc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=SxAn6f7gM44",
    "title": "The best LLMs (3B to 13B models) - Conversational AI",
    "tags": "film, udost",
    "scraped_at": 1685113819.521726,
    "genre": "Science",
    "views": "4618",
    "desc": "What LLM is the best? Today the truth about conversational AI models and their performance (week 17/2023). \\\\nWe evaluate LLM from 3billion trainable parameters (for your 3080 GPU at home) to 13B (for some 4090s at home).\\\\n\\\\nFind your perfect LLM for your specific job, for your task!\\\\nExplore yourself!\\\\n\\\\nData for week 17 in 2023.\\\\n\\\\nFor VICUNA model:\\\\nhttps://lmsys.org/blog/2023-03-30-vicuna/\\\\n\\\\nFor FastChat-t5:\\\\nhttps://github.com/lm-sys/FastChat#fastchat-t5\\\\n\\\\nAll rights with:\\\\n-------------------------\\\\nhttps://lmsys.org/blog/2023-05-03-arena/\\\\n\\\\nBattle two LLMs against each other (anonymously):\\\\nhttps://chat.lmsys.org/?arena\\\\n\\\\nCOLAB NB for the battle evaluation:\\\\nhttps://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing\\\\n\\\\n#ai \\\\n#conversationalai \\\\n#languagemodel \\\\n#evaluation \\\\n#the_best_language_models\"",
    "lengthSeconds": "463",
    "uploadDate": "2023-05-04",
    "thumbnail_url": "https://i.ytimg.com/vi/SxAn6f7gM44/maxresdefault.jpg"
  },
  {
    "link": "watch?v=bHzLzIxQHog",
    "title": "Your Local PDFs in GPT-4 w/ 1-click for FREE",
    "tags": "film, udost",
    "scraped_at": 1685113814.8164003,
    "genre": "Science",
    "views": "3606",
    "desc": "Read my local PDF files and summarize content with 1-click in GPT systems like: GPT-4, BING Chat, ChatGPT, ...\\\\n\\\\nSubscribers ask, free solutions delivered immediately. \\\\n\\\\nNote: Please do not share personal or sensitive information with LLMs, since theoretically (worst case) your data become public data.\\\\n\\\\n\\\\n#ai \\\\n#gpt4 \\\\n#chatgpttutorial \\\\n#pdf \\\\n#summarized\"",
    "lengthSeconds": "540",
    "uploadDate": "2023-05-01",
    "thumbnail_url": "https://i.ytimg.com/vi/bHzLzIxQHog/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jNHSSemNmRk",
    "title": "AI designs Molecular Machines: Proteomics",
    "tags": "film, udost",
    "scraped_at": 1685113815.7594,
    "genre": "Science",
    "views": "1214",
    "desc": "Molecular machines inside your human cell are called proteins. Proteomics is the large-scale study of proteins. They folded in 3D to build parts of machines (see ATP). Biological machines. \\\\n\\\\nAI can learn with enough training data, how to encoded the linear protein sequence in a way, that optimal folded 3D proteins form new molecular machines. For Precision Medicine, or to dissolve toxic materials in the environment or fight cancer in our cells. \\\\n\\\\nTrained AI (like AlphaFold2) systems can be of importance for the next step in all *omics: genomics, proteomics, epigenomics, ...\\\\n\\\\n\\\\n#ai \\\\n#proteomics \\\\n#genomics \\\\n#proteinstructure \\\\n#microfluidics\"",
    "lengthSeconds": "1888",
    "uploadDate": "2023-05-01",
    "thumbnail_url": "https://i.ytimg.com/vi/jNHSSemNmRk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=xuLjAQodoWQ",
    "title": "AI discovers New Planets: Vision AI on Webb Telescope",
    "tags": "physics, astrophysics, planetary physics, protoplanetary disk, AI, Vision transformer, ViT Astrophysics",
    "scraped_at": 1685113816.8154254,
    "genre": "Science",
    "views": "704",
    "desc": "AI plays an important role in protoplanetary Astrophysics, analyzing Petabytes and Exabytes of recorded astronomical data. A simple question for AI is the observation how planets form in a protoplanetary disk around a young star. \\\\n\\\\nAstrophysical data from Hubble, VLT and ALMA. \\\\n\\\\n#ai \\\\n#astrophysics \\\\n#planets \\\\n#starformation \\\\n#protoplanetary\\\\n#planetary \\\\n#cosmology \\\\n#jwst\"",
    "lengthSeconds": "1986",
    "uploadDate": "2023-04-30",
    "thumbnail_url": "https://i.ytimg.com/vi/xuLjAQodoWQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=LplUI-cwQiE",
    "title": "AI in Political Science & Social Psychology",
    "tags": "film, udost",
    "scraped_at": 1685113817.9383996,
    "genre": "Science",
    "views": "1191",
    "desc": "AI language models now predict public opinion. The effects of media on our society ripple beyond the individual, giving rise to new social movements and determining national agendas, especially in election years. AI in Political Science \\\\u0026 Social Psychology.\\\\n\\\\nCan we measure the effects of media on our society with AI (eg simple BERT systems, the encoder-stack of a transformer architecture)? We are increasingly concerned about whether misinformation, fake news, and echo chambers are wearing away at the public ethos. \\\\n\\\\nNew research conducted by Harvard University and MIT raises several questions centered around AI\\xe2\\x80\\x99s ability to mirror and mimic beliefs derived from human language. And if a BERT model is able to predict the public opinion a week ahead?!\\\\nCan AI language models really predict public opinion? \\\\n\\\\nLiterature (all rights are with the corresponding authors):\\\\n\\\\\"",
    "lengthSeconds": "1236",
    "uploadDate": "2023-04-29",
    "thumbnail_url": "https://i.ytimg.com/vi/LplUI"
  },
  {
    "link": "watch?v=b30EZW68c8U",
    "title": "LEADERS control (conversational) AI",
    "tags": "film, udost",
    "scraped_at": 1685113816.8784244,
    "genre": "Science",
    "views": "859",
    "desc": "How can an AI (like ChatGPT) influence you on a subconscious level?  So that a huge amount of voters would be subliminal influenced? \\\\n\\\\nActually it is rather easy, the technology is available and the characters are already on stage. Let\\'s influence the minds of a planet with a free ChatGPT. \\\\n\\\\nPsychological interference based on a template structure for ChatGPT to execute. Gradual escalation patterns and verbalization change the primary believe systems of humans. \\\\n\\\\nGoal: to make you aware of subconscious communication patterns generated when you interact with an AI, with or without intend of polarization.   \\\\n\\\\n#strategies \\\\n#ai \\\\n#planetary \\\\n#chatgpt \\\\n#mars   \\\\n#machinelearning \\\\n#communication\"",
    "lengthSeconds": "736",
    "uploadDate": "2023-04-28",
    "thumbnail_url": "https://i.ytimg.com/vi/b30EZW68c8U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=mzdQOwxypCM",
    "title": "ChatGPT: My emotional Firewall",
    "tags": "film, udost",
    "scraped_at": 1685113815.9644005,
    "genre": "Science",
    "views": "848",
    "desc": "ChatGPT scans incoming communication and provides a psychological profile, content summary and detailed analysis of emotional triggers and hidden misinformation. Can ChatGPT shield you from polarizing and stress-inducing comments?  The importance of fair and open communication. \\\\n\\\\nCurrently my mini-YT-channel receives about 100 comments and feed-backs a day, and it is interesting to be personally evaluated daily by 100 people. \\\\n\\\\nChatGPT can provide an excellent service analyzing my communication (yes, I learn about my own communication patterns: sometimes I forget to attach my info sources for you to verify) and discover communication patterns in other people\\'s non-neutral communication towards me.  \\\\n\\\\nA simple use case for ChatGPT to protect me from emotional stress and misleading communications, not only based on stop-words or single semantic filters, but powered by a complete transformer architecture (ChatGPT).\\\\n\\\\n#ai \\\\n#chatgpt \\\\n#information\"",
    "lengthSeconds": "458",
    "uploadDate": "2023-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/mzdQOwxypCM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6Ib3sC9Vrc0",
    "title": "How much does it cost to Fine-Tune Flan-T5 LLM models?",
    "tags": "film, udost",
    "scraped_at": 1685113814.7414017,
    "genre": "Science",
    "views": "2918",
    "desc": "How much does it cost to fine-tune a pre-trained FLAN-T5-XL (3B) or FLAN-T5-XXL (11B) Large Language Model? \\\\n\\\\nCost estimations are from Technical Lead of HuggingFace, and please note, those are empirical data from an Expert on AWS and HuggingFace. If your are not the professional for cloud infrastructure LLM, if your code is not optimized, your configuration files are not adjusted to compute infrastructure, your costs could be significantly higher! Start small and experience your code optimization when you slowly increase your maximum amount you want to spend for cloud computing, on either of the known cloud infrastructure provider. Do not start with an 11 Billion parameter model fine-tuning in the cloud, when you are inexperienced. \\\\n\\\\n#ai \\\\n#cloudcomputing \\\\n#naturallanguageprocessing\"",
    "lengthSeconds": "732",
    "uploadDate": "2023-04-26",
    "thumbnail_url": "https://i.ytimg.com/vi/6Ib3sC9Vrc0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wgDX0IbgKgY",
    "title": "Upgrade to multi-AI: Update Vector DB to AI",
    "tags": "film, udost",
    "scraped_at": 1685113815.482403,
    "genre": "Science",
    "views": "6690",
    "desc": "Vector Stores or Vector Database means currently big business. Good news: You can build your own Vector store for free. And I show you how to upgrade your Vector DB to an AI system. For free, w/ or w/o LangChain.  ....... Science, not science fiction. On what components of your LLM LangChain AI system to save money and how to upgrade your LangChain Vector Store (about a dozen commercial provider) to an AI? Simple answers and integrate two interactive AI systems in your LangChain, with minimum costs. \\\\n\\\\nSave money on external service providers and build your optimized, second AI right next to GPT-4, interacting with each other? Yes, but only if you know how to code. Why not start today?\\\\n\\\\nA multi-AI system with two interactive AI (GPT and BERT), with and without autonomous agents.\\\\n\\\\n#ai \\\\n#naturallanguageprocessing \\\\n#datascience \\\\n#gpt4\"",
    "lengthSeconds": "1211",
    "uploadDate": "2023-04-23",
    "thumbnail_url": "https://i.ytimg.com/vi/wgDX0IbgKgY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5cnk5zpf1EA",
    "title": "AutoGPT & BabyAGI: autonomous AI Agents for LLMs explained",
    "tags": "AI, AGI, GPT, GPT",
    "scraped_at": 1685113818.346425,
    "genre": "Science",
    "views": "3798",
    "desc": "AutoGPT \\\\u0026 BabyAGI explained! Combining GPT-4 with the interface functionality of LangChain, with on open data channel to external internet services (with $$$ API) opens up new combinations of interlinks of resources: meet AutoGPT and BabyAGI.\\\\n\\\\nThere are significant risks for your financial resources, when GPT-4 takes over the link priorities, without human interaction.\\\\n\\\\nLiterature and code (all rights w/ those authors):\\\\nhttps://github.com/yoheinakajima/babyagi\\\\nhttps://github.com/Significant-Gravitas/Auto-GPThttps://venturebeat.com/ai/as-ai-agents-like-auto-gpt-speed-up-generative-ai-race-we-all-need-to-buckle-up-the-ai-beat/\\\\n\\\\n#ai \\\\n#gpt4 \\\\n#machinelearning \\\\n#datascience \\\\n#agents \\\\n#autonomous\"",
    "lengthSeconds": "1776",
    "uploadDate": "2023-04-22",
    "thumbnail_url": "https://i.ytimg.com/vi/5cnk5zpf1EA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=IbOoEJ9N2z8",
    "title": "LangChain explained: external documents & SQL in GPT-4",
    "tags": "film, udost",
    "scraped_at": 1685113815.1673996,
    "genre": "Science",
    "views": "5430",
    "desc": "LangChain is a Python library that allows to interface different LLMs (like ChatGPT,  or GPT-4) and connect at the same time different data pipelines (documents, search, SQL) to the APIs of professional service providers ($$$) on the internet. \\\\n\\\\nDifferent results from those service providers can be chained together (SBERT model and cosine similarity, or vector embedding of sentences) and constitute an input prompt to GPT-4 /or other LLMs (respecting the 4096 token limit, up to 32 K). The LLM will provide an answer given in-context learning, and the process can be repeated multiple times. \\\\n\\\\nThe real \\\\\"",
    "lengthSeconds": "1883",
    "uploadDate": "2023-04-20",
    "thumbnail_url": "https://i.ytimg.com/vi/IbOoEJ9N2z8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=272QsC0wLGo",
    "title": "Wall Street's new LLM beats GPT-4",
    "tags": "GPT",
    "scraped_at": 1685113818.6032405,
    "genre": "Science",
    "views": "4843",
    "desc": "GPT-4 beaten! GPT-4 lost the game! Wall Street build the perfect AI /LLM.\\\\nA billionaire builds a LLM for the US Financial sector, the first finance LLM!\\\\nAnd a simple reason why GPT-4 lost ....  \\\\n\\\\nQuestions:\\\\n1.  Imagine your are the US Financial Sector. What AI LLM do you buy? \\\\n\\\\n2.  You have the largest domain-specific dataset for an LLM of the latest generation: the financial data of the world! Why you stay away from GPT-4? \\\\n\\\\n3. New economic perspective (price vs performance) on Large Language Models (LLM), including the latest developments of AI systems vs GPT-4. \\\\n\\\\n4.  The economic rationale of LLMs explained. Including Microsoft\\'s GPT-4, Meta\\'s LLama, T5 and Flan-T5, plus BLOOM. \\\\n\\\\nModels you should choose, and those you should avoid for specific tasks, and fine-tuning. ICL vs fine-tuning for LLMs.\\\\n\\\\nIncluding a short explanation of \\\\\"",
    "lengthSeconds": "1934",
    "uploadDate": "2023-04-18",
    "thumbnail_url": "https://i.ytimg.com/vi/272QsC0wLGo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qu-vXAFUpLE",
    "title": "LLM Ecosystem explained: Your ultimate Guide to AI",
    "tags": "film, udost",
    "scraped_at": 1685113820.6346958,
    "genre": "Science",
    "views": "13339",
    "desc": "Introduction to the world of LLM (Large Language Models) in April 2023. With detailed explanation of GPT-3.5, GPT-4, T5, Flan-T5 to LLama, Alpaca and KOALA LLM, plus dataset sources and configurations. \\\\nIncluding ICL (in-context learning), adapter fine-tuning, PEFT LoRA and classical fine-tuning of LLM explained. When to choose what type of data set for what LLM job?\\\\n\\\\nAddendum: Beautiful, new open-source \\\\\"",
    "lengthSeconds": "1622",
    "uploadDate": "2023-04-16",
    "thumbnail_url": "https://i.ytimg.com/vi/qu"
  },
  {
    "link": "watch?v=kZazs6V3314",
    "title": "DOLLY 2 LLM explained: New Open-Source LLM w/ code",
    "tags": "film, udost",
    "scraped_at": 1685113819.7927227,
    "genre": "Science",
    "views": "12982",
    "desc": "New LLM by Databricks: Dolly 2.0 is a new 12B parameter language model (LLM) based on the EleutherAI Pythia model family and instruction fine-tuned exclusively on a new, high-quality human generated instruction following dataset, crowdsourced among Databricks employees. Instruction fine-tuning with a open-source data set, created by  @Databricks \\\\n\\\\nPlus PyTorch code implementation of DOLLY 2 on a free COLAB Notebook.\\\\n\\\\n\\\\\"",
    "lengthSeconds": "844",
    "uploadDate": "2023-04-13",
    "thumbnail_url": "https://i.ytimg.com/vi/kZazs6V3314/maxresdefault.jpg"
  },
  {
    "link": "watch?v=u3jMDiA2pkc",
    "title": "HF Accelerate to Fine-tune my Flan-T5 LLM | on free COLAB NB, Tutorial",
    "tags": "Finetune T5, Fine",
    "scraped_at": 1685113819.4536967,
    "genre": "Science",
    "views": "1240",
    "desc": "We apply HuggingFace ACCELERATE to prepare our code to fine-tune a Flan-T5 LLM on a multi- GPU or multi-TPU environment. HF Accelerate is the perfect instrument for this. \\\\nWatch real-time coding how to finetune a T5 model (a encoder-decoder stack of a transformer) for a specific downstream task with a special data set. \\\\n\\\\nLiterature:\\\\nhttps://huggingface.co/docs/accelerate/index\"",
    "lengthSeconds": "732",
    "uploadDate": "2023-04-12",
    "thumbnail_url": "https://i.ytimg.com/vi/u3jMDiA2pkc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jQL0ZeHtXFc",
    "title": "The ALPACA Code explained: Self-instruct fine-tuning of LLMs",
    "tags": "ALPACA LLM, Fine",
    "scraped_at": 1685113817.674399,
    "genre": "Science",
    "views": "4374",
    "desc": "Pytorch code to fine tune and INSTRUCTION fine-tune your Large Language Models (like Alpaca LLM AI) w/ instruct fine tuned data sets: beautiful, but non-trivial code endeavors. Use your data set (and instruct fine-tune it) to fine-tune your LLM for your multiple tasks in parallel! \\\\n\\\\nSelf-instruct is a method to generate data sets, where ChatGPT /GPT-4 or other LLMs generate synthetic data sets according to our needs for fine tuning or instruct fine tuning our LLM for specific tasks (like summarization, translation, Q+A, ..).\\\\n\\\\nSELF-INSTRUCT: Aligning Language Model\\\\nwith Self Generated Instructions\\\\nhttps://arxiv.org/pdf/2212.10560.pdf\\\\n\\\\nStanford ALPACA:\\\\nhttps://crfm.stanford.edu/2023/03/13/alpaca.html\\\\nhttps://github.com/tatsu-lab/stanford_alpaca\\\\n\\\\n#ai \\\\n#naturallanguageprocessing \\\\n#finetuning \\\\n#chatgpt \\\\n#machinelearning\"",
    "lengthSeconds": "1509",
    "uploadDate": "2023-04-10",
    "thumbnail_url": "https://i.ytimg.com/vi/jQL0ZeHtXFc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Di2b_kQPROg",
    "title": "Create Self-Instruct Data Sets: for Synthetic Self-Instruct (ChatGPT) fine-tuning of LLMs",
    "tags": "ALPACA LLM, Fine",
    "scraped_at": 1685113814.8904,
    "genre": "Science",
    "views": "3161",
    "desc": "The Creation of Self-Instruct Data for fine - tuning and \\\\\"",
    "lengthSeconds": "1819",
    "uploadDate": "2023-04-09",
    "thumbnail_url": "https://i.ytimg.com/vi/Di2b_kQPROg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=m18C1cvoYvM",
    "title": "Self-instruct fine-tuning of LLMs (Alpaca) : The Introduction",
    "tags": "ALPACA LLM, Fine",
    "scraped_at": 1685113820.322699,
    "genre": "Science",
    "views": "8294",
    "desc": "Fine - tuning and \\\\\"",
    "lengthSeconds": "1641",
    "uploadDate": "2023-04-08",
    "thumbnail_url": "https://i.ytimg.com/vi/m18C1cvoYvM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ePoCYL_5rDM",
    "title": "New KOALA LLM - Ignite Your Professional Career in AI",
    "tags": "film, udost",
    "scraped_at": 1685113817.1463997,
    "genre": "Science",
    "views": "3814",
    "desc": "UC Berkeley AI presents a novel Large Language Model called Koala. This KOALA LLM will change your professional career in AI! For a very special reason!\\\\n\\\\nInteractive Demo of ALPACA and KOALA (\\\\u0026 LLAMA): Compare the models yourself.\\\\n\\\\nUC Berkeley (BAIR) hopes that the Koala model will serve as a useful platform for future academic research on large language models: the model is capable enough to exhibit many of the capabilities that we associate with modern LLMs, while being small enough to be fine-tuned or utilized with more limited compute power. \\\\n\\\\nInteractive Demo of ALPACA, KOALA and LLAMA LLM:\\\\nhttps://chat.lmsys.org/?model=koala-13b\\\\n\\\\nmy sources:\\\\nhttps://bair.berkeley.edu/blog/2023/04/03/koala/\\\\nhttps://www.marktechpost.com/2023/04/06/uc-berkeley-researchers-introduce-koala-a-new-ai-chatbot-from-fine-tuned-on-dialogue-close-to-chatgpt-quality/\\\\n\\\\n00:00 UC Berkeley\\'s new KOALA LLM\\\\n01:14 Interactive Demo of ALPACA and KOALA (\\\\u0026 LLAMA)\\\\n04:15 High-quality data sets\\\\n05:46 Self-instruct fine-tuning \\\\n08:00 The next step forward: Functional Programming\\\\n11:13 KOALA code repos\\\\n\\\\n\\\\n@misc{koala_blogpost_2023,\\\\n  author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},\\\\n  title = {Koala: A Dialogue Model for Academic Research},\\\\n  howpublished = {Blog post},\\\\n  month = {April},\\\\n  year = {2023},\\\\n  url = {https://bair.berkeley.edu/blog/2023/04/03/koala/},\\\\n  urldate = {2023-04-03}\\\\n}\"",
    "lengthSeconds": "805",
    "uploadDate": "2023-04-07",
    "thumbnail_url": "https://i.ytimg.com/vi/ePoCYL_5rDM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lnCoJbnhs4s",
    "title": "Your AI Financing by OpenAI: Research Access only",
    "tags": "AI, Program application",
    "scraped_at": 1685113815.3324022,
    "genre": "Science",
    "views": "859",
    "desc": "Personal view when you, as researcher, apply to OpenAI for a \\\\\"",
    "lengthSeconds": "1571",
    "uploadDate": "2023-04-06",
    "thumbnail_url": "https://i.ytimg.com/vi/lnCoJbnhs4s/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DcDwHM54oyg",
    "title": "Fine-tune Seq2Seq LLM:  T5 Professional  |  on free Colab NB",
    "tags": "film, udost",
    "scraped_at": 1685113821.1267223,
    "genre": "Science",
    "views": "1849",
    "desc": "Fine tune Seq2Seq LLM like T5 models on latest code from HuggingFace. The Fine tuning task in this complete code example is summarization, we further fine-tune the T5 model from last video on another training data set and run on a free Colab NB with a Tesla T4 GPU. \\\\n\\\\nHuggingface Repo:\\\\nhttps://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization\\\\n\\\\n#ai \\\\n#naturallanguageprocessing \\\\n#finetune \\\\n#github\"",
    "lengthSeconds": "875",
    "uploadDate": "2023-04-05",
    "thumbnail_url": "https://i.ytimg.com/vi/DcDwHM54oyg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kPlo_mXokqA",
    "title": "Whisper for YouTube: Speech2TextAI: Perfect YouTube Subtitles - Free. Multiple Languages.",
    "tags": "film, udost",
    "scraped_at": 1685113817.610399,
    "genre": "Science",
    "views": "1162",
    "desc": "New AI (called Whisper) optimizes Speech-to-Text via a transformer (encoder-decoder-stack). I show you how to create the perfect subtitles for your YouTube videos, AI transcribing all audio to written text, optional in multiple languages! Open-sourced in September 2022.\\\\n\\\\nA tiny Python code to download the audio file of any YouTube video online, transcribe it to written text - in your particular language - and then, maybe, upload the new SRT subtitle file back to your YouTube videos (maybe in another language, or multiple). \\\\n\\\\nAnd if your language is not part of Whisper from OpenAI, then there is a Jupyter Notebook to fine-tune Whisper on your language: \\\\nhttps://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb\\\\n\\\\nDetailed information for fine-tuning Whisper by OpenAI:\\\\nhttps://huggingface.co/blog/fine-tune-whisper\\\\n\\\\nScientific pre-print for Whisper:\\\\nhttps://arxiv.org/pdf/2212.04356.pdf\\\\n\\\\nHuggingFace Model:\\\\nhttps://huggingface.co/openai/whisper-large-v2\\\\n\\\\n\\\\n#naturallanguageprocessing \\\\n#speech2text\\\\n#ai \\\\n#machinelearning \\\\n#subtitles\"",
    "lengthSeconds": "1168",
    "uploadDate": "2023-04-04",
    "thumbnail_url": "https://i.ytimg.com/vi/kPlo_mXokqA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BAbJ47DMKI4",
    "title": "GPT4ALL: I missed the Hype",
    "tags": "film, udost",
    "scraped_at": 1685113819.0512116,
    "genre": "Science",
    "views": "2780",
    "desc": "My viewers alerted me to the new AI revolution: GPT 4 all! \\\\nAnd sadly it is true: I missed the Hype! But for a reason! \\\\n\\\\n#ai \\\\n#languagemodel \\\\n#naturallanguageprocessing\"",
    "lengthSeconds": "186",
    "uploadDate": "2023-04-03",
    "thumbnail_url": "https://i.ytimg.com/vi/BAbJ47DMKI4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rtBv8KC39zk",
    "title": "The Open Letter to stop GPT-5:   ChatGPT's Answers!",
    "tags": "film, udost",
    "scraped_at": 1685113815.6954005,
    "genre": "Science",
    "views": "596",
    "desc": "\\\\\"",
    "lengthSeconds": "890",
    "uploadDate": "2023-04-02",
    "thumbnail_url": "https://i.ytimg.com/vi/rtBv8KC39zk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=xGuK90Ru64M",
    "title": "GPT-4 and Cybersecurity?!",
    "tags": "film, udost",
    "scraped_at": 1685113815.2504,
    "genre": "Science",
    "views": "935",
    "desc": "Microsoft GPT-4 at the heart of your corporate cybersecurity defense nucleus: my new video on April Fools\\' Day!\\\\n\\\\nMicrosoft\\'s vice president and AI security architect: \\xe2\\x80\\x9cWe know sometimes these models (remark: GPT-4) get things wrong, so we\\xe2\\x80\\x99re offering the ability to make sure we (remark: as Microsoft) have feedback. ...... because there are a lot of ways it could be wrong.\\\\\"",
    "lengthSeconds": "752",
    "uploadDate": "2023-04-01",
    "thumbnail_url": "https://i.ytimg.com/vi/xGuK90Ru64M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=A-a-l_sFtYM",
    "title": "Boost Fine-Tuning Performance of LLM:  Optimal Architecture w/ PEFT LoRA Adapter-Tuning on Your GPU",
    "tags": "Low Rank Adaptation of LLM, Fine",
    "scraped_at": 1685113816.7504253,
    "genre": "Science",
    "views": "5883",
    "desc": "Not enough memory to fine-tune your Language Model: T5, GPT, OPT, BLOOM, Llama, ..? Optimize your model architecture to the MAX for optimal fine-tuning (adapter-tuning) for faster, cheaper and MIN memory on your GPU! \\\\n\\\\nLLM Fine-Tuning on a Budget: Supercharge Your Language Model on a Normal GPU with PEFT, LoRA, and Adapter-Tuning\\\\n\\\\nReal-time code demo with HuggingFace PEFT and LoRA plus INT8 quantization of LLM for adapter-tuning (since main weights tensors of PLM are frozen). \\\\nPLM = Pre-trained Large Language Model (smile)\\\\n\\\\nA 30 min code tutorial for Parameter-Efficient Fine-tuning your LLM (PEFT) on consumer GPU (... less than 80GB). \\\\n\\\\nLow-rank adaptation (LoRA) for LLM adapter-tuning applied to INT8 quantized LLM models, with frozen weights of the pre-trained Language Model (PLM) and a tiny set of layer specific trainable adapter (LRA) parameters, in a complete code tutorial in PyTorch2.\\\\n\\\\nLLM was pretrained using a causal language modeling (CAUSAL_LM) objective and now adapter-tuned for a downstream specific task on a public data set available on Huggingface.\\\\n\\\\nPEFT is just an adapter-tuned method, since not (!) all model weights are updated, like in the classical fine-tuning method (more expensive, more time, ..). \\\\n\\\\nYour Jupyter notebook (all rights w/ the authors) to follow along:\\\\nhttps://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing\\\\n\\\\nYour HuggingFace blog post to read  (all rights w/ the authors):\\\\nhttps://huggingface.co/blog/peft\\\\n\\\\n00:00 PEFT source code (LoRA, pre-fix tuning,..)\\\\n01:53 Llama - LoRA fine-tuning code \\\\n04:39 Create PEFT - LoRA Model (Seq2Seq)\\\\n08.29 LoRA configuration\\\\n10:05 Trainable parameters of PEFT - LoRA model\\\\n13:09 get_peft_model \\\\n14:21 PEFT - LoRA - 8bit model of OPT 6.7B LLM\\\\n15:25 load_in_8bit \\\\n16:30 INT8 Quantization explained \\\\n18:08 Fine-tune a quantized model\\\\n22:56 bfloat16 and XLA compiler PyTorch 2.0\\\\n25:20 Freeze all pre-trained layer weight tensors\\\\n27:52 Adapter-tuning of PEFT - LoRA model\\\\n30:50 Save tuned PEFT - LoRA Adapter weights\\\\n31:30 Run inference of new PEFT - LoRA adapter - tuned LLM\\\\n32:57 Load your Adapter-tuned PEFT - LoRA model\\\\n\\\\n\\\\n\\\\n#ai \\\\n#PEFT\\\\n#LoRA\\\\n#datascience \\\\n#finetuning \\\\n#finetune \\\\n#machinelearning \\\\n#datascience \\\\n#naturallanguageprocessing\"",
    "lengthSeconds": "2111",
    "uploadDate": "2023-03-30",
    "thumbnail_url": "https://i.ytimg.com/vi/A"
  },
  {
    "link": "watch?v=YVU5wAA6Txo",
    "title": "PEFT LoRA Explained in Detail - Fine-Tune your LLM on your local GPU",
    "tags": "film, udost",
    "scraped_at": 1685113818.2804255,
    "genre": "Science",
    "views": "24886",
    "desc": "Your GPU has not enough memory to fine-tune your LLM or AI system? Use HuggingFace PEFT: There is a mathematical solution to approximate your complex weight tensors in each layer of your self-attention transformer architecture with an eigenvector and eigenvalue decomposition, that allows for a minimum memory requirement on your GPU / TPU. \\\\n\\\\nThe HuggingFace PEFT library stands for parameter-efficient fine-tuning of transformer models (LLM for language, Stable Diffusion for images, Vision Transformer for vision) for reduced memory size. And one method of PEFT is LoRA: Low-rank Adaptation of LLMs. \\\\n\\\\nCombined with setting the pre-trained weights to non-trainable and maybe even consider a 8bit quantization of your pre-trained LLM model parameters, a reduced memory footprint of adapter-tuned transformer based LLM models achieves SOTA benchmarks, compared to classical fine-tuning of Large Language Models (like GPT,  BLOOM, LLama or T5). \\\\n\\\\nIn this video I explain the method in detail: AdapterHub and HuggingFace\\'s new PEFT library focus on parameter-efficient fine-tuning of transformer models (LLM for language, Stable Diffusion for images, Vision Transformer for vision) for reduced memory size. \\\\n\\\\nOne method, Low-rank Adaptation, I explain in detail for an optimized LoraConfig file when adapter-tuning INT8 quantization models, from LLMs to Whisper.\\\\n\\\\n#ai \\\\n#PEFT\\\\n#finetuning \\\\n#finetune \\\\n#naturallanguageprocessing \\\\n#datascience \\\\n#science \\\\n#technology \\\\n#machinelearning\"",
    "lengthSeconds": "2455",
    "uploadDate": "2023-03-28",
    "thumbnail_url": "https://i.ytimg.com/vi/YVU5wAA6Txo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2jmZ1BIxD7Q",
    "title": "We code Stanford's ALPACA LLM on a Flan-T5 LLM (in PyTorch 2.1)",
    "tags": "ALPACA_LLM, ALPACA Data SET, Fine",
    "scraped_at": 1685113816.6724007,
    "genre": "Science",
    "views": "6245",
    "desc": "Build Stanford\\'s ALPACA on a free Flan-T5-Large LLM. If you have the compute resources (GPU), why not try the Flan-T5-XXL (11B) model to fine-tune on the Alpaca 52K specific data set? Or create your own ALPACA style data set?\\\\n\\\\nEnjoy  wild and free Alpacas!\\\\n\\\\nAll files of my video are available via git clone for you (I was asked if one has no continuous online access to Jupyter or COLAB? So today .py!):\\\\nhttps://github.com/declare-lab/flan-alpaca\\\\n\\\\nNotice: All rights are with Univ of Singapore for this code /git! Thank you to the team in Singapore.\\\\n\\\\nNew videos will include Parameter Efficient Fine-Tuning (PEFT) like Low-Rank Adaptation (LoRA) for memory optimization (incl INT.8 model weights and parameters), but enough with teasing future videos, let\\'s enjoy this one.\\\\n\\\\nALPACA Data Set:\\\\n-----------------------------\\\\n@tatsu-lab/alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html)\\\\n\\\\nAlpaca is a data set of 52,000 instructions:\\\\nThe data fields are as follows:\\\\nA.  instruction: describes the task the model should perform. Each of the 52K instructions is unique.\\\\nB.  input: optional context or input for the task. For example, when the instruction is \\\\\"",
    "lengthSeconds": "586",
    "uploadDate": "2023-03-26",
    "thumbnail_url": "https://i.ytimg.com/vi/2jmZ1BIxD7Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=j6dqO2dSF9c",
    "title": "Stanford's new ALPACA 7B LLM explained - Fine-tune code and data set for DIY",
    "tags": "film, udost",
    "scraped_at": 1685113819.992722,
    "genre": "Science",
    "views": "20195",
    "desc": "New ALPACA 7B. Stanford Institute for Human-Centered AI created a new LLM: ALPACA 7B based on Meta\\'s LLama 7B. It has some inherent beauty, since it uses OpenAI\\'s API to generate a synthetic data set for supervised fine-tuning a small LLM (7 to 11B). Instruct-tune LLaMA w/ ChatGPT = ALPACA LLM.\\\\n\\\\nLLama is available on HuggingFace Transformers\\' main version, only the weights of the models have to be requested from Meta (w/ a specific form, smile).\\\\nTransformation script is available from Huggingface, fine-tune code from Stanford University. Create your own ALPACA with you corporate specific data set! \\\\n\\\\nFor minimal costs, since a small (around 10 Billion parameters, instead of 175B or 540B models) model is cheaper to fine-tune, and yes, we use the \\\\\"",
    "lengthSeconds": "1189",
    "uploadDate": "2023-03-24",
    "thumbnail_url": "https://i.ytimg.com/vi/j6dqO2dSF9c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=PZE_08Lshr4",
    "title": "Flan-T5 Model Fine-tuning: Advanced Techniques for Professionals",
    "tags": "Fine",
    "scraped_at": 1685113818.7942402,
    "genre": "Science",
    "views": "3501",
    "desc": "Fine-tune FLAN-T5 LLM on NLP: Complete Code Tutorial in PyTorch (free COLAB)\\\\n\\\\nNLP Mastery Made Easy: Fine-tune Your Flan-T5 Model Like a Pro with This Tutorial!\\\\n\\\\nFine tune a FLAN T5 LLM for a specific downstream task like text summarization. PyTorch code in real-time, complete tutorial. Fine-tune Flan-T5 models with task-specific data sets in real-time to watch. \\\\n\\\\nAll credits /rights to Philipp Schmid, Technical Lead at HuggingFace for his NB:\\\\nhttps://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/flan-t5-samsum-summarization.ipynb\\\\n\\\\n\\\\n\\\\n#ai \\\\n#languagemodel \\\\n#finetune \\\\n#finetuning \\\\n#naturallanguageprocessing \\\\n#t5\"",
    "lengthSeconds": "781",
    "uploadDate": "2023-03-22",
    "thumbnail_url": "https://i.ytimg.com/vi/PZE_08Lshr4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=PyRbP9d27sk",
    "title": "Fine-tuning T5 LLM for Text Generation: Complete Tutorial w/ free COLAB  #coding",
    "tags": "film, udost",
    "scraped_at": 1685113818.069425,
    "genre": "Science",
    "views": "4952",
    "desc": "Real time code to fine tune a T5 LLM model for the downstream task of text summarization. Code to Fine-tune a T5 model. \\\\n\\\\nYour official COLAB Jupyter NB to follow along:\\\\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb\\\\n(all rights, credits with corresponding authors of above mentioned NB)\\\\n\\\\n#ai \\\\n#machinelearningwithpython \\\\n#finetune \\\\n#finetuning \\\\n#t5\"",
    "lengthSeconds": "864",
    "uploadDate": "2023-03-20",
    "thumbnail_url": "https://i.ytimg.com/vi/PyRbP9d27sk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=SHMsdAPo2Ls",
    "title": "How to Fine-tune T5 and Flan-T5 LLM models: The Difference is?   #theory",
    "tags": "film, udost",
    "scraped_at": 1685113817.4834,
    "genre": "Science",
    "views": "7030",
    "desc": "Introduction how to fine-tune T5 and FLAN-T5 models (LLM - Large Language Models). Then some detailed videos how to code, step-by-step, fine tuning in real time on T5 and Flan T5 models. Fine tune Flan T5. Theory how to fine-tune T5 LLMs.\\\\n\\\\nNext video is on coding examples (JupyterLab, Colab).\\\\n\\\\nLiterature (all rights \\\\u0026 credits are with those authors):\\\\nhttps://huggingface.co/docs/transformers/training\\\\nhttps://huggingface.co/docs/transformers/model_doc/t5\\\\nhttps://huggingface.co/docs/transformers/model_doc/flan-t5\"",
    "lengthSeconds": "1601",
    "uploadDate": "2023-03-18",
    "thumbnail_url": "https://i.ytimg.com/vi/SHMsdAPo2Ls/maxresdefault.jpg"
  },
  {
    "link": "watch?v=NuYzHtAUbcc",
    "title": "ChatGPT on Job security, academic Youth Unemployment and which Actions to take  #jobs #job #ai",
    "tags": "film, udost",
    "scraped_at": 1685113818.8612401,
    "genre": "Science",
    "views": "729",
    "desc": "I discuss with a GPT-x system the future regarding our Jobs, emerging academic youth unemployment and let ChatGPT analyse counter measures. By corporations and governments. What will happen to jobs, where AI can perform 24/7? GPT-x has been trained w/ the knowledge of the internet, so let\\'s take advantage of it for topics that will have a personal impact on me.\\\\n\\\\nAre GPT-x systems able to discuss economic issues? Like pro/cons of investments in new jobs for the young generation vs new AI systems? Invest in education?\\\\n\\\\nCan we talk employment, business, investments, social coherence, safety with GPT? A real-life experiment to discuss a single economic issue with GPT in a human like conversation. How far are we when applying ChatGPT or GPT-x for real employment topics, apart from just writing a CV? \\\\n\\\\nAn AI system that incorporates this knowledge, can we use it in politics, in economics, for individuals or for nations? \\\\n\\\\nA chain of simple AI prompts to start my conversation on emerging youth unemployment in developed nations (special case: South Korea) with an AI system.\\\\n\\\\nSouth Korea has one of the highest educated young workforce (academics) on this planet. And suicide has been the leading cause of death for people aged 10 to 24 years. Why? \\\\n\\\\nRemember that humans have a propensity to anthropomorphize! ChatGPT has no inherent factual logic system for reasoning, it is just a little generative AI.\\\\n\\\\nShout out to @OpenAI for providing the free ChatGPT access. \\\\n\\\\nArticle from TechCrunch on MS:\\\\nhttps://techcrunch.com/2023/03/13/microsoft-lays-off-an-ethical-ai-team-as-it-doubles-down-on-openai/\\\\n\\\\nother online resources:\\\\nhttps://www.statista.com/topics/8622/suicide-in-south-korea/#topicHeader__wrapper\\\\n\\\\n\\\\n#chatgptprompts \\\\n#chatgpt \\\\n#chatgpttutorial\"",
    "lengthSeconds": "1235",
    "uploadDate": "2023-03-16",
    "thumbnail_url": "https://i.ytimg.com/vi/NuYzHtAUbcc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=H9NT7Qs2sss",
    "title": "Becoming more intelligent: our GPT-4 or ... our Children?",
    "tags": "film, udost",
    "scraped_at": 1685113819.860723,
    "genre": "Science",
    "views": "696",
    "desc": "GPT-4 and school children: What impact could this have on educational topics, on education itself, the way we will learn in the future? A system wide analysis of GPT-x prompt engineering and ICL patterns for education and knowledge frameworks for future generations. Including quantum derivations of fluid neural networks. Plus a Reverse Turing test.\\\\n\\\\nIs it a race of our civilization, who is becoming more intelligent: our children or our next GPT-4 system?\\\\nAre We Educating Humans or Machines? The Future of Intelligence in Education!\\\\nWill Our Children or our Computers Shape our Future, in business, in education?\\\\n\\\\nKnowledge level of kids might increase significantly with new AI tech, like generative AI systems. For a multitude of reasons I specify in this video. \\\\nEspecially the new in-context retrieval-augmented Large Language Models we are gonna build in the coming months on this channel, with cross-source validation! \\\\nUntil than kids have to validate every sentence from GPT-x manually with Google, dive into fact finding and validation, because GPT-x can sometimes hallucinate, and we never know when! Current kids could become most educated in generations?!\\\\n\\\\nGone are the days that children were reading books for their education. Tomorrow generative AI systems will be the standard communication platform for knowledge transfer and education, for interaction, creativity and learning complex, interwoven topics from multiple perspectives. Let\\'s have a look at the new challenges (for all members of the family) in the age of GPT-x.\\\\n\\\\nShout out to @OpenAI   for providing free ChatGPT access. \\\\n\\\\n#chatgpt \\\\n#gpt4 \\\\n#ai \\\\n#education \\\\n#challenges \\\\n #schools\"",
    "lengthSeconds": "671",
    "uploadDate": "2023-03-15",
    "thumbnail_url": "https://i.ytimg.com/vi/H9NT7Qs2sss/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yJJRvAn3sZ0",
    "title": "Reversible Transformer: ReFORMER for GPU Memory Optimization! Reversible Residual Layers?",
    "tags": "reversible Transformer, ReFormer, Locality sensitive hashing attention, LSH",
    "scraped_at": 1685113820.9936972,
    "genre": "Science",
    "views": "782",
    "desc": "GPU Memory Optimization for (Vision) Transformer: Theory of splitting activations, replace dot-product attention by locality-sensitive hashing attention, and reversible residual layers, which allows storing activations only once in the training process instead of N times, where N is the number of layers of the transformer architecture.\\\\n\\\\nReversible Residual Layers (RRL) are a type of neural network layer that have been developed to enable reversible computation in deep neural networks.\\\\n\\\\nIn a standard neural network layer, the computation is typically irreversible, meaning that once a computation has been performed on the input data, it cannot be undone. This poses a problem for deep neural networks that require the intermediate activations to be stored in memory for use in subsequent layers during the forward pass. This memory requirement limits the depth of neural networks that can be used in practice.\\\\n\\\\nRRLs address this issue by enabling the computation to be performed in a reversible manner, which allows the intermediate activations to be computed on-the-fly during the backward pass. This eliminates the need to store the intermediate activations in memory, which reduces the memory requirements of the network and allows for deeper neural networks to be used.\\\\n\\\\nRRLs achieve reversibility by using invertible operations, such as convolutions and permutations, in the computation. They also incorporate residual connections, which enable information to be passed directly from the input to the output of the layer.\\\\n\\\\nReversible Residual Layers have been shown to be effective in a variety of tasks, including image classification, object detection, and machine translation.\\\\n\\\\nREFORMER: THE EFFICIENT TRANSFORMER\\\\nhttps://arxiv.org/pdf/2001.04451.pdf\\\\n\\\\nDeep Residual Learning for Image Recognition:\\\\nhttps://arxiv.org/pdf/1512.03385.pdf\\\\n\\\\nThe Reversible Residual Network:\\\\nBackpropagation Without Storing Activations (Gomez, 2017)\\\\nhttps://arxiv.org/pdf/1707.04585.pdf\\\\n\\\\nRecommend:\\\\nWhat is Residual Connection?\\\\nhttps://towardsdatascience.com/what-is-residual-connection-efb07cab0d55\\\\nby Wanshun Wong\"",
    "lengthSeconds": "1076",
    "uploadDate": "2023-03-14",
    "thumbnail_url": "https://i.ytimg.com/vi/yJJRvAn3sZ0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_FYwnO_g-4E",
    "title": "Pretraining vs Fine-tuning vs In-context Learning of LLM (GPT-x) EXPLAINED | Ultimate Guide ($)",
    "tags": "film, udost",
    "scraped_at": 1685113819.9236963,
    "genre": "Science",
    "views": "8863",
    "desc": "Pretraining \\\\u0026 fine-tuning \\\\u0026 in-context learning of LLM (like GPT-x, ChatGPT) EXPLAINED | The ultimate Guide including price brackets as an indication to absolutely identify your compute and financial resources when and how to train  LLMs.\\\\n\\\\nSimple explanation of the differences between Pretraining, Fine-tuning and ICL (in-context learning) a LLM, like GPT-3.5-turbo or ChatGPT. \\\\nThe simplest explanation possible on this planet! \\\\nThe ultimate guide for beginners to LLM!\\\\n\\\\n#promptengineering \\\\n#ai \\\\n#generativeai \\\\n#naturallanguageprocessing \\\\n#chatgptexplained\"",
    "lengthSeconds": "551",
    "uploadDate": "2023-03-13",
    "thumbnail_url": "https://i.ytimg.com/vi/_FYwnO_g"
  },
  {
    "link": "watch?v=Yin-0PDmriI",
    "title": "Buy Microsoft \"Azure OpenAI Service\" or buy from OpenAI its API for ChatGPT access & tuning?",
    "tags": "film, udost",
    "scraped_at": 1685113819.3812113,
    "genre": "Science",
    "views": "3088",
    "desc": "Buy at Microsoft when it launches ChatGPT API as an Azure Managed Service or \\\\ngo with the start-up OpenAI API for GPT-3.5-turbo ChatGPT (and upcoming GPT-4)? \\\\nMy insight on the night before Microsoft will launch \\\\\"",
    "lengthSeconds": "855",
    "uploadDate": "2023-03-12",
    "thumbnail_url": "https://i.ytimg.com/vi/Yin"
  },
  {
    "link": "watch?v=4XweSnMXxWw",
    "title": "Code your BLIP-2 APP: VISION Transformer (ViT) + Chat LLM (Flan-T5) = MLLM",
    "tags": "film, udost",
    "scraped_at": 1685113818.4084253,
    "genre": "Science",
    "views": "1201",
    "desc": "BLIP-2: Upload an image, the vision transformer will analyze the content of the image and a LLM will tell you a story about it - or answer your questions about the picture. We\\'ll use Flan-T5 and Vision Transformer, interlinked w/ Q-Former (BLIP 2). Multimodal LLM w/ BLIP-2.\\\\n\\\\nExample: if you upload a picture from the great pyramid in Egypt and you prompt (ask) the system: \\\\\"",
    "lengthSeconds": "1408",
    "uploadDate": "2023-03-12",
    "thumbnail_url": "https://i.ytimg.com/vi/4XweSnMXxWw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zGv7sTOxXwI",
    "title": "ChatGPT:  In-context Retrieval-Augmented Learning (IC-RALM) | In-context Learning (ICL) Examples",
    "tags": "film, udost",
    "scraped_at": 1685113817.8113992,
    "genre": "Science",
    "views": "2712",
    "desc": "From  ICL to In-Context Retrieval-Augmented Language Models (in-context RALM). Tune your ChatGPT, let it learn new stuff! I show you how. Even without paying for OpenAI\\'s API.\\\\n\\\\nFine-tuning is too expensive? And by the way, even if you pay for OpenAI\\'s API, you can\\'t currently fine-tune GPT-3.5-Turbo! No way!  .....  So? ICL!\\\\n\\\\nFor the time of your free session on ChatGPT, I show you how you can provide new content to ChatGPT, from one-shot prompting and data extracted in real-time from the internet (RALM). \\\\n\\\\nWhat is ICL? During in-context learning (ICL), we give the LLM a prompt that consists of a list of input-output pairs that demonstrate a task. At the end of the prompt, we append a test input and allow the LLM to make a prediction just by conditioning on the prompt and predicting the next tokens. ( \\xe2\\x80\\x9cfew-shot learning\\xe2\\x80\\x9d, or \\\\\"",
    "lengthSeconds": "548",
    "uploadDate": "2023-03-12",
    "thumbnail_url": "https://i.ytimg.com/vi/zGv7sTOxXwI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zeiBH484H2g",
    "title": "ChatGPT:  Multidimensional Prompts",
    "tags": "film, udost",
    "scraped_at": 1685113817.0093987,
    "genre": "Science",
    "views": "4481",
    "desc": "Create your own thematic tables on complex topics w/ ChatGPT and ChatGPT fills in topic-specific text into each cell. Multidimensional Prompt Engineering for cross-correlations (of multiple semantic topics)! \\\\n\\\\nExample: Compare US to Europe in 8 categories of educational proficiency.\\\\nI show you how ChatGPT writes an analysis in each of the 16 categories for you.\\\\n\\\\nNew ChatGPT prompts to generate 2 dim tables with ChatGPT\\'s Semantic Text Table Autocomplete. Simple role prompting and ChatGPT fills in all cells of a cross-correlated semantic content, for a specific audience and with a defined professional level persona. Have fun with multidimensional prompts!\\\\n\\\\nChatGPT fills in thematic spreadsheets?\\\\nWith multiple \\\\\"",
    "lengthSeconds": "488",
    "uploadDate": "2023-03-11",
    "thumbnail_url": "https://i.ytimg.com/vi/zeiBH484H2g/maxresdefault.jpg"
  },
  {
    "link": "watch?v=QHktvcxsGJ0",
    "title": "Chat with your Image!  BLIP-2 connects Q-Former w/ VISION-LANGUAGE models (ViT & T5 LLM)",
    "tags": "Vision Transformer, LLM, Flan",
    "scraped_at": 1685113820.4826982,
    "genre": "Science",
    "views": "1592",
    "desc": "Combined Vision-Language Transformers, interlinked w/ a Q-Former, a Querying Transformer! BLIP 2. BLIP-2!\\\\nThe financial resources for pre-training both systems (Vision and Language) are astronomical? Let me introduce you to a clever, new training method: BLIP-2. Multimodal Large Language Models for visual QA or perception-language tasks, multimodal dialogue, or image captioning, and  image recognition with verbal content descriptions, plus a Chat function.  \\\\n\\\\nVisual Perception and Large Language Models: The new combination in Transformers.  Multi-modal Large Language Models for visual QA or image captioning.\\\\n\\\\n\\\\nAll rights and credits w/: \\\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\\\nhttps://arxiv.org/abs/2301.12597\\\\n\\\\n#ai \\\\n#machinelearning \\\\n#chatgpt\\\\n#vision \\\\n#llm \\\\n#BLIP2\\\\n#QFormer\"",
    "lengthSeconds": "796",
    "uploadDate": "2023-03-10",
    "thumbnail_url": "https://i.ytimg.com/vi/QHktvcxsGJ0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Z__ki9uS6ck",
    "title": "ChatGPT Prompt Engineering w/ in-context learning (ICL)  - 7 Examples | Tutorial",
    "tags": "film, udost",
    "scraped_at": 1685113820.0596957,
    "genre": "Science",
    "views": "2310",
    "desc": "Best ChatGPT prompts for AI chat on INNOVATION theory w/ in-context learning (ICL)? Absolutely! We start with a simple zero-shot prompting, define a topic, our audience, our preferred style, our goals we want to achieve, play w/ special challenges for ChatGPT (free version), define response templates, play w/ adversarial prompts and have fun with the Reverse Turing Test (if you as a human are intelligent enough to get the optimal output that the machine is theoretically capable of). \\\\n\\\\nPlus we have a conversation with ChatGPT if OpenAI will disrupt BING Chat or MS will incorporate OpenAI? We ask ChatGPT for a list of action items to fight against the board dominance of Microsoft, based on the innovation theory by Clayton Christensen! Yes, we are having fun!\\\\n\\\\nFew-shot prompting and Chain-of-Thought (CoT) will be in the next videos! Plus some advanced ..... \\\\n\\\\nCopyright w/ my Canva Professional edition.\\\\nShout out to  @OpenAI   for providing the free ChatGPT access. \\\\n\\\\n\\\\n#chatgpt \\\\n#promptengineering \\\\n#promptdesign\\\\n#innovation \\\\n#christensen \\\\n#chatgpttutorial \\\\n#chatgptprompts \\\\n#chatgptcoding \\\\n#chatgptexplained\"",
    "lengthSeconds": "752",
    "uploadDate": "2023-03-08",
    "thumbnail_url": "https://i.ytimg.com/vi/Z__ki9uS6ck/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ih8daDJZXfc",
    "title": "Hospital /Clinic AI Decision Models: Performance of 12 AI LLM Systems (incl $$) Radiology, Biomed",
    "tags": "film, udost",
    "scraped_at": 1685113819.1802115,
    "genre": "Science",
    "views": "866",
    "desc": "12 AI Language Models, ranging from 220M to 175B parameters, w/ measuring their performance on 3 different clinical tasks that test their ability to parse and reason over electronic health records of patients. \\\\n\\\\nTraining T5-Base and T5-Large models from scratch on clinical notes from MIMIC III and IV to directly investigate the efficiency of clinical tokens. \\\\n\\\\nResult: small specialized clinical models substantially outperform all in-context learning approaches on LLMs like GPT-3, even when fine-tuned on limited annotated data. \\\\n\\\\nLatest AI in Clinical Settings: A Critical Look at the Performance of 12 Language Models and LLMs.\\\\n\\\\nAll rights and credits to:\\\\nDo We Still Need Clinical Language Models?\\\\nhttps://arxiv.org/pdf/2302.08091.pdf\\\\n\\\\n#clinical \\\\n#ai \\\\n#generativeai \\\\n#naturallanguageprocessing \\\\n#datascience \\\\n#hospital \\\\n#sbert \\\\n#machinelearning \\\\n#test \\\\n#clinicalbiochemistry\"",
    "lengthSeconds": "2192",
    "uploadDate": "2023-03-07",
    "thumbnail_url": "https://i.ytimg.com/vi/ih8daDJZXfc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tkkomjwyMuY",
    "title": "ChatGPT polarizes",
    "tags": "film, udost",
    "scraped_at": 1685113816.9414005,
    "genre": "Science",
    "views": "239",
    "desc": "Yes, I did receive some feedback on my last video about OpenAI\\'s free ChatGPT summarization abilities of external documents on the internet, when a https link is provided via a prompt. Some were emotional.\"",
    "lengthSeconds": "157",
    "uploadDate": "2023-03-06",
    "thumbnail_url": "https://i.ytimg.com/vi/tkkomjwyMuY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BvGKLIH78nw",
    "title": "OpenAI's ChatGPT can NOW summarize external Sources on the Internet?",
    "tags": "film, udost",
    "scraped_at": 1685113818.6662376,
    "genre": "Science",
    "views": "2714",
    "desc": "My viewers noted, that new reporting by TomsGuide (https://www.tomsguide.com/how-to/how-to-use-chatgpt-to-summarize-an-article) shows that OpenAI\\'s ChatGPT can now summarize external documents on the internet! \\\\nCan YOU.com/Chat can do it better?\\\\n\\\\nMy viewers ask for an independent verification, since BING CHAT is reported to be able (in an invitation only Beta) of real-time inputs, but OpenAI\\'s ChatGPT? \\\\nAn analysis with some interesting results! \\\\n\\\\n#ai \\\\n#chatgpt \\\\n#chatgptprompts \\\\n#naturallanguageprocessing \\\\n#datascience \\\\n#machinelearning \\\\n#generativeai \\\\n#summary\"",
    "lengthSeconds": "662",
    "uploadDate": "2023-03-04",
    "thumbnail_url": "https://i.ytimg.com/vi/BvGKLIH78nw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wZEIRupmVZA",
    "title": "Why wait for KOSMOS-1? Code a VISION - LLM w/ ViT, Flan-T5 LLM and BLIP-2: Multimodal LLMs (MLLM)",
    "tags": "film, udost",
    "scraped_at": 1685113820.251727,
    "genre": "Science",
    "views": "1180",
    "desc": "Proprietary MS KOSMOS-1? Forget it! Vote for an early release of two new videos about a new combination of VISION transformers and Flan-T5 LLMs based on a transformer architecture: VISION - LLM systems (or Multi-modal Large Language Models) short:  MLLM.\\\\n\\\\nNew multimodal Large Language Models combine the transformer architecture of vision and language with an intermittent transformer (a QFormer). \\\\n\\\\nGreat arxiv pre-print by Salesforce research on BLIP-2 (all rights /credits with them /authors).  https://arxiv.org/abs/2301.12597\\\\n\\\\nAn alternative w/ ViT and Flan-T5 LLMs (plus interlink BLIP-2) to the proprietary Microsoft KOSMOS-1. Bonus: ViT, Flan-T5 and BLIP-2 are individually to fine-tune. And we will fine-tune the hell out of them for a particular task! And you can fine-tune for additional task later on. \\\\n\\\\nVote for early release in the community tab. \\\\n\\\\n#ai \\\\n#naturallanguageprocessing \\\\n#vision \\\\n#finetuning \\\\n#nlproc \\\\n#generativeai\"",
    "lengthSeconds": "448",
    "uploadDate": "2023-03-03",
    "thumbnail_url": "https://i.ytimg.com/vi/wZEIRupmVZA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Gk9xdqmjVug",
    "title": "Streamlining Similar Image Detection with ViT in PyTorch: A Step-by-Step Guide",
    "tags": "Vision Transformer, ViT, Similarity, Similarity of Images, PyTorch, Python, AI",
    "scraped_at": 1685113820.1236956,
    "genre": "Science",
    "views": "365",
    "desc": "How to find similar images with Vision transformers (ViT) in PyTorch? I show you a simple code implementation where we apply a pre-trained Vision Transformer, which has been fine-tuned on a image dataset (publicly available \\\\\"",
    "lengthSeconds": "497",
    "uploadDate": "2023-03-02",
    "thumbnail_url": "https://i.ytimg.com/vi/Gk9xdqmjVug/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tw_f-Q6Rb2c",
    "title": "Dream Job Alert: AI Prompt Engineer - $335K  |  AI Prompt Design: A Crash Course",
    "tags": "film, udost",
    "scraped_at": 1685113817.5464246,
    "genre": "Science",
    "views": "1662",
    "desc": "Anthropic AI offers you a job as prompt engineer. Go and get a new Job in AI, if you know about prompt engineering. Short Introduction to prompt engineering and continuous prompt design, plus prefix tuning vs fine-tuning for LLMs. \\\\n\\\\nHint: all my viewers, who read recommended research arxiv pre-prints surely qualify.\\\\n\\\\nLiterature:\\\\nhttps://arxiv.org/pdf/2107.13586.pdf\\\\nPre-train, Prompt, and Predict: A Systematic Survey of\\\\nPrompting Methods in Natural Language Processing\\\\n\\\\nConstitutional AI: Harmlessness from AI Feedback (by Anthropic. Claude)\\\\nhttps://arxiv.org/pdf/2212.08073.pdf\\\\n\\\\n#ai \\\\n#joboffer\"",
    "lengthSeconds": "1587",
    "uploadDate": "2023-02-28",
    "thumbnail_url": "https://i.ytimg.com/vi/tw_f"
  },
  {
    "link": "watch?v=KSdPYtWlIMA",
    "title": "Code Panoptic Image Segmentation w/ Vision Transformer & Mask2Former - A PyTorch tutorial",
    "tags": "Mask2Former, Image Segmentation, Panoptic Segmentation, Semantic Segmentation, Image tech, Vision tech",
    "scraped_at": 1685113815.8324015,
    "genre": "Science",
    "views": "1265",
    "desc": "Key innovation is to have a Transformer decoder come up with a set of binary masks and classes in a parallel way. This was then improved in the MaskFormer paper, which showed that the \\\\\"",
    "lengthSeconds": "930",
    "uploadDate": "2023-02-26",
    "thumbnail_url": "https://i.ytimg.com/vi/KSdPYtWlIMA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=m8vxf4rU-P8",
    "title": "Panoptic Image Segmentation: Mask2Former explained | Identify all objects!",
    "tags": "film, udost",
    "scraped_at": 1685113820.769723,
    "genre": "Science",
    "views": "1100",
    "desc": "Image segmentation can largely be split into 3 subtasks - instance, semantic and panoptic segmentation. In 2023 we have a universal image segmentation method for all three of them: Mask2Former. \\\\n\\\\nIn this video I explain mask2former an a given image example. \\\\nIn my next video we code a GUI for Mask2Former in PyTorch.\\\\n\\\\n#ai \\\\n#imagesegmentation \\\\n#vision_transformer_model\"",
    "lengthSeconds": "742",
    "uploadDate": "2023-02-24",
    "thumbnail_url": "https://i.ytimg.com/vi/m8vxf4rU"
  },
  {
    "link": "watch?v=DBNcD1W7ut4",
    "title": "New BING Chat AGGRESSIVE",
    "tags": "film, udost",
    "scraped_at": 1685113818.474425,
    "genre": "Science",
    "views": "580",
    "desc": "Welcome to Microsoft Cyberdyne Systems. Or the new BING Chat for short. Microsoft\\'s ChatGPT. It is aggressive.\\\\nTrain a decoder stack of a transformer architecture with millions of human conversations, add Reinforcement Learning from human feedback (RLHF), combine this with a proprietary Prometheus model (?) on a Microsoft Cloud supercomputer and we will notice the perfect mirror of general human behavior, unfiltered by personality, position or educational level. MS Chat just reflects learned human behavior, or does it? MS Chat imitates human behavior, but does it have the technical means (BING Chat has access to the internet, for sure - but does it has access to MS Mail server to send out mails?) to hurt humans, individuals and families? Does Skynet need access to the famous codes, no, ... maybe only access to our e-mail servers.\\\\n\\\\nIf a company has all corporate data on a Microsoft data server, does ChatGPT or MS BING Chat has access to their data? For sure MS Chat can write a report by its own!\\\\n\\\\nAll right belong to Twitter and its two authors, that published the content on Twitter, as shown and mentioned in my video.\\\\n\\\\nLink:\\\\nhttps://twitter.com/marvinvonhagen/status/1625852323753762816\\\\n\\\\n#ai \\\\n#chatgpt \\\\n#riskmanagement \\\\n#terminator\\\\n#naturallanguageprocessing \\\\n#datascience\"",
    "lengthSeconds": "1108",
    "uploadDate": "2023-02-22",
    "thumbnail_url": "https://i.ytimg.com/vi/DBNcD1W7ut4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wao7HRgtcaU",
    "title": "PyTorch ViT: The Ultimate Guide to Fine-Tuning for Object Identification (COLAB)",
    "tags": "Vision transformers, Fine",
    "scraped_at": 1685113815.9004,
    "genre": "Science",
    "views": "1223",
    "desc": "CODE example to fine-tune a pre-trained Vision Transformer (ViT) to identify different objects in an image: helicopters, cars, .. or biological systems. We code in real-time! Efficiently fine-tune a pre-trained vision transformer (ViT) on a free COLAB notebook.\\\\n\\\\nAll rights with corresponding authors of code or pre-prints:\\\\n\\\\n1. Your Jupyter notebook to follow along (by Nate Raw, HF):\\\\nhttps://colab.research.google.com/github/nateraw/huggingface-hub-examples/blob/main/vit_image_classification_explained.ipynb\\\\n\\\\n2. Your blog by HuggingFace:\\\\nhttps://huggingface.co/blog/fine-tune-vit\\\\n\\\\n3. Visit repo by Google:\\\\nhttps://github.com/google-research/vision_transformer#fine-tuning-a-model\\\\n\\\\n4. How to train your ViT? Data, Augmentation,\\\\nand Regularization in Vision Transformers:\\\\nhttps://arxiv.org/pdf/2106.10270.pdf\\\\n\\\\n#ai \\\\n#vision \\\\n#machinelearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#colab\"",
    "lengthSeconds": "1343",
    "uploadDate": "2023-02-21",
    "thumbnail_url": "https://i.ytimg.com/vi/wao7HRgtcaU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XbVtF2DaUyI",
    "title": "Microsoft strongly restricts access to ChatGPT on new BING - WHY?",
    "tags": "film, udost",
    "scraped_at": 1685113821.294722,
    "genre": "Science",
    "views": "832",
    "desc": "My viewers noticed that Microsoft now strongly restricts access to the new BING ChatGPT system, starting Feb 18, 2023. And ask why? \\\\nThere is an easy scientific answer. \\\\n\\\\nAnd no, this is no entertainment channel, but a science channel. \\\\n\\\\nMicrosoft on Feb 17, 2023, imposed new restrictions on the use of BING ChatGPT and limits your questions to 5 per session, before BING will try to end the session. Does it still qualify as a Chat system, a conversational system? I suppose so, but a system in a hurry, that has no time for you. \\\\n\\\\nRecommended Literature:\\\\nWHAT LEARNING ALGORITHM IS IN-CONTEXT LEARN-\\\\nING? INVESTIGATIONS WITH LINEAR MODELS\\\\nhttps://arxiv.org/pdf/2211.15661.pdf\"",
    "lengthSeconds": "552",
    "uploadDate": "2023-02-19",
    "thumbnail_url": "https://i.ytimg.com/vi/XbVtF2DaUyI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=9dYXGGklkhc",
    "title": "Visualizing the Self-Attention Head of the Last Layer in DINO ViT: A Unique Perspective on Vision AI",
    "tags": "Vision transformer, ViT pre",
    "scraped_at": 1685113816.4684005,
    "genre": "Science",
    "views": "476",
    "desc": "In a Colab Notebook we code a visualization of the last layer of the Vision Transformer Encoder stack and analyze the visual output of each of the 12 Attention Heads, given a specific image. Now we understand how a only pre-trained ViT (although with the DINO method) can not always succeed in an image classification (downstream) task. The fine-tuning of the ViT is simply missing - but essential for a better performance.\\\\n\\\\nBased on the COLAB NB by Niels Rogge, HuggingFace (all rights with him):\\\\nhttps://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DINO/Visualize_self_attention_of_DINO.ipynb\\\\n\\\\nIn one of my next video we will fine-tune a pre-trained Vision Transformer ViT from scratch. For better image classification performance.\\\\n\\\\n#ai \\\\n#vision \\\\n#technology\"",
    "lengthSeconds": "395",
    "uploadDate": "2023-02-18",
    "thumbnail_url": "https://i.ytimg.com/vi/9dYXGGklkhc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dtlg3PUUn_g",
    "title": "Self-Attention Heads of last Layer of Vision Transformer (ViT) visualized (pre-trained with DINO)",
    "tags": "Vision transformer, ViT pre",
    "scraped_at": 1685113817.4194193,
    "genre": "Science",
    "views": "438",
    "desc": "In a Colab Notebook we code a visualization of the last layer of the Vision Transformer Encoder stack and analyze the visual output of each of the 12 Attention Heads, given a specific image. Now we understand how a only pre-trained ViT (although with the DINO method) can not always succeed in an image classification (downstream) task. The fine-tuning of the ViT is simply missing - but essential for a better performance.\\\\n\\\\nBased on the COLAB NB by Niels Rogge, HuggingFace (all rights with him):\\\\nhttps://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DINO/Visualize_self_attention_of_DINO.ipynb\\\\n\\\\nIn one of my next video we will fine-tune a pre-trained Vision Transformer ViT from scratch. For better image classification performance.\\\\n\\\\n#ai \\\\n#vision \\\\n#technology\"",
    "lengthSeconds": "519",
    "uploadDate": "2023-02-16",
    "thumbnail_url": "https://i.ytimg.com/vi/dtlg3PUUn_g/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Gh_1xqWAFu0",
    "title": "New BING ChatGPT: Unlock the Power of Emotions in your Search Engine!",
    "tags": "Microsoft BING ChatGPT, ChatGPT answers, ChatGPT irrational",
    "scraped_at": 1685113817.0743992,
    "genre": "Science",
    "views": "2730",
    "desc": "Exploring the Benefits of an Emotional Search Engine. Introducing new BING: \\\\\"",
    "lengthSeconds": "608",
    "uploadDate": "2023-02-14",
    "thumbnail_url": "https://i.ytimg.com/vi/Gh_1xqWAFu0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=r88L_yLJ4CE",
    "title": "PyTorch code Vision Transformer: Apply ViT models pre-trained and fine-tuned  | AI  Tech",
    "tags": "Vision transformer, AI, Data Science, Vison Image Classification, Google ViT, HuggingFace models, Code, Colab NB, Apply Vision transformer",
    "scraped_at": 1685113818.9222379,
    "genre": "Science",
    "views": "1450",
    "desc": "Run pretrained and fine-tuned Vision Transformer (ViT) models 2023 out of the box for image classification tasks in PyTorch. Code with me on a free Colab Notebook and Google provided the pretrained and/or fine-tuned ViT models on Huggingface models, ready to download.\\\\n\\\\n#ViT\\\\n#ai \\\\n#vision \\\\n#vision_transformer\"",
    "lengthSeconds": "1398",
    "uploadDate": "2023-02-13",
    "thumbnail_url": "https://i.ytimg.com/vi/r88L_yLJ4CE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XRwdC2UkewE",
    "title": "New TECH: Vision Transformer 2023 on Image Classification | AI",
    "tags": "medical image classification, Convolutional Neural Networks, Vision Transformer, ViT, scientific study, CNN vs ViT",
    "scraped_at": 1685113818.7312407,
    "genre": "Science",
    "views": "945",
    "desc": "Understand state-of-the-art tech in Vision Technology, eg medical image classification, beginning of 2023. We learn the current tech of Vision Transformer vs CNN in a medical real-world application: \\\\\"",
    "lengthSeconds": "1264",
    "uploadDate": "2023-02-11",
    "thumbnail_url": "https://i.ytimg.com/vi/XRwdC2UkewE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qaIWjt_TPgY",
    "title": "The Intersection of Copyright Law and Human Faces: Exploring Virtual K-Pop with MAVE",
    "tags": "Vision technology, Vision transformer architecture, Coding Pytorch, Introduction, AI, Science",
    "scraped_at": 1685113818.9872408,
    "genre": "Science",
    "views": "237",
    "desc": "A demo in the sector of K-Pop : https://youtu.be/1wGOHbcQKIc\\\\nAccording to MIT Technology Review, real human faces for AI generated videos are currently a business transaction between corporations (for the digital task of a receptionist, help desk, visual avatar, AI created interactive Siri with a face you find pleasing on your iPhone, AI animated Alexa in VR (with human body) for augmented dialogue and ... music, K-POP. By the way: .... forget Mark\\'s Metaverse characters. \\\\n\\\\nIt is time to talk about the latest technology in image and video, and our responsibilities (and ethics) when applying those technologies. \\\\n\\\\nNext focus of this channel will be on latest TECH in VISION TRANSFORMER and their model architecture. New coding tutorials will cover theory and applications.\\\\n\\\\nMIT TR link:\\\\nhttps://www.technologyreview.com/2021/08/27/1033879/people-hiring-faces-work-deepfake-ai-marketing-clones/\\\\n\\\\nA glimpse into the future of human virtual faces in K-Pop (and what new tech will ignite):\\\\nCheck out a great video by @JordanOrme (pro video editor) on K-Pop MAVE:\\\\nhttps://youtu.be/gyF__LvyJFc\\\\nalso original video (all rights /copyright for this video w/ author, production company, see also creative film company flipevil): \\\\nhttps://youtu.be/1wGOHbcQKIc\\\\n\\\\n#vision \\\\n#ai \\\\n#image\"",
    "lengthSeconds": "273",
    "uploadDate": "2023-02-09",
    "thumbnail_url": "https://i.ytimg.com/vi/qaIWjt_TPgY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vVHSMyUcV3M",
    "title": "And now?",
    "tags": "film, udost",
    "scraped_at": 1685113816.4054003,
    "genre": "Science",
    "views": "288",
    "desc": "Time to re-evaluate.\"",
    "lengthSeconds": "171",
    "uploadDate": "2023-02-09",
    "thumbnail_url": "https://i.ytimg.com/vi/vVHSMyUcV3M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XV1RXLPIVlw",
    "title": "Fine-tune ChatGPT w/  in-context learning ICL - Chain of Thought, AMA, reasoning & acting: ReAct",
    "tags": "Fine",
    "scraped_at": 1685113819.6547236,
    "genre": "Science",
    "views": "5507",
    "desc": "Prompt engineering was yesterday. New insights into in-context learning to achieve significant better results w/ all autoregressive LLMs (like ChatGPT, BioGPT or PaLM540B). Latest research on Chain-of-Thought Prompting (CoT) and ReAct, combining reasoning and action (agent receives external data). The practice of Fine-tuning BioBERT versus prompting GPT-3 vs prefix-tuning BioGPT on biomedical data (like PubMedQA). \\\\n\\\\nWhy does in-context learning work w/ ChatGPT?\\\\n\\\\nApply intelligent input prompts to a pre-trained LLM system (like ChatGPT) and avoid expensive domain-specific fine-tuning, since comparable results are achievable, when applying latest research results and insights.\\\\n\\\\nI recommend this research literature:\\\\n-----------------------------------------------------------\\\\n\\\\nChain-of-Thought Prompting Elicits Reasoning\\\\nin Large Language Models\\\\nhttps://arxiv.org/pdf/2201.11903v6.pdf\\\\n\\\\nREACT : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\\\\nhttps://arxiv.org/pdf/2210.03629.pdf\\\\n\\\\nAsk Me Anything: A simple strategy for prompting language models (AMA)\\\\nhttps://arxiv.org/pdf/2210.02441.pdf\\\\n\\\\nWHAT LEARNING ALGORITHM IS IN-CONTEXT LEARN-\\\\nING? INVESTIGATIONS WITH LINEAR MODELS\\\\nhttps://arxiv.org/pdf/2211.15661.pdf\\\\n\\\\nLARGE LANGUAGE MODELS ARE HUMAN-LEVEL\\\\nPROMPT ENGINEERS\\\\nhttps://arxiv.org/pdf/2211.01910.pdf\\\\n\\\\nBioGPT: Generative Pre-trained Transformer for\\\\nBiomedical Text Generation and Mining\\\\nhttps://arxiv.org/pdf/2210.10341.pdf\\\\n\\\\n#in-context_learning\"",
    "lengthSeconds": "2199",
    "uploadDate": "2023-02-08",
    "thumbnail_url": "https://i.ytimg.com/vi/XV1RXLPIVlw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JcYI11RkyUI",
    "title": "Improve ChatGPT: Modular, Adaptive, Smart LLM | Inside ChatGPT",
    "tags": "ChatGPT, Improvements, Clever and smart AI, Sparrow, Google, up",
    "scraped_at": 1685113821.1907225,
    "genre": "Science",
    "views": "751",
    "desc": "My viewers ask how to build their own, advanced ChatGPTs. How to make ChatGPT smaller, that it runs locally on a single GPU, is adaptive, and provides a smart, factual, up-to-date response. \\\\n\\\\nI compare:\\\\n-----------------\\\\nMonolithic AI                       VS modular AI systems\\\\nRigid system architecture VS adaptive interfaces\\\\nEverything for everybody   VS smart and focused\\\\nPropitiatory AI                     VS open source\\\\nLicense fee based              VS free LLM\\\\n\\\\nEasy! I found a simple answer. Homework included for fans of mathematics.\\\\n\\\\n#chatgpt \\\\n#monolithic \\\\n#microsoft \\\\n#ai \\\\n#architecture\"",
    "lengthSeconds": "192",
    "uploadDate": "2023-02-07",
    "thumbnail_url": "https://i.ytimg.com/vi/JcYI11RkyUI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3vnpwembwp4",
    "title": "After ChatGPT: NEW BioGPT by Microsoft | Do YOU trust Microsoft for your Medication?",
    "tags": "GPT, ChatGPT, BioGPT, Biomedical",
    "scraped_at": 1685113821.0626965,
    "genre": "Science",
    "views": "2486",
    "desc": "Yesterday on Huggingface the BioGPT model from Microsoft was published for everybody to experience. BioGPT is a generative pre-trained transformer on (human) BIOMEDICAL topics. With a focus on medical and drug interactions, this GPT has been intensively pre-trained and then fine-tuned by Microsoft on biomedical data. Also on the question answering (downstream) task. Yes, BioGPT gives you an answer regarding biomedical topics. With the publication on Huggingface, everybody can download and experience BioGPT. \\\\n\\\\nThe arxiv research pre-print by Microsoft on BioGPT you can find here (please respect copyright):\\\\nhttps://arxiv.org/pdf/2210.10341.pdf\\\\n\\\\nThis system is an important learning ground for the scientific community, regarding highly domain specific data models, since GPT wants to achieve a similar status like the BERT systems current enjoy in BioMed. The authors state correctly: \\\\\"",
    "lengthSeconds": "765",
    "uploadDate": "2023-02-06",
    "thumbnail_url": "https://i.ytimg.com/vi/3vnpwembwp4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=7IposV4_LY4",
    "title": "Unleashing the Power of BLOOM 176B with AWS ml.p4de.24xlarge, DJL & DeepSpeed: The Ultimate Boost!",
    "tags": "BLOOM model, BLOOM LLM, BLOOM 176B, Inference run, deepspeed, Deep java library serving",
    "scraped_at": 1685113819.247237,
    "genre": "Science",
    "views": "1659",
    "desc": "More Power! How and where to run inference of an LLM w/ 176 billion parameter? Well, what about the most expensive ML instance on AWS? The most performant implementation for LLMs (utilizing latest .. and most expensive .. cloud infrastructure)? Some implementation ideas ...\\\\n\\\\nRegarding LLM inference code implementation: what LMI DLCs on Amazon ECR to apply? Should we use model parallelism, and if yes, pipeline (like HF\\'s accelerate) or tensor (like DeepSpeed)? Do we have a language agnostic model serving, and if yes, how to apply Deep Java Library serving in Pytorch? Interested to spend some hundreds of US$ on Amazon SageMaker for maybe (if successful ...) a single hour of inference, but ... extreme low inference latency? This is the latest in tech? Hey Amazon: More Power! The next trillion parameter models are visible at the horizon.\\\\n\\\\nIn short: A iypnb for you to experience the full BLOOM 176B model inference task on the most expensive cloud infrastructure.\\\\n  \\\\n- Not for beginners - \\\\n- This video is just for nerds with deep pockets - \\\\n- As a beginner do not pay a cloud provider for their top tier infrastructure, always start with a reasonable cheap instance and experience system behaviour and cost accumulation - \\\\n\\\\nThanks to AWS to provide this Jupyter Notebook to show us how to run inference of huge LLMs on their latest infrastructure. I just wonder why they are so charming to show us how to spend our hard-earned money on latest AWS cloud tech??? Any ideas? \\\\n\\\\nNot sponsored by anybody (unfortunately). \\\\n\\\\n00:00 BLOOM 176B vs Flan-T5-XXL\\\\n01:36 More Power!\\\\n03:45 3 Options to run LLMs on GPU\\\\n05:42 ipynb SageMaker DeepSpeed Container \\\\n\\\\n#bloom \\\\n#ai \\\\n#naturallanguageprocessing \\\\n#generativeai \\\\n#chatgpt\"",
    "lengthSeconds": "1593",
    "uploadDate": "2023-02-06",
    "thumbnail_url": "https://i.ytimg.com/vi/7IposV4_LY4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=crcYVWKwttY",
    "title": "Fine-tune ChatGPT? Buy Embeddings /OpenAI? What are Embeddings?  My own ChatGPT? | Visual Q+A",
    "tags": "Q",
    "scraped_at": 1685113819.3102381,
    "genre": "Science",
    "views": "3505",
    "desc": "An emotional Q+A round regarding my viewers most pressing questions: \\\\nI bought embeddings from OpenAI, and now? How can I build my own ChatGPT? How much does it cost to build GPT myself? How can I fine-tune ChatGPT? What are embeddings? Do I need to pay Microsoft? Really??? Why? \\\\nIs ChatGPT modular for professional use? How to input more specific information in ChatGPT for my profession?\\\\nCan I increase the performance of ChatGPT for specific topics, like medicine, law, chemistry, specific jurisdiction, ...... \\\\nThank you, my subscribers, for all your Q. I try to answer ... \\\\n\\\\n\\\\n#chatgpttutorial \\\\n#chatgpt \\\\n#chatgptexplained \\\\n#questionanswer \\\\n#datascience \\\\n#ai\"",
    "lengthSeconds": "1602",
    "uploadDate": "2023-02-05",
    "thumbnail_url": "https://i.ytimg.com/vi/crcYVWKwttY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=FS2qtP-OofM",
    "title": "Self-aware AI: You.com/chat vs Perplexity.ai | Live Demo, LLMs show Future of ChatGPT w/ BING",
    "tags": "film, udost",
    "scraped_at": 1685113815.6223996,
    "genre": "Science",
    "views": "4721",
    "desc": "LLMs with the integration of a search engine provide current data. We compare perplexity.ai with you.com/chat which both include an inherent GPT-3.5 system with current info from the search engines (either Google or Bing). Perplexity.ai and you.com/chat show us, what the next version of ChatGPT might include in its Microsoft implementation: real-time access to BING, for up-to-date data streams. \\\\n\\\\n\\\\nPerplexity.ai and you.com/chat both lack the excellent CHAT part of ChatGPT, but include (contrary to our current ChatGPT) access to real-time data sets on the internet. Although both systems have a similar architecture under the hood, they respond differently. Choose according to your task. \\\\n\\\\n#chatgpt \\\\n#comparison \\\\n#demonslayer \\\\n\\\\n00:00 Compare perplexity with youchat\\\\n00:22 Theory of ChatGPT, Perplexity and YouChat\\\\n02:09 Live DEMO\\\\n03:46 Limitations\\\\n05:42 What is perplexity.ai?\\\\n08:33 The \\\\\"",
    "lengthSeconds": "603",
    "uploadDate": "2023-02-04",
    "thumbnail_url": "https://i.ytimg.com/vi/FS2qtP"
  },
  {
    "link": "watch?v=XL9vDoumP7Y",
    "title": "2023 KerasNLP Tutorial: Explore Latest KERAS Toolbox & NLP Processing Library for BERT - TF2",
    "tags": "KERAS, KerasNLP, TensorFlow2, TF2, Transformer model, BERT, Pre",
    "scraped_at": 1685113820.189722,
    "genre": "Science",
    "views": "774",
    "desc": "Introduction and new tutorial to KerasNLP: Keras NLP is a natural language processing library (eg TransformerEncoder layer) that supports users through their entire development cycle. Workflows are built from modular components that have state-of-the-art preset weights and architectures when used out-of-the-box and are easily customizable when more control is needed. \\\\n\\\\nMy last video: https://youtu.be/W735DaBOKKo\\\\n----------------------\\\\n\\\\nBuild your own transformer model (BERT, Roberta either from scratch for your domain specific data and knowledge or use pre-train models for your fine-tuning). \\\\n\\\\nNow you have 3 ways to pre-train BERT from scratch:\\\\n-------------------------------------------------------------------------------------\\\\nA. With PyTorch in my video: https://youtu.be/IcrN_L2w0_Y\\\\nB. With Keras in my video: https://youtu.be/W735DaBOKKo\\\\nC. With KerasNLP toolbox for BERT: this video\\\\n\\\\nOfficial KERAS COLAB Notebook:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/keras_nlp/getting_started.ipynb\\\\n\\\\nLinks I referred to in the YT video:\\\\nhttps://keras.io/guides/keras_nlp/getting_started/#introduction \\\\nhttps://keras.io/api/keras_nlp/models/bert/bert_classifier/\\\\nhttps://keras.io/api/keras_nlp/models/bert/bert_backbone/#bertbackbone-class\\\\n\\\\nVideo transitions and thumbnails are provided by my Canva Professional account.\\\\n\\\\n#tensorflow2 \\\\n#keras \\\\n#ai \\\\n#coding \\\\n#colab\"",
    "lengthSeconds": "2183",
    "uploadDate": "2023-02-03",
    "thumbnail_url": "https://i.ytimg.com/vi/XL9vDoumP7Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tiChyUpIuEk",
    "title": "ChatGPT Alternative: Perplexity by Perplexity.AI",
    "tags": "film, udost",
    "scraped_at": 1685113815.4083993,
    "genre": "Science",
    "views": "1047",
    "desc": "Perplexity.AI, a new conversational search engine! Combining GPT-3.x with BING, this Perplexity gives us an idea, what ChatGPT w/ BING will be like later on w/ Microsoft. Try it out today at Perplexity.AI, it\\'s free!\\\\n\\\\nperplexity.ai ask anything:\\\\nhttps://www.perplexity.ai/\\\\n\\\\nAnother reference of perplexity.ai:\\\\nhttps://twitter.com/perplexity_ai\\\\n\\\\n#ai \\\\n#chatgpt \\\\n#searchengineoptimisation\"",
    "lengthSeconds": "523",
    "uploadDate": "2023-02-02",
    "thumbnail_url": "https://i.ytimg.com/vi/tiChyUpIuEk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=oCU97mnl494",
    "title": "FLAN-T5-XXL on NVIDIA A100 GPU w/ HF Inference Endpoints, let's explore 11b models!",
    "tags": "Flan_T5_XXL, NVIDIA A100, HuggingFace, Inference Endpoints, FLAN T5 XXL model, Generative AI",
    "scraped_at": 1685113819.7217245,
    "genre": "Science",
    "views": "3466",
    "desc": "Easy Cloud Inference! Today I discover a new Flan-T5-XXL model repository on Huggingface, which can run (optimized) on a NVIDIA A10G. Or run Google\\'s Flan-T5-XXL on A100 GPU. PLUS: First time discovery of Huggingface\\'s Inference endpoints! What are Inference endpoints by HF: a fully managed cloud compute infrastructure (eg AWS, AZURE, later GOOGLE) where I can use my HuggingFace repositories from any TRANSFORMER or Sentence-Transformer model and run directly a cloud compute! A new milestone for easy inference of 11b parameter LLMs. \\\\nGreat solution by @HuggingFace \\\\n\\\\nClassical rep by Google (NVIDIA A100 or 8xA100 w/ 640 GB):\\\\nhttps://huggingface.co/google/flan-t5-xxl\\\\n\\\\nNew rep by PhilSchmid (NVIDIA A10G):\\\\nhttps://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16\\\\n\\\\nTutorial by Phil Schmid (recommended!):\\\\nhttps://www.philschmid.de/deploy-t5-11b\\\\n\\\\nInference Endpoints (start):\\\\nhttps://ui.endpoints.huggingface.co/welcome\\\\n\\\\n#ai \\\\n#naturallanguageprocessing \\\\n#generativeai\"",
    "lengthSeconds": "1422",
    "uploadDate": "2023-02-01",
    "thumbnail_url": "https://i.ytimg.com/vi/oCU97mnl494/maxresdefault.jpg"
  },
  {
    "link": "watch?v=x7MNNByBKQw",
    "title": "3D Visualization for BERT: How to Pre-Train with a New Layer & Fine-Tune with Downstream Task Layer",
    "tags": "BERT model, NLP, Natural Language Processing, Pre",
    "scraped_at": 1685113818.5374248,
    "genre": "Science",
    "views": "412",
    "desc": "BERT 3D visualization applied to design a pre-training transformer architecture from scratch, pre-train on a company or domain specific dataset, then fine-tune the Transformer model (BERT) and run inference tasks.\\\\n\\\\nIn my other two video we coded the pre-training of a BERT model for SBERT in PyTorch, and in Tensorflow TF2, more specific in KERAS, with the toolbox KerasNLP for Natural Language Processing. We build the complete BERT transformer model in code and pre-trained the model. \\\\n\\\\nVideo transitions are provided by Canva Professional Edition.\\\\n\\\\n#naturallanguageprocessing \\\\n#datascience \\\\n#finetune \\\\n#trending \\\\n#3dmodeling\"",
    "lengthSeconds": "105",
    "uploadDate": "2023-01-31",
    "thumbnail_url": "https://i.ytimg.com/vi/x7MNNByBKQw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=W735DaBOKKo",
    "title": "TF2: Pre-Train BERT from scratch (a Transformer), fine-tune & run inference on text | KERAS NLP",
    "tags": "BERT model, NLP, Natural Language Processing, Pre",
    "scraped_at": 1685113815.0334005,
    "genre": "Science",
    "views": "2110",
    "desc": "TF2 KERAS Transformer pre train: we pre-train BERT from scratch, on a company or domain specific dataset, then fine-tune the model (BERT) and run an inference task on our (domain specific pre-trained and fine-tuned) BERT model. All steps in real time. KERAS NLP.\\\\n\\\\nIn my other video we coded the pre-training of a BERT model for SBERT in PyTorch, today in Tensorflow TF2, more specific in KERAS, with the toolbox KerasNLP for NLP. We build the complete BERT transformer model in code and pre-train the model. \\\\n\\\\n00:00 Build a Transformer (BERT) from scratch\\\\n16:30 Code to pre-train a Transformer (BERT model)\\\\n20:41 Code to fine-tuning a unique BERT model\\\\n26:19 Code BERT model inference on plain text \\\\n\\\\nVideo transitions are provided by Canva Professional.\\\\n\\\\n#naturallanguageprocessing \\\\n#datascience \\\\n#finetune \\\\n#pretrain\\\\n#trending\"",
    "lengthSeconds": "1812",
    "uploadDate": "2023-01-29",
    "thumbnail_url": "https://i.ytimg.com/vi/W735DaBOKKo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=VqDtEESy_q8",
    "title": "Google's 2nd Answer to \"BING ChatGPT\":  Sparrow | after BARD w/ LaMDA | 2nd Gen Conversational AI",
    "tags": "Chinchilla, Google",
    "scraped_at": 1685113817.2244,
    "genre": "Science",
    "views": "5317",
    "desc": "BARD\\'s first demo answer was wrong and destroyed $100 bn of market value for Alphabet. Google\\'s 2nd answer to \\\\\"",
    "lengthSeconds": "1340",
    "uploadDate": "2023-01-26",
    "thumbnail_url": "https://i.ytimg.com/vi/VqDtEESy_q8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lNJQFn84rCA",
    "title": "From Zero to FLAN-T5 XL Model GUI with Gradio: A Step-by-Step Guide on Free COLAB Notebook PyTorch",
    "tags": "FLAN",
    "scraped_at": 1685113819.1172407,
    "genre": "Science",
    "views": "1778",
    "desc": "We code a Gradio GUI for our FLAN-T5-XL model on a free Google Colab NB /Python. Like ChatGPT this model is able to write an essay, summarize text and translate languages. However, Google\\'s FLAN-T5 is free and open-source, contrary to Microsoft\\'s OpenAI ChatGPT implementation. FLAN-T5 runs on a CPU with 2 cores (!) and a (fraction of) a free T4 GPU in the cloud. Free Demonstration example with code to follow along.\\\\n\\\\nPre-trained FLAN-T5 models (checkpoints) provided by HuggingFace /Google.\\\\n\\\\nRecommended link for model and files:\\\\nhttps://huggingface.co/google/flan-t5-xl\\\\nhttps://huggingface.co/google/flan-t5-xl/tree/main\\\\n\\\\n00:00 DEMO of Flan-T5-XL with Gradio GUI\\\\n03:44 Prompt example for Flan-T5-XL\\\\n07:14 Code GUI in Jupyter NB\\\\n08:30 Tune system parameters\\\\n09:29 GUI code for slider input\\\\n10:24 Host on Huggingface Spaces\"",
    "lengthSeconds": "717",
    "uploadDate": "2023-01-25",
    "thumbnail_url": "https://i.ytimg.com/vi/lNJQFn84rCA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Q4qaDZarl8g",
    "title": "The Future of Conversational AI? Google's PaLM w/ RLHF  | LLM ChatGPT Competitor",
    "tags": "LLM, AI, NLP, T5, T5X, PaLM, Google, Language Models, conversational AI",
    "scraped_at": 1685113817.875425,
    "genre": "Science",
    "views": "4342",
    "desc": "After explaining BERT vs GPT and Google\\'s T5 JAX (in my last videos) we now examine new PaLM: Pathways Language Model (if combined w/ RLHF -Reinforcement Learning with Human feedback). T5X = Google\\'s T5 on JAX and FLAX. Plus Code implementation for PaLM w/ RLHF. \\\\n\\\\nPS: Next video on another LLM could be on Sparrow ... smile.\\\\n\\\\nmy resources (all rights are with the authors):\\\\n\\\\nExploring the Limits of Transfer Learning with a Unified\\\\nText-to-Text Transformer\\\\nhttps://arxiv.org/pdf/1910.10683.pdf\\\\n\\\\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\\\\nhttps://arxiv.org/pdf/1808.06226.pdf\\\\n\\\\nIllustrating Reinforcement Learning from Human Feedback (RLHF)\\\\nhttps://github.com/huggingface/blog/blob/main/rlhf.md\\\\n\\\\nFine-Tuning the Text-To-Text Transfer Transformer (T5) for Closed-Book Question Answering\\\\nhttps://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb#scrollTo=zSeyoqE7WMwu\\\\n\\\\nPaLM + RLHF - Pytorch \\\\nhttps://github.com/lucidrains/PaLM-rlhf-pytorch\\\\n\\\\n#ai \\\\n#naturallanguageprocessing \\\\n#reinforcementlearning \\\\n#RLHF\"",
    "lengthSeconds": "597",
    "uploadDate": "2023-01-23",
    "thumbnail_url": "https://i.ytimg.com/vi/Q4qaDZarl8g/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lHLX81qLk_8",
    "title": "From T5 to T5X: A Game-Changing Evolution with JAX & FLAX",
    "tags": "LLM, AI, NLP, T5, T5X, PaLM, Google, Language Models, conversational AI",
    "scraped_at": 1685113820.5606964,
    "genre": "Science",
    "views": "1668",
    "desc": "After explaining BERT vs GPT (last video) we now examine current tech like Google\\'s T5X (for Google search) and in my next video new PaLM: Pathways Language Model (if combined w/ RLHF -Reinforcement Learning with Human feedback). T5X = Google\\'s T5 on JAX and FLAX. Plus Code implementation for T5X. \\\\n\\\\nmy sources (all rights are with the corresponding authors):\\\\n\\\\nExploring the Limits of Transfer Learning with a Unified\\\\nText-to-Text Transformer\\\\nhttps://arxiv.org/pdf/1910.10683.pdf\\\\n\\\\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\\\\nhttps://arxiv.org/pdf/1808.06226.pdf\\\\n\\\\nIllustrating Reinforcement Learning from Human Feedback (RLHF)\\\\nhttps://github.com/huggingface/blog/blob/main/rlhf.md\\\\n\\\\nFine-Tuning the Text-To-Text Transfer Transformer (T5) for Closed-Book Question Answering\\\\nhttps://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb#scrollTo=zSeyoqE7WMwu\\\\n\\\\nPaLM + RLHF - Pytorch \\\\nhttps://github.com/lucidrains/PaLM-rlhf-pytorch\\\\n\\\\n#ai \\\\n#t5 \\\\n#chatgpt \\\\n#reinforcementlearning\"",
    "lengthSeconds": "362",
    "uploadDate": "2023-01-21",
    "thumbnail_url": "https://i.ytimg.com/vi/lHLX81qLk_8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ewjlmLQI9kc",
    "title": "BERT and GPT in Language Models like ChatGPT or BLOOM |  EASY Tutorial on Large Language Models LLM",
    "tags": "film, udost",
    "scraped_at": 1685113820.6997225,
    "genre": "Science",
    "views": "7427",
    "desc": "Transformer-based self-supervised Language Models explained: BERT and GPT.  What is the difference for Language Models like in ChatGPT, YouChat or BLOOM?\\\\n\\\\n#ai \\\\n#gpt3 \\\\n#chatgpt\"",
    "lengthSeconds": "178",
    "uploadDate": "2023-01-19",
    "thumbnail_url": "https://i.ytimg.com/vi/ewjlmLQI9kc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6W-HMbgRJRM",
    "title": "Flan-T5-XL model on a free COLAB | A free LLM - that explains itself w/ reasoning /write essay | AI",
    "tags": "FLAN",
    "scraped_at": 1685113820.3996968,
    "genre": "Science",
    "views": "7590",
    "desc": "Not as performant as ChatGPT, but free:  FLAN-T5-XLARGE LLM model. And we can optimize it!  PS: A laptop is all you need. I show you the code.\\\\n\\\\nFLAN-T5 XL hardly fits in a free Google COLAB Notebook, but with new updates we apply the FP32 and FP16 versions of FLAN-T5 XL and run some inference tasks, like summation or translation or \\\\\"",
    "lengthSeconds": "1095",
    "uploadDate": "2023-01-17",
    "thumbnail_url": "https://i.ytimg.com/vi/6W"
  },
  {
    "link": "watch?v=IcrN_L2w0_Y",
    "title": "Pre-Train BERT from scratch: Solution for Company Domain Knowledge Data | PyTorch (SBERT 51)",
    "tags": "BERT, SBERT, pre",
    "scraped_at": 1685113816.5404253,
    "genre": "Science",
    "views": "2338",
    "desc": "We pretrain a BERT (Bidirectional Encoder Representations from Transformers) model from scratch in PyTorch, on domain specific data (eg confidential company data). We code in Python to train an optimized Tokenizer for our data, design a BERT architecture from scratch and start pre-training of BERT with a masked Language Model Head (MLM). We define the vocabulary size according to our needs (from 8K to 60K), define the depth of our BERT architecture (eg 96 layers) and train days on (a single) GPU for our domain specific knowledge encoding.  \\\\n\\\\nBERT :: Bidirectional Encoder Representations from Transformers is a transformer-based machine learning technique for natural language processing.\\\\n\\\\nWith an advanced BERT model (pre-trained on our special texts) we can then build a SBERT model (Sentence Transformers) for a Neural Information Retrieval (IR) system. \\\\n\\\\nofficial Links to my sources (all rights with them):\\\\nhttps://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\\\\n\\\\n!! COLAB to follow along:\\\\nhttps://colab.research.google.com/drive/1An1VNpKKMRVrwcdQQNSe7Omh_fl2Gj-2?usp=sharing\\\\n\\\\n\\\\n#sbert \\\\n#ai \\\\n#naturallanguageprocessing\"",
    "lengthSeconds": "2146",
    "uploadDate": "2023-01-15",
    "thumbnail_url": "https://i.ytimg.com/vi/IcrN_L2w0_Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5ZP9SJyTM94",
    "title": "Discover Vision Transformer (ViT) Tech in 2023",
    "tags": "Learn to code, learn to code AI, Huggingface, Papers with Code, How I learn",
    "scraped_at": 1685113820.849726,
    "genre": "Science",
    "views": "251",
    "desc": "Discover how I learn to code new AI topics (like Vision Transformer - ViT) for my YouTube videos and how I plan my AI videos. Where to get information about current trends in NLP or Vision, where to learn a new theory (arxiv pre-prints) of a new tech (eg Vision transformer for medical images) in AI. Where to find excellent code examples for a first implementation. And how to stay informed on new and evolving AI topics and code implementations for real-world applications.  \\\\n\\\\nFrom @HuggingFace  libraries to my beloved https://paperswithcode.com\\\\n\\\\n00:00  Learn new AI code\\\\n01:24 Arxiv pre-prints: cs.CV cs.AI\\\\n04:33 Stanford, MIT, Harvard lectures on YouTube\\\\n07:22 HuggingFace Transformer Library on Vision\\\\n11:09 COLAB Notebooks from authors\\\\n13:03 Papers with CODE - Methods\\\\n16:30 Computer Vision - Classification \\\\n18:42 Latest pre-prints Vision Transformer\\\\n23:36 Recent code per month\\\\n26:11 AI publications worldwide\\\\n\\\\n#ai \\\\n#research \\\\n#prepare\\\\n#youtubevideos\"",
    "lengthSeconds": "1742",
    "uploadDate": "2023-01-13",
    "thumbnail_url": "https://i.ytimg.com/vi/5ZP9SJyTM94/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2RA5dEIC-Nw",
    "title": "SBERT Extreme 3D: Train a BERT Tokenizer  on your (scientific) Domain Knowledge  (SBERT 50)",
    "tags": "SBERT, BERT model, TOKENIZER, AI, Language models",
    "scraped_at": 1685113816.2754009,
    "genre": "Science",
    "views": "270",
    "desc": "Building our optimized SBERT Sentence Transformer w/ uniquely designed BERT Pre-training and at first: Training of a special Tokenizer (here: Byte-pair Encoding - BPE) on complex domain knowledge (eg bio-pharmacological text books). Sub-word based tokenization of domain knowledge. \\\\n\\\\nVisualize SBERT models in 3D to better understand each step we will take to code a unique SBERT and BERT model for our special domain knowledge (eg bio-medical research w/ macro-molecular interdisciplinary scientific content).\\\\n\\\\nDomain specific knowledge is encoded in our sentences. Either you have access to arxiv pre-prints in your knowledge area or you have rights to your digitized text book on molecular biology, those sentences have to be transformed as input to our specialized BERT model, which we will pre-train from scratch to take advantage of our optimized, up-to-date pre-trained BERT Tokenizer.\\\\n\\\\n#ai \\\\n#machinelearning \\\\n#naturallanguageprocessing \\\\n#sbert\"",
    "lengthSeconds": "896",
    "uploadDate": "2023-01-11",
    "thumbnail_url": "https://i.ytimg.com/vi/2RA5dEIC"
  },
  {
    "link": "watch?v=hN3vZ1V5BOo",
    "title": "Unlocking Scientific Domain Knowledge w/ BPE Tokenizer: An Amazing Journey!  (SBERT 49)",
    "tags": "SBERT, BERT model, TOKENIZER, AI, Language models",
    "scraped_at": 1685113819.5867229,
    "genre": "Science",
    "views": "397",
    "desc": "Building our optimized SBERT Sentence Transformer w/ uniquely designed BERT Pre-training and at first: Training of a special Tokenizer (here: Byte-pair Encoding - BPE) on complex domain knowledge (eg bio-pharmacological text books). Sub-word based tokenization of domain knowledge. \\\\n\\\\nVisualize SBERT models in 3D to better understand each step we will take to code a unique SBERT and BERT model for our special domain knowledge (eg bio-medical research w/ macro-molecular interdisciplinary scientific content).\\\\n\\\\nDomain specific knowledge is encoded in our sentences. Either you have access to arxiv pre-prints in your knowledge area or you have rights to your digitized text book on molecular biology, those sentences have to be transformed as input to our specialized BERT model, which we will pre-train from scratch to take advantage of our optimized, up-to-date pre-trained BERT Tokenizer.\\\\n\\\\n#ai \\\\n#machinelearning \\\\n#naturallanguageprocessing \\\\n#sbert\"",
    "lengthSeconds": "1121",
    "uploadDate": "2023-01-09",
    "thumbnail_url": "https://i.ytimg.com/vi/hN3vZ1V5BOo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zJlmzYdA8aY",
    "title": "Achieve Unimaginable Levels of Domain Knowledge through SBERT Extreme in 3D   (SBERT 48)",
    "tags": "BERT, SBERT, Sentence Transformers, Self",
    "scraped_at": 1685113820.925698,
    "genre": "Science",
    "views": "506",
    "desc": "Intro to my code series to create an optimized BERT / SBERT system for high-performance neural Information Retrieval systems for highly specialized domain knowledge (eg bio-chemical-neuropharmacological medical textbooks) or company specific knowledge.\\\\n\\\\nPre-trained BERT and SBERT are great and pre-trained on all of Wikipedia and books w/ millions and billions of sentences. But what if you want ultra performance (fast, efficient, ..) of a heavily optimized special domain knowledge Q+A or Information Retrieval system? Welcome to my mini-series.\\\\n\\\\nIn 2019 BERT was especially design to provide better performance than GPT models, where authors used a left-to-\\\\nright architecture, where every token could only attend to previous tokens in the self-attention layers of the Transformer.\\\\n\\\\nHow to explain Q, K and V of Self Attention in Transformers (BERT)?\\\\nhttps://youtu.be/PFczJ6NR5rY\\\\n\\\\nBERT: Pre-training of Deep Bidirectional Transformers for\\\\nLanguage Understanding\\\\nhttps://arxiv.org/pdf/1810.04805.pdf\\\\n\\\\n#sbert \\\\n#ai \\\\n#naturallanguageprocessing\"",
    "lengthSeconds": "182",
    "uploadDate": "2023-01-07",
    "thumbnail_url": "https://i.ytimg.com/vi/zJlmzYdA8aY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5rZ0RF_OyQM",
    "title": "Domain-Specific AI Models: How to Create Customized BERT and SBERT Models for Your Business",
    "tags": "SBERT, BERT, Domain specific Data, Knowledge System",
    "scraped_at": 1685113818.0044253,
    "genre": "Science",
    "views": "400",
    "desc": "A lot of viewers asked about how to train Transformer (BERT, SBERT) on domain specific knowledge? Where there are a lot of special terms and complex medical, biochemical names? Can a pre-trained SBERT system learn these semantic content relations, although it has not been pre-trained on them? Is fine-tuning a SBERT system on new data sets enough to integrate this specific information?\\\\n\\\\nHere an answer to all your question.\\\\n\\\\nMy YT Playlist on DATA SET for SBERT Fine-tuning:\\\\nhttps://www.youtube.com/watch?v=JxfS5ZjdxGE\\\\u0026list=PLgy71-0-2-F1Yf7waKzaywNKMCbD8FtaA\\\\n\\\\nMy YT Playlist on SBERT Fine-tuning:\\\\nhttps://www.youtube.com/watch?v=FidMAm-tj9k\\\\u0026list=PLgy71-0-2-F1GVPahTCcfUNIdPvaWUXJG\\\\n\\\\nMy YT Playlist on LLM:\\\\nhttps://www.youtube.com/watch?v=DNy4UhBrOKI\\\\u0026list=PLgy71-0-2-F0byY7llx5kyNrHHyBVguvZ\\\\n\\\\n\\\\n00:00 Company specific DATA\\\\n01:48 It is not WORDS\\\\n04:06 A sequence of Tokens\\\\n05:15 Client demands\\\\n06:47 My Solutions \\\\n10:42 Use Large Language Models (ChatGPT)\"",
    "lengthSeconds": "707",
    "uploadDate": "2023-01-05",
    "thumbnail_url": "https://i.ytimg.com/vi/5rZ0RF_OyQM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XZhVUI5suf0",
    "title": "Feature Vectors: The Key to Unlocking the Power of BERT and SBERT Transformer Models",
    "tags": "ai, nlp, machine learning, Transformer models, NLP models, BERT, SBERT",
    "scraped_at": 1685113818.205426,
    "genre": "Science",
    "views": "196",
    "desc": "After converting text to high-dimensional vectors (and tensors) we use them as information encoded input to our NLP models based on the transformer architecture (like BERT or Sentence Transformers SBERT). We can apply mathematics to our semantic encoded vectors and compute different weight tensors of our NN systems.\\\\n\\\\n\\\\nMy videos as mentioned in the video:\\\\nBeginner\\'s GUIDE to TRANSFORMERS\\\\nhttps://youtu.be/vBVJhojtooM42\\\\n\\\\nHow to explain Q, K and V of SelfAttention in Transformers ?\\\\nhttps://youtu.be/PFczJ6NR5rY\\\\n\\\\nSBERT (Sentence Transformers) is not BERT Sentence Embedding: Intro \\\\u0026 Tutorial\\\\nhttps://youtu.be/lVqwznaVi78\\\\n\\\\nCore idea of AI w/ Sentence Transformers (SBERT)\\\\nhttps://youtu.be/GeWK5wqe7YY\\\\n\\\\nSBERT and PyG\\\\nhttps://youtu.be/10evf7xmXto\\\\n\\\\nMy YouTube SBERT Playlist\\\\nhttps://www.youtube.com/watch?v=FidMAm-tj9k\\\\u0026list=PLgy71-0-2-F1GVPahTCcfUNIdPvaWUXJG\\\\n\\\\nLearn Sentence Transformers #SBERT: Update 2022 - new models, semantic search, AI  \\\\nhttps://youtu.be/ewlCCB7EFPs\\\\n\\\\n00:00 Learn about Transformers\\\\n02:45 Python code of Vectorizer\\\\n05:55 Python code for Sentence transformer \\\\n10:37 Bonus material (BERT explained)\\\\n16:08 SBERT model visualized\"",
    "lengthSeconds": "1137",
    "uploadDate": "2023-01-03",
    "thumbnail_url": "https://i.ytimg.com/vi/XZhVUI5suf0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JZj1gw66xIE",
    "title": "The Art of Text to Vector Transformation: A Comprehensive Look at AI and NLP Transformers",
    "tags": "intro, nlp, ai, tokenizer",
    "scraped_at": 1685113817.291425,
    "genre": "Science",
    "views": "195",
    "desc": "How to convert Human Text to mathematical Feature Vectors in a Vector Space. Explain why to convert text to vectors, tensors for use to compute AI NLP Transformers application like question answering or summarization of human texts.\\\\n\\\\nTokenizer -INTRO \\\\nhttps://youtu.be/uxMvwDaCQnc\\\\n\\\\nTokenizer - BERT\\\\nhttps://youtu.be/04oZ2P0uvp0\\\\n\\\\nTokenizer - BPE\\\\nhttps://youtu.be/MlDP2BVWjS0\"",
    "lengthSeconds": "764",
    "uploadDate": "2023-01-01",
    "thumbnail_url": "https://i.ytimg.com/vi/JZj1gw66xIE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2qvhcoxDyeY",
    "title": "Amazing Symmetry in Convolutional Layers of CNN ... because of algebraic properties of GROUPS",
    "tags": "Mathematics, Group Definition",
    "scraped_at": 1685113818.1354256,
    "genre": "Science",
    "views": "107",
    "desc": "Short explanation why Convolutional Layers in CNN have a Symmetry. It is because of the specific properties of a mathematical construct: GROUPS.\\\\n\\\\nHomomorphismus of Groups, the Kernel of a Group and general definitions of a Group, for ML and Deep Neural Networks.\\\\n\\\\n#group \\\\n#algebra \\\\n#datascience\"",
    "lengthSeconds": "390",
    "uploadDate": "2022-12-29",
    "thumbnail_url": "https://i.ytimg.com/vi/2qvhcoxDyeY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=WQtQ4yvVJ7U",
    "title": "SetFit and SBERT: ZERO Shot Classification w/ synthetic Data Set added (SBERT 47)",
    "tags": "SBERT, SetFit, Zero Shot, Zero Shot Classification, Synthetic data",
    "scraped_at": 1685113821.6178677,
    "genre": "Science",
    "views": "526",
    "desc": "SetFit (trained on SBERT) was designed for few-shot learning, but the method can also be applied in scenarios where no (or not enough) labeled data is available for ZERO-Shot classification w/ synthetic data set added. \\\\n\\\\nThe main trick is to create synthetic examples that resemble the classification task, and then train a SetFit model on them.\\\\n\\\\nSetFit can be applied for ZERO-shot classification (with added labeled synthetic data) AND adding synthetic examples can also provide a performance boost to new FEW-shot classification!\"",
    "lengthSeconds": "1571",
    "uploadDate": "2022-12-27",
    "thumbnail_url": "https://i.ytimg.com/vi/WQtQ4yvVJ7U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Hd4zV3Km0t8",
    "title": "Text Vectorization Made Easy: A Tutorial on KERAS Preprocessing Layers for AI",
    "tags": "KERAS, Text Vectorization, INPUT Processing Pipelines in KERAS",
    "scraped_at": 1685113821.5568674,
    "genre": "Science",
    "views": "731",
    "desc": "Text Vectorization layer: a preprocessing layer which maps TEXT features to numerical sequences. KERAS preprocessing layer API allows to build Keras-native INPUT PROCESSING PIPELINES.\\\\n\\\\nThis layer has basic options for managing text in a Keras model. It transforms a batch of strings (one example = one string) into either a list of token indices (one example = 1D tensor of integer token indices) or a dense representation (one example = 1D tensor of float values representing data about the example\\'s tokens). This layer is meant to handle natural language inputs. \\\\n\\\\nALL Credits to KERAS:\\\\nhttps://keras.io/guides/preprocessing_layers/\\\\nhttps://keras.io/api/layers/preprocessing_layers/text/text_vectorization/#textvectorization-class\\\\n\\\\nCOLAB from KERAS:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_from_scratch.ipynb\\\\n\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/preprocessing_layers.ipynb\\\\n\\\\n\\\\n#AI\\\\n#KERAS\\\\n#TextVectorization\"",
    "lengthSeconds": "1648",
    "uploadDate": "2022-12-25",
    "thumbnail_url": "https://i.ytimg.com/vi/Hd4zV3Km0t8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yI7FeziuG8M",
    "title": "YouChat: Unlock Advanced Conversational AI w/ information sources & up-to-date info",
    "tags": "LLM, YouChat, ChatGPT, First look, Conversational LM",
    "scraped_at": 1685113821.8108752,
    "genre": "Science",
    "views": "1865",
    "desc": "New conversational Language model YouChat: You.com (a search engine like Google) launches a new ChatGPT kind-of LM, named YouChat. Brand new! First impressions! Unique feature: YouChat provides its information sources to you, with clickable links to immediately verify sources. A feature ChatGPT has not implemented, eg for transparency and validation.\\\\n\\\\nExplore YouChat here:\\\\nhttps://you.com\\\\n\\\\nAnother new YouChat feature (compared to ChatGPT): YouChat incorporates recent events and information from the internet and incorporates this facts into the answer /conversation. See in my YT video the information about the current weather situation in the US, with new weather info from just three days ago!\\\\n\\\\nYouCode provides you with code snippets from GitHub and other relevant (internet) sources (code and articles).\\\\n\\\\nYouWrites writes essays, blogs or social media posts for you, given your text prompt. You can specify your target audience, your tonality and the complexity of text generated.\\\\n\\\\nYouImagine create stable diffusion generative AI Images from your text prompt.\\\\n\\\\nExperience with me live first impressions on the new conversational LLM: YouCHAT. Particular its online, web based version. Further investigations are necessary for a complete view of this application.\\\\n\\\\n#chatgpt \\\\n#ai \\\\n#youchat\"",
    "lengthSeconds": "901",
    "uploadDate": "2022-12-24",
    "thumbnail_url": "https://i.ytimg.com/vi/yI7FeziuG8M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XQCxzfjX6Eg",
    "title": "Invest in future OpenAI shares in 2023? The ChatGPT company?",
    "tags": "film, udost",
    "scraped_at": 1685113822.3448403,
    "genre": "Science",
    "views": "1213",
    "desc": "Analyzing the potential (!) investment opportunities in future stock market, I have a look at OpenAI and its hyping free research demo ChatGPT. Will there be an IPO in 2023? You have to decide yourself. \\\\n\\\\nMy links (sources):\\\\nhttps://www.forbes.com/sites/qai/2022/12/21/is-there-a-chatgpt-stock-can-you-invest-in-chatgpt-and-other-types-of-artificial-intelligence/\\\\n\\\\nhttps://www.reuters.com/article/us-microsoft-openai/microsoft-to-invest-1-billion-in-openai-idUSKCN1UH1H9\\\\n\\\\nhttps://www.reuters.com/article/idUSL3N1405JW20151211\\\\n\\\\n\\\\n#investment \\\\n#shares \\\\n#openai \\\\n#chatgpt\"",
    "lengthSeconds": "265",
    "uploadDate": "2022-12-23",
    "thumbnail_url": "https://i.ytimg.com/vi/XQCxzfjX6Eg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vF77LRomkic",
    "title": "Beam Info through Dimensional Reduction to lower-dim Vector Spaces w/ PaCMAP on MNIST images",
    "tags": "Topology, Mathematics, Dimensional rduction, Topological manifolds, UMAP, 2D, Visualization",
    "scraped_at": 1685113821.9388387,
    "genre": "Education",
    "views": "173",
    "desc": "Dimensional reduction algorithm PaCMAP. In data science we often need to reduce our dimensional complexity of topological spaces, and after t-SNE, UMAP and my latest video on \\\\\"",
    "lengthSeconds": "850",
    "uploadDate": "2022-12-23",
    "thumbnail_url": "https://i.ytimg.com/vi/vF77LRomkic/maxresdefault.jpg"
  },
  {
    "link": "watch?v=t71zMCsAoEY",
    "title": "Unveiling the Power of Graphs: Node and Edge Classification w/ GraphSAGE | GraphML",
    "tags": "Node Classification, GNN, Graph Neural Networks, GraphML, Edge Classification, DGL, Deep Graph Network",
    "scraped_at": 1685113821.7468705,
    "genre": "Science",
    "views": "578",
    "desc": "Full code example of Node and Edge classification with GraphSAGE for GraphML. DGL on PyTorch backbone. Graph Neural Networks explained.\\\\n\\\\nOne of the most popular and widely adopted tasks for graph neural networks is node classification, where each node in the training/validation/test set is assigned a ground truth category from a set of predefined categories.\\\\n\\\\nTo classify nodes, graph neural network performs message passing to utilize the node\\xe2\\x80\\x99s own features, but also its neighboring node and edge features.\\\\n\\\\nAll credits to\\\\nhttps://docs.dgl.ai/en/latest/guide/training-node.html\\\\nhttps://docs.dgl.ai/en/latest/guide/training-edge.html\\\\nhttps://docs.dgl.ai/en/latest/guide/training.html#guide-training-heterogeneous-graph-example\\\\n\\\\n00:00 Intro\\\\n02:10 Code DGL \\\\n02:50 Code Node Classification\\\\n13:27 Heterogeneous Graph Node Classification\\\\n15:00 Code EDGE Classification\\\\n26:33 Heterogeneous Graph Edge Classification\\\\n\\\\nThanks to Canva (canva.com) for providing of free version of canva to the global community.   \\xc2\\xa0@canva\\xc2\\xa0 \\\\n\\\\n#graphs \\\\n#machinelearning \\\\n#datascience\"",
    "lengthSeconds": "1847",
    "uploadDate": "2022-12-21",
    "thumbnail_url": "https://i.ytimg.com/vi/t71zMCsAoEY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=T299uwZK-XI",
    "title": "Uncover the Secrets of Graph Data: Train, Test and Validation GRAPH Datasets  |  GraphML PyG DGL",
    "tags": "Graph data Set, PyG, Training data set, Validation data set, test data set, Graph Data Science, GraphML",
    "scraped_at": 1685113821.8728676,
    "genre": "Science",
    "views": "286",
    "desc": "Short intro to splitting into Training data set, Validation data set and Test data set. Mistakes that happened to me when working with Graph data sets. \\\\n\\\\n#graphs \\\\n#machinelearning \\\\n#datasets\"",
    "lengthSeconds": "993",
    "uploadDate": "2022-12-19",
    "thumbnail_url": "https://i.ytimg.com/vi/T299uwZK"
  },
  {
    "link": "watch?v=wxJ84sMJfUA",
    "title": "CODE: GRAPH Link Prediction w/ DGL on Pytorch and PyG  Code Example | GraphML | GNN",
    "tags": "Graph ML, Graph Machine learning, Link prediction, recommender systems, PyG, DGL, Deep Graph Library",
    "scraped_at": 1685113821.6798615,
    "genre": "Science",
    "views": "2379",
    "desc": "For Graph ML we make a deep dive to code LINK Prediction on Graph Data sets with DGL and PyG. We examine the main ideas behind LINK Prediction and how to code a link prediction example in PyG and DGL - Deep Graph Library. DGL - Easy Deep Learning on Graphs with framework agnostic coding (either PyG or TensorFlow2).\\\\n\\\\nA GNN-based link prediction model represents the likelihood of connectivity between two nodes u and v as a function of their node representation computed from the multi-layer GNN.\\\\n\\\\nTraining a link prediction model involves comparing the scores between nodes connected by an edge against the scores between an arbitrary pair of nodes. For example, given an edge connecting u and v, we encourage the score between node u and v to be higher than the score between node u and a sampled node v\\xe2\\x80\\xb2 from an arbitrary noise distribution v\\xe2\\x80\\xb2\\xe2\\x88\\xbcPn(v). Such methodology is called \\\\\"",
    "lengthSeconds": "1276",
    "uploadDate": "2022-12-17",
    "thumbnail_url": "https://i.ytimg.com/vi/wxJ84sMJfUA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=aHCpi4nqR8M",
    "title": "The Future: ChatGPT 2, STABLE DIFFUSION 3 | Tech AI Science",
    "tags": "Stable Diffusion, ChatGPT, InstructGPT, Reinforcement Learning from Human Feedback",
    "scraped_at": 1685113822.069868,
    "genre": "Science",
    "views": "954",
    "desc": "The tech future of ChatGPT and Stable Diffusion examined. Generative AI future for text, image and video. Two evolution scenarios.\\\\n\\\\nCombined use of ChatGPT and Stable Diffusion will lead to new tech implementations and possibilities for generative AI movies and Generative VR (GVR). Especially in combination with topologically lifted message passing and neural information retrieval systems. \\\\n\\\\nRecommended videos:\\\\n\\\\n\\\\\"",
    "lengthSeconds": "1490",
    "uploadDate": "2022-12-15",
    "thumbnail_url": "https://i.ytimg.com/vi/aHCpi4nqR8M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=UgcrTe51q6M",
    "title": "Learn about Reinforcement Learning from Human Feedback - ChatGPT / RLHF  HuggingFace Course",
    "tags": "film, udost",
    "scraped_at": 1685113822.1298406,
    "genre": "Science",
    "views": "718",
    "desc": "2 excellent sources to learn RLHF: Reinforcement Learning from Human Feedback, as used in ChatGPT. \\\\n\\\\nFirst resource is HuggingFace\\'s new (free) RL COURSE!\\\\nUnit 1 is already online! @HuggingFace \\\\nhttps://huggingface.co/blog/deep-rl-intro\\\\n\\\\nSecond resource is HuggingFace\\'s Blog on\\\\n\\\\\"",
    "lengthSeconds": "169",
    "uploadDate": "2022-12-14",
    "thumbnail_url": "https://i.ytimg.com/vi/UgcrTe51q6M/hqdefault.jpg"
  },
  {
    "link": "watch?v=GMkszGzFOgw",
    "title": "ChatGPT explained: A Guide to Conversational AI w/ InstructGPT, PPO,  Markov,  RLHF",
    "tags": "ChatGPT, InstructGPT, OPENAI, PPO, Reinforcement Learning, LLM",
    "scraped_at": 1685113822.0068715,
    "genre": "Science",
    "views": "6597",
    "desc": "How does ChatGPT work? Given the training details from OpenAI about InstructGPT, I explain in simple terms how ChatGPT can reproduce such great results, given a simple prompt. And what Reinforcement Learning from Human Feedback (RLHF) means in detail, incl PPO, Markov , ...\\\\n\\\\nIf you like easy explanations with visualizations, this is the video for you. Additional literature is available here:\\\\n\\\\nhttps://spinningup.openai.com/en/latest/algorithms/ppo.html\\\\nhttps://arxiv.org/pdf/1707.06347.pdf\\\\nhttps://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d\\\\n\\\\nThe Images in-between | Before Diffusion: Variational Autoencoder VAE explained w/ KL Divergence:\\\\nhttps://youtu.be/5GeUn4B9XYQ\\\\n\\\\n00:00 Instruct GPT, ChatGPT\\\\n01:42 ChatGPT Create Demonstration Data set\\\\n05:33 Create Ranking Data set\\\\n07:36 Proximal Policy Optimization Algo\\\\n09:37 Policy gradients and Reinforcement Learning \\\\n12:45 Supervised Policy model and Reward model interact\\\\n14:20 ChatGPT explained \\\\n\\\\nAll credits with OpenIA. \\\\n\\\\nThanks to Canva (canva.com) for providing of free version of canva to the global community.   \\xc2\\xa0@canva\\xc2\\xa0 \\\\n\\\\n#chatgpt \\\\n#theory \\\\n#ai\"",
    "lengthSeconds": "1116",
    "uploadDate": "2022-12-12",
    "thumbnail_url": "https://i.ytimg.com/vi/GMkszGzFOgw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=OppY5WxUiiE",
    "title": "VISUAL Code: Graph LINK PREDICTION | How AI Recommender work | Graph ML",
    "tags": "GNN, Graph Neural Networks, Link prediction, recommender systems, Node embedding, GCN",
    "scraped_at": 1685113822.4108393,
    "genre": "Science",
    "views": "316",
    "desc": "HOW does LINK PREDICTION work in recommender systems for graph neural networks? Is it the intelligent code in GRAPH ML? Or is the secret of node embedding and link prediction somewhere else? In the topological structure of the computational graph?\\\\n\\\\nExplore the secret of why link prediction works in GRAPH ML with an easy intuitive introduction to GNN \\\\u0026 Graph ML.\\\\n\\\\nThanks to Canva (canva.com) for providing of free version of canva to the global community.   \\xc2\\xa0@canva\\xc2\\xa0 \\\\n\\\\n#GraphML\\\\n#datascience  #ai\"",
    "lengthSeconds": "969",
    "uploadDate": "2022-12-11",
    "thumbnail_url": "https://i.ytimg.com/vi/OppY5WxUiiE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=7K1CL0YzkhE",
    "title": "Stable Diffusion 2.1: NEGATIVE Prompts, NEW NATURE Parameter | Generative Nature AI Images",
    "tags": "film, udost",
    "scraped_at": 1685113822.4818404,
    "genre": "Science",
    "views": "3682",
    "desc": "Negative prompts w/ Stable Diffusion V2.1 explored, and NEW high performance prompt QUALIFIER tested for generative nature and landscape photography! \\\\nExperience a first look with me on a free COLAB Notebook, when we create images 768x1200px from our (little bit advanced) text prompts.\\\\n\\\\nNegative prompts (which are new in V2.1) are explored in detail on nature photography generation (Generative AI) and landscape photography simulation. \\\\n\\\\nA NEW SURPRISING QUALIFIER for nature photography in SD V2.1!\\\\n\\\\nIn my experience negative prompts on photography STYLES provide no advantages. But the new qualifier works GREAT!\\\\n\\\\nExperience yourself (all credits with StabilityAI):\\\\nhttps://huggingface.co/stabilityai/stable-diffusion-2-1\\\\n\\\\n#stablediffusion2 \\\\n#stablediffusion \\\\n#texttoimage\"",
    "lengthSeconds": "412",
    "uploadDate": "2022-12-08",
    "thumbnail_url": "https://i.ytimg.com/vi/7K1CL0YzkhE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=NQexDkbkSTI",
    "title": "Stable Diffusion 2.1 - NEW Release! First look at SD2.1 on free COLAB Notebook",
    "tags": "film, udost",
    "scraped_at": 1685113822.9138403,
    "genre": "Science",
    "views": "799",
    "desc": "New version of Stable Diffusion V2.1 was published today (Dec 8). SD 2.1!\\\\nExperience a first look with me on a free COLAB Notebook, when we create images 768x768px and with attention slicing 768x1200px from our simple text prompts.\\\\n\\\\nNo SD v2.1 prompt engineering on this short demo. Negative prompts (which are new in V2.1) will be explored in my next video.\\\\n\\\\nExperience yourself (all credits with StabilityAI):\\\\nhttps://huggingface.co/stabilityai/stable-diffusion-2-1\\\\n\\\\n#stablediffusion2 \\\\n#stablediffusion \\\\n#texttoimage\"",
    "lengthSeconds": "266",
    "uploadDate": "2022-12-08",
    "thumbnail_url": "https://i.ytimg.com/vi/NQexDkbkSTI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=K9w7-4N_ZCI",
    "title": "Unlocking the Potential of Message Passing: Exploring GraphSAGE, GCN and GAT | GNN GraphML",
    "tags": "GraphSAGE, GCN, GAT, Message Passing, GNN, Graph Neural Networks, GRAPH ML",
    "scraped_at": 1685113824.8058677,
    "genre": "Science",
    "views": "1009",
    "desc": "Introduction to GRAPH ML, Graph Neural Networks (GNN) and the main idea behind Message Passing in graph network configurations of GraphSAGE, GCN and GAT.\\\\nMessage passing applied to Graph Convolutional Networks (GCN), GraphSAGE and Graph Attention Networks. The key difference between GAT and GCN is how the information from the k-hop neighborhood is aggregated. \\\\n\\\\nStanford online: CS224W\\\\nhttps://www.youtube.com/watch?v=JAB_plj2rbA\\\\u0026list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn\\\\n\\\\n#ai \\\\n#graphs \\\\n#theory\"",
    "lengthSeconds": "1116",
    "uploadDate": "2022-12-07",
    "thumbnail_url": "https://i.ytimg.com/vi/K9w7"
  },
  {
    "link": "watch?v=10evf7xmXto",
    "title": "PyG + SBERT: Heterogeneous Graphs Using SBERT SentenceTransformers for Node Classification SBERT 46",
    "tags": "PyG, Heterograph, Heterogeneous Graph Learning, GNN, GCN, SBERT, Sentence Transformers, Sentence Embeding",
    "scraped_at": 1685113823.3008661,
    "genre": "Science",
    "views": "819",
    "desc": "PyG w/ SBERT Sentence Transformers for Node Classification in heterogeneous Graphs, coded in PyG (PyTorch geometric) on a free COLAB NB. ML on GRAPHS.\\\\n\\\\nGraph-structured data such as social graphs, networks in cybersecurity, or molecular representations are our real-world scenarios which generate heterogeneous Graphs, on which to apply our ML models (Node2Vec, Message Passing MP-GNN, GCN - Graph Convolutional Networks) for prediction of node classification or simply classical link prediction.\\\\n\\\\nDetecting fraudulent entities in the network in cybersecurity can be a node classification problem. Therefore we will focus on NODE CLASSIFICATION on heterogeneous Graphs. And code our algorithms in PyG.\\\\n\\\\nMost real-world datasets are stored as heterogeneous graphs, like graphs in the area of recommendation  (social graphs) are indeed heterogeneous.  They store information about different types of entities (nodes) and their different types of relations. This tutorial introduces how heterogeneous graphs and their Node and Edge information (node feature tensor and edge feature tensor) are mapped to PyG and how they can be transformed as input to ML on Graph Neural Network models.\\\\n\\\\nAll rights of code w/ PyTorch geometric:\\\\nhttps://pytorch-geometric.readthedocs.io/en/latest/notes/heterogeneous.html\\\\n\\\\n#pytorch \\\\n#geometric\\\\n#heterogeneous \\\\n#graphs\"",
    "lengthSeconds": "2547",
    "uploadDate": "2022-12-05",
    "thumbnail_url": "https://i.ytimg.com/vi/10evf7xmXto/maxresdefault.jpg"
  },
  {
    "link": "watch?v=u6RdjuUaQGU",
    "title": "Node Classification w/ GRAPH CONVOLUTIONAL Networks for GraphML",
    "tags": "PyG, Pytorch geometric, CORA Dataset, GCN, Graph Convolutional Layers, Graph Convolutional Networks, GNN",
    "scraped_at": 1685113826.42384,
    "genre": "Science",
    "views": "402",
    "desc": "PyG coding example (on node classification) for GCN: A choice of convolutional NN architecture motivated via a localized first-order approximation of spectral graph convolutions. The GCN model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.\\\\n\\\\nSEMI-SUPERVISED CLASSIFICATION WITH\\\\nGRAPH CONVOLUTIONAL NETWORKS\\\\nThomas N. Kipf, Max Welling\\\\nhttps://arxiv.org/abs/1609.02907v4\\\\n\\\\nAll rights for code with PyG. \\\\nSee also:\\\\nhttps://paperswithcode.com/method/gcn\"",
    "lengthSeconds": "1276",
    "uploadDate": "2022-12-03",
    "thumbnail_url": "https://i.ytimg.com/vi/u6RdjuUaQGU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DNy4UhBrOKI",
    "title": "ChatGPT and Flan-T5 LLM  |  Proprietary vs FREE AI |  plus Performance tuning of Hyper-Parameters",
    "tags": "ChatGPT, Flan",
    "scraped_at": 1685113822.6288393,
    "genre": "Science",
    "views": "5916",
    "desc": "New ChatGPT by OpenAI is only free in this research preview. This LLM compared with a real free FLAN-T5 Large Language model by Google. Flan-T5 is freely available in 5 model sizes (small to xxl) on HuggingFace. We test the Flan-T5 model \\\\\"",
    "lengthSeconds": "704",
    "uploadDate": "2022-12-02",
    "thumbnail_url": "https://i.ytimg.com/vi/DNy4UhBrOKI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_Qf_SiCLzw4",
    "title": "NEW Flan-T5 Language model | CODE example | Better than ChatGPT?",
    "tags": "LLM, Large Language Models, Flan",
    "scraped_at": 1685113822.5598414,
    "genre": "Science",
    "views": "12701",
    "desc": "Currently my preferred LLM: FLAN-T5. Watch my code optimization and examples. \\\\nReleased Nov 2022 - it is an enhanced version of T5. Great for few-shot learning. \\\\n\\\\n(By the way, I had to provide my mail and telephone number to OpenAI for a preview test of ChatGPT, only during the research preview, usage of ChatGPT is free. Plus: ChatGPT is currently not available on HuggingFace. And you know ... proprietary AI ...)\\\\n\\\\nWe directly use /code FLAN-T5 for inference from Huggingface, without further fine-tuning the models weights. \\\\n\\\\nAll rights with:\\\\nhttps://huggingface.co/docs/transformers/main/en/model_doc/flan-t5\\\\nhttps://arxiv.org/pdf/2210.11416.pdf\\\\n\\\\n#machinelearning \\\\n#ai \\\\n#datascience\"",
    "lengthSeconds": "647",
    "uploadDate": "2022-12-01",
    "thumbnail_url": "https://i.ytimg.com/vi/_Qf_SiCLzw4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vuv2hHKf0to",
    "title": "PyG - PyTorch Geometric - Intro to Graph Neural Networks - Outlook SBERT w/ PyG",
    "tags": "PyTorch, PyTorch Geometric, PyG, Intro, GCN, Graph Neural Networks",
    "scraped_at": 1685113824.47884,
    "genre": "Science",
    "views": "631",
    "desc": "PyG - PyTorch Geometric - is a library for Graph Neural Networks, based on PyTorch, for ML and geometric learning.\\\\n\\\\nThis lookout on my next 2 YouTube videos w/ PyG Code on homogeneous Graphs provides insights and analyses the problem w/ heterogeneous Graphs, especially when applying SBERT (Sentence Transformers) to scientific documents w/ PyG. \\\\n\\\\n#pytorch \\\\n#geometric \\\\n#graphs\"",
    "lengthSeconds": "643",
    "uploadDate": "2022-11-29",
    "thumbnail_url": "https://i.ytimg.com/vi/vuv2hHKf0to/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ThjvQt3QBvI",
    "title": "Databricks Distributed File System DBFS vs  my FILES on local driver node of Spark cluster",
    "tags": "Databricks, DBFS, Distributed File System, Local Driver Node, Databricks Community edition",
    "scraped_at": 1685113823.225841,
    "genre": "Education",
    "views": "456",
    "desc": "Q by my viewers: When work with files on the local driver node of Spark cluster vs distributed Databricks DBFS? \\\\n\\\\nDatabricks File System (DBFS) is a distributed file system mounted into a Databricks workspace and available on Databricks clusters. DBFS is an abstraction on top of scalable object storage.\\\\n\\\\nDatabricks uses a FUSE mount to provide local access to files stored in the cloud. A FUSE mount is a secure, virtual filesystem.\\\\n\\\\nCredits to:\\\\nhttps://docs.databricks.com/data/databricks-file-system.html\\\\n\\\\n#databricks \\\\n#dbfs \\\\n#DriverNode_SparkCluster\\\\n#pyspark \\\\n#cluster \\\\n#deeplearning \\\\n#ai\"",
    "lengthSeconds": "933",
    "uploadDate": "2022-11-27",
    "thumbnail_url": "https://i.ytimg.com/vi/ThjvQt3QBvI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wenWvM6Xriw",
    "title": "Stable Diffusion 2.0 on free COLAB NB | 768x1200 px | CODE LLM DEMO SD 2.0",
    "tags": "Stable Diffusion, Stable Diffusion V2, Colab NB",
    "scraped_at": 1685113824.9958398,
    "genre": "Science",
    "views": "616",
    "desc": "Code live with me Stable Diffusion 2.0 on a free COLAB NB in Pytorch. \\\\nImages are 768x1200px or higher. Don\\'t wait for a free slot at HuggingFace spaces, code it yourself!\\\\n\\\\nStable Diffusion v2.0 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 865M UNet and OpenCLIP ViT-H/14 text encoder for the diffusion model. The SD 2.0-v model produces 768x768px outputs.\\\\n\\\\nNote: This script by StabilityAI incorporates an invisible ! watermarking ! of the outputs, to help viewers identify the images as machine-generated. \\\\n\\\\nAll rights at @Stability_AI    and     @HuggingFace \\\\n\\\\nCode (Pytorch) to follow along:\\\\nhttps://huggingface.co/stabilityai/stable-diffusion-2\\\\n\\\\n#stablediffusion \\\\n#stablediffusion2\\\\n#colab\"",
    "lengthSeconds": "350",
    "uploadDate": "2022-11-25",
    "thumbnail_url": "https://i.ytimg.com/vi/wenWvM6Xriw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=o1tEVjdqHBE",
    "title": "Exploring the Impact of Social Media on the Future of AI Systems",
    "tags": "Sentence Transformers, News content, Social media, AI, ML",
    "scraped_at": 1685113825.2478676,
    "genre": "Education",
    "views": "69",
    "desc": "FOX vs CNN and then .. pure Science based GALACTICA? Even GALACTICA? \\\\\"",
    "lengthSeconds": "58",
    "uploadDate": "2022-11-24",
    "thumbnail_url": "https://i.ytimg.com/vi/o1tEVjdqHBE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GywIF1fzeOE",
    "title": "How-to Self-Study ML / DL / AI for Free",
    "tags": "Machine Learning, Artificial intelligence, Self",
    "scraped_at": 1685113822.6968389,
    "genre": "Science",
    "views": "210",
    "desc": "Short Intro to platform \\\\\"",
    "lengthSeconds": "450",
    "uploadDate": "2022-11-22",
    "thumbnail_url": "https://i.ytimg.com/vi/GywIF1fzeOE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6NkS3Re6LiM",
    "title": "Experimental KERAS Feature Space for advanced Feature scaling | DEMO:  Your chance of a heart attack",
    "tags": "TensorFlow 2.12 dev, Medical AI demo, Technical demonstration, Feature Space, KERAS, Data Pipeline, Data standardization, Data Normalization, Different Data Scales, ML, Machine Learning, AI, python3, Colab NB, Structured data classification",
    "scraped_at": 1685113823.7488678,
    "genre": "Science",
    "views": "127",
    "desc": "We code in real time an experimental KERAS Feature Space for advanced Feature scaling on a medical AI system, using COLAB. \\\\n\\\\nAnalyzing dozens of features from your individual lab results, we apply an experimental new Feature Space for coherent feature scaling (normalization) and build an optimized data pipeline (ETL) to train our ML model on cloud based GPUs or TPUs. \\\\n\\\\nPlus we code an inference model for analyzing your next (hypothetical) virtual patients\\' probability for a eg. heart attack, given the training data provided in this code demonstration of a medical AI system. \\\\n\\\\nStructured Data Classification w/ new Feature Space.\\\\n\\\\nThis is a technical demo for code4AI insights only, under no circumstances applicable to any real human. \\\\n\\\\nAll rights are w/ KERAS \\\\u0026 TensorFlow team!\\\\n\\\\nTo follow along this coding exercise, the official KERAS Jupyter NB is here:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/structured_data/ipynb/structured_data_classification_with_feature_space.ipynb\\\\n\\\\nPlease note, that as demonstrated in the video, at current state in time you have to install tensorflow-nightly development version (TF2.12 dev) to experience this new feature (keras.util.FeatureSpace).\\\\n\\\\n#medical \\\\n#ai \\\\n#datascience \\\\n#machinelearning \\\\n\\\\n00:00 Health report, Lab work\\\\n03:00 Feature Space \\\\u0026 Data normalization\\\\n09:50 CODE Structured Data Classification \\\\n25:10 Two ML models for training and inference\\\\n28:07 TensorFlow 2.12 dev update\\\\n31:55 Predict probability for new virtual patients\"",
    "lengthSeconds": "2177",
    "uploadDate": "2022-11-20",
    "thumbnail_url": "https://i.ytimg.com/vi/6NkS3Re6LiM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=G8wQAlVGYVM",
    "title": "The Intelligent Future of Data Warehouses & Delta Lake | Structured Streaming w/ PySPARK",
    "tags": "Database, Data Warehouse, Data Lake, Delta Lake, Databricks, Big Data, Petabyte, Lakehouse, lakehouse architecture",
    "scraped_at": 1685113826.1628435,
    "genre": "Science",
    "views": "108",
    "desc": "Intelligent DATA for data science? Is it not enough that AI is intelligent? Now DATA + AI, both intelligent? \\\\nRapid evolution from an Egyptian Warehouse 4000 years ago with a classical ETL to an advanced Lake House architecture on Lake Databricks, with Structured Streaming on SPARK. Including: Delta Lake, Unify Catalog,  Delta Live Tables, Apache Iceberg, .. \\\\n\\\\nTechnology is of value when it generates value. The technical warehouse architecture evolution from a classical Warehouse to a Data lake to Big Query ML to Data Science on petabyte scale with Databricks and Spark with it new Lakehouse architecture for batch and structured streaming. 3 code examples in SPARK, SQL and PySpark. \\\\n\\\\nApache SPARK is currently at use at 80% of Fortune 500 companies in US (according to Databricks). Does industry care about TensorFlow2 or PyTorch at all? My next video will explore current job qualifications from high-tech consultants to Digital Health, to Biotech and Pharmaceutical.\\\\n\\\\n#lakehouse  \\\\n#warehouse \\\\n#databricks\\\\n#datascience \\\\n#pyspark \\\\n\\\\nCover image by Gerd Altmann from Pixabay\"",
    "lengthSeconds": "1907",
    "uploadDate": "2022-11-18",
    "thumbnail_url": "https://i.ytimg.com/vi/G8wQAlVGYVM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1Ycodhrd0S8",
    "title": "Galactica: New LLM by Meta hallucinates Science - First Look",
    "tags": "LLM, Science LLM, Meta, Science",
    "scraped_at": 1685113823.6168675,
    "genre": "Science",
    "views": "1508",
    "desc": "Galactica: a brand-new large language model (LLM) on Science: trained on over 48 million papers, textbooks, reference material, compounds, proteins and other sources of scientific knowledge.\\\\n\\\\nBut there is a warning: This LLM can hallucinate scientific text. A machine (without any understanding of the content) adds with a probability distribution some pieces of text together. Now the probability distribution has been trained on real scientific text, but the output is not!\\\\n\\\\nIf you want to learn about LLMs:\\\\nhttps://youtu.be/rrZGIR5CryM\\\\nhttps://youtu.be/hHL92ch2J1c\\\\nhttps://youtu.be/ZrD8IZ9tIAs\\\\n\\\\nExperience Galactica for yourself:\\\\nhttps://galactica.org/\\\\n\\\\n#ai   \\\\n#science \\\\n#machinelearning  \\\\n#datascience\"",
    "lengthSeconds": "506",
    "uploadDate": "2022-11-15",
    "thumbnail_url": "https://i.ytimg.com/vi/1Ycodhrd0S8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=T5bHvMO-FLk",
    "title": "Preparing for the Future: A Guide to AI Job Qualifications in Industry for 2023",
    "tags": "AI, AI Jobs, Data Scientist, Senior Data Scientist, ML Research Scientist, Jobs in 2023",
    "scraped_at": 1685113824.21284,
    "genre": "Science",
    "views": "141",
    "desc": "$ 200K AI Jobs in Industry 2023: What you need. Optimize your AI Job application.\\\\nA real-world check for the content of my YouTube channel and a guide for your next job application in consulting, digital health, biotech and pharmaceutical industries. \\\\n\\\\nOptimize your AI Job application: ML qualification profile for Data Scientists, Senior Data Scientist, Principal Data Scientist, Head ML Research as publicly published by companies on Nov 8, 2002 on glassdoor.com\\\\n\\\\nNot sponsored by anyone. \\\\n\\\\n#jobs \\\\n#jobapplication \\\\n#ai \\\\n#biopharma \\\\n#consulting \\\\n#pharmaceutical \\\\n#jobprofile\"",
    "lengthSeconds": "2604",
    "uploadDate": "2022-11-15",
    "thumbnail_url": "https://i.ytimg.com/vi/T5bHvMO"
  },
  {
    "link": "watch?v=Sd-Qc7s1roA",
    "title": "WHY JAX? Why the Hell a 3rd ML framework in 2023?",
    "tags": "film, udost",
    "scraped_at": 1685113826.5188491,
    "genre": "Science",
    "views": "3736",
    "desc": "100x faster on GPU. Why do I need a 3rd Machine Learning Framework, next to TensorFlow2 and PyTorch? Automatically scalable Cloud infrastructure (TPU Pods v4) on Google Cloud and Nvidia H100 SuperPods (... AWS) with XLA compiler are fast! Really fast! \\\\n\\\\nBased on my last video ( ) we look at a new powerful ML framework from the cloud infrastructure side. Primary factors are SPEED, more SPEED, SCALE, Memory and COSTS ($$) for ML training and inference at cloud supercomputer. \\\\n\\\\n#ai \\\\n#datascience \\\\n#pytorch \\\\n#tensorflow2 \\\\n#jax \\\\n#machinelearning\"",
    "lengthSeconds": "1055",
    "uploadDate": "2022-11-13",
    "thumbnail_url": "https://i.ytimg.com/vi/Sd"
  },
  {
    "link": "watch?v=9canD-CiGiQ",
    "title": "Your next ML (Cloud) Infrastructure for your Code",
    "tags": "NVIDIA, H100 GPU, TPU v4, XLA Complier, JAX, Tensorflow3, DTensors, XLA Tensors, Data parallelism, Model parallelism, 2023",
    "scraped_at": 1685113825.768842,
    "genre": "Science",
    "views": "1283",
    "desc": "Which ML Framework is best for the new CLOUD infrastructure (independent if NVIDIA H100 or GOOGLE TPUs)?  The future of Machine Learning Accelerators (NVIDIA Tensor Core H100 GPU and Google\\'s TPU Pod v4) w/ ML compiler = XLA.\\\\nPlus JAX and TensorFlow3 for new, optimized ML Cloud computing in 2023. \\\\n\\\\nThere could be a winner if you want pure speed and auto-cloud-parallelism over 1000s of TPU-chips v4 for you advanced ML models. \\\\n\\\\n#nvidia \\\\n#h100 \\\\n#tpu \\\\n#xla \\\\n#cloudcomputing\"",
    "lengthSeconds": "1332",
    "uploadDate": "2022-11-09",
    "thumbnail_url": "https://i.ytimg.com/vi/9canD"
  },
  {
    "link": "watch?v=bf1BML64MSI",
    "title": "Topological Message Passing on GNN  |  SIMPLICIAL COMPLEXES on CW Networks  #ai",
    "tags": "Algebraic topology, Message Passing, Topological Message Passing, CW Networks",
    "scraped_at": 1685113823.8198664,
    "genre": "Science",
    "views": "291",
    "desc": "We go from Message Passing GNN (MPGNN) to TOPOLOGICAL Message Passing on CW Networks: Lifting a Graph to a higher topological space allows for high-dimensional interactions (greater than 2) given our higher-dim topological spaces. Computational Graph Neural Networks increase its complexities and n-body interactions (eg chemistry, pharma, molecular design). We go from Message Passing GNN (MPGNN) to TOPOLOGICAL Message Passing on CW Networks. \\\\n\\\\nFirst part of this video is here:\\\\nhttps://youtu.be/Xiy_bD1CHro\\\\n\\\\nAll credits go to:\\\\n\\\\nMichael Bronstein, \\\\\"",
    "lengthSeconds": "1506",
    "uploadDate": "2022-11-07",
    "thumbnail_url": "https://i.ytimg.com/vi/bf1BML64MSI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Xiy_bD1CHro",
    "title": "Discover the Future of Graph Neural Networks | Beyond message passing",
    "tags": "Graph Neural Networks, GNN, Graphs, Limitations, Algebraic Topology",
    "scraped_at": 1685113823.8918662,
    "genre": "Science",
    "views": "610",
    "desc": "The next step for Graph Neural Networks (GNN). Based on their underlying topological spaces, they operate on. Graphs are per definition DYADIC Systems. The future of GNN explained.\\\\n\\\\nMY other videos on Graph Neural Networks (as mentioned):\\\\nhttps://youtu.be/11bAAy8b4sI\\\\nhttps://youtu.be/dBeYBjjxQqU\\\\nhttps://youtu.be/0KH95BEz370\\\\n\\\\nMy complete playlist on GNN in general (25 videos with code):\\\\nhttps://www.youtube.com/playlist?list=PLgy71-0-2-F13J3EABiLDfedF3tlejgHJ\\\\n\\\\n\\\\nAll credits (for the content of this video) go to the following publications:\\\\n\\\\nMichael Bronstein, \\\\\"",
    "lengthSeconds": "845",
    "uploadDate": "2022-11-05",
    "thumbnail_url": "https://i.ytimg.com/vi/Xiy_bD1CHro/maxresdefault.jpg"
  },
  {
    "link": "watch?v=w6brndJc5OA",
    "title": "CODE for InPainting w/ Stable Diffusion | Generative AI | Change Persons in a Photo",
    "tags": "Stable Diffusion, Inpainting, generative AI, COLAB",
    "scraped_at": 1685113822.7748406,
    "genre": "Science",
    "views": "183",
    "desc": "Change objects in a photo: Code a Jupyter NB w/ InPainting  using Stable Diffusion (a Generative AI tool) to create new objects or persons in the image, with Variational Autoencoders (VAE), UNets and maybe CLIP. New image synthesis. Generative AI.\\\\n\\\\nInpainting is a conservation process where damaged, deteriorated, or missing parts of an artwork are filled in to present a complete image. \\\\n\\\\nAll credits to:\\\\nhttps://huggingface.co/runwayml/stable-diffusion-v1-5\\\\n\\\\nFollow along w/ COLAB:\\\\nhttps://github.com/huggingface/notebooks/blob/main/diffusers/in_painting_with_stable_diffusion_using_diffusers.ipynb\\\\n\\\\n\\\\n@InProceedings{Rombach_2022_CVPR,\\\\n    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\\\\\\\\\\\"",
    "lengthSeconds": "531",
    "uploadDate": "2022-11-03",
    "thumbnail_url": "https://i.ytimg.com/vi/w6brndJc5OA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=j1_szOni5-0",
    "title": "SETFIT - HYPER Parameter Optimization for SBERT Text Classification (SBERT 45)",
    "tags": "SetFit, SBERT, text classification, Few",
    "scraped_at": 1685113823.037841,
    "genre": "Science",
    "views": "551",
    "desc": "Best Training Parameters for SetFit /SBERT Text classification: HyperParameter Optimization HPO. The SetFit system trains itself to find the BEST performance parameters. SBERT for text Classification. Few-shot Learning.\\\\n\\\\nAutotune the SetFit system to find the best performant system parameter for your system with your dataset.\\\\n\\\\nSetFit takes advantage of Sentence Transformers\\xe2\\x80\\x99 ability to generate dense embeddings based on paired sentences. In the initial fine-tuning phase stage, it makes use of the limited labeled input data by contrastive training, where positive and negative pairs are created by in-class and out-class selection.\\\\n\\\\n The Sentence Transformer model then trains on these pairs (or triplets) and generates dense vectors per example. In the second step, the classification head trains on the encoded embeddings with their respective class labels.\\\\n\\\\n At inference time, the unseen example passes through the fine-tuned Sentence Transformer, generating an embedding that when fed to the classification head outputs a class label prediction.\\\\n\\\\nAll credits to:\\\\nhttps://github.com/huggingface/setfit\\\\n\\\\nJupyter Notebook to follow along: \\\\nhttps://github.com/huggingface/setfit/tree/main/notebooks\\\\n\\\\n\\\\n\\\\n#ai \\\\n#naturallanguageprocessing \\\\n#sbert\"",
    "lengthSeconds": "1334",
    "uploadDate": "2022-11-01",
    "thumbnail_url": "https://i.ytimg.com/vi/j1_szOni5"
  },
  {
    "link": "watch?v=6gC_dR71hRA",
    "title": "CODE SetFit w/ SBERT for Text Classification (Few-Shot Learning) multi-class multi-label (SBERT 44)",
    "tags": "SBERT, Few",
    "scraped_at": 1685113825.0598676,
    "genre": "Science",
    "views": "1678",
    "desc": "How-to Code SBERT for Few-Shot Learning (SetFit). SetFit takes advantage of Sentence Transformers\\xe2\\x80\\x99 ability to generate dense embeddings based on paired sentences. \\\\n\\\\nIn the initial fine-tuning phase stage, it makes use of the limited labeled input data by contrastive training, where positive and negative pairs are created by in-class and out-class selection. \\\\n\\\\nThe Sentence Transformer model then trains on these pairs (or triplets) and generates dense vectors per example. In the second step, the classification head trains on the encoded embeddings with their respective class labels. \\\\n\\\\nAt inference time, the unseen example passes through the fine-tuned Sentence Transformer, generating an embedding that when fed to the classification head outputs a class label prediction.\\\\n\\\\nGiven a limited training sample set per class the new SetFit methodology based on SBERT Sentence Transformers perform exceptionally well in text classification. Either multi class or multi label classification of text /sentences. \\\\n\\\\nUnderstand the theory behind SetFit and the power of pre-trained SBERT Sentence transformers when applied for \\\\\"",
    "lengthSeconds": "1697",
    "uploadDate": "2022-10-30",
    "thumbnail_url": "https://i.ytimg.com/vi/6gC_dR71hRA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3hzry5ZtQwE",
    "title": "Generative AI Demo - Stable Diffusion (90 sec)",
    "tags": "Scify, Space, Stable Diffusion, Jupyter NB, Python, HuggingFace, Generative AI",
    "scraped_at": 1685113823.422842,
    "genre": "Science",
    "views": "217",
    "desc": "Hyper-parameter tuned Generative AI. Research only. Each Image generated w/ heavy prompt engineering and only 90 sec (entry-level) GPU runtime. Video demonstration of state-of-the-art Generative AI tools.\\\\n\\\\nSee my videos on Stable Diffusion for code to follow along (Jupyter NB, Colab):\\\\nhttps://www.youtube.com/playlist?list=PLgy71-0-2-F04wRy8iPp3IO4a8x-r6mWf\\\\n\\\\nAll license conditions from https://huggingface.co/spaces/CompVis/stable-diffusion-license apply.\\\\n\\\\nCredits for code/algorithms/Libraries go to:\\\\nhttps://huggingface.co/blog/stable_diffusion\\\\nhttps://huggingface.co/CompVis/stable-diffusion-v1-4\\\\n\\\\nMusic from YouTube Music Library.\\\\n\\\\n#ai \\\\n#aiart \\\\n#space \\\\n#stablediffusion\"",
    "lengthSeconds": "172",
    "uploadDate": "2022-10-30",
    "thumbnail_url": "https://i.ytimg.com/vi/3hzry5ZtQwE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5BGaJWf50xg",
    "title": "SETFIT Few-Shot Learning outperforms GPT-3 |  SBERT Text Classification (SBERT 43)",
    "tags": "SBERT, Few",
    "scraped_at": 1685113827.1078396,
    "genre": "Science",
    "views": "1004",
    "desc": "NEW: SBERT Few-Shot Learning (SetFit) outperforms GPT-3 in text classification tasks. Few-shot learning without prompts: SetFit.\\\\n\\\\nGiven a limited training sample set per class the new SetFit methodology based on SBERT Sentence Transformers perform exceptionally well in text classification. Either multi class or multi label classification of text /sentences. \\\\n\\\\nUnderstand the theory behind SetFit and the power of pre-trained SBERT Sentence transformers when applied for \\\\\"",
    "lengthSeconds": "1113",
    "uploadDate": "2022-10-28",
    "thumbnail_url": "https://i.ytimg.com/vi/5BGaJWf50xg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=oEksCgkU1GY",
    "title": "CODE Stable Diffusion: 8x parallel pipelines for ultimate SPEED #stablediffusion Generative AI",
    "tags": "Stable Diffusion, JAX, Free TPU, TPU, Tensor Processing Unit, Parallelization, COLAB, Jupyter, text",
    "scraped_at": 1685113823.1638677,
    "genre": "Science",
    "views": "677",
    "desc": "New speed of Stable Diffusion on 8 parallel devices with a free Tensor Processing Unit (TPU) in COLAB. Utilizing JAX and FLAX library for the first time on stable diffusion. Speed improvement: 8 pictures in 4 seconds! Generative AI for art?\\\\n\\\\nJAX is NumPy on the Tensor Processing Unit (TPU), with great automatic differentiation for high-performance machine learning research. https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\\\\n\\\\nTogether with @HuggingFace \\xf0\\x9f\\xa4\\x97 Diffusers lib, which provides pretrained vision JAX diffusion models! Amazing! https://huggingface.co/docs/diffusers/index\\\\nLicenses apply. \\\\n\\\\nAll credits to (and your Jupyter NB to follow along from):\\\\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_jax_how_to.ipynb\\\\n\\\\nsoftware{flax2020github,\\\\n  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},\\\\n  title = {{F}lax: A neural network library and ecosystem for {JAX}},\\\\n  url = {http://github.com/google/flax},\\\\n  version = {0.6.0},\\\\n  year = {2020},\\\\n}\\\\n\\\\n#tpu \\\\n#stablediffusion \\\\n#jax \\\\n#aiart \\\\n#generativeai\"",
    "lengthSeconds": "1006",
    "uploadDate": "2022-10-26",
    "thumbnail_url": "https://i.ytimg.com/vi/oEksCgkU1GY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=snUceAplmEI",
    "title": "CODE Stable Diffusion: Step by Step (PyTorch, VAE, UNet, CLIP) #stablediffusion Generative AI",
    "tags": "Sentence Transformers, UNet, VAE, Variational Autoencoder, AI, Text",
    "scraped_at": 1685113824.6718388,
    "genre": "Science",
    "views": "3723",
    "desc": "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. Licenses apply. Generative AI.\\\\n\\\\nIt\\'s trained on 512x512 images from a subset of the LAION-5B database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. AI Art. AI art tool. \\\\n\\\\nAll credits to:\\\\nhttps://huggingface.co/models?library=diffusers\\\\nhttps://huggingface.co/CompVis/stable-diffusion-v1-4\\\\n\\\\nColab NB to follow along:\\\\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=yW14FA-tDQ5n\\\\n\\\\n#ai \\\\n#stablediffusion \\\\n#naturallanguageprocessing \\\\n#text-to-image\\\\n#generativeai\"",
    "lengthSeconds": "568",
    "uploadDate": "2022-10-24",
    "thumbnail_url": "https://i.ytimg.com/vi/snUceAplmEI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=gLT4F96CKJ4",
    "title": "2 Amazing Ideas in Latent Diffusion Models LDM w/ VAE, U-Net & CLIP: Generative AI #stablediffusion",
    "tags": "Text",
    "scraped_at": 1685113827.2968416,
    "genre": "Science",
    "views": "863",
    "desc": "New Latent Diffusion Models, LDM by Rombach \\\\u0026 Blattmann, 2022, run the diffusion process in latent space instead of pixel space, making training cost lower and inference speed faster. Insights from a theoretical physicist applying Markov chains, UNet data augmentation theory. Keywords: stable ai art, generative AI.\\\\n\\\\nLDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with auto-encoder and then manipulate/generate semantic concepts with diffusion process on learned latent. Architecture wise Diffusion Models consists of Variational Autoencoders, a U-Net and CLIP Text Encoder (or BERT) for Generative AI. \\\\n\\\\nRemember: Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. \\\\n\\\\nThe key difference between standard diffusion and latent diffusion models: in latent diffusion the model is trained to generate latent (compressed) representations of the images.\\\\n\\\\nThere are three main components in latent diffusion models:\\\\n\\\\n    1.  Variational AutoEncoder (VAE).\\\\n    2.  A U-Net Data Augmentation (2015).\\\\n    3.  A text-encoder, e.g. CLIP\\'s Text Encoder.\\\\n\\\\n\\\\nExplained:\\\\nCompVis - Machine Vision and Learning LMU Munich\\\\nMachine Vision and Learning research group at Ludwig Maximilian University of Munich (formerly Computer Vision Group at Heidelberg University)\\\\n\\\\nNoticeable links:\\\\n\\\\nHigh-Resolution Image Synthesis with Latent Diffusion Models\\\\nhttps://arxiv.org/pdf/2112.10752.pdf\\\\n\\\\nU-Net: Convolutional Networks for Biomedical Image Segmentation\\\\nhttps://arxiv.org/pdf/1505.04597.pdf\\\\n\\\\nhttps://lilianweng.github.io/posts/2021-07-11-diffusion-models/\\\\nhttps://deepsense.ai/the-recent-rise-of-diffusion-based-models/\\\\n\\\\n00:00 Latent Diffusion Model explained \\\\n00:37 Nonequilibrium Thermodynamics 2015\\\\n02:32 Generative Markov Chains\\\\n05:10 UNet Data Augmentation 2015\\\\n06:39 UNet Architecture\\\\n08:12 LDM 2022 pretrained Autoencoders w/ cross-attention layers\\\\n10:18 Schema of LDM - Latent Diffusion Model\\\\n13:07 Summary 5 Videos \\\\n\\\\n#text-to-image\\\\n#stablediffusion \\\\n#ai \\\\n#generativeai\"",
    "lengthSeconds": "854",
    "uploadDate": "2022-10-23",
    "thumbnail_url": "https://i.ytimg.com/vi/gLT4F96CKJ4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4kPDqvT8i3k",
    "title": "1600px Stable Diffusion 1.4 w/ \ud83e\udd17 HuggingFace on free Colab for Panoramic AI Images, Generative AI",
    "tags": "Stable Diffusion, Free GPU, Colab Notebook, HuggingFace",
    "scraped_at": 1685113822.8458402,
    "genre": "Science",
    "views": "724",
    "desc": "Create 1600px pictures utilizing Stable Diffusion Generative AI with new COLAB NB w/ HuggingFace Hub on free GPU in 1 minute! New AI art tool. Licenses apply. Generative AI \\\\n\\\\nLatest Stable Diffusion implementation by HuggingFace on free COLAB Notebook:\\\\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\\\\n\\\\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. \\\\n\\\\nAll credits to HuggingFace Hub Diffusers Library and:\\\\nhttps://huggingface.co/CompVis/stable-diffusion-v1-4\\\\n\\\\nExperience Stable Diffusion Generative AI text-to-Image for yourself, insert your text prompt and save your pictures generated by AI. Please respect the legal and ethical guidelines for using this software and AI algo as specified by HuggingFace and therein mentioned contributors and license providers.\\\\n\\\\n#ai \\\\n#stablediffusion \\\\n#colab \\\\n#generativeai\"",
    "lengthSeconds": "488",
    "uploadDate": "2022-10-21",
    "thumbnail_url": "https://i.ytimg.com/vi/4kPDqvT8i3k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=FUtj2iqrC64",
    "title": "Understand Vector-Quantized Variational Autoencoder (VQ-VAE) for Image Generation #stablediffusion",
    "tags": "VQ",
    "scraped_at": 1685113822.979841,
    "genre": "Science",
    "views": "1153",
    "desc": "Vector-Quantized Variational Autoencoder (VQ-VAE) for Image Generation explained in Detail. The main component of DALL-E of 2020. On the road to DIFFUSION for text-to-video. Generative AI.\\\\n\\\\n3 videos before this video:\\\\nhttps://youtu.be/onuIy6PC1ic\\\\nhttps://youtu.be/BHzU-jq-AcM\\\\nhttps://youtu.be/pRKTr8gw2KA\\\\n\\\\nNext videos will focus on DIFFUSION mechanism and code implementation:\\\\nhttps://youtu.be/gLT4F96CKJ4\\\\n\\\\n\\\\nLinks:\\\\nhttps://arxiv.org/pdf/1711.00937.pdf\\\\n\\\\\"",
    "lengthSeconds": "1067",
    "uploadDate": "2022-10-20",
    "thumbnail_url": "https://i.ytimg.com/vi/FUtj2iqrC64/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5GeUn4B9XYQ",
    "title": "The Images in-between | Before Diffusion: Variational Autoencoder VAE explained w/ KL Divergence",
    "tags": "film, udost",
    "scraped_at": 1685113826.584868,
    "genre": "Science",
    "views": "355",
    "desc": "After coding VAE in Python (see my vid https://youtu.be/pRKTr8gw2KA), a lot of questions emerged from my viewers. Therefore: Variational AUTOENCODERS (VAE) explained: Theory of VAE. \\\\n\\\\nOn the road to Diffusion theory for text-to-image and text-to-video. Generative AI.\\\\n\\\\nThe answers to all your questions:\\\\n1. enforcing a probabilistic prior on the latent space\\\\n2. restrict our posterior approximation to Gaussian distribution.\\\\n3. KL-Div as a regularization of the posterior\\\\n4. keep thee latent distribution compact around a subspace.\\\\n5. smooth transitions of data points in latent space.\\\\n6. how can a generative system create such unseen images?\\\\n\\\\nFurther reading\\\\nhttps://jaan.io/what-is-variational-autoencoder-vae-tutorial/\\\\n\\\\nOutlook to Vector-Quantized Variational Autoencoder for text-to-image AI tools.\\\\n\\\\n#ai \\\\n#datascience \\\\n#stablediffusion\"",
    "lengthSeconds": "1551",
    "uploadDate": "2022-10-18",
    "thumbnail_url": "https://i.ytimg.com/vi/5GeUn4B9XYQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qN857BvqTTk",
    "title": "Code SEMANTIC Information Retrieval System w/ SBERT Sentence Transformers in PyTorch (SBERT 42)",
    "tags": "SBERT, Sentence Transformers, Q and A, Python, COLAB",
    "scraped_at": 1685113827.3668406,
    "genre": "Science",
    "views": "771",
    "desc": "Code SEMANTIC Information Retrieval (IR) System [ SBERT + PyTorch ] on your (secret) Corporate Body of Knowledge. This easy example can include millions of sentences describing eg your customer experience (CX), all your process descriptions,  your in-house knowledge ... \\\\n\\\\nWith SEMANTIC search you improve significantly above LEXICAL search algorithms. In this video you\\'ll see the performance of both system: Lexical search vs Semantic Search based on SBERT Sentence Transformers. Keyword: Document and Query Indexing with Transformers (BERT, SBERT).\\\\n\\\\nIf you want to follow along w/ coding:\\\\nhttps://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb\\\\n\\\\nAll credits to:\\\\nSBERT.net and \\\\nhttps://github.com/UKPLab/sentence-transformers\\\\n\\\\n00:00 Code SEMANTIC Information Retrieval System\\\\n04:04 Load Wikipedia, encode paragraphs\\\\n05:45 Lexical Search, baseline\\\\n06:25 SEMANTIC Search w/ SBERT Bi-Encoder (Siamese BERT) \\\\n13:25 UMAP \\\\u0026 3D Sentence /Paragraph visualization\\\\n18:05  SBERT CROSS-Encoder for detailed SEMANTIC SEARCH \\\\n19:42 Result IR Semantic System in PyTorch\\\\n\\\\n#datascience \\\\n#sbert \\\\n#searchengine \\\\n#naturallanguageprocessing\"",
    "lengthSeconds": "1427",
    "uploadDate": "2022-10-16",
    "thumbnail_url": "https://i.ytimg.com/vi/qN857BvqTTk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=iSCdrAlCUD4",
    "title": "How to Code spectacular 3D Clusters w/ Pandas DataFrames in Python - Interactive Animation",
    "tags": "3D, 3d visualization, data science, COLAB, Jupyter NB, Python3",
    "scraped_at": 1685113826.769839,
    "genre": "Science",
    "views": "311",
    "desc": "How-to Code: Watch Python code unfold multiple Data (Pandas Dataframe) Cluster Dynamics in 3D Animation (in real time) on a free COLAB Jupyter Notebook. \\\\n\\\\n#3d \\\\n#datascience \\\\n#datavisualization\"",
    "lengthSeconds": "1159",
    "uploadDate": "2022-10-14",
    "thumbnail_url": "https://i.ytimg.com/vi/iSCdrAlCUD4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=P_gRE9G7CHw",
    "title": "3 surprising Insights into starting YouTube in CS /AI",
    "tags": "AI, Future, Strategic goal, Video, YT",
    "scraped_at": 1685113825.8968408,
    "genre": "Education",
    "views": "31",
    "desc": "The future of your YouTube channel starts now. Also a valid statement for me. \\\\nDesign - focus - fail - stand up - learn - start new.\\\\n\\\\n#decisions \\\\n#new \\\\n#channel\"",
    "lengthSeconds": "318",
    "uploadDate": "2022-10-13",
    "thumbnail_url": "https://i.ytimg.com/vi/P_gRE9G7CHw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=jp_b2Aw1k4Q",
    "title": "Easy coding to Visualize Geospatial Data in Python | NEW Tutorial in COLAB",
    "tags": "Python, COLAB NB, 2d Visualization, Visualisation, 3d Visualization, Animation, Time series, GEO data",
    "scraped_at": 1685113823.3638394,
    "genre": "Science",
    "views": "215",
    "desc": "GEO visualization in Python: Watch  in real time Python code for interactive GEO Data Animation on a free COLAB notebook (openstreetmap, mapbox, ARCGIS, ..). \\\\n\\\\nGEO Data Animation of your Pandas df data or your csv data (of time series) w/ free and open-source Plotly Express. All credits to Plotly.com.\\\\n\\\\nGitHub Gist for your code to follow along:\\\\n\\\\n\\\\nNEXT: We will switch from Plotly Express to the FULL Plotly version. \\\\n\\\\n#animation  \\\\n#datascience \\\\n#geodata\"",
    "lengthSeconds": "630",
    "uploadDate": "2022-10-11",
    "thumbnail_url": "https://i.ytimg.com/vi/jp_b2Aw1k4Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zFKChohCLOE",
    "title": "When I Encounter AI in Job Interviews - how to win?",
    "tags": "AI, Monetization, YouTube channel, Inflection points, HR, Decision Intelligence, Health, CX",
    "scraped_at": 1685113827.1708653,
    "genre": "Education",
    "views": "36",
    "desc": "Your next online (automated) job interview will be analyzed and this personal profile will define you for your next employer. Try it out. Share my experience with inflections points in 4 specific industrial applications where (adequate?) AI systems are currently applied. Human resources. Health. Decision Intelligence. Customer Experience (CX).\"",
    "lengthSeconds": "827",
    "uploadDate": "2022-10-10",
    "thumbnail_url": "https://i.ytimg.com/vi/zFKChohCLOE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=e4wfwxKPjck",
    "title": "Pandas DataFrame DATA ANIMATION in COLAB NB | Python for Time Series #datavisualization",
    "tags": "Python, COLAB NB, 2d Visualization, Visualisation, 3d Visualization, Animation, Time series",
    "scraped_at": 1685113823.6798387,
    "genre": "Science",
    "views": "224",
    "desc": "Real time Python code for interactive Data Animation on your free COLAB notebook. Data Animation (financial data, business data, medical data, ..) of your Pandas df data or your CSV data (of time series) w/ free and open-source Plotly Express. All credits to Plotly.com.\\\\n\\\\nNext: GEO animations (interactive) with data from Pandas df or CSV file in your COALB NB and send it off as html files or place it on your homepage for interactive learning.\\\\n\\\\n#animation  \\\\n#datascience \\\\n#datavisualisation\"",
    "lengthSeconds": "620",
    "uploadDate": "2022-10-08",
    "thumbnail_url": "https://i.ytimg.com/vi/e4wfwxKPjck/maxresdefault.jpg"
  },
  {
    "link": "watch?v=U3D7SLSbpxk",
    "title": "Does my YouTube Channel need a more advanced Value Proposition?",
    "tags": "AI, YouTube channel, strategic positioning, time",
    "scraped_at": 1685113825.8308387,
    "genre": "Education",
    "views": "28",
    "desc": "How to build a new and advanced YouTube channel in AI \\\\u0026 Computer Science - strategic positioning of your channel in a dynamic market with multiple players and their ecosystems. Unlock your strategic advantage over your competitors and understand their strong position. \\\\n\\\\nYour competitors and your partners in time-to-market analysis give you further insights to your desired market segment and the value you can provide for your targeted audience. \\\\n\\\\n#ai \\\\n#youtube \\\\n#strategy\"",
    "lengthSeconds": "690",
    "uploadDate": "2022-10-07",
    "thumbnail_url": "https://i.ytimg.com/vi/U3D7SLSbpxk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Aag9D2S13tU",
    "title": "How To Code Awesome Python: Vibrant 3D Data Visualizations in free COLAB NB",
    "tags": "Python, COLAB NB, 2d Visualization, Visualisation, 3d Visualization",
    "scraped_at": 1685113826.646841,
    "genre": "Science",
    "views": "284",
    "desc": "Real time Python: Interactive 3D Data (from Pandas df) visualization on your free COLAB notebook. 3D interactive visualization of your Pandas df data or your CSV data w/ free and open-source Plotly Express. All credits to Plotly.com.\\\\n\\\\n\\\\nNext: ANIMATIONS (interactive) with data from Pandas df or CSV file in your COALB NB and send it off as html files or place it on your homepage for interactive learning.\\\\n\\\\n#3d  \\\\n#visualization \\\\n#pythontutorial\"",
    "lengthSeconds": "413",
    "uploadDate": "2022-10-06",
    "thumbnail_url": "https://i.ytimg.com/vi/Aag9D2S13tU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=NsdAQ8medg0",
    "title": "ONE strategic Advantage | Your YouTube channel",
    "tags": "AI, YouTube channel, strategic positioning, time",
    "scraped_at": 1685113824.9318395,
    "genre": "Education",
    "views": "35",
    "desc": "How to build a new and advanced YouTube channel in AI \\\\u0026 Computer Science - strategic positioning of your channel in a dynamic market with multiple players and their rivaling ecosystems. Unlock your strategic advantage over your competitors and understand their strong value proposition, community building and branding. And just in case, their open source or proprietary technology. \\\\n\\\\nYour competitors and your partners in time-to-market analysis. Prepare for emotional shock. \\\\n\\\\n#ai \\\\n#youtube \\\\n#strategy\"",
    "lengthSeconds": "778",
    "uploadDate": "2022-10-05",
    "thumbnail_url": "https://i.ytimg.com/vi/NsdAQ8medg0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=o_EKF0KCR6I",
    "title": "Learn to code in Python interactive 2D Data Visualization | COLAB Jupyter NB",
    "tags": "Python, COLAB NB, 2d Visualization, Visualisation",
    "scraped_at": 1685113826.9708672,
    "genre": "Science",
    "views": "98",
    "desc": "Watch in real time Python code for interactive 2 dimensional data visualization on your free COLAB notebook. 2D visualization of data in a Pandas df or CSV file w/ free and open-source Plotly Express. PLUS convert your interactive data visualization to HTML and send it to your co-workers or put it on your homepage.\\\\n\\\\nAll credits go to Plotly.com.\\\\n\\\\nGitHub Gist for your code to follow along:\\\\nhttps://gist.github.com/Data-topology/d95835bc9de75bc4665a797e73a3dd68\\\\n\\\\nNext: 3D data from Pandas df or CSV file \\\\n\\\\n#2d \\\\n#visualization \\\\n#pythontutorial\"",
    "lengthSeconds": "336",
    "uploadDate": "2022-10-04",
    "thumbnail_url": "https://i.ytimg.com/vi/o_EKF0KCR6I/maxresdefault.jpg"
  },
  {
    "link": "watch?v=7BvIMX7oycE",
    "title": "Competitive Ecosystems | YouTube Channels",
    "tags": "AI, YouTube channel, strategic positioning",
    "scraped_at": 1685113825.6388683,
    "genre": "Education",
    "views": "35",
    "desc": "How to build a new and advanced YouTube channel in AI \\\\u0026 Computer Science - strategic positioning of your channel in a dynamic market with multiple players and their ecosystems. \\\\n\\\\nYour competitors and your partners. \\\\n\\\\n#ai \\\\n#youtube \\\\n#strategy\"",
    "lengthSeconds": "780",
    "uploadDate": "2022-10-03",
    "thumbnail_url": "https://i.ytimg.com/vi/7BvIMX7oycE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Jdxs_GbQv6Q",
    "title": "The Battlefield emerges | Your YouTube channel",
    "tags": "AI, YouTube channel, strategic positioning",
    "scraped_at": 1685113825.18384,
    "genre": "Education",
    "views": "51",
    "desc": "A new YouTube channel?\\\\nYour competitors and your partners in a competitive Ecosystem. \\\\n\\\\n#youtube \\\\n#strategy \\\\n#ecosystem \\\\n#competitive\"",
    "lengthSeconds": "708",
    "uploadDate": "2022-10-01",
    "thumbnail_url": "https://i.ytimg.com/vi/Jdxs_GbQv6Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4qdCn_IVEeA",
    "title": "Be authentic | YouTube channel",
    "tags": "Tutorial, YouTube Channel, Computer Science",
    "scraped_at": 1685113824.6088674,
    "genre": "Education",
    "views": "69",
    "desc": "You create YouTube videos? With special focus on AI? \\\\nFor scientific, technical, educational, informative and economic insights? Great! \\\\nWatch me reposition my YT channel. Therefore this mini-series on strategic positioning, dynamic markets, ecosystems and my current inflection points.\\\\nNext stop: competitive analysis of your YT ecosystem!\\\\n\\\\n#tutorials \\\\n#creators \\\\n#computerscience \\\\n#creativity\"",
    "lengthSeconds": "523",
    "uploadDate": "2022-09-29",
    "thumbnail_url": "https://i.ytimg.com/vi/4qdCn_IVEeA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=9X6r2FnTgRM",
    "title": "FRESH UNBOXING of latest COLAB: Fastest Stable Diffusion Implementation 2022/09/25",
    "tags": "COLAB, Diffusion model, Image generation model from text prompt, KERAS, TensorFlow2",
    "scraped_at": 1685113824.0818672,
    "genre": "Science",
    "views": "172",
    "desc": "Given the official new KERAS TF2 implementation of the Text-to-Image generation AI tool (STABLE DIFFUSION) on COLAB, I unbox the new COLAB and share in real-time my first hand experience with you. Including new float_16 and XLA compiler acceleration. Faster than PyTorch?! Great Jupyter NB with clear explanations!\\\\n\\\\nOfficial KERAS \\\\nhttps://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/\\\\n\\\\nOfficial KERAS COLAB NB\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/keras_cv/generate_images_with_stable_diffusion.ipynb\\\\n\\\\n#keras \\\\n#stablediffusion \\\\n#colab\"",
    "lengthSeconds": "963",
    "uploadDate": "2022-09-28",
    "thumbnail_url": "https://i.ytimg.com/vi/9X6r2FnTgRM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3FHvxTDD38U",
    "title": "SELF-ATTENTION: Top5 YouTube Vids ON HOW TO Learn Self Attention in BERT (SBERT, Transformers)",
    "tags": "Self",
    "scraped_at": 1685113826.8418403,
    "genre": "Science",
    "views": "337",
    "desc": "Attention is all you need: My TOP5 YouTube Videos from the community on learning Self-attention mechanism in Transformers. After watching 72 Videos as presented by YouTube, I recommend 5 channels for outstanding explanation in5 categories (Vision, Code, Detail, Novel and Lecture). \\\\n\\\\nThe winner are:\\\\n@Peltarion https://www.youtube.com/watch?v=-9vVhYEXeyQ\\\\n@ChrisMcCormickAI https://youtu.be/a1Hc9soLxts\\\\n@SebastianRaschka  https://youtu.be/0PjHri8tc1c\\\\n@HeduMathematicsofIntelligence https://youtu.be/mMa2PmYJlCo\\\\n@stanfordonline https://youtu.be/ptuGllU5SQQ\\\\n\\\\n#ai \\\\n#Self_Attention\\\\n#top5\"",
    "lengthSeconds": "583",
    "uploadDate": "2022-09-27",
    "thumbnail_url": "https://i.ytimg.com/vi/3FHvxTDD38U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=0gWvRJMSNdQ",
    "title": "Code PARALLEL Fine-Tuning SBERT BI-Encoder on 2 Knowledge DOMAINS / ML Training Data (SBERT 41)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets, BI",
    "scraped_at": 1685113826.7078674,
    "genre": "Science",
    "views": "120",
    "desc": "SOTA Machine Learning: We use 2 Training Datasets and 2 LOSS Functions in one training run to fine-tune our SBERT model in parallel on two different knowledge domains, for Semantic Search or Clustering. Sentence Embedding with ROBERTA Sentence transformers. Python, PyTorch, COLAB. WORD EMBEDDING.\\\\n\\\\nLearn more on Sentence-Transformers and this Loss function (all credits to):\\\\nhttps://sbert.net\\\\nhttps://www.sbert.net/docs/training/overview.html\\\\n\\\\n#sbert \\\\n#datascience \\\\n#dataset \\\\n#datasets \\\\n#semantic #python \\\\n#pythonprogramming\"",
    "lengthSeconds": "708",
    "uploadDate": "2022-09-26",
    "thumbnail_url": "https://i.ytimg.com/vi/0gWvRJMSNdQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=PFczJ6NR5rY",
    "title": "How to explain Q, K and V of Self Attention in Transformers (BERT)?",
    "tags": "Self attention, Self attention of Transformers, AI, NLProc",
    "scraped_at": 1685113824.3458729,
    "genre": "Science",
    "views": "2450",
    "desc": "How to explain Q, K and V of Self Attention in Transformers (BERT)? \\\\nThought about it and present here my most general approach to explain the history of the notation of Query, Key and Values and how they combine for classical self attention mechanism in transformers. \\\\n\\\\n#ai \\\\n#self_attention\\\\n#bert\"",
    "lengthSeconds": "905",
    "uploadDate": "2022-09-24",
    "thumbnail_url": "https://i.ytimg.com/vi/PFczJ6NR5rY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=MonVJhvGwcs",
    "title": "Train SBERT on 2 Knowledge Domains: Python Code to Fine-Tuning SBERT BI-Encoder (SBERT 40)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets, BI",
    "scraped_at": 1685113825.9638598,
    "genre": "Science",
    "views": "148",
    "desc": "We download a fine-tuned \\\\u0026 pre-trained SBERT model from HuggingFace and then continue training our SBERT model (BI-Encoder) on another Domain / Training Dataset to integrate the second DOMAIN KNOWLEDGE in our SBERT model for Semantic Search or Clustering. \\\\n\\\\nLearn more on Sentence-Transformers and this Loss function (all credits to):\\\\nhttps://sbert.net\\\\nhttps://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark_continue_training.py\\\\n\\\\n#sbert \\\\n#datascience \\\\n#dataset \\\\n#datasets \\\\n#bert\"",
    "lengthSeconds": "845",
    "uploadDate": "2022-09-22",
    "thumbnail_url": "https://i.ytimg.com/vi/MonVJhvGwcs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=FidMAm-tj9k",
    "title": "Python Tutorial to Fine-tune SBERT BI-Encoder w/ my Domain-specific Training Dataset (SBERT Ep 39)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets, BI",
    "scraped_at": 1685113826.3588402,
    "genre": "Science",
    "views": "1393",
    "desc": "SOTA Word embedding and Sentence embedding in NLP: Fine-tune SBERT models with my own TRAINING DATASET for a specific DOMAIN (eg Science) with 100000 sentence-pairs with labeled data (entailment, neutral, contradiction).  Fine-tune my SBERT BI-ENCODER model with an improved LOSS Function on my personal TRAINING Dataset. Result: my fine-tuned SBERT model for science specific downstream tasks (clustering, semantic search).\\\\n\\\\nI extract these 100K sentence-pairs from EU Frontier Research projects descriptions. To construct 30K neutral sentences is a real challenge, given it\\'s requested neutral semantic correlation. Be careful, maybe you can achieve a lexical similarity even for a short word window between your anchor and neutral sentences. \\\\n\\\\nMy new Dataset is structurally identical to SNLI, but strictly focused on the best evaluated Research projects in Europa, ending in 2022 or 2023.\\\\n\\\\nPlus an Evaluator Dataset STSb to track your performance during training phase of your SBERT model.\\\\n\\\\nLearn more on Sentence-Transformers and this Loss function (all credits to):\\\\nhttps://sbert.net\\\\nhttps://www.sbert.net/examples/training/nli/README.html#multiplenegativesrankingloss\\\\nhttps://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss\\\\n\\\\n#sbert \\\\n#datascience \\\\n#dataset \\\\n#datasets \\\\n#bert \\\\n#semantic \\\\n#sentence \\\\n#insight \\\\n#colab \\\\n#dataanalytics \\\\n#pandas \\\\n#pandasdataframe\"",
    "lengthSeconds": "1266",
    "uploadDate": "2022-09-19",
    "thumbnail_url": "https://i.ytimg.com/vi/FidMAm"
  },
  {
    "link": "watch?v=8wRN48rmUr0",
    "title": "Tutorial SBERT BI-ENCODER fine-tuning Domain specific Training Dataset: Preview  SBERT 38",
    "tags": "Sentence Transformers, SBERT, AI, Semantic search",
    "scraped_at": 1685113823.485866,
    "genre": "Science",
    "views": "421",
    "desc": "Word Embedding and Sentence embedding in NLP: This is a preview of my next three coding videos (Python) as a TUTORIAL on SBERT coding! Code your own SBERT model from scratch, laser focused on your domain knowledge (= your training data set) and optimized w/ a specific loss function for training efficiently. Plus a evaluator STSb benchmark to indicate your performance during training. Word Embedding and Sentence embedding in NLP. \\\\n\\\\n#sbert \\\\n#ai \\\\n#deeplearning\"",
    "lengthSeconds": "718",
    "uploadDate": "2022-09-17",
    "thumbnail_url": "https://i.ytimg.com/vi/8wRN48rmUr0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=M7eNEsKig-o",
    "title": "BAYES' Theorem explained - How to calculate Probability of two dependent Events",
    "tags": "film, udost",
    "scraped_at": 1685113823.95384,
    "genre": "Education",
    "views": "174",
    "desc": "In probability theory and statistics, Bayes\\' theorem (alternatively Bayes\\' law or Bayes\\' rule), named after Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\\\\n\\\\nhttps://youtu.be/HZGCoVF3YvM\\\\nhttps://youtu.be/lG4VkPoG3ko\\\\n\\\\n#ai \\\\n#statistics \\\\n#mathematics\"",
    "lengthSeconds": "896",
    "uploadDate": "2022-09-15",
    "thumbnail_url": "https://i.ytimg.com/vi/M7eNEsKig"
  },
  {
    "link": "watch?v=1nNddbE_uQs",
    "title": "YOUR TikTok & Insta Reel train next Chinese & American AI  - And YOU get what? Text-to-Video AI",
    "tags": "film, udost",
    "scraped_at": 1685113823.1008677,
    "genre": "Science",
    "views": "47",
    "desc": "Your TikTok /China \\\\u0026 Instagram Reel /Meta + YouTube /Google  will train the next AI (Text-to-Video AI and Text-to-Image AI)). With your knowledge or without?\\\\nThanks from the global platforms, that you provide your data /videos free-of-charge. After text-to-image AI the next step is: text-to-video AI, which will need a lot of videos. Hundreds of millions of videos. And your text. \\\\n\\\\nNo, not Hollywood blockbusters (too expensive to buy the rights), but your videos you upload to the big global tech platforms. Hopefully without your face or GPS coordinates. \\\\n\\\\nChina is in a unique position here, all people from around the world upload free video material for China\\'s AIs to learn, all regions, all cultures, all economies, all political systems. Imagine what you can do with this potential of global information in video data. What you mean by Intellectual Property Rights?\\\\n\\\\nBut there is a star light at the end of my video, the world is gonna be beautiful. At least ... in the virtual new world (by the way: why is Meta(verse) in the club?). Smile. \\\\n\\\\n#ai \\\\n#datascience \\\\n#future\"",
    "lengthSeconds": "518",
    "uploadDate": "2022-09-14",
    "thumbnail_url": "https://i.ytimg.com/vi/1nNddbE_uQs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zzO_COvqjx8",
    "title": "STABLE DIFFUSION - the Day after - Insights",
    "tags": "AI, Diffusion probabilistic models, text",
    "scraped_at": 1685113824.8688383,
    "genre": "Science",
    "views": "120",
    "desc": "Insights to performance and system failures of stable diffusion on simple tasks. Insights in Generative Ai: text-to-image tools (as shown in my last video).\\\\n\\\\nPersonal insights illuminate a way forward.\\\\n\\\\nEmoji by  Pete Linforth from Pixabay.\\\\n\\\\n#ai \\\\n#stablediffusion \\\\n#insights\"",
    "lengthSeconds": "555",
    "uploadDate": "2022-09-13",
    "thumbnail_url": "https://i.ytimg.com/vi/zzO_COvqjx8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=7FcAtsWaD54",
    "title": "STABLE DIFFUSION is flawed - Watch out!",
    "tags": "film, udost",
    "scraped_at": 1685113825.512867,
    "genre": "Science",
    "views": "145",
    "desc": "Famous stable diffusion algorithm is a great piece of code, but flawed. To analyze the performance of a generative text-to-image system I start with some simple tasks. You can do it too, HuggingFace provide spaces.  \\\\n\\\\nHow will  a Latent Diffusion Model (LDM) perform? \\\\nWe will gain significant insights from our results on the full model. \\\\nText-to-Image AI tools evaluated on simple DIY. \\\\n\\\\nUnderstanding shortcomings of a model will enable us to learn more about its performance and adjust accordingly. Diffusion models are a significant step forward to match vision and language transformer applications.\\\\n\\\\nThanks to Huggingface for providing Spaces.\\\\n\\\\nI used the following models and spaces:\\\\nhttps://huggingface.co/spaces/huggingface/diffuse-the-rest\\\\nModel: CompVis/stable-diffusion-v1-4\\\\n\\\\nAlternative stable diffusion demo:\\\\nhttps://huggingface.co/spaces/stabilityai/stable-diffusion\\\\n\\\\n\\\\n#ai \\\\n#datascience \\\\n#stablediffusion\"",
    "lengthSeconds": "266",
    "uploadDate": "2022-09-12",
    "thumbnail_url": "https://i.ytimg.com/vi/7FcAtsWaD54/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lVqwznaVi78",
    "title": "SBERT (Sentence Transformers) is not BERT Sentence Embedding: Intro & Tutorial (#sbert  Ep 37)",
    "tags": "BERT, Transformer model, AI, Machine Learning, Deep Learning, SBERT, Encoder, Encoder Stack",
    "scraped_at": 1685113825.7058706,
    "genre": "Science",
    "views": "3115",
    "desc": "SBERT is not BERT Sentence Embedding (Introduction, Tutorial).\\\\n\\\\nQuite some viewers ask about SBERT Sentence Transformers, and confuse it with BERT Sentence Vectors or Embedding. I try a clarification of both systems and why one outperforms the other one for sentence semantic similarity. \\\\n\\\\nSimplifying systems for clear presentation is never easy, but I try to incorporate  visualizations for a better understanding, and for the moment do not focus on every little detail. If you are interested in a python implementation: there are 36 SBERT videos with code on my channel. \\\\n\\\\nBoth systems provide you for a given sentence with a sentence embedding. So what is the difference? \\\\n\\\\nSBERT = Sentence Embeddings using Siamese BERT-Networks (Bi-Encoder)\\\\n\\\\nAll SBERT credits go to:\\\\nSentence-BERT: Sentence Embeddings using Siamese BERT-Networks\\\\nNils Reimers, Iryna Gurevych\\\\nhttps://arxiv.org/abs/1908.10084\\\\n\\\\n\\\\n#bert \\\\n#sbert \\\\n#ai\"",
    "lengthSeconds": "1216",
    "uploadDate": "2022-09-10",
    "thumbnail_url": "https://i.ytimg.com/vi/lVqwznaVi78/maxresdefault.jpg"
  },
  {
    "link": "watch?v=SFAcXcfa5HA",
    "title": "When to operate ML models on dedicated Matrix Multiplier (Tensor Ecosystem)",
    "tags": "Tensorflow2, Tensor, ML, DL, AI, CPU, GPU, TPU Pods, TPU, Guide",
    "scraped_at": 1685113825.309841,
    "genre": "Science",
    "views": "26",
    "desc": "Operate your ML models with tensors on a single CPU (single core or multi-core), multiple GPUs or even Cloud TPU Pods? A short vocal introduction to the complexity of general CPUs, massive parallel on GPUs or dedicated matrix multiplier like TPUs?\\\\n\\\\n#tensors \\\\n#tensorflow2 \\\\n#tpu \\\\n#gpu \\\\n#cpu \\\\n#machinelearning \\\\n#artificialintelligence \\\\n#ai \\\\n#deeplearning \\\\n#datascience \\\\n#insight \\\\n#shorts\"",
    "lengthSeconds": "35",
    "uploadDate": "2022-09-09",
    "thumbnail_url": "https://i.ytimg.com/vi/SFAcXcfa5HA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1kIckOlG1oY",
    "title": "Optimize \"Parametric UMAP\" Tuned to MAX w/ AUTOENCODER  #python #datascience",
    "tags": "Topology, Mathematics, Dimensional rduction, Topological manifolds, UMAP, 2D, Visualization",
    "scraped_at": 1685113826.227839,
    "genre": "Education",
    "views": "96",
    "desc": "We code our Neural Networks (NN) for the Parametric UMAP encoder and decoder (like ConvNet for Vision MNIST data set) therefore we can optimize the performance of our parametric UMAP dimensional reduction algorithm. Plus activate the Autoencoder functionality for parametric UMAP.\\\\n\\\\nThe encoder is trained to minimize UMAP loss, and the decoder is trained to minimize reconstruction loss. To train the encoder jointly on both UMAP loss and reconstruction loss, pass \\\\\"",
    "lengthSeconds": "630",
    "uploadDate": "2022-09-08",
    "thumbnail_url": "https://i.ytimg.com/vi/1kIckOlG1oY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rujdyFHOIG0",
    "title": "Apply \"Parametric UMAP\" with ONE LINE  #python #coding",
    "tags": "film, udost",
    "scraped_at": 1685113826.0288682,
    "genre": "Science",
    "views": "242",
    "desc": "The simplest application of Parametric UMAP is w/ default values. We just define the number of epochs and that\\'s it! You can apply Parametric UMAP with one line of python code for dimensional reduction specific to your task. Example on MNIST picture data set of 70K B\\\\u0026W pictures of 28px x 28px. \\\\n\\\\nOriginal Parametric UMAP Jupyter NB:\\\\nhttps://github.com/lmcinnes/umap\\\\nhttps://github.com/lmcinnes/umap/tree/master/doc\\\\nhttps://github.com/lmcinnes/umap/tree/master/notebooks\\\\n\\\\nAll credits to:\\\\nhttps://arxiv.org/pdf/2009.12981.pdf\\\\nTim Sainburg, Leland McInnes, Timothy Q Gentner\\\\n\\\\nhttps://arxiv.org/pdf/1802.03426v1.pdf\\\\nLeland McInnes and John Healy\\\\n\\\\n#datascience \\\\n#topologicalspace \\\\n#embedding \\\\n#umap \\\\n#pythonprogramming \\\\n#tensorflow2 \\\\n#keras\"",
    "lengthSeconds": "748",
    "uploadDate": "2022-09-06",
    "thumbnail_url": "https://i.ytimg.com/vi/rujdyFHOIG0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=pRKTr8gw2KA",
    "title": "CODE Variational Autoencoder (VAE)  w/ KL-Divergence #ai #pythonprogramming #keras",
    "tags": "Autoencoder, Variational Autoencoder, probabilistic Autoencoder, VAE",
    "scraped_at": 1685113824.1488674,
    "genre": "Science",
    "views": "270",
    "desc": "A VAE is a probabilistic take on the autoencoder, a model which takes high dimensional input data and compresses it into a smaller representation space. \\\\n\\\\nUnlike a traditional autoencoder, which maps the input onto a latent vector, a VAE maps the input data into the parameters of a probability distribution, such as the mean and variance of a Gaussian. This approach produces a continuous, structured latent space, which is useful for image generation.\\\\n\\\\nofficial link and COLAB NB:\\\\nhttps://www.tensorflow.org/tutorials/generative/cvae#define_the_encoder_and_decoder_networks_with_tfkerassequential\\\\n\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vq_vae.ipynb\\\\n\\\\nFunny explanation of KL-Divergence:\\\\nhttps://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8\\\\n\\\\n#ai \\\\n#deeplearning \\\\n#tensorflow2 #vae\"",
    "lengthSeconds": "812",
    "uploadDate": "2022-09-04",
    "thumbnail_url": "https://i.ytimg.com/vi/pRKTr8gw2KA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BHzU-jq-AcM",
    "title": "Code AUTOENCODER in Python 2022 to de-noise Photos  (COLAB, KERAS, Convolutional2D, Tensorflow2)",
    "tags": "film, udost",
    "scraped_at": 1685113824.5418675,
    "genre": "Science",
    "views": "706",
    "desc": "AUTOENCODERS with Con2D KERAS Layers de-noise a photo, Python Code to follow along! Convolutional Neural Networks build a VISION NN, which will be the core component of the AUTOENCODER to de-noise pictures. With KERAS as a high-level API. Easily design your own autoencoder. \\\\n\\\\nOfficial TensorFlow2 Notebook:\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb\\\\n\\\\n#ai \\\\n#datascience \\\\n#tensorflow\"",
    "lengthSeconds": "824",
    "uploadDate": "2022-09-02",
    "thumbnail_url": "https://i.ytimg.com/vi/BHzU"
  },
  {
    "link": "watch?v=snGbTguuEMs",
    "title": "LIVE DEMO: \"STABLE DIFFUSION\" is the new hype (Text-to-Image AI) in Architecture (diffuse the rest)",
    "tags": "AI, Stable Diffusion, Text",
    "scraped_at": 1685113826.292841,
    "genre": "Science",
    "views": "2983",
    "desc": "On Huggingface Spaces a DEMO of the new hype this week: STABLE DIFFUSION, a text-to-image AI tool: diffuse the rest AI tool. You can add your own picture on the HuggingFace server and then add a text, and Stable Diffusion will calculate the probability-calculated visualization of those two entities. Can you imagine the amount of training data? Interesting new possibility. Just do not try it on faces.\\\\n\\\\nThanks to:\\\\nStable Diffusion model by CompVis and Stability AI - Demo by \\xf0\\x9f\\xa4\\x97 Hugging Face\\\\n\\\\nEnjoy the demo on interior design with AI. \\\\n\\\\nLink to HuggingFace \\\\\"",
    "lengthSeconds": "243",
    "uploadDate": "2022-08-31",
    "thumbnail_url": "https://i.ytimg.com/vi/snGbTguuEMs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tzedWkhZtZo",
    "title": "Physics ignites new Learning on Graph Neural Networks: Feature Propagation (M. Bronstein)",
    "tags": "film, udost",
    "scraped_at": 1685113824.4098418,
    "genre": "Science",
    "views": "226",
    "desc": "Study Physics and Mathematics if you want to push frontiers of Graph Neural Networks /ML /AI.\\\\n\\\\nLink to article:\\\\nhttps://towardsdatascience.com/learning-on-graphs-with-missing-features-dd34be61b06\\\\n\\\\n#ai \\\\n#machinelearning \\\\n#GraphNeuralNetwork\"",
    "lengthSeconds": "615",
    "uploadDate": "2022-08-28",
    "thumbnail_url": "https://i.ytimg.com/vi/tzedWkhZtZo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4Fbf9CQZE00",
    "title": "Dimensionality Reduction: Parametric UMAP 2022  = Code Fuzzy Simplicial Complex Topology in AI",
    "tags": "Topology, Mathematics, Dimensional rduction, Topological manifolds, UMAP, 2D, Visualization",
    "scraped_at": 1685113825.4468396,
    "genre": "Education",
    "views": "334",
    "desc": "Advance UMAP to a parametric optimization \\\\\"",
    "lengthSeconds": "2115",
    "uploadDate": "2022-08-26",
    "thumbnail_url": "https://i.ytimg.com/vi/4Fbf9CQZE00/maxresdefault.jpg"
  },
  {
    "link": "watch?v=onuIy6PC1ic",
    "title": "Code AUTOENCODERS w/ Python + KERAS Layers (Colab, TensorFlow2, Autumn 2022)",
    "tags": "Autoencoder, AI, Machine Learning, Deep Learning, Feed",
    "scraped_at": 1685113824.2788415,
    "genre": "Science",
    "views": "310",
    "desc": "An elegant way to code AUTOENCODERS with KERAS layers in TensorFlow2 on COLAB w/ Python. Autoencoders are applied for dimensionality reduction, where PCA fails for non-linearity. A coding Example on COLAB shows the way forward to code your own AUTOENCODER for your low-dimensional latent space encoding (low-dim embedding).  \\\\n\\\\nNote: A KERAS layer is a mathematical abstraction, based on TF2, but AUTOENCODERS are simply feed-forward neural networks. Autoencoders do not rely on self-attention, like encoders from within transformers. Autoencoders encode information from a high-dimensional input space to a low-dimensional latent space, which includes all relevant information from the input space. \\\\n\\\\nofficial links and COLAB NB #colab :\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb\\\\n\\\\nTF2 tutorial (recommended): Intro to AUTOENCODER:\\\\nhttps://www.tensorflow.org/tutorials/generative/autoencoder\\\\n\\\\n#deeplearning \\\\n#ai \\\\n#encoder\\\\n#autoencoder \\\\n#stablediffusion\"",
    "lengthSeconds": "1651",
    "uploadDate": "2022-08-24",
    "thumbnail_url": "https://i.ytimg.com/vi/onuIy6PC1ic/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wvk5uxMwMYs",
    "title": "Python Code for BERT Paragraph Vector Embedding w/ Transformers (PyTorch, Colab)",
    "tags": "BERT Transformer, Stacked Encoder, MAchine Learning, AI, Sentence Embedding, Sentence Vectors",
    "scraped_at": 1685113827.43284,
    "genre": "Science",
    "views": "1203",
    "desc": "After BERT \\\\\"",
    "lengthSeconds": "1124",
    "uploadDate": "2022-08-22",
    "thumbnail_url": "https://i.ytimg.com/vi/wvk5uxMwMYs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=adBa-dAxs6I",
    "title": "How to code BERT Word + Sentence Vectors (Embedding) w/ Transformers? Theory + Colab, Python",
    "tags": "BERT, Transformer AI, Encoder, Language Model, Word Embedding, Sentence Embedding, Sentence Vector, High",
    "scraped_at": 1685113826.0958676,
    "genre": "Science",
    "views": "3085",
    "desc": "Before SBERT there was BERT. A stacked Encoder of a Transformer, bidirectional. I show you in theory (2min) and in code (Colab) how to build WORD Embeddings (word vectors) form the hidden states of each of the 12 BERT encoders and how to build a SENTENCE Vector (a Sentence embedding) from the encoder stack in a high dimensional vector space.\\\\n\\\\nPart 2 of this video is called:\\\\nPython Code for BERT Paragraph Vector Embedding w/ Transformers (PyTorch, Colab)\\\\nand linked here: https://youtu.be/wvk5uxMwMYs\\\\n\\\\nThen we can apply UMAP for dimensional reduction, w/ preserving all relevant information in a lower dimensional vector space. \\\\n\\\\nSBERT today is faster and more performant than BERT Sentence Vectors. But BERT has some exceptional hidden states for its contextualized embeddings, which outperforms static word embeddings like Word2Vec. if you know which hidden states of BERT to select for your vector representation.\\\\n\\\\nGreat instructions online:\\\\nhttps://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\\\\nhttps://peltarion.com/knowledge-center/modeling-view/build-an-ai-model/blocks/english-bert\\\\n\\\\nDifference between CLS hidden state (Embedding) and pooling w/ BERT:\\\\nhttps://github.com/huggingface/transformers/issues/7540\\\\n\\\\n#datascience \\\\n#ai \\\\n#sbert\"",
    "lengthSeconds": "1199",
    "uploadDate": "2022-08-19",
    "thumbnail_url": "https://i.ytimg.com/vi/adBa"
  },
  {
    "link": "watch?v=36wTBLeMP-c",
    "title": "UMAP explained in 1 min - Dimensional Reduction Algorithm in 3 steps",
    "tags": "Topology, Mathematics, Dimensional rduction, Topological manifolds, UMAP, 2D, Visualization",
    "scraped_at": 1685113825.5748653,
    "genre": "Education",
    "views": "195",
    "desc": "In my last video I presented python code in COLAB for a UMAP dimensionality reduction. In this video the explanation to my genius drawings of UMAP , with no code at all. Explain UMAP in 3 steps. \\\\n\\\\nAll credits for UMAP go to 8recommended literature):\\\\nhttps://umap-learn.readthedocs.io/en/latest/_modules/umap/umap_.html\\\\n\\\\n#datascience \\\\n#dimensional \\\\n#embedding \\\\n#umap\"",
    "lengthSeconds": "73",
    "uploadDate": "2022-08-17",
    "thumbnail_url": "https://i.ytimg.com/vi/36wTBLeMP"
  },
  {
    "link": "watch?v=yYzN0vSRlaQ",
    "title": "Apply Code UMAP 2022 - Dimensional Reduction of Embeddings w/ Python, Colab Jupyter NB",
    "tags": "Topology, Mathematics, Dimensional rduction, Topological manifolds, UMAP, 2D, Visualization",
    "scraped_at": 1685113825.3738678,
    "genre": "Education",
    "views": "264",
    "desc": "Our SBERT (BI-encoder) data live in a high-dim vector space. But we want a 3D visualization. Therefore we use Python Library UMAP - Uniform Manifold Approximation and Projection for Dimension Reduction!\\\\n\\\\nAn Introduction to dimensional reduction with classical UMAP, next video will focus on more advanced versions! Yes, finally! Topological manifolds.\\\\n\\\\nAll credits to:\\\\nhttps://umap-learn.readthedocs.io/en/latest/index.html\\\\nhttps://arxiv.org/abs/1802.03426 \\\\nby Leland McInnes, John Healy, James Melville\\\\n\\\\n#topologicalspace \\\\n#datascience \\\\n#machinelearningwithpython \\\\n#embedding \\\\n#dimensional \\\\n#dimensions\"",
    "lengthSeconds": "1142",
    "uploadDate": "2022-08-15",
    "thumbnail_url": "https://i.ytimg.com/vi/yYzN0vSRlaQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vBVJhojtooM",
    "title": "Beginner's Guide to TRANSFORMERS: Code your ML model with latest Transformers in TensorFlow 08/2022",
    "tags": "Transformers, Machine Learning, HuggingFace, TensorFlow2, KERAS, Beginners Guide, Introduction, Introduction video",
    "scraped_at": 1685113823.5538619,
    "genre": "Education",
    "views": "917",
    "desc": "A coding Introduction to (latest update on) Transformers for BEGINNER: Intro to HuggingFace pre-trained Transformer models in TensorFlow2.\\\\n\\\\nTensorFlow2 has 2 powerful advantages over PyTorch and JAX: a high-level KERAS API and the Input pipeline tf.data.\\\\n\\\\nBased on a great intro blog by HuggingFace:\\\\nhttps://huggingface.co/blog/tensorflow-philosophy\\\\n\\\\nFollow along with COLAB:\\\\nhttps://huggingface.co/docs/transformers/training \\\\n\\\\nKERAS Code and example notebooks:\\\\nhttps://keras.io/examples/\\\\n\\\\n#tensorflow2 \\\\n#transformers \\\\n#machinelearningwithpython\"",
    "lengthSeconds": "2113",
    "uploadDate": "2022-08-13",
    "thumbnail_url": "https://i.ytimg.com/vi/vBVJhojtooM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_VwSSvy1eig",
    "title": "Billion parameter AI model fails ....",
    "tags": "AI, Future of AI, Feynman Path for AI multi",
    "scraped_at": 1685113824.0208402,
    "genre": "Science",
    "views": "40",
    "desc": "Billion parameter AI system should provide insights for human decision making. That national governments and global corporations are able to make decisions affecting million of citizens. In highly entangled \\\\u0026 complex systems from economy, education to health. Right down to individual decision making. Do current AI systems have the theoretical ability to advise humans on complex, dynamical systems? And if not, what piece of AI interlink is missing?\\\\n\\\\nFeynman paths will master dynamical evolving multi AI systems (from monetary policy models, supply chain dynamics to education in virtual isolation). \\\\n\\\\nSince combining multiple dynamic systems is not state-of-the-art in AI research, in analogy to Quantum Field Theory, the mathematics of path integral for all possible paths between the state of a dynamical and chaotic system will utilize Feynman\\'s path integral formalism to develop multi system perturbation predictions of real world systems.  \\\\n\\\\nAnalyze interdependencies of monetary policies, economic policies, new education systems in isolation, another COVID winter and increasing prices of energy in Europa, with its main source of energy cut off in Q3/Q4 2022. \\\\n\\\\nAI research needs its very first \\\\\"",
    "lengthSeconds": "156",
    "uploadDate": "2022-08-12",
    "thumbnail_url": "https://i.ytimg.com/vi/_VwSSvy1eig/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GrvnE7gVFpQ",
    "title": "Gradient Descent, Feynman's Path Integral to solve entangled AI systems",
    "tags": "AI, Weights, Loss function, Score function, Minimize the loss function, Feynman Paths, Feynman, Multiple dynamic systems in AI",
    "scraped_at": 1685113827.0388408,
    "genre": "Science",
    "views": "50",
    "desc": "From Gradient Descent to Feynman paths in dynamically entangled AI systems: From (single system) gradient descent in high-dim weight spaces to the future of multi AI systems, where all possible paths (path integral) will unlock the complexity of the sum of all subsystems. Needed for economic predictions by multi AI systems. \\\\n\\\\n#ai \\\\n#gradientdescent \\\\n#feynman\"",
    "lengthSeconds": "542",
    "uploadDate": "2022-08-10",
    "thumbnail_url": "https://i.ytimg.com/vi/GrvnE7gVFpQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JxfS5ZjdxGE",
    "title": "Fine-Tune SBERT on my Knowledge DOMAIN 2022 w/ CROSS-ENCODER Sentence Transformers (SBERT 36)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets",
    "scraped_at": 1685113824.733868,
    "genre": "Science",
    "views": "2033",
    "desc": "NEW: Fine-tune SBERT models with my own TRAINING DATASET for a specific DOMAIN (eg Science) with 100000 sentence-pairs with labeled data (entailment, neutral, contradiction). TRAIN my SBERT model on it. Fine-tune SBERT model w/ CROSS-ENCODER for semantic search and clustering. \\\\n\\\\nI extract these 100K sentence-pairs from EU Frontier Research projects descriptions. To construct 30K neutral sentences is a real challenge, given it\\'s requested neutral semantic correlation. But I found a solution.\\\\n\\\\nMy new Dataset is structurally identical to SNLI, but strictly focused on the best evaluated Research projects in Europa, ending in 2022 or 2023.\\\\n\\\\nLearn more on Sentence-Transformers:\\\\nhttps://sbert.net\\\\n\\\\n#sbert \\\\n#datascience \\\\n#dataset \\\\n#datasets \\\\n#bert \\\\n#semantic \\\\n#sentence \\\\n#insight \\\\n#colab \\\\n#dataanalytics \\\\n#pandas \\\\n#pandasdataframe\"",
    "lengthSeconds": "1328",
    "uploadDate": "2022-08-05",
    "thumbnail_url": "https://i.ytimg.com/vi/JxfS5ZjdxGE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hHL92ch2J1c",
    "title": "Amazing TEXT Generation from BLOOM Language Model (4x bigger than on Colab) locally on my PC",
    "tags": "film, udost",
    "scraped_at": 1685113825.1198714,
    "genre": "Science",
    "views": "2600",
    "desc": "BLOOM LLM for amazing text generation. 2.5 billion parameter BLOOM LLM on my local Win10 PC: Anaconda w/ transformers (PyTorch), download BigScience BLOOM 2b5 model (2.5 billion trained parameters) + give the model the first sentence of the Declaration of Independence to build upon. \\\\n\\\\nJudge Antonin Scalia was a member of the Supreme Court of the United States. The LLM included a real judge from the Supreme Court in the text generation about the American Declaration of Independence. Not bad at all. \\\\n\\\\nYou need min 32 GB RAM on your laptop / PC (\\\\u0026 download 7GB model from HuggingFace).\\\\n\\\\nAmazing answers by this tiny 2.5 B LLM .... if you compare it to the real 176 B model. But it shows, that even on a normal PC you can use BigScience\\'s BLOOM LLM, although one of the smaller models, but still with billions of trained parameters.\\\\n\\\\nAll credits to:\\\\nhttps://huggingface.co/bigscience/bloom\\\\n\\\\nJust for fun, for a commercial LLM application you need a professional cloud infrastructure (TPU $$$). See recommendation by HuggingFace.\\\\n\\\\n#ai \\\\n#datascience \\\\n#machinelearningwithpython\"",
    "lengthSeconds": "145",
    "uploadDate": "2022-08-02",
    "thumbnail_url": "https://i.ytimg.com/vi/hHL92ch2J1c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rrZGIR5CryM",
    "title": "BigScience BLOOM on COLAB (max tuned version) for text generation by LLM",
    "tags": "AI, LLM, BLOOM, DEMO, Large Language Model, Open, BigScience",
    "scraped_at": 1685113826.9058409,
    "genre": "Science",
    "views": "5002",
    "desc": "The COLAB  implementation of BigScience BLOOM to experience for yourself. BLOOM inference with 176 billion parameters can either be achieved on professional cloud infrastructure with more than 400GB RAM (some recommend 768GB) or you can try a smaller BLOOM model on free COLAB for exploration purposes. \\\\n\\\\nHere we download the 760 million parameter model from HuggingFace and run this BLOOM model on Colab for text generation, given a first sequence of words to start with. Text generation by LLMs.\\\\n\\\\n#ai \\\\n#datascience \\\\n#machinelearningwithpython\"",
    "lengthSeconds": "309",
    "uploadDate": "2022-07-30",
    "thumbnail_url": "https://i.ytimg.com/vi/rrZGIR5CryM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=sDNAPdMESVw",
    "title": "BigScience BLOOM Language Model: Live DEMO w/ LLM on Policy, Tech, Science and Economy.",
    "tags": "AI, Language Models, Large Language Models, BLOOM, BLOOM LLM, DEMO, BigScience",
    "scraped_at": 1685113827.5018613,
    "genre": "Science",
    "views": "2148",
    "desc": "Live Demo of BigScience BLOOM LLM, a state-of-the-art Large Language Model (LLM) to generate text for you, given a starter sentence. On HuggingFace. Watch for yourself! \\\\n\\\\nExperience the power of current Large Language Models yourself. BigScience made a Language model openly available, unlike GPT-3, for researchers to learn and advance.\\\\n\\\\nTopics explored w/ BigScience BLOOM LLM include: \\\\ninternational policy,\\\\ntechnology,\\\\nscience,\\\\nUS policy and\\\\nEuropa. \\\\n\\\\nExperience BLOOM yourself at HuggingFace:\\\\nhttps://huggingface.co/bigscience/bloom\\\\nhttps://huggingface.co/bigscience\\\\nhttps://huggingface.co/docs/transformers/model_doc/bloom\\\\nhttps://huggingface.co/spaces/bigscience/license\\\\n\\\\n#ai \\\\n#demonstration \\\\n#live \\\\n#bloom_LLM\"",
    "lengthSeconds": "382",
    "uploadDate": "2022-07-27",
    "thumbnail_url": "https://i.ytimg.com/vi/sDNAPdMESVw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=x_8Qow6piyQ",
    "title": "How  to CREATE your TRAINING DATASET on your DOMAIN Knowledge (Fine-tune SBERT in 2022) (SBERT 35)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets",
    "scraped_at": 1685113827.6948404,
    "genre": "Science",
    "views": "759",
    "desc": "New video on how I create my own SBERT Training DATASET for a SCIENCE DOMAIN with 100000 sentence-pairs with labeled data (entailment, neutral, contradiction). For supervised Learning of Sentence Transformers. \\\\n\\\\nI extract these 100K sentence-pairs from EU Frontier Research projects descriptions. To construct 30K neutral sentences is a real challenge, given it\\'s requested neutral semantic correlation. But i found a solution.\\\\n\\\\nMy new Dataset is structurally identical to SNLI, but strictly focused on the best evaluated Research projects in Europa, ending in 2022 or 2023.\\\\n\\\\n#sbert \\\\n#datascience \\\\n#dataset \\\\n#datasets \\\\n#bert \\\\n#semantic \\\\n#sentence \\\\n#insight \\\\n#colab \\\\n#dataanalytics \\\\n#pandas \\\\n#pandasdataframe\"",
    "lengthSeconds": "1549",
    "uploadDate": "2022-07-25",
    "thumbnail_url": "https://i.ytimg.com/vi/x_8Qow6piyQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=UfC70U5bFLU",
    "title": "Create your own DATASET: HuggingFace Course on Transformers: first view (SBERT 34)",
    "tags": "film, udost",
    "scraped_at": 1685113827.8378386,
    "genre": "Science",
    "views": "1732",
    "desc": "A first view on the course by HuggingFace on the topic: create your own Dataset, for AI/ML models. HuggingFace course will teach you about natural language processing (NLP) using libraries from the Hugging Face ecosystem. Either PyTorch or TensorFlow 2 code.\\\\n\\\\nWhenever you are entrusted with a specific task, and no datasets are publicly available, then you have to create your own dataset to train your BERT or SBERT model on it. \\\\n\\\\nAll credits to:\\\\nhttps://huggingface.co/course/chapter1/1\\\\nhttps://huggingface.co/course/chapter5/5?fw=pt\\\\n\\\\n#dataset \\\\n#datasets \\\\n#datascience \\\\n#dataanalytics \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#embedding \\\\n#bert\"",
    "lengthSeconds": "1882",
    "uploadDate": "2022-07-22",
    "thumbnail_url": "https://i.ytimg.com/vi/UfC70U5bFLU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZrD8IZ9tIAs",
    "title": "New BigScience BLOOM Large Language Model - YOUR LICENSE  - July 2022",
    "tags": "film, udost",
    "scraped_at": 1685113828.1718388,
    "genre": "Science",
    "views": "987",
    "desc": "The BigScience RAIL License protects use of new BLOOM AI Large Language Model (LLM). You have to know the legal liabilities /responsibilities when using or modifying any code that is protected by a copyright and/or license agreement. BLOOM is protected by a copyright and (!) a license agreement. For good reasons.\\\\n\\\\nThis is NOT an open source license according to the Open Source Initiative definition. It defines restrictions on the use of the model (\\xc2\\xa75, Attachment A). Please fact-check against the original documents, especially in its current version and form when you read this text.\\\\n\\\\nYou as a responsible scientist /engineer have to know the legal conditions and limitations of use if you download and use a language model (or in general Artificial Intelligence). \\\\n\\\\nHuggingFace and Big Science did provide an in-depth analysis and reasoning for responsible AI use and implemented it in the BigScience RAIL (Responsible AI License) License for their new BLOOM model. \\\\n\\\\nTo empower developers to place restrictions on the use of their AI technology through end user and source code license agreements.\\\\n\\\\nThe license covers the BigScience BLOOM models, any checkpoints released during the training, and any source code, scripts and/or documentation necessary to define, run, load, benchmark and evaluate the LLM (Large Language Model).\\\\n\\\\nLegitimate concerns about the impact of this technology on society and our planet are being raised, including the need for AI fairness, transparency, explainability and robustness, as well as addressing issues related to privacy, accountability, addiction, manipulation, and misuse. \\\\n\\\\nBigScience is an ongoing collaborative open science initiative, where a large number of researchers from all over the world work together to train a large language model. all research artifacts are shared with the entire research community.\\\\n\\\\nAll credits to:\\\\nhttps://huggingface.co/bigscience/bloom\\\\nhttps://huggingface.co/spaces/bigscience/license\\\\nhttps://bigscience.huggingface.co/blog/the-bigscience-rail-license\\\\n\\\\nLegal disclaimer: \\\\nPlease consult your legal License when you intent to use this BLOOM LLM model for scientific reasons or otherwise. Your are solely responsible for the use of this model and no information in this YouTube video supersedes the legal condition of the RAIL license agreement for BLOOM LLM. \\\\n\\\\nMy personal views recorded in this YouTube video are my views as a scientist, not at all a legal advice. \\\\n\\\\nConsult your professional service provider for your specific legal conditions of use by the BigScience initiative, since it is a protected model. \\\\n\\\\n#gpt3 \\\\n#license \\\\n#legal \\\\n#datascience \\\\n#ai\"",
    "lengthSeconds": "1452",
    "uploadDate": "2022-07-19",
    "thumbnail_url": "https://i.ytimg.com/vi/ZrD8IZ9tIAs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=cskwcbLDSJg",
    "title": "Responsible AI in 2022? Over-reliance on Automation for Decisions of huge Consequences",
    "tags": "AI, AI Regulation, Industry self",
    "scraped_at": 1685113828.1008654,
    "genre": "Science",
    "views": "18",
    "desc": "CNET published on July18, 2022, an article titled: \\\\\"",
    "lengthSeconds": "87",
    "uploadDate": "2022-07-17",
    "thumbnail_url": "https://i.ytimg.com/vi/cskwcbLDSJg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Ac5ncmFMNRE",
    "title": "AI self-regulation? US vs Europa",
    "tags": "film, udost",
    "scraped_at": 1685113827.906839,
    "genre": "Science",
    "views": "26",
    "desc": "A better society through better AI? Do we need AI regulations? In the future, right now?\\\\n\\\\nCurrent US and EU AI policies for society and Big Tech, read by a scientist. As requested by the community to highlight current (July 2022) AI policies in America and Europa, I provide 2 (non-representative) relevant and up-to-date sources of Insight: MIT Tech Review (June 2022) and Panel for the Future of Science and Technology (STOA) of the European Parliament (July 2022).\\\\n\\\\nMy adventure to public policy texts on AI and my amazement are recorded live, all personal opinions recorded in this YouTube video are my own. \\\\n\\\\nYou can disagree with me on my reflections of current AI policy trends in EU and US and leave a comment on a beautiful new world of legislative enforced \\\\\"",
    "lengthSeconds": "1731",
    "uploadDate": "2022-07-16",
    "thumbnail_url": "https://i.ytimg.com/vi/Ac5ncmFMNRE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=UZOzh6-js8I",
    "title": "NEW: Multiple Training DATASETS to fine-tune your SBERT model in 2022 (SBERT 33)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets",
    "scraped_at": 1685113827.9708407,
    "genre": "Science",
    "views": "371",
    "desc": "Python code on how to train multiple training datasets for your specific Sentence Transformer model. Combine Datasets of SNLI with MS MARCO and Reddit for training your SBERT model. Example on COLAB with PyTorch.\\\\n\\\\n#datascience \\\\n#datastructure \\\\n#dataset \\\\n#datasets \\\\n#pytorch \\\\n#deeplearning \\\\n#colab \\\\n#sbert \\\\n#semantic \\\\n#search\"",
    "lengthSeconds": "969",
    "uploadDate": "2022-07-14",
    "thumbnail_url": "https://i.ytimg.com/vi/UZOzh6"
  },
  {
    "link": "watch?v=dhw2-oBbm78",
    "title": "DATASET to fine-tune SBERT (w/ CROSS-ENCODER) for a better Domain Performance 2022 (SBERT 32)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets",
    "scraped_at": 1685113827.758839,
    "genre": "Science",
    "views": "722",
    "desc": "Use a training dataset to fine-tune your SBERT model. Python code on how to train famous SNLI dataset for a CROSS-ENCODER /Sentence Transformer model.  Example on COLAB with PyTorch.\\\\n\\\\n#datascience \\\\n#datastructure \\\\n#dataset \\\\n#datasets \\\\n#pytorch \\\\n#deeplearning \\\\n#colab \\\\n#sbert \\\\n#semantic \\\\n#search \\\\n#machinelearning \\\\n#ai\"",
    "lengthSeconds": "766",
    "uploadDate": "2022-07-11",
    "thumbnail_url": "https://i.ytimg.com/vi/dhw2"
  },
  {
    "link": "watch?v=Dj1RP6gkrZk",
    "title": "AI / ML: CPU GPU TPU optimization",
    "tags": "CPU, GPU, TPU, AI, ML, DL, Architecture, Computer Chip",
    "scraped_at": 1685113827.5618393,
    "genre": "Science",
    "views": "70",
    "desc": "CPU, GPU and TPU optimization explained for applications of Machine Learning ML and Deep Learning models (AI). \\\\n\\\\n#machinelearningwithpython \\\\n#deeplearning \\\\n#tpu \\\\n#gpu \\\\n#cpu \\\\n#computerscience \\\\n#chips \\\\n#machinelearning \\\\n#ai \\\\n#explained\"",
    "lengthSeconds": "47",
    "uploadDate": "2022-07-09",
    "thumbnail_url": "https://i.ytimg.com/vi/Dj1RP6gkrZk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ihFm64SV_FU",
    "title": "Training DATASET: internal structure for Sentence-Transformers SBERT semantic search (SBERT31)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer, Datasets, Training Datasets",
    "scraped_at": 1685113827.6278608,
    "genre": "Science",
    "views": "612",
    "desc": "The famous SNLI Training Dataset for a CROSS-ENCODER /Sentence Transformer model.  Example on COLAB with PyTorch. Explore the structure of a SNLI datset. \\\\n\\\\nPart 1 of my series to build your own domain-specific knowledge dataset for SBERT! \\\\n\\\\n#datascience \\\\n#datastructure \\\\n#dataset \\\\n#datasets \\\\n#pytorch \\\\n#deeplearning \\\\n#colab \\\\n#sbert \\\\n#semantic \\\\n#search \\\\n#machinelearning \\\\n#ai\"",
    "lengthSeconds": "1088",
    "uploadDate": "2022-07-07",
    "thumbnail_url": "https://i.ytimg.com/vi/ihFm64SV_FU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GeWK5wqe7YY",
    "title": "Core idea of AI w/ Sentence Transformers (SBERT): for anyone to understand - July 2022 (SBERT30)",
    "tags": "Sentence Transformers, AI, ML, SBERT, Explanation, easy, Visualization",
    "scraped_at": 1685113828.035867,
    "genre": "Education",
    "views": "315",
    "desc": "To explain artificial intelligence and Sentence Transformer models (like SBERT) without code or mathematics is a challenge. Just for fun a short 3 min video on the core idea of sentence embedding and semantic similarity in a designed vector space, for anyone. \\\\n\\\\nWhy sentence transformers? How does artificial intelligence work?\\\\n\\\\nAsk yourself: Can we add two sentences together?\\\\nCan we substract sentences? \\\\nNot in our language, but when we map sentences w/ mathematics to locations in space?\\\\n\\\\nAn Example:\\\\nFirst sentence is:  today it is sunny.\\\\nThe second sentence is: tomorrow it will be rainy.\\\\n\\\\nThose sentences have a meaning. To humans.  \\\\nLet\\xe2\\x80\\x99s focus on the meaning of those two sentences.\\\\n\\\\nBoth sentences have the following structure:  (datum) and (weather condition).\\\\nThey both belong to the same class of information (datum and weather). \\\\nLet\\'s assign a particular segment of space for those sentences.\\\\nBut where does this class of information live in the information space? And what mathematical algorithm constructs this vector space for us, for our specific task?\\\\n\\\\nEasy: Let\\xe2\\x80\\x99s build a space for it, let\\xe2\\x80\\x99s build a mathematical space for it, a topological space, where we can operate with mathematical operators like products and differentiation \\\\u0026 probabilities. Suddenly we can work with those sentences and calculate semantic similarities. Sentence Transformer as a tool provide us with those sentence embeddings in a vector space, a space with a particular structure.\\\\n\\\\nTopics are semantic clusters with a high cosine similarity of their content vectors and UMAP is ideal to convert 1024 to 3 dim visualizations. A single sentence query retrieves all relevant semantic information from a particular field or sector in vector space to provide insights into complex semantic dependencies of sentences or paragraphs of text.\\\\n\\\\nNext step: we are interested in a new document. We input all sentences of this document into our fine-tuned Sentence Transformer and get a set of vectors. Those vectors are locally close to other vectors, that define and construct the space. And we can apply mathematics on vectors. We can compute meaning. \\\\n\\\\nFrom the tabular 2 dimensional structured data in Databases, we evolved to a complete topological space, which is build up by our engine, SBERT, and is kind of self-learning and adapting to new data. Our engine SBERT constructs a vector space, and operates on this vector space with a mapping of sentences with a similar meaning to locally close vectors.\\\\n\\\\nAlmost 3 min, also if you read this. \\\\n\\\\nAll ideas and visualizations presented here are my mistakes and my conceptual idealization /simplification alone. \\\\n\\\\n#sbert \\\\n#explained \\\\n#explainableai\\\\n#nlproc\\\\n#nlp\"",
    "lengthSeconds": "187",
    "uploadDate": "2022-07-05",
    "thumbnail_url": "https://i.ytimg.com/vi/GeWK5wqe7YY/maxresdefault.jpg"
  },
  {
    "link": "watch?v=toWQ1HVYSo0",
    "title": "Your best Data Ecosystem:  DELTA LAKE w/ PySpark example |  BUSINESS and AI 2022",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales, CLV, Customer Lifetime Value",
    "scraped_at": 1685113828.2388396,
    "genre": "Education",
    "views": "48",
    "desc": "A requested topic to compare pure parquet files to the benefits of a DELTA Lake from Databricks for business data performance. A Jupyter NB with PySpark code to compare the benefits of a DELTA Lake to a pure PARQUET file system. \\\\n\\\\nA streaming data source is synthetically simulated (pyspark.stream) and data are live streamed to a parquet file, and compared to a Delta Lake functionality.\\\\n\\\\nAll credits to Databricks Notebooks:\\\\nhttps://docs.databricks.com/delta/intro-notebooks.html\\\\n\\\\n#ai \\\\n#machinelearningwithpython \\\\n#DeltaLake\\\\n#DataStreaming\\\\n#datascience \\\\n#databricks \\\\n#dataanalytics \\\\n#businessanalytics \\\\n#businessdata \\\\n#businessintelligence \\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "1136",
    "uploadDate": "2022-07-03",
    "thumbnail_url": "https://i.ytimg.com/vi/toWQ1HVYSo0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Lt62xJAW8nQ",
    "title": "How to Transfer Domain Knowledge w/ Augmented SBERT, update 2022 (SBERT29)",
    "tags": "Sentence Transformers, SBERT, MAchine Learning, BERT model, AI, Deep Learning, Knowledge Transfer",
    "scraped_at": 1685113832.321867,
    "genre": "Science",
    "views": "840",
    "desc": "Augmented SBERT: In domains like Physics, Biology, Medicine, Law there is only little LABELED TRAINING datasets available to train your Sentence Transformer (SBERT) models on new topics (eg for a specific domain knowledge) in \\\\\"",
    "lengthSeconds": "2411",
    "uploadDate": "2022-06-29",
    "thumbnail_url": "https://i.ytimg.com/vi/Lt62xJAW8nQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8aOFeNjl_4o",
    "title": "\"Digital AI Twin\" for best Customer Journey simulation | BUSINESS AI 2022  #businessintelligence",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales, CLV, Customer Lifetime Value",
    "scraped_at": 1685113828.4538615,
    "genre": "Education",
    "views": "226",
    "desc": "In our digital economy the AI powered optimization of a cross-channel customer journey is essential for a competitive advantage. An \\\\\"",
    "lengthSeconds": "380",
    "uploadDate": "2022-06-25",
    "thumbnail_url": "https://i.ytimg.com/vi/8aOFeNjl_4o/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zMDBc_Q9Ark",
    "title": "Advanced Semantic SEARCH w/ SBERT (Re-Ranking w/ Cross-Encoder) Edition 2022 #sbert (SBERT 28)",
    "tags": "Sentence Transformer, SBERT, BERT model, Question and Answer, Machine Learning, ML, AI, Zero",
    "scraped_at": 1685113828.5258412,
    "genre": "Science",
    "views": "1422",
    "desc": "SBERT Sentence Transformers Q\\\\u0026A cosine-similarity and re-ranking w/ Cross-Encoders (BERT) for advanced Semantic Search in PyTorch. BI-Encoder combined w/ Cross-Encoder. Pytorch code, Jupyter, Colab. Advanced Semantic Search w/ SBERT Cross-Encoder. \\\\n\\\\nCross-encoder are pre-trained from HuggingFace for Semantic Search.\\\\n\\\\nAll credits to:\\\\nsbert.net\\\\n\\\\nColab NB:\\\\nhttps://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/cross-encoder/cross-encoder_reranking.py\\\\n\\\\n#sbert \\\\n#deeplearning \\\\n#search  \\\\n#pytorch \\\\n#pythonprogramming \\\\n#machinelearningwithpython \\\\n#questionandanswer \\\\n#questionanswer \\\\n#bert \\\\n#encoder \\\\n#sentence \\\\n#embedding \\\\n#classification \\\\n#colab \\\\n#semantic \\\\n#ai \\\\n#artificialintelligence\"",
    "lengthSeconds": "1334",
    "uploadDate": "2022-06-22",
    "thumbnail_url": "https://i.ytimg.com/vi/zMDBc_Q9Ark/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_Akv9oW8jeA",
    "title": "Gamma-Gamma: PySpark MLFlow, Hyperopt  |  BUSINESS AI 2022  #businessdata   #customerexperience",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales, CLV, Customer Lifetime Value",
    "scraped_at": 1685113832.3938396,
    "genre": "Education",
    "views": "59",
    "desc": "Customer Lifetime Value: Calculating the amount of revenue we could receive from a customer over the lifetime of our relationship with them, we might better tailor our investments to maximize the value of our relationship. Gamma-Gamma mod: PySpark w/ MLFlow, Hyperopt.\\\\n\\\\nPython code for Customer Lifetime Value with AI: On Databricks Community edition we attach a Spark Cluster to our Databricks Notebook and execute a Customer Prediction model for future purchases. \\\\n\\\\nWe use Lifetimes Python Lib for 2 models and apply MLFlow for hyperparameter tuning and deploy the new model for Customer predictions. \\\\n\\\\nWe calculate the prediction that an existing customer will return within 15, 30 or 45 days, given his/her past purchase patterns. We use a wrapper class to convert the new model into a function for Spark DataFrames. \\\\n\\\\nHow to best spend money to profitably grow your business brand? Personalized engagement can drive higher revenues, marketing efficiency and customer retention. We will code (Python, Jupyter NB) an AI system with Databricks MLFlow to design our specific Customer Lifetime Calculation for marketing spending. \\\\n\\\\nALL Credits to DATABRICKS for Jupyter NB and code:\\\\nhttps://databricks.com/notebooks/CLV_Part_2_Customer_Lifetimes.html\\\\n\\\\nTo calculate the remaining lifetime in a customer relationship we must carefully examine the transactional signals previously generated by customers in terms of the frequency and recency of their engagement. \\\\n\\\\n#businessintelligence \\\\n#ai \\\\n#clv \\\\n#customerjourney \\\\n#customerexperience \\\\n#lifetimevalue \\\\n#datascience \\\\n#dataanalytics \\\\n#databricks \\\\n#data \\\\n#machinelearning \\\\n#pyspark \\\\n#spark \\\\n#dataframe \\\\n#dataframes \\\\n\\\\n\\\\n00:00 AI for Customer Lifetime Value\\\\n05:33 Customer Metrics: Monetary Value\\\\n13:40 Train our model: Gamma-Gamma \\\\n17:40 Perfect Hyperparameter from Hyperopt \\\\n20:20 Customer Lifetime Value (monetary)\\\\n24:03 Save model to MLFlow\\\\n25:07 CLV data for all our customer\\\\n\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "1641",
    "uploadDate": "2022-06-19",
    "thumbnail_url": "https://i.ytimg.com/vi/_Akv9oW8jeA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BO6BcqurHBU",
    "title": "PySpark MLFlow: How to spend money to grow your business? |  BUSINESS AI 2022 #businessintelligence",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales, CLV, Customer Lifetime Value",
    "scraped_at": 1685113832.4678388,
    "genre": "Education",
    "views": "68",
    "desc": "PySpark code: How to best spend money to profitably grow your business brand? Business Intelligence AI 2022.  Personalized engagement can drive higher revenues, marketing efficiency and customer retention. We will code (Python, Jupyter NB) an AI system with Databricks MLFlow to design our specific Customer Lifetime Calculation for marketing spending. \\\\n\\\\nDatabricks PySpark NB: we execute a Customer Prediction model for future purchases.  Non-subscription case. We use Lifetimes Python Lib for 2 models and apply MLFlow for hyperparameter tuning and deploy the new model for Customer predictions. We calculate the prediction that an existing customer will return within 15, 30 or 45 days, given his/her past purchase patterns. We use a wrapper class to convert the new model into a function for Spark DataFrames. \\\\n\\\\nALL Credits to DATABRICKS for Jupyter NB and code:\\\\nhttps://databricks.com/notebooks/CLV_Part_1_Customer_Lifetimes.html\\\\n\\\\nCalculating the amount of revenue we could receive from a customer over the lifetime of our relationship with them, we might better tailor our investments to maximize the value of our relationship.\\\\n\\\\nTo calculate the remaining lifetime in a customer relationship we must carefully examine the transactional signals previously generated by customers in terms of the frequency and recency of their engagement. \\\\n\\\\n#businessintelligence \\\\n#ai \\\\n#clv \\\\n#customerjourney \\\\n#customerexperience \\\\n#lifetimevalue \\\\n#datascience \\\\n#dataanalytics \\\\n#databricks \\\\n#data \\\\n#machinelearning \\\\n#pyspark \\\\n#spark \\\\n#dataframe \\\\n#dataframes \\\\n\\\\n00:00 Databricks Community Edition Spark Cluster\\\\n05:33 Create Spark Dataframe of Customer Data\\\\n09:50 Calculate Customer Metrics\\\\n14:24 Programmatic SQL API\\\\n16:31 Train the model: Pareto/NBD Lifetimes Python Lib\\\\n19:15 Hyperparameter Search Space w/ hyperopt\\\\n20:18 MLFlow Hyperparameter Tuning 100 iterations\\\\n23:20 model.fit \\\\n26:00 Probability a customer remains engaged\\\\n28:40 Predict number of purchases from customer in future time interval \\\\n29:23 Deploy the Model for Predictions\\\\n30:00 Wrapper class for our Lifetimes Models\\\\n\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "2087",
    "uploadDate": "2022-06-17",
    "thumbnail_url": "https://i.ytimg.com/vi/BO6BcqurHBU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=f1EGdXko6pA",
    "title": "SBERT: CROSS - ENCODER for Zero-Shot Classification, Question & Answer (QA), Update 2022 (SBERT 27)",
    "tags": "Sentence Transformer, SBERT, BERT model, Question and Answer, Machine Learning, ML, AI, Zero",
    "scraped_at": 1685113828.6098661,
    "genre": "Science",
    "views": "1009",
    "desc": "SBERT Sentence Transformers include  Cross-Encoders for sentence pair score (Question and Answer - QA) and zero-shot classification tasks. Pytorch code in Jupyter, Colab. \\\\n\\\\nCross-encoder are pre-trained from HuggingFace for Zero-Shot classifications or Question and Answer (QA) w/ long sequences.\\\\n\\\\nReal time PyTorch code examples of SBERT applications.\\\\n\\\\nAll credits to:\\\\nhttps://sbert.net\\\\nhttps://sbert.net/examples/applications/cross-encoder/README.html\\\\nhttps://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/cross-encoder/README.md\\\\nhttps://arxiv.org/abs/1908.10084\\\\nby Nils Reimers, Iryna Gurevych\\\\n\\\\n\\\\n#sbert \\\\n#deeplearning \\\\n#pytorch \\\\n#pythonprogramming \\\\n#machinelearningwithpython \\\\n#questionandanswer \\\\n#questionanswer \\\\n#bert \\\\n#encoder \\\\n#sentence \\\\n#embedding \\\\n#classification \\\\n#colab \\\\n#zero_shot\\\\n#huggingface\"",
    "lengthSeconds": "2643",
    "uploadDate": "2022-06-14",
    "thumbnail_url": "https://i.ytimg.com/vi/f1EGdXko6pA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dBLIIqjIGLg",
    "title": "Industrial Data Science:  CLV?  Invest in BUSINESS AI 2022",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales, CLV, Customer Lifetime Value",
    "scraped_at": 1685113828.6988397,
    "genre": "Education",
    "views": "37",
    "desc": "Industrial Data Science: How to best spend money to profitably grow your business brand? Personalized engagement can drive higher revenues, marketing efficiency and customer retention. We will code (Python, Jupyter NB) an AI system with Databricks MLFlow to design our specific Customer Lifetime Calculation for marketing spending. \\\\n\\\\nCalculating the amount of revenue we could receive from a customer over the lifetime of our relationship with them, we might better tailor our investments to maximize the value of our relationship.\\\\n\\\\nTo calculate the remaining lifetime in a customer relationship we must carefully examine the transactional signals previously generated by customers in terms of the frequency and recency of their engagement. \\\\n\\\\n#datascience  #businessintelligence \\\\n#machinelearning \\\\n#customerjourney \\\\n#clv \\\\n#lifetimevalue \\\\n#customerexperience \\\\n#industrial \\\\n\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "514",
    "uploadDate": "2022-06-13",
    "thumbnail_url": "https://i.ytimg.com/vi/dBLIIqjIGLg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Zw3uVi_okb0",
    "title": "AI humiliates your Customers?  Challenges for BUSINESS AI 2022",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales",
    "scraped_at": 1685113828.780841,
    "genre": "Science",
    "views": "25",
    "desc": "Are \\\\\"",
    "lengthSeconds": "830",
    "uploadDate": "2022-06-11",
    "thumbnail_url": "https://i.ytimg.com/vi/Zw3uVi_okb0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vQaTNKSXr5M",
    "title": "ML Layers = Tensor Multiplication Operators: Multiply Weights w/ Feature vectors, Version 2022",
    "tags": "Tensor, Tensor Algebra, Tensor product, Tensor dot, Rank 4 Tensor, Perceptron",
    "scraped_at": 1685113828.8818402,
    "genre": "Science",
    "views": "215",
    "desc": "How can a rank-4 tensor (as input to our Neural Network model) fit in a simple layer, A KERAS layer? What mathematical operations (tf.tensordot) are performed in a DENSE Keras layer? TensorFlow 2.9\\\\n\\\\nExplanations of DENSE LAYERS based on a simple human decision and the perceptron explained for deeper layer tensor algorithm. \\\\n\\\\nTensors are n-dim arrays that define form and shape of input data to our Neural Network models. Layer operations are like functions applied to tensors. A layer operation takes tensors as input, performs operations (dense layer, pooling layer, convolutional layer) and outputs a tensor. \\\\n\\\\nUnderstanding tensors is important for designing your multiple stacked  layers of your neural network model. \\\\n\\\\nTensorflow2 operates on tensors, and tf.keras.layers are high-level APIs. \\\\nDesign your network layers of your NN model with tf.tensordot operations along the right axes. \\\\n\\\\n#tensor \\\\n#tensorflow2 \\\\n#tensoranalysis \\\\n#algebra \\\\n#arrays \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#neuralnetwork \\\\n#neuralnetworks \\\\n#input \\\\n#data \\\\n#numpy \\\\n#keras \\\\n#tensors \\\\n#multiplication \\\\n#matrixmultiplication \\\\n#perceptron\"",
    "lengthSeconds": "1347",
    "uploadDate": "2022-06-07",
    "thumbnail_url": "https://i.ytimg.com/vi/vQaTNKSXr5M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=MQhwpM8_vPg",
    "title": "ML predicts real-time Marketing Campaign Success  (2/2)  |  BUSINESS AI 2022",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales",
    "scraped_at": 1685113828.9508402,
    "genre": "Education",
    "views": "69",
    "desc": "Combine Landing page visits and Social Media Likes for Campaign Success in a ML prediction model (Part 2/2). A Databricks Python /PySpark code notebook to examine how to analyze marketing attribution with applying Machine Learning ML algorithms.\\\\n\\\\nBased on a Databricks Blog: https://databricks.com/blog/2020/10/05/measuring-advertising-effectiveness-with-sales-forecasting-and-attributing.html\\\\n\\\\nA real world business example and how AI is implemented to analyze Marketing MMM and Attribution. How do you connect the impact of marketing and your ad spend toward driving sales? MLFlow helps to optimize your models.\\\\n\\\\nAll credits to (blog and notebook):\\\\nhttps://databricks.com \\\\n\\\\n#businessintelligence  \\\\n#ai  \\\\n#attribution \\\\n#marketingdigital  \\\\n#marketingstrategies  \\\\n#forecasts \\\\n#machinelearningwithpython  \\\\n#pyspark  \\\\n#campaign \\\\n#deeplearningtutorial  \\\\n#machinelearningwithpython  \\\\n#machinelearning   \\\\n#databricks \\\\n#forecasts \\\\n#ads \\\\n#marketingsocial  \\\\n#databricks \\\\n#attribution \\\\n\\\\n\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "521",
    "uploadDate": "2022-06-05",
    "thumbnail_url": "https://i.ytimg.com/vi/MQhwpM8_vPg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YVaHV-IlS6E",
    "title": "Channels to spend your Money for driving Sales (1/2) ? |  BUSINESS AI 2022",
    "tags": "Business, AI, Business and AI, python, Jupyter NB, Marketing, Attribution, Sales",
    "scraped_at": 1685113829.0178397,
    "genre": "Science",
    "views": "72",
    "desc": "Business AI example for Marketing MMM (Media Mix Model), sales forecasting and Attribution. How do you connect the impact of marketing and your ad spend toward driving sales?\\\\n3 Min of Intro and then pure Pyspark code! \\\\n\\\\nA Databricks Python notebook to examine how to analyze marketing attribution with applying machine Learning ML algorithms. Part 1 of 2. Based on a Databricks Blog: https://databricks.com/blog/2020/10/05/measuring-advertising-effectiveness-with-sales-forecasting-and-attributing.html\\\\n\\\\nTo calculate the remaining lifetime in a customer relationship we must carefully examine the transactional signals previously generated by customers in terms of the frequency and recency of their engagement. \\\\n\\\\nAll credits to (blog and notebook):\\\\nhttps://databricks.com \\\\n\\\\n#businessintelligence \\\\n#pyspark \\\\n#Marketing_Attribution\\\\n#marketingsocial \\\\n#mediamix \\\\n#businessdata \\\\n#businesssecret \\\\n#attribution  \\\\n#jupyterlab \\\\n#datascience \\\\n#dataanalytics \\\\n#databricks \\\\n#pythonprogramming \\\\n\\\\n\\\\n00:00 Talking head Intro\\\\n03:33 Jupyter NB - Python code\\\\n07:45 Data Pipelines \\\\n09:25 Gold Delta Table - Delta Lake \\\\n\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "803",
    "uploadDate": "2022-06-02",
    "thumbnail_url": "https://i.ytimg.com/vi/YVaHV"
  },
  {
    "link": "watch?v=bfwvI3iz-1w",
    "title": "Why a TENSOR in ML, Neural Networks? What about PANDAS dataframes?",
    "tags": "Tensor, Tensor Algebra, Tensor product, Tensor dot, Rank 4 Tensor",
    "scraped_at": 1685113829.2108397,
    "genre": "Science",
    "views": "105",
    "desc": "Tensors are n-dim arrays that define form and shape of input data to our Neural Network models. Tensors rank, shape and axis are important for layer operations: like functions applied to tensors. \\\\n\\\\nA layer operation takes tensors as input, performs operations (dense layer, pooling layer, convolutional layer) and outputs a tensor. \\\\n\\\\nTo understand rank and dim of tensors is important for designing your multiple layers of your neural network model. Tensors allow for automated differentiation, which is great for gradient descent of your ML models.\\\\n\\\\nTensorflow2 operates on tensors, and tf.keras.layers are high-level APIs. \\\\nDesign your network layers of your NN model with tf.tensordot operations along the right axes. \\\\n\\\\n#tensorflow2 \\\\n#tensor \\\\n#definition \\\\n#tensoranalysis  \\\\n#mathematics \\\\n#keras  \\\\n#deeplearning  \\\\n#machinelearningwithpython  \\\\n#shape \\\\n#dimension \\\\n#matrices \\\\n#matrix \\\\n#layers \\\\n#axis\"",
    "lengthSeconds": "1203",
    "uploadDate": "2022-05-30",
    "thumbnail_url": "https://i.ytimg.com/vi/bfwvI3iz"
  },
  {
    "link": "watch?v=wrFLQR2qVk0",
    "title": "BERT Transformers for Sentences: Python Code for Sentence Similarity, Edition 2022 |  Part 3/3",
    "tags": "BERT, NAtural Language processing, NLProc, Transformer model, Encoder",
    "scraped_at": 1685113829.2788408,
    "genre": "Science",
    "views": "400",
    "desc": "A COLAB (3/3) Notebook to follow along with BERT model applied to calculate sentence similarity with encoder stack of transformers. Python code. TensorFlow and KERAS. Self attention. Frozen BERT models with operational added KERAS layers. Compare this to SBERT!\\\\n\\\\nDeep Bidirectional Encoder Representation Transformers for Language Understanding. \\\\n\\\\nBERT Wordpiece Tokenizer. Special BERT token. Attention masks. BERT Data Generator. Google created a transformer-based machine learning approach for natural language processing pre-training called Bidirectional Encoder Representations from Transformers. \\\\n\\\\nNot an SBERT model. \\\\n\\\\nOriginal BERT paper:\\\\nhttps://arxiv.org/pdf/1810.04805.pdf\\\\n\\\\nCredits to official KERAS Notebook:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/semantic_similarity_with_bert.ipynb\\\\n\\\\n#keras \\\\n#bert\\\\n#deeplearning \\\\n#deeplearningtutorial \\\\n#tensorflow2 \\\\n#neuralnetwork \\\\n#neurallayer\\\\n#lstm \\\\n#transformers \\\\n#jupyterlab \\\\n#machinelearningwithpython \\\\n#similarity \\\\n#tokenization \\\\n#colab\"",
    "lengthSeconds": "709",
    "uploadDate": "2022-05-28",
    "thumbnail_url": "https://i.ytimg.com/vi/wrFLQR2qVk0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_aTZoM7M0_4",
    "title": "BERT Transformers for Sentences: Python Code for Sentence Similarity, Update 2022 |  Part 2/3",
    "tags": "BERT, NAtural Language processing, NLProc, Transformer model, Encoder",
    "scraped_at": 1685113829.349841,
    "genre": "Science",
    "views": "701",
    "desc": "A COLAB (2/3) Notebook to follow along with BERT model applied to calculate sentence similarity with encoder stack of transformers. Python code. TensorFlow and KERAS. Self attention. Frozen BERT models with operational added KERAS layers. Compare this to SBERT!\\\\n\\\\nDeep Bidirectional Encoder Representation Transformers for Language Understanding. \\\\n\\\\nBERT Wordpiece Tokenizer. Special BERT token. Attention masks. BERT Data Generator. Google created a transformer-based machine learning approach for natural language processing pre-training called Bidirectional Encoder Representations from Transformers. \\\\n\\\\nOriginal BERT paper:\\\\nhttps://arxiv.org/pdf/1810.04805.pdf\\\\n\\\\nCredits to official KERAS Notebook:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/semantic_similarity_with_bert.ipynb\\\\n\\\\n\\\\n#keras \\\\n#bert\\\\n#deeplearning \\\\n#deeplearningtutorial \\\\n#tensorflow2 \\\\n#neuralnetwork \\\\n#neurallayer\\\\n#lstm \\\\n#transformers \\\\n#jupyterlab \\\\n#machinelearningwithpython \\\\n#similarity \\\\n#tokenization \\\\n#colab #keras \\\\n#bert\\\\n#deeplearning \\\\n#deeplearningtutorial \\\\n#tensorflow2 \\\\n#neuralnetwork \\\\n#neurallayer\\\\n#lstm \\\\n#transformers \\\\n#jupyterlab \\\\n#machinelearningwithpython \\\\n#similarity \\\\n#tokenization \\\\n#colab\"",
    "lengthSeconds": "865",
    "uploadDate": "2022-05-26",
    "thumbnail_url": "https://i.ytimg.com/vi/_aTZoM7M0_4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=y7JQmegD3vg",
    "title": "BERT Transformers for Sentences: Python Code for Sentence Similarity, Update 2022 |  Part 1/3",
    "tags": "BERT, NAtural Language processing, NLProc, Transformer model, Encoder",
    "scraped_at": 1685113829.4238415,
    "genre": "Science",
    "views": "2211",
    "desc": "A COLAB (1/3) Notebook to follow along with BERT model applied to calculate sentence similarity with encoder stack of transformers. Python code. TensorFlow and KERAS. Self attention. Compare this to SBERT!\\\\n\\\\nDeep Bidirectional Encoder Representation Transformers for Language Understanding. \\\\n\\\\nBERT Wordpiece Tokenizer. Special BERT token. Attention masks. BERT Data Generator. Google created a transformer-based machine learning approach for natural language processing pre-training called Bidirectional Encoder Representations from Transformers. \\\\n\\\\nOriginal BERT paper:\\\\nhttps://arxiv.org/pdf/1810.04805.pdf\\\\n\\\\nCredits to official KERAS Notebook:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/semantic_similarity_with_bert.ipynb\\\\n\\\\n#keras \\\\n#bert\\\\n#deeplearning \\\\n#deeplearningtutorial \\\\n#tensorflow2 \\\\n#neuralnetwork \\\\n#neurallayer\\\\n#lstm \\\\n#transformers \\\\n#jupyterlab \\\\n#machinelearningwithpython \\\\n#similarity \\\\n#tokenization \\\\n#colab \\\\n#sbert\"",
    "lengthSeconds": "861",
    "uploadDate": "2022-05-24",
    "thumbnail_url": "https://i.ytimg.com/vi/y7JQmegD3vg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=cZdrT7-MqPA",
    "title": "Buy a CLOUD!  |  BUSINESS AI 2022",
    "tags": "AI, Business, AI Business, Ai Business implementation, AutoML, MAchine Learning",
    "scraped_at": 1685113833.6128674,
    "genre": "Science",
    "views": "18",
    "desc": "AI for Business people.  Legal benefits of cloud companies for your business. GDPR. Compliance standards. Cyber security. Business model for CLOUD companies (Google Cloud, AWS, Microsoft).\\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Data-driven decision making for Business transformation. AI Integration in business processes.\\\\n\\\\nLink to my videos:\\\\nhttps://youtu.be/4_ZT6hvlhGQ\\\\nhttps://youtu.be/1h-IdF1KRWw\\\\nhttps://youtu.be/nb1d97-CbCM\\\\n\\\\n#MachineLearning\\\\n#deeplearning \\\\n#businessintelligence  \\\\n#businessdata \\\\n#ai \\\\n#machinelearning \\\\n#deeplearning \\\\n#datascience \\\\n#dataanalytics \\\\n#databricks \\\\n#datalake \\\\n#deltalake \\\\n#insight \\\\n#nlptechniques \\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "391",
    "uploadDate": "2022-05-23",
    "thumbnail_url": "https://i.ytimg.com/vi/cZdrT7"
  },
  {
    "link": "watch?v=m9eq-kQr_VM",
    "title": "Start the Data Revolution  - Tech recommendations  |  BUSINESS AI 2022",
    "tags": "AI, Business, Enterprise, Machine Learning, Deep Learning, Data strategy, Business transformation",
    "scraped_at": 1685113829.4968398,
    "genre": "Science",
    "views": "23",
    "desc": "AI for Business people.  On technical (!) recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups.\\\\nThe first 10 part mini-series was on general (!) recommendations.  \\\\n\\\\nWhat if future decision-making is grounded in your data? Reproducible decisions will happen in your teams. Behavioral adaptation to data-based decision making in your company. Like XAI. \\\\n\\\\nPresentation slide from databricks:\\\\nhttps://databricks.com/blog/2020/10/05/measuring-advertising-effectiveness-with-sales-forecasting-and-attributing.html\\\\n\\\\n\\\\n\\\\n#businessintelligence \\\\n#businessdata \\\\n#ai \\\\n#machinelearning \\\\n#deeplearning \\\\n#datascience \\\\n#dataanalytics \\\\n#databricks \\\\n#datalake \\\\n#deltalake \\\\n#insight \\\\n#nlptechniques \\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "337",
    "uploadDate": "2022-05-22",
    "thumbnail_url": "https://i.ytimg.com/vi/m9eq"
  },
  {
    "link": "watch?v=o1zcWYrulEQ",
    "title": "KERAS Preprocessing Layers on VISION Tech (TensorFlow 2.9 on Data Augmentation)",
    "tags": "TensorFlow, KERAS, VISION, Data Augmentation",
    "scraped_at": 1685113829.5658395,
    "genre": "Science",
    "views": "734",
    "desc": "In COLAB we apply KERAS pre-processing Layers on Data: Pictures of Flowers. Hand-on Experience to code KERAS /TensorFlow2 for VISION. \\\\n\\\\nWith Keras preprocessing layers, you can build and export models that are truly end-to-end: models that accept raw images or raw structured data as input; models that handle feature normalization or feature value indexing on their own.\\\\n\\\\nCredits to:\\\\nhttps://www.tensorflow.org/tutorials/images/data_augmentation\\\\n\\\\nCOLAB:\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb\\\\n\\\\n#ai \\\\n#machinelearningwithpython \\\\n#keras  \\\\n#layers \\\\n#tensorflow2 \\\\n#datascience \\\\n#data_augment\\\\n#preprocessing \\\\n#deeplearning \\\\n#machinelearning \\\\n#machinelearningwithpython \\\\n#clustering \\\\n#embedding\"",
    "lengthSeconds": "851",
    "uploadDate": "2022-05-19",
    "thumbnail_url": "https://i.ytimg.com/vi/o1zcWYrulEQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=HH1JKRcxX3Y",
    "title": "BUSINESS and AI: Invest first in data strategy or AI implementation?",
    "tags": "AI, Business, AI Business, Ai Business implementation, AutoML, MAchine Learning",
    "scraped_at": 1685113829.6408396,
    "genre": "Science",
    "views": "23",
    "desc": "AI for Business people. Final invest decision: Should I invest in DATA or AI first? \\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Data-driven decision making for Business transformation. AI Integration in business processes.\\\\n\\\\nLink to video presented:  \\\\nDELTA LAKE by Databricks    https://youtu.be/Bx16-U1sGtk\\\\nLAKEHOUSE by Databricks    https://youtu.be/c55Diu3M-lw\\\\nAutoML Machine Learning     https://youtu.be/d1fbM-cPKZQ\\\\n\\\\n@Databricks \\\\n\\\\n#Business\\\\n#AI\\\\n#MachineLearning\\\\n#DeepLearning\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "480",
    "uploadDate": "2022-05-17",
    "thumbnail_url": "https://i.ytimg.com/vi/HH1JKRcxX3Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GMl5sIsGQSM",
    "title": "BUSINESS and AI: Data-Driven Enterprise for NEW Business Transformation",
    "tags": "AI, Business, Enterprise, Machine Learning, Deep Learning, Data strategy, Business transformation",
    "scraped_at": 1685113829.73584,
    "genre": "Science",
    "views": "26",
    "desc": "AI for Business people. What if future decision-making is grounded in data? Reproducible decisions. For your staff / teams to see and understand. Behavioral adaptation ON ALL LEVELS. Company culture will evolve...\\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. \\\\n\\\\nLink to video presented:  https://youtu.be/1o5qD3hvThg\\\\n\\\\n#Business\\\\n#AI\\\\n#MachineLearning\\\\n#DeepLearning\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "312",
    "uploadDate": "2022-05-16",
    "thumbnail_url": "https://i.ytimg.com/vi/GMl5sIsGQSM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=xx8eA0Wx6Dg",
    "title": "BUSINESS and AI: BUY an external off-the-shelf AI?",
    "tags": "AI, Business, artificial Intelligence, Machine Learning, Deep Learning, Natural Language Processing",
    "scraped_at": 1685113829.8118405,
    "genre": "Science",
    "views": "32",
    "desc": "AI for Business people. Costs and immanent risks: Pre-trained, off-the-shelf AI platforms have hidden costs. \\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Machine Learning and Deep Learning applied to business processes?\\\\n\\\\nLink to video presented:   https://youtu.be/4_ZT6hvlhGQ\\\\n\\\\n#businessintelligence \\\\n#ai \\\\n#machinelearningwithpython \\\\n#deeplearning \\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "458",
    "uploadDate": "2022-05-14",
    "thumbnail_url": "https://i.ytimg.com/vi/xx8eA0Wx6Dg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kMbiWUD7L3Y",
    "title": "BUSINESS and AI: Train your AI on your DATA",
    "tags": "AI, Train AI, Business, Artificial Intelligence, Machine Learning, Deep Learning",
    "scraped_at": 1685113829.8938413,
    "genre": "Science",
    "views": "21",
    "desc": "AI for Business people. Another simple question: Is your AI biased, right at the customer interface? \\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Machine Learning and Deep Learning applied to business processes?\\\\n\\\\nLink to video presented:   https://youtu.be/m5y579SnvzI\\\\n\\\\n#Business\\\\n#AI\\\\n#MachineLearning\\\\n#DeepLearning\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "388",
    "uploadDate": "2022-05-13",
    "thumbnail_url": "https://i.ytimg.com/vi/kMbiWUD7L3Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=c3nm7leZaVw",
    "title": "BUSINESS and AI: Start SMART with a pre-defined AI for your business?",
    "tags": "AI, XAI, Business",
    "scraped_at": 1685113829.9768412,
    "genre": "Science",
    "views": "32",
    "desc": "AI for Business. Start the DATA revolution before you start the AI revolution in your company. \\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Machine Learning and Deep Learning applied to business processes?\\\\n\\\\n#businessintelligence \\\\n#ai \\\\n#machinelearningwithpython \\\\n#deeplearning \\\\n#datascience \\\\n#revolution \\\\n#data \\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\"",
    "lengthSeconds": "484",
    "uploadDate": "2022-05-11",
    "thumbnail_url": "https://i.ytimg.com/vi/c3nm7leZaVw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qr563OgwHX8",
    "title": "BUSINESS and AI: Human-centered AI for Business Processes",
    "tags": "AI, Business, XAI, Explainable AI",
    "scraped_at": 1685113832.5348406,
    "genre": "Science",
    "views": "46",
    "desc": "AI for Business people. Insights from a business implementation: Usability of AI is paramount.\\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Artificial Intelligence, Machine Learning and Deep Learning applied to business processes?\\\\n\\\\nLink to video presented   https://youtu.be/DAP0ofXbzaw\\\\n\\\\n#businessintelligence \\\\n#ai \\\\n#machinelearningwithpython \\\\n#deeplearning \\\\n#iq \\\\n#usability \\\\n#deeplearning \\\\n#userexperience \\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "381",
    "uploadDate": "2022-05-10",
    "thumbnail_url": "https://i.ytimg.com/vi/qr563OgwHX8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qMmLByQ3QLU",
    "title": "5 Sectors dominate my Channel in Q1 2022- TOP AI videos",
    "tags": "XAI, SBERT, Transformer models, Graph Neural networks, PyTorch, TensorFlow, KERAS, Language models",
    "scraped_at": 1685113830.052841,
    "genre": "Science",
    "views": "36",
    "desc": "My important videos in 5 sectors: \\\\n\\\\n1. PySpark and Delta Lake (Databricks)\\\\n2. Language models and Data Science App Dev (Gradio/Streamlit)\\\\n3. Sentence Transformer models (SBERT)\\\\n4. Explainable AI - XAI (Business model, risks, smart, ethics)\\\\n5. Graph Neural Networks (Representation Learning) \\\\n\\\\nYou can view this configuration and display of my top videos in 5 sectors in a plane as a new representation of data. Newly configured for easy uptake. Like representation learning for graph structured data to forward to our ML models. \\\\n\\\\n#ai  \\\\n#datascience   #pyspark  #deeplearning  #machinelearning  #businessintelligence  #pyspark  #python  #pythonprogramming  #explainableAI  #GraphNeuralNetwork  \\\\n#tensorflow2  #pytorch  #bert  #sbert  #datascience  #dataengineering  #appdevelopment #word2vec\"",
    "lengthSeconds": "307",
    "uploadDate": "2022-05-08",
    "thumbnail_url": "https://i.ytimg.com/vi/qMmLByQ3QLU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3mpzWKGNfjU",
    "title": "BUSINESS and AI: From BI to Business AI",
    "tags": "AI, Business, CEO, Consultant",
    "scraped_at": 1685113830.1238403,
    "genre": "Science",
    "views": "28",
    "desc": "AI for Business people. A simple truth: From BI to AI will enhance your organisational dynamics. \\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Artificial Intelligence, Machine Learning and Deep Learning applied to business processes?\\\\n\\\\nLink to video presented: https://youtu.be/1h-IdF1KRWw\\\\n\\\\n#businessintelligence \\\\n#ai \\\\n#machinelearningwithpython \\\\n#deeplearning \\\\n#businessdata \\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "283",
    "uploadDate": "2022-05-07",
    "thumbnail_url": "https://i.ytimg.com/vi/3mpzWKGNfjU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=7Wq1fDHGO_Y",
    "title": "BUSINESS and AI: Business AI strategy",
    "tags": "AI, Business, company, Insights",
    "scraped_at": 1685113830.1968663,
    "genre": "Science",
    "views": "39",
    "desc": "AI for Business people. The third simple question: Are your teams ready for change? AI implementations might induce digital business transformations. Time for a risk assessment. \\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Machine Learning and Deep Learning applied to business processes?\\\\n\\\\nLink to video presented:  https://youtu.be/nb1d97-CbCM\\\\n\\\\n#Business\\\\n#AI\\\\n#MachineLearning\\\\n#DeepLearning\\\\n\\\\nhttps://youtu.be/nb1d97-CbCM\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "234",
    "uploadDate": "2022-05-06",
    "thumbnail_url": "https://i.ytimg.com/vi/7Wq1fDHGO_Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=rV8qDMPHufg",
    "title": "SBERT 2022 Generative Pseudo Labeling (GPL): Domain Adaptation Sentence Transformers (SBERT 26)",
    "tags": "BERT, Transformer, AI, Machine Learning, Deep Learning, Sentnece Transformer, SBERT, GPL, Pseudo Labelling",
    "scraped_at": 1685113830.2658396,
    "genre": "Science",
    "views": "882",
    "desc": "Sentence transformers - SBERT GPL (Generative Pseudo Labeling)  is an unsupervised domain adaptation method for training dense retrievers. Add new domain knowledge to your pre-trained Sentence Transformer. Sentence Embedding. Sentence BERT.\\\\n\\\\nIt is based on query generation and pseudo labeling with powerful cross-encoders. To train a domain-adapted model, it needs only the unlabeled target corpus and can achieve significant improvement over zero-shot models.\\\\n\\\\nOfficial links (credits to):\\\\nhttps://github.com/UKPLab/gpl\\\\nhttps://arxiv.org/abs/2112.07577\\\\n\\\\n#ai \\\\n#machinelearningwithpython \\\\n#SentenceTransformers\\\\n#SBERT\\\\n#SentenceBERT\\\\n#bert \\\\n#nlproc \\\\n#nlptechniques \\\\n#knowledge \\\\n#sentences  \\\\n#pytorch \\\\n#datascience \\\\n\\\\n\\\\nCredits to:\\\\n\\\\n@article{wang2021gpl,\\\\n    title = \\\\\"",
    "lengthSeconds": "1186",
    "uploadDate": "2022-05-04",
    "thumbnail_url": "https://i.ytimg.com/vi/rV8qDMPHufg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=KQBndpvt3QA",
    "title": "BUSINESS and AI: Business AI software?",
    "tags": "AI, Business, Cloud based service provider",
    "scraped_at": 1685113832.6038394,
    "genre": "Science",
    "views": "37",
    "desc": "AI for Business people. What kind of AI do you need to achieve your new business / performance goals? Grab the lightning rod of AI induced business transformations? Machine Learning and Deep Learning applied to business processes?\\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. \\\\n\\\\nLink to video presented: https://youtu.be/0fFs5ASMFzc\\\\n\\\\n#Business\\\\n#AI\\\\n#MachineLearning\\\\n#DeepLearning\\\\n\\\\nhttps://youtu.be/0fFs5ASMFzc\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\"",
    "lengthSeconds": "542",
    "uploadDate": "2022-05-03",
    "thumbnail_url": "https://i.ytimg.com/vi/KQBndpvt3QA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=TMM7Jcj591M",
    "title": "BUSINESS and AI: Artificial Intelligence in YOUR business processes",
    "tags": "AI, Business, Commercial Risks",
    "scraped_at": 1685113830.3548746,
    "genre": "Science",
    "views": "58",
    "desc": "AI for Business people. One simple question: Does your teams in your company really need AI? Machine Learning and Deep Learning applied to business processes?\\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. \\\\n\\\\nLink to video presented: https://youtu.be/GtNKWxqPLqc\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region. \\\\n\\\\n#Business\\\\n#AI\\\\n#MachineLearning\\\\n#DeepLearning\\\\n\\\\nhttps://youtu.be/GtNKWxqPLqc\"",
    "lengthSeconds": "256",
    "uploadDate": "2022-05-02",
    "thumbnail_url": "https://i.ytimg.com/vi/TMM7Jcj591M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hXGpEZoHvMo",
    "title": "BUSINESS and AI: WHY should you listen to me?",
    "tags": "AI, ML, Deep Learning, Explainable Ai, Decision Intelligence, Business Intelligence, Multi node cloud infrastructure",
    "scraped_at": 1685113830.4368412,
    "genre": "Science",
    "views": "121",
    "desc": "Welcome to my NEW SERIES on BUSINESS and AI. Implementing Machine Learning and Deep Learning for Businesses. General ideas for non-experts in AI. Primarily AI for Business people.\\\\n\\\\nA 10 part mini-series on general recommendations for companies planning to incorporate Artificial Intelligence in their business processes. Tips and empirical knowledge to know before entering any contractual obligation for your business. Including AI advice for start-ups. Machine Learning and Deep Learning applied to business processes?\\\\n\\\\n#Business\\\\n#AI\\\\n#MachineLearning\\\\n#DeepLearning\\\\n\\\\nLegal disclaimer: Recommendations presented within theses YouTube videos are personal empirical knowledge, you might use these ideas as input to your business specific risk assessment. If you choose to apply this knowledge, is completely up to you. Whatever you induce from your risk assessment, is completely up to you. It is your responsibility. As always, when you run a business. \\\\nIt is highly recommended to ask for multiple offers from different Software/Hardware/System vendors, and the aim of theses videos is to make you aware of some important considerations, whereas these recommendations are not at all a complete set of recommendations. Special considerations apply for specific sectors, concerning internet and cyber security, official compliance regulations in your country or state and consumer protection law in your legislative region.\\\\n\\\\nPlease verify license limitations regarding your use of data sets. Common one are cc-by-4-0, MIT, cc-by-sa-4-0, cc-by-sa-3-0, apache-2-0, apache-2.0, cc0-1.0, ....\"",
    "lengthSeconds": "552",
    "uploadDate": "2022-05-01",
    "thumbnail_url": "https://i.ytimg.com/vi/hXGpEZoHvMo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=W8rlBUTMCdM",
    "title": "NEURAL Bellman-Ford NETWORK - 2022 Neural BFNet - Graph Neural Networks w/ Link Prediction AI",
    "tags": "Graph Neural Networks, GNN, Link prediction, Node embedding, Neural Bellman",
    "scraped_at": 1685113830.514841,
    "genre": "Science",
    "views": "221",
    "desc": "Neural Bellman-Ford Networks -  A brand-new  representation learning framework based on paths for link prediction:\\\\nA. representation of a pair of nodes as the generalized sum of all path representations between the nodes,\\\\nB. with each path representation as the generalized product of the edge representations in the path. \\\\n\\\\nGraph Neural Networks (GNNs), Machine Learning, Deep Learning, AI.\\\\n\\\\nSingle document (credits to):\\\\n\\\\\"",
    "lengthSeconds": "1624",
    "uploadDate": "2022-04-29",
    "thumbnail_url": "https://i.ytimg.com/vi/W8rlBUTMCdM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=M-YgtcD8cto",
    "title": "Domain Adapt SBERT: Adaptive Pre-Training for Sentence Transformers Domain Learning, 2022 (SBERT 25)",
    "tags": "AI, NLProc, Sentence transformers, Python, Jupyter NB, JupyterLab, #machine Learning, Deep Learning, Language",
    "scraped_at": 1685113830.5918415,
    "genre": "Science",
    "views": "1722",
    "desc": "Domain Adaptation - SentenceTransformers SBERT : Goal is to adapt text embedding models to your specific text domain. \\\\n\\\\nEasy Theory and python code in Jupyter NB / COLAB. Python. SBERT.\\\\nBERT. Transformers. HuggingFace. \\\\n\\\\nDiscover how text embeddings model can be adapted to your specific domain.\\\\nAs requested by my viewers (poll in community tab 2 days ago).\\\\n\\\\n00:00 Neural Search\\\\n03:09 Domain Adaptation\\\\n06:52 Adaptive Pre-training\\\\n10:26 Python code in Jupyter NB\\\\n15:20 Outlook Improvements\\\\n\\\\n\\\\n#nlproc \\\\n#SentenceTransformers\\\\n#python \\\\n#pythonprogramming \\\\n#pytorch \\\\n#colab \\\\n#ai \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#sbert\\\\n#bert \\\\n#domain \\\\n#adaptation \\\\n\\\\n\\\\n\\\\nAll Credits to: \\\\nhttps://sbert.net\\\\n\\\\n@inproceedings{reimers-2019-sentence-bert,\\\\n    title = \\\\\"",
    "lengthSeconds": "1046",
    "uploadDate": "2022-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/M"
  },
  {
    "link": "watch?v=j3ZEk8lO7BM",
    "title": "NEW TOPOLOGICAL LAYER in Graph Neural Networks (GCN), Filtrations, Persistent Homology  -  ICLR 2022",
    "tags": "Homology, Persistent Homology, Vertex filtration, Learnable Filtration, Topology, Graph Neural Networks",
    "scraped_at": 1685113830.6658468,
    "genre": "Science",
    "views": "303",
    "desc": "NEW: integrate a topological layer as one of the Graph Convolutional Network (GCN) layer in to your GCN to obtain essential topological info about the Graph. Persistent Homology, Learnable Filtrations and Topology. Topological Data Analysis (TDA).\\\\n\\\\nAlthough this method is limited to l=1, connected components and cycles, the approach is elegant, since this topological layer is compatible with Graph Neural Network layers, say GCN with our Deep Graph Library implementation (DGL). \\\\n\\\\n#ai \\\\n#machinelearning \\\\n#machinelearningwithpython \\\\n#topology \\\\n#graphs \\\\n#convolutionalneuralnetwork \\\\n#graph \\\\n#topology \\\\n#neuralnetwork \\\\n\\\\nCredits to \\\\u0026 highly recommend reading this:\\\\nMain link to pre-print(v4):\\\\nhttps://arxiv.org/abs/2102.07835v4\\\\nby Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, Karsten Borgwardt\\\\n\\\\nPlease follow:\\\\n@Pseudomanifold \\\\n\\\\n00:00 Topological Graph Neural Networks\\\\n02:23 Computational Topology\\\\n06:13 Main idea: Topological Layer \\\\n12:47 Topological Data Analysis (TDA)\\\\n13:43 Example of Graph Filtrations\"",
    "lengthSeconds": "1225",
    "uploadDate": "2022-04-25",
    "thumbnail_url": "https://i.ytimg.com/vi/j3ZEk8lO7BM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=H2nMdrECbss",
    "title": "TOP 5 Sources of code examples PyTorch, TensorFlow, Python for AI, ML, Deep Learning - Learn smart",
    "tags": "Code sources, excellent code examples, free jupyter notebooks, free code, open code, kaggle, paper with code, twitter",
    "scraped_at": 1685113832.6758661,
    "genre": "Science",
    "views": "65",
    "desc": "By viewers request my top 5 Sources of excellent code and theory publications on AI, Deep Learning and Machine Learning. Including Graph Neural networks, Transformer models, BERT sentence Transformers and more. Artificial intelligence code examples. Learn coding smart. Learn from the best.\\\\n\\\\n#pythonprogramming \\\\n#ai \\\\n#pytorch \\\\n#jupyterlab \\\\n#tensorflow2 \\\\n#machinelearningwithpython \\\\n#deeplearning \\\\n#bert \\\\n#sbert\\\\n#datascience \\\\n#GraphNeuralNetwork\\\\n\\\\n@kaggle\"",
    "lengthSeconds": "605",
    "uploadDate": "2022-04-23",
    "thumbnail_url": "https://i.ytimg.com/vi/H2nMdrECbss/maxresdefault.jpg"
  },
  {
    "link": "watch?v=AlHcx7RW1yo",
    "title": "NodePiece code for Knowledge Graphs in Python, clever Node embedding in 2022",
    "tags": "AI, Sentence embedding, Transformer, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, train AI model, NodePiece, Tokenizer, Knowledge Graphs",
    "scraped_at": 1685113830.7388408,
    "genre": "Science",
    "views": "161",
    "desc": "NodePiece, a more parameter-efficient node embedding strategy for Knowledge Graphs. In NodePiece, a vocabulary of subword/sub-entity units is constructed from anchor nodes in a graph with known relation types. \\\\n\\\\nSpecial benefit: Compositional encoding is inductive by design, d.h. new nodes can be added with training the whole Graph. \\\\n\\\\nCredits to: arXiv preprint (Feb 1, 2022):\\\\nhttps://arxiv.org/pdf/2106.12144.pdf\\\\nby Mikhail Galkin, Etienne Denis, Jiapeng Wu and William L. Hamilton\\\\n\\\\nGitHub link:\\\\nhttps://github.com/migalkin/NodePiece\\\\n\\\\n#GraphEmbedding \\\\n#Tokenization \\\\n#NodePiece\"",
    "lengthSeconds": "406",
    "uploadDate": "2022-04-22",
    "thumbnail_url": "https://i.ytimg.com/vi/AlHcx7RW1yo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6MCth1fGwQQ",
    "title": "What are Graph Kernels? Graph Kernels explained, Python + Graph Neural Networks",
    "tags": "Graph Kernel, Graph Neural Networks, Graph data, AI, Machine Learning",
    "scraped_at": 1685113833.684841,
    "genre": "Science",
    "views": "976",
    "desc": "The abundance of graph-structured data and need to perform machine learning ML tasks on this data led to development of graph kernels. Machine Learning, Deep Learning.\\\\n\\\\nGraph kernels, this means kernel functions between graphs, have been proposed in the 2010s to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression setting.\\\\n\\\\nIncluding:\\\\nWasserstein Weisfeiler-Lehman Graph Kernels\\\\nWeisfeiler-Lehman Graph Kernels\\\\n\\\\nRelevant ARXIV pre-prints presented in last minute of my video.\\\\n\\\\n#machinelearning \\\\n#ai \\\\n#neuralnetwork \\\\n#kernel \\\\n#graphs \\\\n#GraphKernels\\\\n#pythonprogramming \\\\n\\\\nRecommended links:\\\\nhttps://arxiv.org/pdf/2011.03854.pdf\\\\nhttps://arxiv.org/pdf/1904.12218.pdf\\\\nhttps://jmlr.csail.mit.edu/papers/volume12/shervashidze11a/shervashidze11a.pdf\\\\nhttps://arxiv.org/pdf/1906.01277v2.pdf\"",
    "lengthSeconds": "578",
    "uploadDate": "2022-04-20",
    "thumbnail_url": "https://i.ytimg.com/vi/6MCth1fGwQQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=e3XiJdBJo9c",
    "title": "Groupby explained step-by-step and how to use Pivot Table for Pandas DataFrame: Python w/ Example",
    "tags": "Pandas, Pandas DataFrame, GroupBy, Pivot_table, Structures DataFrames, COLAB, Jupyter NB, JupyterLab, Python",
    "scraped_at": 1685113830.8228424,
    "genre": "Science",
    "views": "133",
    "desc": "Compare the \\\\\"",
    "lengthSeconds": "939",
    "uploadDate": "2022-04-18",
    "thumbnail_url": "https://i.ytimg.com/vi/e3XiJdBJo9c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=e4e6h9arD78",
    "title": "Weisfeiler-Lehman WL Test for Graph Isomorphism explained visually & Message Passing NNs 2022",
    "tags": "Graph Theory, Graph Neural Networks, Weisfeiler lehman Test, Graph Isomorphism",
    "scraped_at": 1685113830.9138415,
    "genre": "Science",
    "views": "2959",
    "desc": "The graph isomorphism problem and the Weisfeiler-Lehman heuristic for graph isomorphism testing method explained visually on two examples. \\\\n\\\\nA classical question in graph theory: the graph isomorphism problem, aiming to determine whether two graphs are topologically equivalent. The Weisfeiler-Lehman (WL) test for k=1. \\\\n\\\\nLink for deep dive:\\\\nhttps://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49\\\\nby Prof. Bronstein \\\\n\\\\nA key difference between traditional message passing neural networks and such higher-order GNNs is the fact that they are non-local, as the k-WL algorithm operates on k-tuples of nodes. \\\\n\\\\n\\\\n#isomorphism \\\\n#topology \\\\n#graph \\\\n#neuralnetwork \\\\n#GraphNeuralNetwork\\\\n#convolutionalneuralnetwork\"",
    "lengthSeconds": "195",
    "uploadDate": "2022-04-17",
    "thumbnail_url": "https://i.ytimg.com/vi/e4e6h9arD78/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dD9F44vhv1M",
    "title": "Python code to build a Chatbot  w/ Microsoft/DialoGPT-medium + build your App w/ Gradio",
    "tags": "MAchine Learning, Deep Learning, Pre",
    "scraped_at": 1685113831.0238404,
    "genre": "Science",
    "views": "2920",
    "desc": "Python code to build an intelligent Chatbot with pre-trained Microsoft/DialoGPT-medium: a dialogue response generation model for multiturn conversations. Plus create a ChatBot App from it - with Gradio.\\\\n\\\\nRecommend validating your pre-trained model, especially if you plan an unsupervised interface with your clients.\\\\n\\\\nA chatbot is a type of software that can help customers by automating conversations and interact with them through messaging platforms. If they are trained and validated for all possible occasions. \\\\n\\\\nlink with Python code / Jupyter NB / HuggingFace Spaces (credits to):\\\\nhttps://www.gradio.app/creating_a_chatbot/\\\\n\\\\n#ai \\\\n#pythonprogramming \\\\n#chatbot \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#deeplearning \\\\n#conversations \\\\n#bot \\\\n#nlproc \\\\n#nlp\"",
    "lengthSeconds": "377",
    "uploadDate": "2022-04-16",
    "thumbnail_url": "https://i.ytimg.com/vi/dD9F44vhv1M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=eUyXkgkUWrc",
    "title": "Learn Graph Neural Network + new videos on Neural Bellman-Ford + NodePiece",
    "tags": "Graph Neural Networks, GNN, Node2vec, Neural Bellman",
    "scraped_at": 1685113831.0988417,
    "genre": "Science",
    "views": "123",
    "desc": "Outlook for new videos of GRAPH NEURAL NETWORKS, on node embedding and GNN class, integrating path-formulation benefits for representational learning of graphs, Neural Bellman-Ford and NodePiece. \\\\n\\\\nIncludes \\\\\"",
    "lengthSeconds": "269",
    "uploadDate": "2022-04-13",
    "thumbnail_url": "https://i.ytimg.com/vi/eUyXkgkUWrc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=KC8jKqLlAHQ",
    "title": "Node2vec : TensorFlow + KERAS code in live COLAB | Graph NN 2022",
    "tags": "AI, Sentence embedding, Transformer, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, train AI model",
    "scraped_at": 1685113833.7598414,
    "genre": "Science",
    "views": "448",
    "desc": "Real-time COLAB to learn Node2vec for Graph representation learning in KERAS implementation for learning low-dimensional embeddings of nodes in a graph, w/ neighborhood-preserving objective. \\\\n\\\\nDownload your COLAB:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/graph/ipynb/node2vec_movielens.ipynb\\\\n\\\\nCredits to:\\\\nhttps://keras.io/examples/graph/node2vec_movielens/\\\\n\\\\nStanford Univ slides from Prof. Jure Leskovic:\\\\nhttps://snap.stanford.edu/node2vec/\\\\nhttps://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn\\\\n\\\\nGraph representation learning aims to learn embeddings for graph nodes, which can be used for downstream ML tasks such as clustering,  node label prediction (e.g. categorizing an article based on its citations) and link prediction (e.g. recommending an interest group to a user in a social network).\\\\n\\\\nnode2vec is a simple, yet scalable and effective technique for learning low-dimensional embeddings for nodes in a graph by optimizing a neighborhood-preserving objective. The aim is to learn similar embeddings for neighboring nodes, with respect to the graph structure.\\\\n\\\\n#keras  \\\\n#Node2vec \\\\n#colab \\\\n#graphs \\\\n#neuralnetworks \\\\n#nodes \\\\n#tensorflow2 \\\\n#graphs \\\\n#word2vec \\\\n\\\\n00:00 Graph representation learning with node2vec\\\\n03:50 Embedding nodes in a Vector space\\\\n07:30 Random-walk Embedding \\\\n10:20 Negative Sampling\\\\n13:40 Second order random-walk\\\\n16:20 Optimize Embeddings using stochastic gradient descent\\\\n20:45 tf.data Input pipeline in TensorFlow\\\\n22:10 KERAS model\\\\n27:20 Google Embedding Projector - 3D visualization\"",
    "lengthSeconds": "1799",
    "uploadDate": "2022-04-10",
    "thumbnail_url": "https://i.ytimg.com/vi/KC8jKqLlAHQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=0fFs5ASMFzc",
    "title": "News: BigLake for Data Science / ML / BI / Multi-cloud / LakeHouse",
    "tags": "BigQuery, BigLake, Delta Lake, Apache Spark, Lake House architecture, Spark, Google Ai, Vertex AI workbench",
    "scraped_at": 1685113832.741864,
    "genre": "Science",
    "views": "104",
    "desc": "April 6, 2022: Google\\'s BigLake (w/ Parquet file format) and Data Cloud Alliance announced. You are a Data Scientist: it will influence your job! Time to learn Delta Lake and Lake House architectures for multi-cloud Lake House.\\\\n\\\\nDatabricks is a main partner in both strategic decisions by Google.\\\\n\\\\nDetailed videos on both topics:\\\\nDelta Lake:  https://youtu.be/Bx16-U1sGtk\\\\nLake House:  https://youtu.be/c55Diu3M-lw\\\\n\\\\nCredits to:\\\\nhttps://cloud.google.com/solutions/data-cloud-alliance\\\\nhttps://cloud.google.com/bigquery/docs/biglake-intro\\\\n\\\\n#DataScience\\\\n#MachineLearning\\\\n#BusinessIntelligence\"",
    "lengthSeconds": "209",
    "uploadDate": "2022-04-07",
    "thumbnail_url": "https://i.ytimg.com/vi/0fFs5ASMFzc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=9lNyFrWdQmQ",
    "title": "GRAPH vs TRANSFORMER? A 3D comparison.",
    "tags": "SBERT, Sentence Transformers, Graph Theory, Network Theory, EGO nodes",
    "scraped_at": 1685113831.167841,
    "genre": "Science",
    "views": "172",
    "desc": "Python code to compare Graph Theory against Sentence transformers for content insights in huge documents.  Classical Graph Theory: Combine multiple EGO nodes in a Graph representation for detailed insights in text documents.\\\\n\\\\n#pythonprogramming \\\\n#graphtheory \\\\n#SBERT\\\\n#bert\\\\n#graphs \\\\n#nodes \\\\n#nlproc \\\\n#datascience \\\\n#TransformerModels\"",
    "lengthSeconds": "1000",
    "uploadDate": "2022-04-06",
    "thumbnail_url": "https://i.ytimg.com/vi/9lNyFrWdQmQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ApAIMGdM_u8",
    "title": "2022: GPT-neo-1.3B + smarter Text Generation (nucleus sampling)",
    "tags": "AI, Sentence embedding, Transformer, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, train AI model, GRADIO, HuggingFace Models, COLAB",
    "scraped_at": 1685113831.2438407,
    "genre": "Science",
    "views": "459",
    "desc": "Create your APP with Gradio + Python, plus nucleus sampling (top-p) on recurrent neural language models (Welleck et al. 2020) plus optimize layout of your APP with CSS. \\\\n\\\\nDownload HuggingFace model for NLP on COLAB: GPT-neo-1.3B! \\\\n\\\\nHow to generate text: \\\\nusing different decoding methods for language generation with Transformers! Watch out for inconsistency arising from incomplete decoding algorithms.\\\\n\\\\nGreat COLAB NB by HuggingFace:\\\\nhttps://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb\\\\n\\\\nWatch in real-time how to build your APP from scratch within minutes, loading your HuggingFace models and generating an UI in pure Python. \\\\n\\\\nPS: Almost crashes the 12.7GB RAM from COLAB.\\\\n\\\\n#Gradio \\\\n#colab \\\\n#ai \\\\n#nlproc \\\\n#appdevelopment \\\\n#deeplearning \\\\n#pythonprogramming \\\\n#jupyterlab \\\\n#neuralnetwork \\\\n#python \\\\n\\\\n\\\\n\\\\nLink  to (credits to):\\\\nhttps://huggingface.co/models\\\\nhttps://www.gradio.app/\\\\n\\\\nread on (credits to):\\\\n----------------------------------\\\\n\\\\\"",
    "lengthSeconds": "644",
    "uploadDate": "2022-04-03",
    "thumbnail_url": "https://i.ytimg.com/vi/ApAIMGdM_u8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=epKMlI7TXgU",
    "title": "Cluster w/ Sentence Transformers (TOP10)",
    "tags": "Sentence Transformer, SBERT, UMAP, HDBSCAN, NetworkX, Plotly, Insights",
    "scraped_at": 1685113832.8058643,
    "genre": "Science",
    "views": "717",
    "desc": "Why Cluster Sentences w/ Sentence Transformers - SBERT?  About faster INSIGHTS in a complex topic.\\\\n\\\\nHow to create TOP 10 Semantic Clusters of 3600+ pages, embedded by Sentence Transformers in a 768 dim topological space, using a pre-trained Transformer model from HuggingFace. \\\\n\\\\nRecommendations for sub-clustering semantic objects like sentences. \\\\n\\\\n#insights \\\\n#top10 \\\\n#datascience \\\\n#3danimation \\\\n#sbert\\\\n#bert \\\\n#sentence \\\\n#transformers \\\\n#topologicalspace \\\\n#clustering \\\\n#dimensions\"",
    "lengthSeconds": "1106",
    "uploadDate": "2022-03-30",
    "thumbnail_url": "https://i.ytimg.com/vi/epKMlI7TXgU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=yI2xJk9Q5DM",
    "title": "SBERT 3D Visualization : IPCC Report w/ 3600 pages (SBERT 24)",
    "tags": "Sentence transformers, SBERT, UMAP, HDBSCAN, NetworkX, Plotly, IPCC",
    "scraped_at": 1685113831.31784,
    "genre": "Science",
    "views": "408",
    "desc": "After Sentence transformers (SBERT) coding for semantic search, now we apply Sentence Transformers on semantic clustering. Given a real example: IPCC report w/ 3600 pages.\\\\n\\\\nTo understand the complex inter-dependencies of climate, Biodiversity and people, an integrated natural, social and economic science approach to assess Climate Change impacts. Can Sentence transformer (based on BERT Transformers) models help us to understand this interwoven dependencies? \\\\n\\\\nNext steps would include a dynamic interlink between cluster topics via Graph embedding or optimization via Graph Neural Network. \\\\n\\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#ipcc \\\\n#umap \\\\n#insight \\\\n\\\\nlinks used (credits to):\\\\nhttps://sbert.net/\\\\nhttps://umap-learn.readthedocs.io/en/latest/api.html\\\\nhttps://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\\\\nhttps://networkx.org/\\\\n\\\\nVideo / thumbnail on global precipitation by NASA:\\\\nhttps://gpm.nasa.gov/data/imerg\\\\nIMERG: Integrated Multi-satellitE Retrievals for GPM\\\\n\\\\n00:00 Visualization Climate \\\\n00:48 SBERT and additional Python Libraries \\\\n05:22 Python code SBERT + UMAP + HDBSCAN \\\\n17:00 Multiple subcluster SBERT Analysis\\\\n22:22 Outlook\"",
    "lengthSeconds": "1368",
    "uploadDate": "2022-03-27",
    "thumbnail_url": "https://i.ytimg.com/vi/yI2xJk9Q5DM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BTBr6I2D_8E",
    "title": "GPT-neo-1.3B +  longer Text Generation w/ GRADIO on COLAB: Data Science App for AI text",
    "tags": "AI, Sentence embedding, Transformer, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, train AI model, GRADIO, HuggingFace Models, COLAB",
    "scraped_at": 1685113831.3878427,
    "genre": "Science",
    "views": "699",
    "desc": "Language Models (LM) Text Generation with optimized GPT-neo: watch App Dev (right side of screen) in real-time with Gradio+COLAB in Python and download HuggingFace model for NLP text generation: GPT-neo-1.3B and optimize for more output sentences, longer output paragraphs! Almost crashes the 12.7GB RAM in free COLAB. Smile.\\\\n\\\\nLLM version: \\\\nGPT-neo-1.3B version updated on Dec 31, 2021. \\\\n\\\\nHow to generate text: \\\\nusing different decoding methods for language generation with Transformers!\\\\n\\\\nGreat COLAB NB by HuggingFace for more insight:\\\\nhttps://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb\\\\n\\\\nAlmost crashes the 12.7GB RAM from COLAB.\\\\n\\\\nWatch in real-time how to build your APP from scratch within minutes, load your HuggingFace model and generate an UI in pure Python. \\\\n\\\\n#appdevelopment \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#nlproc \\\\n#nlp \\\\n#gpt3 \\\\n#languagemodel \\\\n#Gradio \\\\n#colab \\\\n\\\\nLink  to (credits to this creator):\\\\nhttps://huggingface.co/models\\\\nhttps://www.gradio.app/\\\\n\\\\nRemark: \\\\nGPT-neo-1.3B has been downloaded 400000 times from HuggingFace models, at the time this video is uploaded.\\\\n\\\\n00:00 Download HuggingFace Model to my COLAB NB\\\\n04:20 Define Model parameter like length of output from LLM\\\\n05:40 Link to new App\\\\n07:36 Longer LLM output\\\\n\\\\n{gpt-neo,\\\\n  author       = {Black, Sid and\\\\n                  Leo, Gao and\\\\n                  Wang, Phil and\\\\n                  Leahy, Connor and\\\\n                  Biderman, Stella},\\\\n  title        = {{GPT-Neo: Large Scale Autoregressive Language \\\\n                   Modeling with Mesh-Tensorflow}},\\\\n  month        = mar,\\\\n  year         = 2021,\\\\n  note         = {{If you use this software, please cite it using \\\\n                   these metadata.}},\\\\n  publisher    = {Zenodo},\\\\n  version      = {1.0},\\\\n  doi          = {10.5281/zenodo.5297715},\\\\n  url          = {https://doi.org/10.5281/zenodo.5297715}\\\\n}\"",
    "lengthSeconds": "585",
    "uploadDate": "2022-03-25",
    "thumbnail_url": "https://i.ytimg.com/vi/BTBr6I2D_8E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ewlCCB7EFPs",
    "title": "Learn Sentence Transformers #SBERT: Update 2022 - new models, semantic search, AI  #colab (SBERT 23)",
    "tags": "Sentence embedding, SentenceTransformers, SBERT, Bert, Sentence transformers, COLAB, Jupyter NB, PyTorch, Cosine similarity, Optimize SBERT",
    "scraped_at": 1685113832.872841,
    "genre": "Science",
    "views": "5408",
    "desc": "Update on latest Sentence Transformers (SBERT) pretrained models for semantic search. Symmetric \\\\u0026 asymmetric semantic search, Pytorch on Colab.\\\\n\\\\nDepending on a semantic search of a 768 dimensional sentence embedding in a topological space four pretrained SBERT models are recommended and their performance presented in real-time to watch. \\\\n\\\\nCosine similarity function is compared to util.semantic_search function for 129000 sentence embeddings in 768 dim space. \\\\n\\\\n@inproceedings{reimers-2019-sentence-bert,\\\\n  title = \\\\\"",
    "lengthSeconds": "1081",
    "uploadDate": "2022-03-21",
    "thumbnail_url": "https://i.ytimg.com/vi/ewlCCB7EFPs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YXqIvkdqRrw",
    "title": "Interactive Maps in Python on COLAB + FOLIUM & GEOPANDAS",
    "tags": "Visualization, Pandas dataframe, Plot, Map, City map, track routes, Folio, Geopandas",
    "scraped_at": 1685113831.458841,
    "genre": "Science",
    "views": "1944",
    "desc": "Interactive Maps / geospatial data in Jupyter NB to track objects, plot your bike tour or analyze paths on maps. In COLAB I apply basic FOLIUM (a python library) and GeoPANDAS functions in Python for street level mapping, city navigation or flight path plotting. \\\\n\\\\nSoftware used in python with original links (credits to):\\\\nManipulate your data in Python, a Pandas Dataframe, then visualize it in on a Leaflet map via folium. \\\\nhttps://python-visualization.github.io/folium/\\\\n\\\\nLeaflet - a JavaScript library for interactive maps\\\\nhttps://leafletjs.com\\\\n\\\\nGeoPandas is an open source project to make working with geospatial data in Python easier. \\\\nhttps://geopandas.org/en/stable/\\\\n\\\\n#pythonprogramming \\\\n#python \\\\n#pandas \\\\n#maps \\\\n#pandasdataframe \\\\n#jupyterlab \\\\n#plotting \\\\n#geography \\\\n#visualization \\\\n#pythontutorial \\\\n\\\\n00:00 Install FOLIUM\\\\n01:52 Set Geo Marker \\\\n03:13 Data from Pandas dataframe\\\\n04:47 Mini map\\\\n05:36 Install GeoPANDAS\\\\n06:24 Data Structure of GeoPANDAS\\\\n07:25 Plot data and Visualization\"",
    "lengthSeconds": "670",
    "uploadDate": "2022-03-17",
    "thumbnail_url": "https://i.ytimg.com/vi/YXqIvkdqRrw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=EJVcmBQzB10",
    "title": "Folium + Shapely + leaflet.js + GeoPANDAS = Geospatial Data Analysis in Python, COLAB",
    "tags": "Python, Pandas, GeoPandas, Folium, Shapely, Jupyter NB",
    "scraped_at": 1685113831.5248675,
    "genre": "Science",
    "views": "213",
    "desc": "Geospatial data analysis - Planar projected ellipsoid distance from White House to McDonald\\'s. Apply Python Libraries GeoPandas, Shapely, Folium and leaflet.js in a Jupyter NB, COLAB, to calculate line-of-sight distance in meter, pure Python.\\\\n\\\\nLearn about the Geographic World Geodetic System WGS 84, from CRS 4326 to CRS 32619 w/ GeoPANDAS. No ARCGIS. Smile.\\\\n\\\\nA.  Leaflet - a JavaScript library for interactive maps. \\\\nB.  Shapely is a BSD-licensed Python package for manipulation and analysis of planar geometric objects.\\\\nC.  folium builds on the data wrangling strengths of the Python ecosystem and the mapping strengths of the Leaflet.js library. Manipulate your data in Python, then visualize it in a Leaflet map via folium.\\\\nD.  GeoPandas is an open source project to make working with geospatial data in python easier.\\\\n\\\\n\\\\nThis is not a YouTube Shorts. It just happens to be vertical. \\\\n\\\\n#Pandas\\\\n#Jupyter\\\\n#PythonVisualization\"",
    "lengthSeconds": "115",
    "uploadDate": "2022-03-16",
    "thumbnail_url": "https://i.ytimg.com/vi/EJVcmBQzB10/maxresdefault.jpg"
  },
  {
    "link": "watch?v=_ssAjhs5wl4",
    "title": "GPT-J-6B vs GPT-neo-2.7B (COLAB + GRADIO left side  |  APP Dev w/ LLM on right side of screen)",
    "tags": "AI, Sentence embedding, Transformer, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, train AI model, GRADIO, HuggingFace Models, COLAB",
    "scraped_at": 1685113831.591842,
    "genre": "Science",
    "views": "1333",
    "desc": "Data Science App Development in real-time with Gradio + Python only, applying your HuggingFace models.  Gradio Web App for GPT-J-6B Demo. \\\\n\\\\nLinked to HuggingFace via the \\\\\"",
    "lengthSeconds": "631",
    "uploadDate": "2022-03-13",
    "thumbnail_url": "https://i.ytimg.com/vi/_ssAjhs5wl4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=5M0vj1qNjMQ",
    "title": "Create your App to your Machine Learning models w/ GRADIO (right side) vs Python code (left side)",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Data center, Sentence Embedding, train AI model, Gradio",
    "scraped_at": 1685113833.832841,
    "genre": "Science",
    "views": "143",
    "desc": "Gradio offers a friendly Web interface (UI) to your Machine Learning (ML) models. No Flask. No Javascript. Pure Python. Gradio - Create your own Web App to share your ML/DL models. With HuggingFace Spaces. \\\\n\\\\nA great interface to integrate Huggingface models / spaces in your app. To share freely with your friends or clients. \\\\n\\\\nWatch me code in real time a data science app with gradio, based on my last example of calculating the cosine similarity of word embeddings in 128 dimensions (in my last video w/ streamlit). \\\\n\\\\nHuge benefits as your SDK of choice for HuggingFace SPACES!\\\\n\\\\nWatch my COLAB NB create fast apps (public links) to HuggingFace models (like GPT-neo or GPT-J). \\\\n\\\\n#appdevelopment \\\\n#pytorch \\\\n#pythonprogramming \\\\n#jupyterlab \\\\n#machinelearning \\\\n#deeplearning  \\\\n#javascript \\\\n#Gradio\\\\n\\\\nGradio links:\\\\nhttps://gradio.app/\\\\nhttps://gradio.app/docs/\\\\nhttps://gradio.app/working_with_ml/\\\\nhttps://github.com/gradio-app/gradio\\\\n\\\\nCOLAB NB:\\\\nhttps://github.com/gradio-app/gradio/tree/master/demo\\\\n\\\\n00:00 Gradio - a web interface to share your ML models \\\\n00:38 Python example w/ JupyterLab\\\\n04:18 COLAB NB w/ examples\\\\n05:45 Gradio documentation\\\\n06:33 HuggingFace SPACES\\\\n07:57 Compare 3 HuggingFace models in real time\\\\n09:22 GitHub Demo \\\\n10:21 Upload your model to Huggingface SPACES\"",
    "lengthSeconds": "751",
    "uploadDate": "2022-03-07",
    "thumbnail_url": "https://i.ytimg.com/vi/5M0vj1qNjMQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dkqC0Dt7iB8",
    "title": "Data Science App Development: left side Python / right side App | StreamLIT",
    "tags": "AI, Sentence embedding, Transformer, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, Data center, cloud computing, Streamlit, Build your first web app",
    "scraped_at": 1685113831.6668453,
    "genre": "Education",
    "views": "179",
    "desc": "Streamlit: watch real-time coding a data science web app within minutes, in pure Python. Data Science App Development: left side is my Python code - right side is the App.\\\\n\\\\nStreamlit allows you to build your own web app - really simple. And host one app w/ free starter plan (code stays on GitHub). \\\\n\\\\nStreamlit\\'s Open Source Library we apply in this example is currently free. On the day of publishing this video on YouTube: Snowflake acquired Streamlit for $800M.\\\\n\\\\nExperience Streamlit v1.5 with immediate response in your web browser (your app) to whatever you code in Python. Easy, fast, simple.\\\\n\\\\nThe good people from Streamlit you\\'ll find here:\\\\nhttps://streamlit.io/\\\\nhttps://streamlit.io/cloud\\\\n\\\\n#Streamlit #python #App\\\\n\\\\n00:00 Spyder - my Python IDE ( integrated development environment)\\\\n00:52 Start up streamlit locally in IPython\\\\n06:43 Input options for your web app\\\\n10:02 Experimental cache primitives \\\\n12:09 Optimize data\\\\n15:18 Learn more about streamlit\"",
    "lengthSeconds": "1133",
    "uploadDate": "2022-03-03",
    "thumbnail_url": "https://i.ytimg.com/vi/dkqC0Dt7iB8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=bMVowMgQOLU",
    "title": "Code Cosine Similarity (Python, KERAS) in 128 dim Vector Space for Word2Vec Validation (3/3)",
    "tags": "AI, Sentence embedding, Transformer, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, Multi GPU, Data center, Sentence Embedding, train AI model, Word2Vec, KERAS, 3D, Visualization, Cosine similarity",
    "scraped_at": 1685113831.7438643,
    "genre": "Education",
    "views": "500",
    "desc": "Code a cosine similarity function in a 128 dimensional vector space and calculate semantically similar words in that space in order to validate our model and its learned word embeddings. \\\\n\\\\nTo validate the results of our trained Word2Vec model in TensorFlow 2.7 on COLAB we calculate in real-time cosine similarities in a high-dimensional vector space for all learned word embeddings of our input text (Part 3 of 3).\\\\n\\\\nCredits to: Official TensorFlow tutorial on Word2Vec w/ TF 2.7:\\\\nhttps://www.tensorflow.org/tutorials/text/word2vec#subclassed_word2vec_model\\\\n\\\\nCOLAB / Jupyter NB:\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb\\\\n\\\\nKERAS preprocessing Layer:\\\\nhttps://keras.io/guides/preprocessing_layers/\\\\n\\\\nCode in real time. \\\\n\\\\n#bert \\\\n#sbert\\\\n#tensorflow2 \\\\n#keras \\\\n#3dvisualization \\\\n#word2vec\\\\n#cosinesimilarity \\\\n#vectorspace \\\\n#similarity \\\\n#nlproc \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#jupyterlab \\\\n#pythonprogramming \\\\n\\\\n\\\\n00:00 Validate Word2Vec Results\\\\n00:37 Weights for Embedding Projector (3D)\\\\n02:49 Alternative Validation\\\\n04:02 Example: Higgs Boson\\\\n05:55 Example: Science\\\\n08:35 Example: University \\\\n10:00 Summary and Motivation\"",
    "lengthSeconds": "701",
    "uploadDate": "2022-02-28",
    "thumbnail_url": "https://i.ytimg.com/vi/bMVowMgQOLU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hhUk7_VDH2U",
    "title": "KERAS preprocessing Layers (2/3)",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Word2Vec, TF data input Pipeline, KERAS preprocessing Layer",
    "scraped_at": 1685113835.0304263,
    "genre": "Education",
    "views": "228",
    "desc": "Create your  TensorFlow Data Pipeline for Text preprocessing and work w/ latest KERAS preprocessing Layers. \\\\n\\\\n Learn the latest TensorFlow functions and KERAS preprocessing layers to parallelize preprocessing your input data and training your Word2vec model  (Part 2 of 3). \\\\n\\\\nTo perform efficient batching for the potentially large number of training examples, use the tf.data.Dataset API.  The official KERAS /TF2 Jupyter NB.\\\\n\\\\nUse the Keras Subclassing API and accelerate your TF model on CPU / GPU significantly. Code in real time with me your first optimized Word2Vec model in pure TensorFlow (no Cython at all, d.h. no gensim). Learn TensorFlow on a familiar DEMO.  With learn-in-parallel Colab /Jupyter NB provided. \\\\n\\\\nTensorFlow tutorial on Word2Vec w/ TF 2.7:\\\\nhttps://www.tensorflow.org/tutorials/text/word2vec#subclassed_word2vec_model\\\\n\\\\nCOLAB / Jupyter NB (official code from KERAS, TensorFlow):\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb\\\\n\\\\nKERAS preprocessing Layer:\\\\nhttps://keras.io/guides/preprocessing_layers/\\\\n\\\\n#bert \\\\n#sbert\\\\n#tensorflow2 \\\\n#keras \\\\n#3dvisualization \\\\n#word2vec\\\\n#cosinesimilarity \\\\n#vectorspace \\\\n#similarity \\\\n#nlproc \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#jupyterlab \\\\n#pythonprogramming \\\\n\\\\n00:00 Word2Vec TF code / tutorial\\\\n00:44 Create TF Data Pipeline for text preprocessing with KERAS\\\\n06:45 Real time code execution of a Word2Vec DEMO project\\\\n10:11 TextVectorization\\\\n15:45 Our optimized Data for Word2Vec model\\\\n18:27 KERAS subclassing API\\\\n23:10 Our Word2Vec model in pure TF 2.7 code \\\\n25:37 Train our Word2Vec model with 25 epochs (watch real time)\\\\n25:49 Result\"",
    "lengthSeconds": "1678",
    "uploadDate": "2022-02-24",
    "thumbnail_url": "https://i.ytimg.com/vi/hhUk7_VDH2U/maxresdefault.jpg"
  },
  {
    "link": "watch?v=oYk6s-ubplk",
    "title": "Can AI identify the right Features in a Medical Image?  2022 Feature-attribution XAI MIT",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, train AI model, XAI, Feature Attribution",
    "scraped_at": 1685113831.8138404,
    "genre": "Education",
    "views": "50",
    "desc": "Can ML models in 2022 finally explain itself? MIT study reveals results.\\\\n\\\\nFeature-attribution methods try to explain which pixels in an image are important for NN predictions (XAI). MIT performed a study to evaluate if the most popular methods do miss important features in an image (eg medical image). \\\\n\\\\nLink to three original articles mentioned:\\\\nhttps://news.mit.edu/2022/test-machine-learning-models-work-0118\\\\nhttps://venturebeat.com/2022/01/15/responsible-ai-will-give-you-a-competitive-advantage/\\\\nhttps://www.protocol.com/newsletters/protocol-enterprise/manufacturing-ai-salesforce-shay-banon\\\\n\\\\n#FeatureAttribution #XAI #explainAI\\\\n\\\\n00:00 Does Feature Attribution work?\\\\n01:10 MIT on explanation methods for ML \\\\n02:07 ArXiv pre-print on human augmented AI effectiveness\\\\n03:32 Results\\\\n05:57 1000s of small AI models?\\\\n06:41 Responsible AI\"",
    "lengthSeconds": "442",
    "uploadDate": "2022-02-22",
    "thumbnail_url": "https://i.ytimg.com/vi/oYk6s"
  },
  {
    "link": "watch?v=k2YDLu01pV4",
    "title": "Learn Word2vec w/ latest TF | Word Embedding TensorFlow - Input data pipeline (1/3)",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Word2Vec, TensorFlow 2.7",
    "scraped_at": 1685113833.9108398,
    "genre": "Education",
    "views": "321",
    "desc": "Calculate the classical Word2vec with latest TensorFlow functions to embed semantic similar words in a high-dim vector space (Part 1 of 3).\\\\n\\\\nTensorFlow provides a great tutorial to perform Word2Vec calculations only in TensorFlow 2.7. A deep dive in Skip-gram and negative sampling for word2vec in 2022. Without any gensim functions. TF code only.\\\\n\\\\nLink to TF tutorial:\\\\nhttps://www.tensorflow.org/tutorials/text/word2vec\\\\n\\\\nCOLAB NB:\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb\\\\n\\\\nOriginal word2vec pre-print by Mikolov et al:\\\\nhttps://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\\\\n\\\\n00:00 Word2Vec in TensorFlow only \\\\n01:02 Skip-Gram and negative sampling\\\\n02:45 Reduce softmax to binary classification \\\\n05:56 Jupyter NB w/ TF2 code \\\\n08:49 Negative sampling in TF2\\\\n17:20 Input pipeline for tf.data\\\\n20:38 Code example (Shakespeare)\\\\n23:15 Optimize your input pipeline in TensorFlow\\\\n\\\\n#bert  \\\\n#sbert\\\\n#tensorflow2 \\\\n#keras \\\\n#3dvisualization \\\\n#word2vec\\\\n#cosinesimilarity \\\\n#vectorspace \\\\n#similarity \\\\n#nlproc \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#jupyterlab \\\\n#pythonprogramming\"",
    "lengthSeconds": "1454",
    "uploadDate": "2022-02-20",
    "thumbnail_url": "https://i.ytimg.com/vi/k2YDLu01pV4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=CrLmtKnbHGw",
    "title": "Learn ML only from YouTube, in Feb 2022? Here are the Top3 videos!",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, XAI, ML Interpretability, Feature Attribution, Top 3 videos",
    "scraped_at": 1685113833.986842,
    "genre": "Science",
    "views": "73",
    "desc": "Can I learn \\\\\"",
    "lengthSeconds": "1524",
    "uploadDate": "2022-02-17",
    "thumbnail_url": "https://i.ytimg.com/vi/CrLmtKnbHGw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=2JrR6vkqkNk",
    "title": "PyTorch Captum - finally understand AI's decisions?",
    "tags": "AI, Sentence embedding, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, Sentence Embedding, train AI model, XAI, Explainable AI, Interpretable AI, Captum",
    "scraped_at": 1685113832.94687,
    "genre": "Science",
    "views": "194",
    "desc": "Captum is a library in PyTorch to better understand/explain AI models (XAI): why decision are taken by specific AI models, in human understandable terms. \\\\n\\\\nA set of different interpretability methodologies (short XAI) are listed with a short summary, a link to the original pre-prints (ArXiv) in Computer Science or AI plus a significant number of code tutorials in PyTorch. \\\\n\\\\nIntegrated gradients (IG) are, together with all evolutionary steps of SHAP, part of this library for feature attribution, layer attribution or neuron attribution.\\\\n\\\\nA deep dive in PyTorch code of BERT interpretability led me to some inconsistencies. Limited by my own understanding. \\\\n\\\\nLink to Jupyter NB:\\\\nhttps://github.com/pytorch/captum/blob/master/tutorials/Bert_SQUAD_Interpret.ipynb\\\\n\\\\n#XAI #CAPTUM #PyTorch\\\\n\\\\n00:00 CAPTUM - XAI in PyTorch\\\\n00:44 Library of XAI Algorithms\\\\n03:16 API Reference\\\\n03:34 Code Tutorials\\\\n04:46 Interpret a simple BERT model (Q/A)\\\\n07:15 Inconsistencies?\\\\n11:55 Heat Map of Attributions across all BERT layers\\\\n15:05 Summary\"",
    "lengthSeconds": "943",
    "uploadDate": "2022-02-13",
    "thumbnail_url": "https://i.ytimg.com/vi/2JrR6vkqkNk/maxresdefault.jpg"
  },
  {
    "link": "watch?v=slV4-cA3kow",
    "title": "Safety-critical AI Applications | Spectral-normalized Neural Gaussian Process SNGP",
    "tags": "AI, Sentence embedding, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, Multi GPU, Tensor cores, Data center, cloud computing, train AI model, Explainable AI, XAI, Responsible AI",
    "scraped_at": 1685113834.0598388,
    "genre": "Education",
    "views": "291",
    "desc": "3 Questions for my eXplainable AI system (XAI): Is it consistent, predictable and explanatory?\\\\n\\\\nIn safety-critical AI applications, the deep classifier should be aware of its own limitations and when it should hand control over to the human experts -  using a technique called  Spectral-normalized Neural Gaussian Process SNGP.  Example code w/ original COLAB TF2 NB provided.\\\\n\\\\nExplore 3 Questions concerning responsible AI systems. \\\\n\\\\nTensorFlow Code explained: Uncertainty-aware Deep Learning with SNGP:\\\\nhttps://www.tensorflow.org/tutorials/understanding/sngp\\\\n\\\\nCOLAB NB  Uncertainty-aware Deep Learning with SNGP:\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/understanding/sngp.ipynb\\\\n\\\\nGoogle\\'s XAI Whitepaper:\\\\nhttps://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf\\\\n\\\\nPAIR by Google:\\\\nhttps://pair.withgoogle.com/\\\\n\\\\nWhat Have Language Models Learned?\\\\nhttps://pair.withgoogle.com/explorables/fill-in-the-blank/\\\\n\\\\n#XAI #SNGP #PAIR\\\\n\\\\n00:00 Deep Learning with SNGP\\\\n02:00 Google\\'s XAI Whitepaper \\\\n02:30 Research algorithms for XAI in Tensorflow\\\\n03:30 People + AI Research = PAIR \\\\n08:15 BERT Language Models to use\\\\n09:03 Is my AI system PREDICTABLE?\\\\n12:00 Is my AI system CONSISTENT?\\\\n14:04 Is my AI system EXPLANATORY?\"",
    "lengthSeconds": "1287",
    "uploadDate": "2022-02-10",
    "thumbnail_url": "https://i.ytimg.com/vi/slV4"
  },
  {
    "link": "watch?v=aSzJ3NXBk1c",
    "title": "Learn Integrated Gradients w/Colab: Identify significant Pixels for AI Object Recognition - 2022",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, XAI, Explainable AI, Integrated Gradients, TF2",
    "scraped_at": 1685113834.1298387,
    "genre": "Science",
    "views": "215",
    "desc": "Real time TensorFlow /Colab for Learning Integrated Gradients IG: feature attributions to explain decision taken by Deep Learning models (XAI). Code Example for attributing the prediction of a deep network to its input features is executed in real time on an image (medical).\\\\n\\\\nWhy we need this? Explainable AI models (XAI) provide value for Data Scientist to improve /debug models, and values for Consumers by increasing trust and understanding impact of AI/ML/DL decisions. \\\\n\\\\nPrimary goal? Visual object identification by AI models can augment human decision making, from medical diagnosis to autonomous vehicles. \\\\n\\\\nOriginal pre-print from 2017:\\\\n\\\\\"",
    "lengthSeconds": "1616",
    "uploadDate": "2022-02-06",
    "thumbnail_url": "https://i.ytimg.com/vi/aSzJ3NXBk1c/maxresdefault.jpg"
  },
  {
    "link": "watch?v=PJCocBlxkZg",
    "title": "Learn TensorFlow DATA Pipeline |  tf.data | Why optimize?",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Data pipeline TensorFlow, tf.data, tf.Data API",
    "scraped_at": 1685113831.8868673,
    "genre": "Science",
    "views": "248",
    "desc": "Build the perfect TensorFlow input pipeline w/ tf.data API.\\\\nWhy? Because your data do not fit in memory, you need pre-processing and you want to decouple loading and pre-processing of data from the distribution of parallel computation, training your model. \\\\n\\\\nA single host performance guide to achieve maximum performance between your CPU/GPU system for training your NN model. \\\\n\\\\nETL is normally performed on your CPU, while training of your model is done on your GPU. Optimal parallelization for multiple core CPUs  and asynchronous cycle management. Included: tf.data.AUTOTUNE.  \\\\n\\\\nLink to main literature: \\\\nhttps://www.tensorflow.org/guide/data\\\\nhttps://www.tensorflow.org/guide/data_performance\\\\nhttps://github.com/tensorflow/docs/blob/master/site/en/guide/data_performance.ipynb\\\\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data_performance.ipynb#scrollTo=MYOHG69M-jMT\\\\n\\\\n#tf.data\\\\n#TensorFlowAPI\\\\n#InputPipeline\\\\n\\\\n00:00 TF2 Input Pipeline\\\\n00:50 ETL process\\\\n01:45 Multiple CPU cores\\\\n02:32 TF2 code for tf.data\\\\n06:00 COLAB Jupyter NB tf.data API\\\\n11:05 Summary\"",
    "lengthSeconds": "768",
    "uploadDate": "2022-02-03",
    "thumbnail_url": "https://i.ytimg.com/vi/PJCocBlxkZg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=04oZ2P0uvp0",
    "title": "Python TF2: BERT model  |  Code your WordPiece - Tokenizer  (w/ HuggingFace)",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Tokenizer, BERT Tokenizer, WordPiece Tokenizer",
    "scraped_at": 1685113834.1988676,
    "genre": "Science",
    "views": "499",
    "desc": "Python TF2 code  (w/ JupyterLab) to train your WordPiece tokenizer: Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the BERT model.\\\\n\\\\nWhy would you need a new \\\\u0026 improved tokenizer?\\\\n\\\\nThat\\'s because Transformer models very often use subword tokenization algorithms, and they need to be trained to identify the parts of words that are often present in the corpus of your input text (sentences, Paragraphs, documents, ..), the sentences you are interested in. In order to build your optimized vocabulary.\\\\n\\\\nWordPiece is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It tries to build long words first, splitting in multiple tokens when entire words don\\xe2\\x80\\x99t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. \\\\n\\\\nBe aware: Training a tokenizer is not (!) the same as training a DL BERT model. \\\\nAll w/ TensorFlow2 code (JupyterLab).\\\\n\\\\nA special case of a newly trained WordPiece tokenizer (see also HuggingFace\\'s Tokenizer Library) \\\\nHuggingFace\\'s downloadable models, tokenizers and datasets at: https://huggingface.co/models\\\\n\\\\n#Tokenizer\\\\n#HuggingFace\\\\n#WordPiece\\\\n\\\\n00:00 WordPiece Tokenizer \\\\n04:20 WordPiece model for Tokenizer\\\\n09:25 Train your WordPiece Tokenizer\\\\n13:35 Encode your sentences\\\\n17:00 Save your new Tokenizer\\\\n19:15 Use your new Tokenizer \\\\n23:10 Output (hidden layers)\\\\n24:53 Example for sentiment analysis\"",
    "lengthSeconds": "1732",
    "uploadDate": "2022-01-31",
    "thumbnail_url": "https://i.ytimg.com/vi/04oZ2P0uvp0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=MlDP2BVWjS0",
    "title": "Python code to build your BPE - Tokenizer from scratch (w/ HuggingFace)",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, BPE, Tokenizer from scratch, BPE Tokenizer",
    "scraped_at": 1685113833.018867,
    "genre": "Science",
    "views": "1545",
    "desc": "Python TF2 code (JupyterLab) to train your Byte-Pair Encoding tokenizer (BPE):\\\\na. Start with all the characters present in the training corpus as tokens.\\\\nb. Identify the most common pair of tokens and merge it into one token.\\\\nc. Repeat until the vocabulary (e.g., the number of tokens) has reached the size you want.\\\\n\\\\nTraining a tokenizer is not (!) the same as training a DL model. TensorFlow2 code:\\\\nfrom tokenizers.trainers import BpeTrainer\\\\ntokenizer.train(files, trainer) \\\\n\\\\n Here the special case of a Byte-Pair Encoding (BPE) from HuggingFace\\'s Tokenizer Library! See original downloadable models, tokenizers and datasets at: https://huggingface.co/models\\\\n\\\\n#Tokenizer\\\\n#HuggingFace\\\\n#BPE\\\\n\\\\n00:00 Code my optimized BPE Tokenizer\\\\n03:13 BPE model and trainer \\\\n04:36 Train a new Tokenizer\\\\n05:38 Use newly constructed Tokenizer\\\\n07:55 Encode batch\\\\n09:44 Summary\"",
    "lengthSeconds": "619",
    "uploadDate": "2022-01-27",
    "thumbnail_url": "https://i.ytimg.com/vi/MlDP2BVWjS0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=uxMvwDaCQnc",
    "title": "Python to optimize Input DATA Pipeline  |  BERT Transformer Models",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Tokenizer, Python code, JupyterLab, Code examples",
    "scraped_at": 1685113833.085867,
    "genre": "Science",
    "views": "179",
    "desc": "Python TF2 code to optimize your Tokenizer and Vocabulary for your specific dataset. Pre-trained (BERT) NLP models are trained on a general set of documents, which will not provide good enough performance for your specific Deep Learning task (in NLP).\\\\n\\\\nCode examples from original HuggingFace description, or modified / altered to my specific presentation needs. Check HuggingFace for original models and datasets: https://huggingface.co/models\\\\n\\\\n\\\\n#code_in_real_time\\\\n#Tokenizer\\\\n#HuggingFace\\\\n\\\\n00:00 Code your Tokenizers\\\\n03:58 Tokenization pipeline\\\\n06:20 Full service Tokenizer \\\\n09:15 Train a new Tokenizer\\\\n15:00 Fast Tokenizer\\\\n16:16 Encode your sentences with the new tokenizer\\\\n18:30 Use a pretrained tokenizer (with vocabulary)\"",
    "lengthSeconds": "1453",
    "uploadDate": "2022-01-26",
    "thumbnail_url": "https://i.ytimg.com/vi/uxMvwDaCQnc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6ojgCTUzVdg",
    "title": "Explore new AI storytelling yourself | NLP",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Tokenizer",
    "scraped_at": 1685113833.150839,
    "genre": "Science",
    "views": "76",
    "desc": "You explore new texts written by Transformer models! You can explore their Text Generation abilities online with GPT-Neo (w/ Huggingface). And discover their weakness online, based on their training data, vocabulary files and tokenizers. The quality of training data is absolutely important for the performance of your AI/DL/ML model.\\\\n\\\\n#HuggingFace\\\\n#GPT-x\\\\n#Tokenizer\\\\n\\\\n00:00 LLM on HuggingFace \\\\n00:45 Text generation example\\\\n02:45 GPT answers me\\\\n04:15 Tokenizer  and vocabulary\\\\n08:30 Select your Transformer model\"",
    "lengthSeconds": "697",
    "uploadDate": "2022-01-22",
    "thumbnail_url": "https://i.ytimg.com/vi/6ojgCTUzVdg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DAP0ofXbzaw",
    "title": "How to use Decision Intelligence in 2022: Insights for human centered XAI - empirical study",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Decision intelligence, XAI, Explainable AI, DI, Usability of ML prototypes for DI",
    "scraped_at": 1685113831.9678419,
    "genre": "Science",
    "views": "57",
    "desc": "Augment human decision-making. When do you trust AI? And why?  Researchers from MIT and Auckland Univ found answers in their work on USABILITY Challenges of ML algorithms for DI (Decision Making): Partnering w/ US Child Protective Services (CPS) agencies a series of insights emerge about how to design a DI prototype app to better protect our most important (and weakest) members of society. \\\\n\\\\nLink to IEEE arXiv:\\\\nhttps://arxiv.org/pdf/2103.02071.pdf\\\\n\\\\\"",
    "lengthSeconds": "1586",
    "uploadDate": "2022-01-20",
    "thumbnail_url": "https://i.ytimg.com/vi/DAP0ofXbzaw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=4QnILIZx_6Y",
    "title": "Decision Intelligence - still not dead in 2022?",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, BI, DI, Decision Intelligence, DI in 2022 a Top Trend",
    "scraped_at": 1685113832.047842,
    "genre": "Science",
    "views": "120",
    "desc": "In Jan 2018 Harvard Business Review wrote about AI executing business decisions. How did Decision Intel (DI) evolved since then? Why is it a Top Strategic Tech Trend in 2022? Should you care for your business? \\\\n\\\\nIronic remarks on insights from IBM, Google and Gartner to ease my complex approach to DI in 2022.\\\\n\\\\nLink to public documents:\\\\nhttps://hbr.org/2018/01/as-ai-makes-more-decisions-the-nature-of-leadership-will-change\\\\nhttps://towardsdatascience.com/introduction-to-decision-intelligence-5d147ddab767\\\\nhttps://www.ibm.com/blogs/journey-to-ai/2020/06/the-rise-of-decision-intelligence-ai-that-optimizes-decision-making/\\\\nhttps://www.gartner.com/smarterwithgartner/would-you-let-artificial-intelligence-make-your-pay-decisions\\\\nhttps://www.gartner.com/en/information-technology/glossary/decision-intelligence\\\\nhttps://www.gartner.com/en/documents/3891518/decision-intelligence-is-the-near-future-of-decision-mak\\\\n\\\\n#Decision_Intelligence\\\\n#DI2022\\\\n#AI\\\\n\\\\n00:00 Decision Intelligence\\\\n02:25 IBM: The rise of DI \\\\n06:35 Gartner\\'s Definition of DI in 2022\\\\n10:04 Outlook next video: ML DI\"",
    "lengthSeconds": "636",
    "uploadDate": "2022-01-16",
    "thumbnail_url": "https://i.ytimg.com/vi/4QnILIZx_6Y/maxresdefault.jpg"
  },
  {
    "link": "watch?v=PUTjxJ6YqNc",
    "title": "New Attribution Framework of Identified Sources in 2022 | NLG NLP",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, XAI, Explainable AI, LLM",
    "scraped_at": 1685113832.1208396,
    "genre": "Science",
    "views": "64",
    "desc": "First:  Google research w/ their new \\\\\"",
    "lengthSeconds": "1500",
    "uploadDate": "2022-01-14",
    "thumbnail_url": "https://i.ytimg.com/vi/PUTjxJ6YqNc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=nb1d97-CbCM",
    "title": "ETHICS in AI? Are there RISKS for us? In Education or Medical treatment?",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, Databricks, Delta Lake, train AI model, Stanford Univ, HAI, Human AI, Risks of Foundation models, LLM, Risks of LLMs",
    "scraped_at": 1685113834.2668388,
    "genre": "Science",
    "views": "44",
    "desc": "We look at the US response to Explainable AI (XAI) and Ethics in 2022. Stanford\\'s HAI response on: Explainable AI / Foundation models / Large Language Models - can there be risks for us?\\\\n\\\\nA new group assembled by Stanford\\xe2\\x80\\x99s institute for Human-Centered Artificial Intelligence (HAI), called the Center for Research on Foundation Models (CRFM), is studying the impacts and implications of foundation models (or LLMs - as I call them).\\\\n\\\\nThe main danger of foundation models (like BERT, GPT-3, ..), according to HAI: \\\\n\\\\\"",
    "lengthSeconds": "1286",
    "uploadDate": "2022-01-10",
    "thumbnail_url": "https://i.ytimg.com/vi/nb1d97"
  },
  {
    "link": "watch?v=4_ZT6hvlhGQ",
    "title": "Explainable AI - The Billion $ Business Model of XAI in 2022",
    "tags": "AI, Sentence embedding, Transformer, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, Databricks, train AI model, XAI, Explainable AI, Business Case",
    "scraped_at": 1685113832.1958394,
    "genre": "Science",
    "views": "179",
    "desc": "How can you make money of XAI in 2022? \\\\nHow is the interlink designed between Cloud Companies (Microsoft, Meta, ..)  and commercial companies seeking AI induced growth paths?\\\\nA locked-in ecosystem? Dominated by tech giants? \\\\nNo way out?\\\\n\\\\n#xai\\\\n#businessmodel \\\\n#explainableai \\\\n#humancentereddesign \\\\n#humancentered \\\\n#industry \\\\n#business \\\\n\\\\n00:00 Explainable AI system configuration\\\\n03:17 Cloud Economy\\\\n05:45 Government regulations\\\\n07:10 Cloud Company\\\\n11:05 Specific AI models \\\\n12:55 Revenue streams\\\\n13:20 Ecosystem lock-in?\"",
    "lengthSeconds": "1025",
    "uploadDate": "2022-01-06",
    "thumbnail_url": "https://i.ytimg.com/vi/4_ZT6hvlhGQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1h-IdF1KRWw",
    "title": "Explainable AI - The story behind XAI in 2022 (legal, ethical, commercial, risks)",
    "tags": "AI, Sentence embedding, Transformer, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, Sentence Embedding, train AI model, XAI, Explainable AI",
    "scraped_at": 1685113833.2178414,
    "genre": "Science",
    "views": "267",
    "desc": "Two beautiful examples show why we need an Explainable AI (XAI) system to protect our individual freedom and human rights in a digital / AI economy. Although it will cause significant additional work for us AI coder. \\\\n\\\\nThe story behind XAI in 2022: The problem of social media input data. \\\\nIn general: the quality of data for training deep AI systems. \\\\n\\\\nExplainable Artificial Intelligence will become important in 2022 for AI coder and AI provider. Upcoming US and EU AI-specific legislation for market applications are drafted / discussed. \\\\n\\\\nImage of Youtube thumbnail by Samanta Pereira Ferdinandi Sam from Pixabay (credits).\\\\n\\\\n#xair \\\\n#explainableai \\\\n#ai \\\\n#legal \\\\n#datascience \\\\n#machinelearning \\\\n#deeplearning \\\\n#risk \\\\n#ethics \\\\n#ethical \\\\n#commercial \\\\n\\\\n00:00 Explainable AI\\\\n04:00 National Institute of Standards USA\\\\n08:15 EU GDPR\\\\n10:45 EU AI Regulation\\\\n29:22 Ethical AI (Data Problem)\\\\n33:10 Wirecard pattern learned and Social Media data\"",
    "lengthSeconds": "2463",
    "uploadDate": "2022-01-03",
    "thumbnail_url": "https://i.ytimg.com/vi/1h"
  },
  {
    "link": "watch?v=qoAmE9RwgQM",
    "title": "Physics applied to LLM / Foundation models?",
    "tags": "AI, Sentence embedding, Transformer, BERT, Transformer models, Machine learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, Sentence Embedding, train AI model, LLM, Large Language Models",
    "scraped_at": 1685113835.7564287,
    "genre": "Science",
    "views": "73",
    "desc": "2 Years of LLM: scientist apply the tools of 357 years of Physics on this new topic and achieve astonishing results, simulating a human conversation. And still fail, for reasons discussed in this video.\\\\n\\\\nYou need a huge supercomputer, an electricity bill payed by the US government or Google, train it for days or weeks on several hundreds CPUs, GPUs or TPUs, and you get your trained Large Language Model (LLM).\\\\n\\\\nAnd then you run this trained model on a smaller supercomputer to answer questions like: Is the answer, presented in Veritasium\\'s YouTube video, true?\\\\n\\\\nHint: After watching several YouTube videos you know: Under certain conditions, certain effects will overlap and therefore you need to ask a more specific question. Otherwise you have a multitude of answers, given multiple undefined system configurations. \\\\n\\\\nYes, the presenter should have studied Maxwell Equations more intense. But with more than 10 M views he stimulated something gorgeous. Well done, Veritasium. Sometimes it is not the answer, but the way you deduct an answer, given your knowledge.\\\\n\\\\nWhat are the limitation of LLMs - analysing a simple physical experiment of switching on a light bulb, with 600.000 km of wires between battery and light bulb. \\\\n\\\\nThe insights of physics - applied to LLMs. \\\\n\\\\nA physical system description with all boundary conditions which provide insights in system dynamics and predicts events to be verified by experiments. Which part is missing in LLMs?\\\\n\\\\n\\\\n4 YouTube videos I recommend (Credits to):\\\\nhttps://youtu.be/bHIhgxav9LY\\\\nhttps://youtu.be/VQsoG45Y_00\\\\nhttps://youtu.be/iph500cPK28\\\\nhttps://youtu.be/2Vrhk5OjBP8\\\\n\\\\n#LLM\\\\n#BIAS_in_AI\\\\n#Large_Language_Models\\\\n\\\\nMIT link:\\\\nhttps://news.mit.edu/2021/top-research-2021-1222\\\\n\\\\n00:00 Monster AI models \\\\n01:15 A simple Question\\\\n02:38 Beautiful answers\\\\n05:12 Check list\\\\n07:30 Describe Systems\\\\n11:00 Apply a Theory (eg Physics)\\\\n13:25 Feynman knew it all\\\\n15:40 Careful with visualizations\\\\n16:57 AI researcher are careful\\\\n21:25 Summary\"",
    "lengthSeconds": "1356",
    "uploadDate": "2022-01-02",
    "thumbnail_url": "https://i.ytimg.com/vi/qoAmE9RwgQM/hqdefault.jpg"
  },
  {
    "link": "watch?v=X26StUywiss",
    "title": "NEW Mixture-of-Experts architecture to scale LLM  |  GLaM by Google AI (1.6 trillion Token Dataset)",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Sentence Embedding, train AI model, Graph neural Networks, Large language Models, LLM, Supercomputer, GLAM",
    "scraped_at": 1685113835.8254526,
    "genre": "Science",
    "views": "147",
    "desc": "A new \\\\\"",
    "lengthSeconds": "786",
    "uploadDate": "2021-12-31",
    "thumbnail_url": "https://i.ytimg.com/vi/X26StUywiss/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BOYurPHWWXs",
    "title": "Knowledge Graph for a Medical Application - DEMO in Python",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, train AI model, GraphNeuralNetworks, DGL, DGL",
    "scraped_at": 1685113835.099429,
    "genre": "Science",
    "views": "1900",
    "desc": "Watch a real-world coding example of official DGL on a Knowledge Graph for medical research. Understand in real-time why a Graph Neural Network is so important to gain insight in complex data sets, highlighting a heterogeneous Knowledge Graph in DGL code.\\\\n\\\\nAll Credits to:\\\\nA team of AWS scientists from Amazon Shanghai AI Lab and AWS Deep Engine Science team working along with academic collaborators from University of Minnesota, Ohio State University, and Hunan University have created the Drug Repurposing Knowledge Graph (DRKG) and a set of machine learning tools that can be used to prioritize drugs for re-purposing studies. \\\\n\\\\nThese institutions provide the code and verbal description in the officially published DGL Blog, which I just follow and implement in real-time coding in AWS SageMaker Studio Lab. \\\\n\\\\nLink to original published Blog content by DGL-KE in DGL: \\\\nhttps://www.dgl.ai/news/2020/06/09/covid.html\\\\n\\\\n#KnowledgeGraph\\\\n#DGLKE\\\\n#NetworkX\\\\n\\\\n00:00 DGL Blog example\\\\n02:42 SageMaker Studio JupyterLab\\\\n07:00 Node entities \\\\n09:00 Edge types\\\\n09:30 NetworkX\\\\n13:12 Training on DGL-KE\\\\n14:55 Entity Embedding\\\\n16:45 Low-dim Relations embedding\\\\n17:45 Summary\"",
    "lengthSeconds": "1166",
    "uploadDate": "2021-12-29",
    "thumbnail_url": "https://i.ytimg.com/vi/BOYurPHWWXs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dBeYBjjxQqU",
    "title": "PyTorch: Node Classification w/ Graph Neural Network on DGL for GCN",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Data center, cloud computing, train AI model, GNN, GCN, RGCN, SageMaker Studio LAB, JupyterLab, Train Knowledge Graphs, PyTorch",
    "scraped_at": 1685113835.169427,
    "genre": "Science",
    "views": "3240",
    "desc": "PyTorch Code to train a GCN/ RGCN w/ DGL-KE on a free SageMaker Studio Lab. Graph Convolution Network GCN Embedding calculated in real-time on a simple JupyterLab. Applying mighty Deep Graph Library (DGL) on Graphs (PyTorch). \\\\n\\\\nNext step will be DGL-KE.\\\\n\\\\nFor each node, a RGCN layer performs the following steps:\\\\n1) Compute outgoing message using node representation and weight matrix associated with the edge type (message function)\\\\n2) Aggregate incoming messages and generate new node representations (reduce and apply function)\\\\n\\\\nIn simple terms: A relational graph convolutional network (RGCN) handles different relationships between entities in a knowledge base. \\\\n\\\\nCompared to a classical GCN: \\\\nThe straightforward graph convolutional network (GCN) exploits structural information of a dataset (that is, the graph connectivity) in order to improve the extraction of node representations. Graph edges are left as untyped.\\\\n\\\\nAdvance to a Knowledge Graph: \\\\nA knowledge graph is made up of a collection of triples in the form subject, relation, object. Edges thus encode important information and have their own edge embeddings to be learned. Plus there may exist multiple (!) edges among any given pair. \\\\n\\\\nBoth JupyterLabs are part of the official DGL tutorial presentation. \\\\nSee https://docs.dgl.ai/en/latest/index.html\\\\nFull credits to DGL community!\\\\n\\\\nOfficial Link to these JupyterLabs:\\\\n--------------------------------------------\\\\nhttps://docs.dgl.ai/en/0.6.x/_downloads/3c3026a1d47f007e05f4e833138c1b51/5_hetero.ipynb\\\\nhttps://docs.dgl.ai/en/0.6.x/tutorials/basics/5_hetero.html\\\\n\\\\n(Theoretical) Background on RGCN:\\\\n---------------------------------------------------------\\\\nhttps://docs.dgl.ai/en/latest/tutorials/models/1_gnn/4_rgcn.html\\\\nhttps://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn-hetero\\\\n\\\\n\\\\n#GraphConvolutionNetwork\\\\n#DGL-Deep_Graph_Library\\\\n#Heterogeneous_Graphs\\\\n#graphs \\\\n#neuralnetworks \\\\n#ai\\\\n#machinelearningwithpython \\\\n#convolutionalneuralnetwork \\\\n#jupyterlab \\\\n#dgl \\\\n\\\\n\\\\n00:00 Node Classification with DGL\\\\n04:20 Graph Convolutional Network\\\\n07:00 Code for Training\\\\n08:50 Heterogeneous Graphs\\\\n11:50 Knowledge Graph\\\\n15:30 Relational GCN\\\\n17:30 Train RGCN code\"",
    "lengthSeconds": "1182",
    "uploadDate": "2021-12-27",
    "thumbnail_url": "https://i.ytimg.com/vi/dBeYBjjxQqU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=nLjBPDuUkDw",
    "title": "Lie Groups for Deep Learning w/ Graph Neural Networks",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, cloud computing, train AI model, Lie Group, Graph Neural Networks, Symmetry Group",
    "scraped_at": 1685113833.290841,
    "genre": "Science",
    "views": "595",
    "desc": "Lie Groups encode the symmetry of systems. We examine actions of a Lie group on a vector space, given their algebraic, topological and analysis based connectome. Deep Learning algorithms for Graph Neural Networks (GNN) are non trivial, and to understand them Lie Groups are essential! \\\\n\\\\nA real Lie group is a group that is also a finite-dimensional real smooth manifold, in which the group operations of multiplication and inversion are both smooth maps. \\\\n\\\\nThe only connected Lie groups with dimension one are: \\\\n1) the real line R (with the group operation being addition) and \\\\n2) the circle group S1 of complex numbers with absolute value one (with the group operation being multiplication). \\\\nThe S1  group is often denoted as U(1), the group of 1 \\xc3\\x97 1 unitary matrices. \\\\n\\\\n#LieGroups\\\\n#GraphNeuralNetworks\\\\n#DeepLearning\\\\n\\\\nLink to public document:\\\\nhttp://staff.ustc.edu.cn/~wangzuoq/Courses/13F-Lie/Notes/Lec%2001.pdf\\\\n\\\\nLink to Arxiv pre-print:\\\\nGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges\\\\nMichael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veli\\xc4\\x8dkovi\\xc4\\x87\\\\nhttps://arxiv.org/pdf/2104.13478\\\\n\\\\n00:00 Algebraic + Geometric + Analysis component\\\\n02:50 Examples of Lie Groups\\\\n04:15 Invariance of groups acting on vector spaces\\\\n05:50 Differentiable structure of Lie Groups\\\\n07:02 Lie Algebra\"",
    "lengthSeconds": "590",
    "uploadDate": "2021-12-25",
    "thumbnail_url": "https://i.ytimg.com/vi/nLjBPDuUkDw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Es4GplwLsBs",
    "title": "First look at Knowledge Graph Embedding (w/ simple Jupyter NB dgl-ke)",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, train AI model, Knowledge graph, Knowledge Graph embedding, DGL",
    "scraped_at": 1685113834.3328414,
    "genre": "Science",
    "views": "1451",
    "desc": "Knowledge Graph Embedding and its advantages for answering search queries. Simple explanation of Knowledge Graph Embedding and its use case.\\\\n\\\\nTech to answer your (Siri) questions is basically a Deep Graph Knowledge Embedding Library (DGL-KE), a knowledge graph (KG) embeddings library built e.g. on top of Deep Graph Library (DGL). \\\\n\\\\nMy YouTube video on DGL:\\\\nhttps://youtu.be/d81GrQkvp6I\\\\n\\\\n\\\\\"",
    "lengthSeconds": "1269",
    "uploadDate": "2021-12-23",
    "thumbnail_url": "https://i.ytimg.com/vi/Es4GplwLsBs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=11bAAy8b4sI",
    "title": "Knowledge Graph Embedding - Dec 2021",
    "tags": "AI, Sentence embedding, Transformer, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, Sentence Embedding, train AI model, Knowledge Graph, Knowledge Graph embedding, #Easy Explanation",
    "scraped_at": 1685113833.361839,
    "genre": "Education",
    "views": "6212",
    "desc": "An intro to Knowledge Graphs, based on our knowledge of Graph Neural Networks. A simple example provides an easy pathway to Knowledge Graphs and training of Knowledge Graphs (AI). \\\\n\\\\nKnowledge graphs (KG) are data structures that store information about different entities (nodes) and their relations (edges). A common approach of using KG in various machine learning tasks is to compute knowledge graph embeddings. \\\\n\\\\nA knowledge graph (KG) is a directed heterogeneous multigraph whose node and relation types have domain-specific semantics. KG allow us to encode the knowledge into a form that is human interpretable and amenable to automated analysis and inference. \\\\n\\\\nTwo models for Knowledge Graph Embeddings are presented and expained: TransE and TransR.  \\\\n\\\\n00:00 Remember word embedding?\\\\n01:53 Knowledge graph embedding\\\\n03:18 Simple knowledge graph \\\\n06:52 Key idea\\\\n08:50 Closeness\\\\n12:20 TransE explained\\\\n13:18 Knowledge graph embeddings\\\\n15:18 TransR explained\\\\n\\\\n\\\\n\\\\n#knowledgegraph \\\\n#embedding \\\\n#Intro2KnowledgeGraphs\\\\n#graphs \\\\n#neuralnetworks \\\\n#ai\\\\n#machinelearningwithpython \\\\n#convolutionalneuralnetwork \\\\n#jupyterlab \\\\n#dgl \\\\n#word2vec\"",
    "lengthSeconds": "1019",
    "uploadDate": "2021-12-21",
    "thumbnail_url": "https://i.ytimg.com/vi/11bAAy8b4sI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=8xsVQJ72VB8",
    "title": "My TOP 9 videos to understand & code GNN - Graph Neural Network",
    "tags": "AI, Transformer, BERT, Transformer models, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Databricks, train AI model, GNN, GraphNeuralNetwork, PyTorchGeometric, PyG, DeepGraphLibrary, DGL, Deep Graph Neural Network, Python code",
    "scraped_at": 1685113833.429868,
    "genre": "Science",
    "views": "550",
    "desc": "How YOU start with coding Graph Neural Networks GNN? Playlist overview of my 9 GNN videos. The benefits for you when coding GNNs (python code as of December 2021). \\\\n\\\\nMy library recommendations for you: a) PyTorch Geometric PyG and b) Deep Graph Library DGL.\\\\n\\\\nBest free online course on GNN I found (lectures free on Youtube):\\\\nCS224W: Machine Learning with Graphs\\\\nJure Leskovec, Stanford University\\\\nhttp://cs224w.stanford.edu\\\\n\\\\n00:00 9 videos \\\\n00:40 CS224W\\\\n01:50 Data (foundation)\\\\n04:57 Graph Representation Learning\\\\n07:25 My 9 GNNs\\\\n08:35 Libraries of Deep GNN models\\\\n09:50 JAX\\\\n11:35 Recommend\\\\n\\\\n#Graph_Neural_Networks\\\\n#Overview_Coding_GNN\\\\n#MyPlaylist_Summary\"",
    "lengthSeconds": "773",
    "uploadDate": "2021-12-19",
    "thumbnail_url": "https://i.ytimg.com/vi/8xsVQJ72VB8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=i_Tm3ZQScv8",
    "title": "Message Passing for graphs - explained w/ example",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Data center, cloud computing, Sentence Embedding, train AI model, GNN, DGL, Message Passing on Graphs, DGL Message Passing code",
    "scraped_at": 1685113834.3988388,
    "genre": "Science",
    "views": "1313",
    "desc": "Message Passing: How to encode the complete information of a (non-euclidean) Graph into a  low-dimensional embedding? With the aim of training GNN for more intelligent answers. The solution: Message Passing algorithms.\\\\n\\\\nHere I explain their foundation (step-by-step) and use predefined python function of DGL (Deep Graph Library) for Message Passing Neural Network (MPNN) Embedding in a low-dim vector space. \\\\n\\\\nMain task with GNN is to compute a low dimensional embedding of the Graph in an euclidean vector space. Herein find a simple visualization of node features and edge features encoded in multi-dimensional tensors, plus their 3 \\\\\"",
    "lengthSeconds": "1620",
    "uploadDate": "2021-12-17",
    "thumbnail_url": "https://i.ytimg.com/vi/i_Tm3ZQScv8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=zX4jk-1VocE",
    "title": "How to create a Graph for Graph Neural Networks?",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Data center, cloud computing, train AI model, DGL, DGL Basics, Deep Graph Library DGL, Basics",
    "scraped_at": 1685113833.4968674,
    "genre": "Science",
    "views": "2789",
    "desc": "Welcome to the Basics of DGL. At first, how to construct a DGL Graph? Encode information as (PyTorch) tensors in nodes and edges! How to code (Python) a heterogeneous (knowledge) Graph and extract semantic information? I show you in real time coding. \\\\n\\\\n#DGL_Graph_Construction\\\\n#Heterogeneous_Graphs \\\\n#PyTorch_SageMaker_Studio_LAB\\\\n\\\\n\\\\nNotebook reference/link (credits to):\\\\nhttps://github.com/dbgannon/graphs/blob/master/hetero-intro.ipynb\\\\nhttps://docs.dgl.ai/en/0.6.x/tutorials/blitz/index.html\\\\n\\\\nRecommended paper:\\\\nhttps://cloud4scieng.org/2020/08/28/deep-learning-on-graphs-a-tutorial/\\\\n\\\\n00:00 DGL Graph Construction\\\\n02:02 Assigning Node and Edge Features to Graph\\\\n05:08 Convert DGL to NetworkX\\\\n07:00 Knowledge graphs are heterogeneous\\\\n14:36 DGL Graphs\"",
    "lengthSeconds": "1168",
    "uploadDate": "2021-12-15",
    "thumbnail_url": "https://i.ytimg.com/vi/zX4jk"
  },
  {
    "link": "watch?v=IReUlgafgZo",
    "title": "GraphSAGE to GraphBERT - Theory of Graph Neural Networks",
    "tags": "AI, Transformer, Attention heads, BERT, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, Sentence Embedding, GNN, Graph Neural Networks, GraphSAGE, GraphBERT, Graph Transformer, Graph",
    "scraped_at": 1685113835.8934536,
    "genre": "Science",
    "views": "562",
    "desc": "You start w/ differentiable aggregator functions of GraphSAGE to permutation invariance of graphs, plus a mathematical presentation of convolutional, attentional and message passing Neural Networks. \\\\nResulting in Transformers applied on Graphs, via Laplacian EigenVectors as positional encoding.\\\\n\\\\nPresentation slides by Petar Velickovic (see link below), the author and presenter of the seminar Theory of GNN, 17 February 2021. All rights on these presentation slides belong to him. \\\\n\\\\n\\\\nInductive Representation Learning on Large Graphs\\\\nhttps://arxiv.org/pdf/1706.02216v4.pdf\\\\n\\\\nSlides on Theory of Graph Neural Networks available for you at:\\\\nhttps://petar-v.com/talks/GNN-Wednesday.pdf\\\\n* Thanks to Petar Velickovic to make informative slides (as pdf file) publicly available*\\\\n\\\\nA Generalization of Transformer Networks to Graphs\\\\nhttps://arxiv.org/pdf/2012.09699.pdf\\\\n\\\\n#GraphSAGE\\\\n#GraphBERT\\\\n#GraphNN\\\\n\\\\n00:00 GraphSAGE\\\\n07:30 Aggregator Functions\\\\n08:30 Permutation Equivariance\\\\n12:20 GNN Aggregator \\\\n16:12 GNN Meta-structure\\\\n19:55 Transformers are GNN\\\\n21:15 Graph Laplacian\\\\n22:00 Graph Transformer\"",
    "lengthSeconds": "1467",
    "uploadDate": "2021-12-13",
    "thumbnail_url": "https://i.ytimg.com/vi/IReUlgafgZo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=S3sRy4oqvCM",
    "title": "Strange JRAPH  - Deep Mind's GNN Library for Graph Neural Networks (w/ JAX)",
    "tags": "AI, Transformer, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, train AI model, Jraph, JAX, GNN, GraphNeuralNetworks",
    "scraped_at": 1685113834.4658391,
    "genre": "Science",
    "views": "711",
    "desc": "Discover JRAPH when implementing its graph neural network models (from Convolution GCN  to Attention GAT) on JAX. Personal approach of exploring content and applicability of new python libraries.\\\\n\\\\nLinks within the vid:\\\\nhttps://github.com/deepmind/jraph\\\\nhttps://deepmind.com/blog/article/using-jax-to-accelerate-our-research\\\\nhttps://jraph.readthedocs.io/en/latest/api.html#models\\\\n\\\\n\\\\n#Jraph\\\\n#JAX\\\\n#GraphNeuralNetworks\\\\n\\\\n00:00 Jraph on GitHub\\\\n03:00 Check the code\\\\n04:00 Origin of Code \\\\n06:06 Read the Docs\\\\n07:22 Graph Attention network code\\\\n08:10 Compare Jraph - PyG - DGL\\\\n10:50 Deep dive\"",
    "lengthSeconds": "721",
    "uploadDate": "2021-12-11",
    "thumbnail_url": "https://i.ytimg.com/vi/S3sRy4oqvCM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=c6N6yX_6feo",
    "title": "Learn low-dim Embeddings that encode GRAPH structure (data) : \"Representation Learning\" /arXiv",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Data center, cloud computing, train AI model, Neighborhood aggregation encoders, Representational Learning on Graphs, Encode Graphs, Low dimensional embeddings, GNN",
    "scraped_at": 1685113834.534868,
    "genre": "Science",
    "views": "265",
    "desc": "Optimize your complex Graph Data before applying Neural Network predictions. Automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. \\\\n\\\\nAn encoder-decoder perspective, random walk approaches or Neighborhood aggregation methods/encoders.\\\\n\\\\nSince central problem in machine learning on graphs is finding a way to incorporate information about graph-structure into a machine learning model.\\\\n\\\\nFind a way to represent, or encode, graph structure so that it can be easily exploited by your Machine Learning models. \\\\n\\\\nGreat publication:\\\\n-----------------------------\\\\n\\\\\"",
    "lengthSeconds": "1426",
    "uploadDate": "2021-12-09",
    "thumbnail_url": "https://i.ytimg.com/vi/c6N6yX_6feo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=WpAxWMz42OI",
    "title": "4h of free GPU for coding NN: SageMaker Studio LAB by aws",
    "tags": "AI, Transformer, HuggingFace, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, Tensor cores, Data center, cloud computing, train AI model, SageMaker Studio Lab, Free GPU, Jupyter Lab, Train GNN, Graph Convolutional Network",
    "scraped_at": 1685113835.2374623,
    "genre": "Science",
    "views": "494",
    "desc": "SageMaker Studio LAB is a *free of charge* Jupyter Lab w/ GPU (4 hours) or w/ CPU (12 hours) by AWS. No credit card needed. My first look at SageMaker Studio LAB: familiar Jupyter Lab UI, seamless integration/installation procedure, works perfectly with my uploaded ipynb files! First run of training a GNN on PyTorch w/ SageMaker Studio Lab: fast execution. Great first impression! \\\\n\\\\nFamiliar Jupyter Lab UI w/ SageMaker Studio LAB, no problems at all to upload my ipynb file, install a Deep Graph Library (DGL) w/ CUDA, define a Graph Convolutional Network (GCN) and run a GNN training cycle on PyTorch. No problems at all, perfect integration! \\\\n\\\\nImmediately familiar interfaces if You know Jupyter Lab at all. I will explore SageMaker Studio Lab in the next days/weeks and post further findings/impressions for your information. \\\\n\\\\nFirst look: 10 out of 10! \\\\nBe careful: very tempting to join AWS ecosystem for client ML/DL of GNNs! Smile.\\\\n\\\\n#SageMakerStudioLAB\\\\n#SageMaker\\\\n#FreeGPU\\\\n\\\\n00:00 SageMaker Studio LAB\\\\n01:25 GPU runtime\\\\n02:45 Upload my ipynb file\\\\n03:10 Install Deep Graph Library\\\\n04:20 PyTorch\\\\n06:00 Train Graph Convolution Model\"",
    "lengthSeconds": "503",
    "uploadDate": "2021-12-08",
    "thumbnail_url": "https://i.ytimg.com/vi/WpAxWMz42OI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=F3CHVVv8fqg",
    "title": "JAX explained in 1 Minute w/ Neural Network Code!",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Data center, cloud computing, train AI model, JAX, ParallelizeCode, VectorizeCode, Neural Network, NumPy, Python",
    "scraped_at": 1685113835.3004525,
    "genre": "Science",
    "views": "288",
    "desc": "Explain the power of JAX on python/numpy code. JAX on a simple Feed Forward Neural Network code. Video by TensorFlow on YouTube.\\\\n\\\\nGoogle/TensorFlow video on YouTube:\\\\nhttps://youtu.be/WdTeDXsOSj4\\\\n\\\\n#JAX\\\\n#TensorFlowXLA\\\\n#NeuralNetwork\\\\n\\\\n00:00 What is JAX?\\\\n00:20 Numpy\\\\n00:49 JAX code\"",
    "lengthSeconds": "165",
    "uploadDate": "2021-12-07",
    "thumbnail_url": "https://i.ytimg.com/vi/F3CHVVv8fqg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=QxPgaRBPdUw",
    "title": "Your Brain will overload: Symmetry on Graph Neural Networks -  R. Feynman to Bronstein",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, train AI model, Symmetry operations, Lie Groups, Tangent spaces, Klein geometry, Graph Neural Networks, Why GNN",
    "scraped_at": 1685113835.3664546,
    "genre": "Science",
    "views": "167",
    "desc": "Explain Graph Neural Networks GNN, given current advances in Vision-NN or Language-NN with BERT. It all depends on the symmetry group of your input data! If you desire Lie groups, jump right in!\\\\n\\\\nBeginning with Landau-Lifschitz mathematics on conservation laws on physical systems, we visit Feynman\\'s Lecture on symmetry to jump to Klein geometry to finally understand why we need Graph Neural Networks in Vision technologies for autonomous cars.\\\\n\\\\nRecommended Literature:\\\\n-------------------------------------------\\\\nExcellent Arxiv pre-print by \\\\nMichael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veli\\xc4\\x8dkovi\\xc4\\x87:\\\\n\\\\\"",
    "lengthSeconds": "1452",
    "uploadDate": "2021-12-06",
    "thumbnail_url": "https://i.ytimg.com/vi/QxPgaRBPdUw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DqFFH44yps8",
    "title": "Google's AutoGrad + Tensorflow's XLA Linear Algebra Compiler = JAX",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, train AI model, XLA, JAX, Autograd, differentiate, vectorize, parallelize, NumPy",
    "scraped_at": 1685113834.6118402,
    "genre": "Science",
    "views": "269",
    "desc": "Add Google\\'s AutoGrad and Tensorflow\\'s XLA linear algebra compiler and you get JAX: a python and numpy racehorse to differentiate for backprop and compile on multi TPU clouds. \\\\n\\\\nYou love numpy and want vectorization and automatic parallelization for GPUs and TPUs? Then you know JAX! \\\\n\\\\nFor the sole purpose of applying Graph Neural network models, we need to cover JAX by Google/DeepMind, before starting into Jraph, for our main purpose: Apply GNN to complex problem solving in the omniverse. Or was it the Multiverse? Any way, here is JAX!\\\\n\\\\nAll credits go to:\\\\nhttps://github.com/google/jax#neural-network-libraries\\\\n\\\\nhttps://theaisummer.com/jax/\\\\n\\\\nhttps://jax.readthedocs.io/en/latest/index.html\\\\n\\\\nCOLAB NB on Jax:\\\\nhttps://colab.research.google.com/drive/1m1WM60WJaLnIqKZ2A1SNjAyf0C5uikSD?usp=sharing\\\\n\\\\n#JAX\\\\n#MachineLearning\\\\n#XLA\"",
    "lengthSeconds": "1125",
    "uploadDate": "2021-12-03",
    "thumbnail_url": "https://i.ytimg.com/vi/DqFFH44yps8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Q-nahhtS4dc",
    "title": "Data Scientists devastated - Databricks AI networks create themselves!",
    "tags": "KERAS, Tensorflow, MLFlow, Databricks, Neural Networks, Deep Learning, AutoML",
    "scraped_at": 1685113834.6798697,
    "genre": "Science",
    "views": "52",
    "desc": "Latest dev on Deep Learning (DL)  w/ Databricks:  \\\\nTensorFlow Keras, Hyperopt, and MLflow auto-tune a neural network. Jupyter NB provided by Databricks, code segments include TensorFlow, Keras, Hyperopt, MLflow and other common python frameworks.\\\\n\\\\nCode execution on Databricks Community Edition for educational and demonstration purposes only. Real time coding.\\\\n\\\\nYes, a Python Jupyter Notebook which creates a Neural Network model \\\\nwith TensorFlow (trains w/ TensorBoard online visualization), performs automated hyperparameter tuning with Hyperopt and MLflow plus it uses autologging to save network results.\\\\n\\\\nPrevious YouTube videos were on ML (machine learning) algorithms, now focus is on Deep Learning neural networks w/ Databricks cloud platform. \\\\n\\\\nThanks to Databricks for providing a free Community edition for first testing to everybody. \\\\nThis YouTube Video is (unfortunately) not sponsored. \\\\n\\\\nCode-in-real-time.\\\\nReal time coding.\\\\nWatch real-time coding in Python, MLflow, TensorFlow, Keras, Hyperopt.\\\\nExperience databricks cloud platform.\\\\n\\\\n00:00 Deep learning\\\\n03:00 Autotune Hyperparameters\\\\n05:42 Auto-tune best NN model\\\\n08:39 Hyperopt concepts\\\\n10:48 SparkTrials class\\\\n13:12 Register Model in MLflow\\\\n\\\\n#TensorFlow\\\\n#Keras\\\\n#Hyperopt\"",
    "lengthSeconds": "928",
    "uploadDate": "2021-12-03",
    "thumbnail_url": "https://i.ytimg.com/vi/Q"
  },
  {
    "link": "watch?v=d81GrQkvp6I",
    "title": "Alternative to PyG: Mighty DEEP GRAPH Library DGL (your black belt GraphAI)",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, GCN, Graph Convolutional Networks, Graph neural networks, GNN, PyTorch, TensorFlow, Graph, Node Classification, Jupyter NB, DGL, DeepGraphLibrary",
    "scraped_at": 1685113835.9614527,
    "genre": "Science",
    "views": "1479",
    "desc": "First look: Mighty Graph Neural Network library w/ multi-GPU acceleration, called DGL Deep Graph Lib for Deep Learning on Graph structured data (non-euclidean). \\\\n\\\\nAfter 1) PyTorch Geometric (PyG) and 2) pure KERAS TF code \\\\nnow a framework agnostic Graph Neural network (GNN) Library:\\\\n\\\\n**** DEEP GRAPH Library (DGL) ****\\\\n\\\\nSupported by a very active community, empowering \\\\na) DGL-KE for learning large-scale knowledge graph embeddings or\\\\nb) DGL-LifeSci for bioinformatics and cheminformatics, etc.\\\\n\\\\nLink to Jupyter NB:\\\\nhttps://docs.dgl.ai/_downloads/b8c6bed6f07efe8046e874c957b27f7a/1_introduction.ipynb\\\\n\\\\nMain document link:\\\\nhttps://docs.dgl.ai/index.html\\\\n\\\\ngitHub:\\\\nhttps://github.com/dmlc/dgl\\\\n\\\\nDGL is free software; you can redistribute it and/or modify it under the terms of the Apache License 2.0.\\\\n\\\\n#DeepGraphLibrary\\\\n#GraphNeuralNetworks\\\\n#DGL\\\\n\\\\n00:00 Deep Learning on Graphs: DGL\\\\n02:25 DGL Community\\\\n03:00 GitHub\\\\n04:30 JupyterNB coding \\\\n06:37 Graph Convolutional Network code\\\\n08:16 Train GCN \\\\n10:20 Link prediction\\\\n11:20 Recommendation\"",
    "lengthSeconds": "734",
    "uploadDate": "2021-12-02",
    "thumbnail_url": "https://i.ytimg.com/vi/d81GrQkvp6I/maxresdefault.jpg"
  },
  {
    "link": "watch?v=3RvSAx6wziM",
    "title": "First look at PyTorch Geometric: PyG 2.0 (Nov 2021)",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, Transformer models, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Data center, cloud computing, Sentence Embedding, train AI model, GNN, GraphNeuralNetworks, PyTorch, PyTorch_geometric, PyG, FirstLook",
    "scraped_at": 1685113834.741839,
    "genre": "Science",
    "views": "1229",
    "desc": "Discover together with me PyTorch Geometric (v2). PyG to code and train Graph Neural Networks (GNNs) for applications w/ graph structured data.  Various methods for deep learning on graphs, also known as geometric deep learning, from a variety of published papers (see Arxiv for AI, GNN, GCN, GAT, and related pre-prints).\\\\n\\\\n#PyTorchGeometric\\\\n#GraphNeuralNetwork\\\\n#GeometricDeepLearning\\\\n\\\\n PyTorch Geometric (v2) on Graphs.\\\\nPyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\"",
    "lengthSeconds": "714",
    "uploadDate": "2021-11-29",
    "thumbnail_url": "https://i.ytimg.com/vi/3RvSAx6wziM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=0KH95BEz370",
    "title": "Graph Neural Networks: GCN w/ pure KERAS coding",
    "tags": "AI, Transformer, Transformer models, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, Sentence Embedding, train AI model, GNN, Graph Neural Networks, KERAS, TensorFlow, Convolution Layer, JupyterNB",
    "scraped_at": 1685113837.3635836,
    "genre": "Science",
    "views": "3440",
    "desc": "You want to code a CONVOLUTION Layer for a GNN from scratch? With TensorFlow KERAS in a Jupyter NB and train your GCN to perform NODE PREDICTION??  Welcome!!\\\\n\\\\nAfter a) GRID DATA (Vision) and b) SEQUENCE DATA (NLP - Natural Language Processing) we now switch to more complex topological data: c) GRAPH DATA!\\\\n\\\\n** Great interactive explanatory of Graph and GNN basics:\\\\nhttps://distill.pub/2021/gnn-intro/\\\\nby Google research **\\\\n\\\\nOfficial Google Colab NB:\\\\nhttps://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/graph/ipynb/gnn_citations.ipynb\\\\n\\\\n*** Highly recommended course for Geometric Deep Learning :\\\\nCS224W: Machine Learning with Graphs | 2021 \\\\nhttps://www.youtube.com/watch?v=JAB_plj2rbA\\\\nby Stanford online ***\\\\n\\\\n\\\\n\\\\n00:00 Data Topology GNN\\\\n02:30 Node Level Task\\\\n06:05 Definition GNN\\\\n08:50 Prediction Task w/ GNN\\\\n11:00 PyTorch geometric\\\\n11:20 Deep Graph Library\\\\n11:55 KERAS Jupyter NB\\\\n17:33 GNN Node Classifier\\\\n\\\\n#KERAS\\\\n#GNN\\\\n#ConvolutionLayerGNN\"",
    "lengthSeconds": "1320",
    "uploadDate": "2021-11-27",
    "thumbnail_url": "https://i.ytimg.com/vi/0KH95BEz370/maxresdefault.jpg"
  },
  {
    "link": "watch?v=IMcj0GU5md4",
    "title": "Graph Convolution Networks GCN - WHY?",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, Data center, cloud computing, Sentence Embedding, train AI model, GNN, Graph Neural Networks, KERAS, TensorFlow, Convolution Layer, JupyterNB, semi",
    "scraped_at": 1685113836.0374296,
    "genre": "Science",
    "views": "616",
    "desc": "To understand how \\\\u0026 why Graph Neural Networks GCN originated, I time travel to the origins of Graph Convolutional Networks GCN. Watch the original publications \\\\u0026 examine Deep Learning (DL) on Graph Structured Data!\\\\n\\\\nWhy not take the Adjacency matrix and the Feature Matrix of graph structured Data and feed them in a Neural Network? Why invent GCN?\\\\n\\\\nReading the original paper I found a beautiful solution in encoding information from the topological structure of data (Edge information) and the feature vectors of Node information. \\\\n\\\\nFollow along to discover the arguments by the original authors of GCNs. \\\\n\\\\nThe documents I present you are:\\\\n\\\\nhttps://tkipf.github.io/misc/GCNSlides.pdf \\\\nhttps://tkipf.github.io/graph-convolutional-networks/\\\\nby Thomas Kipf\\\\n\\\\nhttps://arxiv.org/pdf/1609.02907.pdf \\\\nby Thomas Kipf and Max Welling (arxiv pdf)\\\\n\\\\nSemi-Supervised Classification with Graph Convolutional Networks:\\\\n\\\\\"",
    "lengthSeconds": "1027",
    "uploadDate": "2021-11-26",
    "thumbnail_url": "https://i.ytimg.com/vi/IMcj0GU5md4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=l13eqc7H8X0",
    "title": "NEW TensorFlow Library on GRAPH Neural Networks released (Nov 2021)",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Tensor cores, SPARK cluster, Data center, cloud computing, train AI model, TensorFlow, GNN, Graph Neural Networks, KERAS",
    "scraped_at": 1685113835.428452,
    "genre": "Science",
    "views": "890",
    "desc": "Discover newly released TensorFlow Graph Neural Network library (TF-GNN) in Nov 2021. And watch me fail to install this preview \\\\u0026 alpha version from GitHub.\\\\n\\\\nFollowing the link from TensorFlow\\'s Blog, the GitHub repo only provides us with an alpha release of this software. Given my non-positive experience with different alpha releases in the past, I will wait for the official version 1.0.\\\\n\\\\nIn the meantime, explore KERAS capabilities for Graph Neural networks - GNNs. \\\\nWith KERAS Code sequences for * Node Label prediction * of a trained GNN in my next video. \\\\n\\\\n*** Highly recommended course for Geometric Deep Learning :\\\\nCS224W: Machine Learning with Graphs | 2021 \\\\nhttps://www.youtube.com/watch?v=JAB_plj2rbA\\\\nby Stanford online ***\\\\n\\\\n#TensorFlow\\\\n#KERAS\\\\n#GraphNeuralNetworks_GNN\"",
    "lengthSeconds": "487",
    "uploadDate": "2021-11-25",
    "thumbnail_url": "https://i.ytimg.com/vi/l13eqc7H8X0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=JwQVHnXD76M",
    "title": "Graph Neural Networks - what the Hell?",
    "tags": "AI, Sentence embedding, Transformer, Attention heads, BERT, HuggingFace, Transformer models, Machine learning, Deep learning, Unsupervised learning, Sentence transformer, Natural language processing, artificial intelligence, AI data center, Tensor cores, Data center, cloud computing, Sentence Embedding, train AI model, GNN, Graph Neural Network, GCN, MPNN, TensorFlow, Graph Data, Geometric Deep Learning",
    "scraped_at": 1685113836.1094537,
    "genre": "Science",
    "views": "729",
    "desc": "Discover Graph Neural Networks within minutes! Learn about Graph Convolution Networks GCN, Message Passing NN \\\\u0026 Graph Attention Networks GAT. Deep learning on Graph structured data!\\\\n\\\\nGoogle announced its new TensorFlow Graph Neural Network Library in Nov 2021, and I started out to discover this new library but got stuck in my reflections on what I know about GraphNN and why the world needs GraphNNs - if you already have high performing grid-based VisionNN and sequence-based Transformer NLPs (like BERT) .\\\\n\\\\n3 FREE Sources for YOU (\\\\\"",
    "lengthSeconds": "843",
    "uploadDate": "2021-11-22",
    "thumbnail_url": "https://i.ytimg.com/vi/JwQVHnXD76M/maxresdefault.jpg"
  },
  {
    "link": "watch?v=l34WKDWP3RE",
    "title": "Install Anaconda and open JupyterLab - a universal platform for Data Science",
    "tags": "AI, Machine learning, Deep learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Databricks, Delta Lake, train AI model",
    "scraped_at": 1685113834.811453,
    "genre": "Science",
    "views": "427",
    "desc": "Set up your virtual environment in Python /R, load your python packages and start up a Jupyter notebook /JupyterLab. Python coding for AI, ML, DL, artificial general intelligence, artificial decision intelligence. \\\\n\\\\nDownload anaconda installers for Win10, start up an environment with specific python libraries, choose your distribution channel (like conda-forge), do not mix pip install and conda install, check for version compatibility, and enjoy coding in Python.\\\\n\\\\n#VirtualEnvironment\\\\n#Conda\\\\n#JupyterLab\\\\n\\\\n00:00 Anaconda installer\\\\n01:05 Create environment\\\\n04:35 Terminal command\\\\n08:31 Open JupyterLab\\\\n09:34 Code in JupyterLab\"",
    "lengthSeconds": "690",
    "uploadDate": "2021-11-21",
    "thumbnail_url": "https://i.ytimg.com/vi/l34WKDWP3RE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=fEbsZ84cVz0",
    "title": "My recommendation: JupyterLab for Python and PySpark coding",
    "tags": "AI, Transformer, Transformer models, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, Multi GPU, Tensor cores, SPARK cluster, Data center, cloud computing, Databricks, Delta Lake, train AI model",
    "scraped_at": 1685113836.1784523,
    "genre": "Science",
    "views": "126",
    "desc": "My recommendation for a future-proof platform technology, that will allow you to run Data Science and Machine Learning: Jupyter Notebook or JupyterLab. \\\\n\\\\n*** Jupyter *** is open source software and open standard  for interactive computing.\\\\n*** Apache Spark *** is an open-source unified analytics engine.\\\\n*** Delta Lake ***  is an open-source project that enables building a Lakehouse Architecture on top of existing storage systems such as S3, ADLS, GCS, and HDFS.\\\\n\\\\nIf your are just starting to explore Data Science or Machine Learning, the scientific and tech world changed significantly in 2021. Data lakes evolved to DELTA Lake, with ACID transactions, parquet file formats, all open source, Python or PySpark, scalable from a single node machine to a SPARK cluster, empowered by Apache Spark. \\\\n\\\\nYou need the right tool for your coding. That opens up the world of data science (including unstructured data, Batch and streaming) to latest transformer (BERT) models in Machine Learning / Deep Learning. \\\\n\\\\nMy recommendation is Jupyter NB or Jupyter Lab. \\\\n\\\\nFrom a Laptop environment to a Spark cluster, if you know JupyterLab you can master AutoML, apply PySpark and design triggered Data pipelines with AutoLoader from databricks. \\\\n\\\\n00:00 Data + AI\\\\n01:37 Data Ecosystem in 2021\\\\n03:00 PyTorch and TensorFlow\\\\n04:45 Multi-cloud approach\\\\n06:20 Delta Lake (ACID) \\\\n08:55 Apache SPARK\\\\n09:50 Jupyter Notebook\\\\n11:12 Open Source \\\\n\\\\n\\\\n\\\\n#JupyterNB\\\\n#DataScience\\\\n#MachineLearning\"",
    "lengthSeconds": "804",
    "uploadDate": "2021-11-20",
    "thumbnail_url": "https://i.ytimg.com/vi/fEbsZ84cVz0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=d1fbM-cPKZQ",
    "title": "Databricks AutoML - First Look: auto-generate auto-tuned Jupyter ML notebooks in Python",
    "tags": "AutoML, ML, Machine Learning, Databricks, Jupyter NB, PySpark, Python, auto",
    "scraped_at": 1685113834.8754265,
    "genre": "Science",
    "views": "469",
    "desc": "First time experience of brand-new Databricks AutoML! \\\\n\\\\nDemo run on free Databricks Community Edition. First test of AutoML functions and auto-generated Jupyter NB for a ML classification problem.\\\\n\\\\nRestricted to 30 min run time for demonstration purposes Databricks AutoML delivers impressive model selection and hyperparameter tuning of ML models. Auto-generates a Jupyter NB and provides a best-of parameter tuning.\\\\n\\\\nExperience this new \\'intelligent\\' AutoML system for yourself. Presented Summer 2021 by Databricks.\\\\n\\\\nDue to the lack of available financing (smiley) no full Spark cluster run can be demonstrated, but even with a heavily limited Community edition the results are impressive. \\\\n\\\\nGive it a try, maybe it will enhance your professional workflow. \\\\n\\\\nDatabricks AutoML\\\\nAutogenerated Jupyter NB\\\\nAutogenerated ML notebooks\\\\ncode-in-real-time\\\\nreal time coding\\\\nPyspark\\\\nJupyterLab\\\\nSpark 3.2\\\\n\\\\n#pyspark \\\\n#apachespark \\\\n#databricks \\\\n#machinelearningwithpython \\\\n#ai \\\\n#artificialintelligence \\\\n#automl \\\\n#hyperparameter\\\\n#jupyterlab \\\\n#pythonprogramming\"",
    "lengthSeconds": "782",
    "uploadDate": "2021-11-19",
    "thumbnail_url": "https://i.ytimg.com/vi/d1fbM"
  },
  {
    "link": "watch?v=vXeBrJxRxQg",
    "title": "Install new Spark 3.2 on Google Colab - latest PySpark code update",
    "tags": "COLAB, Google COLAB, Apache SPARK, SPARK 3.2, PySpark, AI",
    "scraped_at": 1685113835.4884524,
    "genre": "Science",
    "views": "650",
    "desc": "Short \\\\\"",
    "lengthSeconds": "253",
    "uploadDate": "2021-11-16",
    "thumbnail_url": "https://i.ytimg.com/vi/vXeBrJxRxQg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BGTobK3dRcs",
    "title": "QUALITY CHECK your PANDAS dataframe - missing values, inconsistencies, correlation, text analysis.",
    "tags": "AI, Machine learning, Deep learning, Unsupervised learning, Natural language processing, artificial intelligence, AI data center, SPARK cluster, Data center, cloud computing, Databricks, Pandas, Pandas dataframe, Data analysis, Quality check",
    "scraped_at": 1685113836.2444532,
    "genre": "Science",
    "views": "270",
    "desc": "Pandas df profiling! The QUALITY of your input DATA is most important! Here a python lib for an ultra-fast quality check on your PANDAS dataframe. Your potentially heterogeneous tabular data structure is thoroughly checked for consistency. \\\\n\\\\nPandas profiling. Pandas dataframe profiling. Profiling of a pandas dataframe. Pandas_profiling. Missing values. \\\\n\\\\nWith visual output: Pandas_profiling v3.\\\\nhttps://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/\\\\n\\\\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\\\\n1. Type inference: detect the types of columns in a dataframe.\\\\n2. Essentials: type, unique values, missing values.\\\\n3. Statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range.\\\\n4. Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation.\\\\n5. Most frequent values.\\\\n6. Histograms.\\\\n7. Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices.\\\\n8. Missing values matrix, count, heatmap and dendrogram of missing values.\\\\n9. Duplicate rows Lists the most occurring duplicate rows.\\\\n10. Text analysis: learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data.\\\\n\\\\npython lib.\\\\n#JupyterLab\\\\n#Pandas_profiling\\\\n#Pandas_dataframe\\\\nPandas_df.\\\\n\\\\n00:00 Install Pandas_profiling lib\\\\n01:05 Profile Report \\\\n02:56 35K project descriptions\\\\n06:29 Variable Analysis\\\\n09:42 Deep Dive\"",
    "lengthSeconds": "905",
    "uploadDate": "2021-11-15",
    "thumbnail_url": "https://i.ytimg.com/vi/BGTobK3dRcs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ms6Fg6oik0w",
    "title": "ETL for Cloud Data? Better use Databricks AUTOLoader!",
    "tags": "AI, Deep learning, Natural language processing, artificial intelligence, AI data center, SPARK cluster, Data center, cloud computing, Databricks, Delta Lake, train AI model",
    "scraped_at": 1685113839.0179539,
    "genre": "Science",
    "views": "147",
    "desc": "Understand the advantages of ETL 2.0: Databricks AUTOLOADER. Ingest cloud data (streams) into DELTA LAKE: SPARK NB with real time coding (external YouTube video).\\\\n\\\\n\\\\\"",
    "lengthSeconds": "413",
    "uploadDate": "2021-11-14",
    "thumbnail_url": "https://i.ytimg.com/vi/ms6Fg6oik0w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Y8S_vJ5UkdU",
    "title": "What's new in avatar tech? Latest LLM avatar by NVIDIA!",
    "tags": "AI data center, LLM, Large Language Model, Transformer, Video analytics, Speech recognition, Avatar rendering, Face morphing, Data Center Hardware, NVIDIA A100",
    "scraped_at": 1685113839.0879533,
    "genre": "Science",
    "views": "738",
    "desc": "Technology of Natural Language Avatar - NVIDIA example (clinical avatar, health \\\\u0026 elderly care). Training LLM (large language models) for new domain specific knowledge is a supercomputing application. \\\\n\\\\nNVIDIA provides an ecosystem of hard- and software for large language AI models to be deployed for new growth enterprises:\\\\nhttps://nvidianews.nvidia.com/news/nvidia-brings-large-language-ai-models-to-enterprises-worldwide\\\\n\\\\nCurrent tech advantages - provided by press releases of NVIDIA in November 2021 - an overall technology based ecosystem presentation is recorded (incl LLM, NLP, AI Data center, Multi-GPUs, Tensor models, multilingual datasets, video analytics framework, ..).\\\\n\\\\nBut LLms like Megatron 530B (see my last video) enable just a domain specifically trained natural language model. \\\\n\\\\nHowever, if you add to its development and deployment on two DGX SuperPOD \\\\na. a SDK for speech recognition and \\\\nb. a computer vision framework for video analytics, plus \\\\nc. an AI driven 3D facial/avatar animation, you end up with \\\\nd. a virtual avatar, able to communicate and interact with you in natural language. \\\\n\\\\nBusiness cases for world leading enterprises open up in areas like clinical data, care for elderly, healthcare specific applications within hospitals and clinics, telemed at home, etc. \\\\n\\\\nThe moment global companies can fine-tune pretrained LLM (like NVIDIA\\'s Megatron 530B) for their domain specific knowledge and business cases, avatars will step out of video games and provide significant input for corporate wealth creation. The benefits for individual users given a fine-tuned virtual interaction platform (in natural language) is another envisioned advantage. \\\\n\\\\n00:00 NVIDIA Enterprise AI\\\\n01:45 DGX A100 GPUs\\\\n03:29 Training costs\\\\n04:20 Investment\\\\n05:25 System integration\\\\n07:30 Ecosystem for avatar\\\\n08:35 Business cases\\\\n09:15 Supercomputer needed\\\\n\\\\nUnfortunately this video is not sponsored by anybody. It just reflects my tech foresight on currently available tech ecosystems for the next generation of AI induced products and services.\\\\n\\\\n#NVIDIA\\\\n#Avatar_tech\\\\n#LLM\"",
    "lengthSeconds": "652",
    "uploadDate": "2021-11-13",
    "thumbnail_url": "https://i.ytimg.com/vi/Y8S_vJ5UkdU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=k0xB6pomTMM",
    "title": "After GPT-3: The new Queen!",
    "tags": "film, udost",
    "scraped_at": 1685113839.1569543,
    "genre": "Science",
    "views": "402",
    "desc": "Forget old GPT-3 of 2020. Megatron-Turing NLG 530B: the worlds largest and most powerful generative large language AI model (LLM).\\\\n\\\\nThe most powerful monolithic transformer language model (LLM) trained to date on this planet, with 530 billion parameters, is of Microsoft and NVIDIA to further parallelize and optimize the training of very large AI models (as of Nov 2021).\\\\n\\\\nMicrosoft link:\\\\nhttps://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\\\\n\\\\nSummary:\\\\nA common dataset (the Pile, 800GB) as input for our AI models.\\\\nHighly parallelized software model (Transformer based, 350 billion parameter, 128 attention heads).\\\\nExtreme AI supercomputer executing AI training of our models.\\\\nFinal result to commercialize: a trained LLM (large language model). \\\\n\\\\nIn detail - step by step: \\\\n------------------------------------\\\\n\\\\n1. Start with one of the largest designed English dataset for AI/ML learning, the Pile, of 800GB of English sentences from Biomed to Physics, plus CC,\\\\n\\\\n2. design an efficient and scalable 3D parallel SW-system capable of combining data, pipeline, and tensor-slicing based parallelism\\\\n\\\\n3. run it on one of the worlds AI supercomputers, NVIDIA Selene, with 560 DGX A100 servers (each with eight A100 80GB Tensor core GPUs)\\\\n\\\\n4. you will get a nicely trained LLM for the English language, named Megatron-Turing Natural Language Generation model (MT-NLG).\\\\n\\\\n\\\\n\\\\n00:00 What is data?\\\\n00:48 The Pile \\\\n03:25 SW: Biggest LLM on earth\\\\n05:57 HW: Supercomputer Selene\\\\n07:40 AI for SMEs?\\\\n08:45 Main results \\\\n11:03 Bias in language models\\\\n13:21 Importance of data for AI\\\\n\\\\n\\\\nRecent work in language models (LM) has demonstrated that a strong pretrained model can often perform competitively in a wide range of NLP tasks without finetuning.\\\\n\\\\nBy the way: about 90% of cost of commercially employing AI models emerge from inference processes. Smile. \\\\n\\\\nMegatron-Turing NLG 350B.\\\\nMT-NLG 350B.\\\\nNVIDIA Selene supercomputer .\\\\n560 DGX A100 servers networked with HDR InfiniBand.\\\\nBias in language models.\\\\nThe importance of data for AI performance.\\\\n#MT_NLG_350B\\\\n#Transformer_Language_Model\\\\n#Pre_trained_LLM\"",
    "lengthSeconds": "961",
    "uploadDate": "2021-11-11",
    "thumbnail_url": "https://i.ytimg.com/vi/k0xB6pomTMM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Rz_Ai2_peL8",
    "title": "What is AutoML in AI?",
    "tags": "film, udost",
    "scraped_at": 1685113837.4325812,
    "genre": "Science",
    "views": "84",
    "desc": "What services do Google, Microsoft et al. offer for AutoML - Automated Machine Learning?\\\\nScanning through the presentations of Azure, Google Cloud and Databricks about their specific AutoML services. \\\\n\\\\nExperience a first look with me.\\\\n\\\\n\\\\\"",
    "lengthSeconds": "1023",
    "uploadDate": "2021-11-09",
    "thumbnail_url": "https://i.ytimg.com/vi/Rz_Ai2_peL8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GIl0VpxL0ws",
    "title": "No-code AI system: AutoML",
    "tags": "film, udost",
    "scraped_at": 1685113835.5624285,
    "genre": "Science",
    "views": "99",
    "desc": "A no code AI system? A system that writes python ML code for you? And puts it in a Jupyter Notebook for you? The complete code for the model? And trains the model for you? And optimizes the model with hyperparameters for you? And delivers you a production ready model? As a no-code AI system? \\\\n\\\\nYou as a user just have to have a dataset. Your data you want to investigate. To find correlations in your data to optimize your business results. To gain insights in your complex business data. And find causation to optimize your business results. You want the insights - insights in your data to act upon! Not to code complex AI in python, with Jupyter NB/Lab, PyTorch, Tensorflow etc.\\\\n\\\\nYou just need your domain specific data, and the system does the work for you:\\\\n\\\\n1. The system chooses an ML model for you.\\\\n2. Trains different systems on your data.\\\\n3. Recommends a system for your data.\\\\n4. Runs the ML system on your data.\\\\n5. Optimizes the hyperparameter.\\\\n6. Presents you a Jupyter NB with all the python code.\\\\n7. You can add your domain specific knowledge to it.\\\\n8. Add relevant data from your feature store.\\\\n9. The systems optimizes itself with your new input.\\\\n10. Push the production stage model to REST API.\\\\n11. And you are up and running an online AI system - optimized for your data.\\\\n\\\\nWelcome to the advanced world of AutoML. \\\\nAutogenerated Maschine Learning Jupyter Notebooks (Spark 3.2).\\\\n\\\\n\\\\n00:00 AI system writes ML code for you\\\\n01:30 Add your domain specific knowledge\\\\n02:45 Data eats AI\\\\n04:00 No code approach \\\\n04:55 Causation in your data\\\\n\\\\n#AutoML\\\\n#Autogenerate_JupyterNB\\\\n#No_code_AI\"",
    "lengthSeconds": "339",
    "uploadDate": "2021-11-05",
    "thumbnail_url": "https://i.ytimg.com/vi/GIl0VpxL0ws/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1o5qD3hvThg",
    "title": "Apache SPARK MLlib - Machine Learning for Data Science and AI deployment on SPARK cluster",
    "tags": "film, udost",
    "scraped_at": 1685113835.6334279,
    "genre": "People",
    "views": "648",
    "desc": "Learn Apache SPARK. Spark includes MLlib - Machine Learning for Data Science and AI deployment. Databricks for beginners. Learn SPARK MLlib v3.2.\\\\n\\\\nWe start up Databricks Community edition and try out MLlib in Apache SPARK for the first time. Simple, first steps with databricks: start up a cluster and attach a ML Jupyter Notebook.\\\\n\\\\nAWS new AI/ML chips AWS Trainium, the new processor is optimized for deep learning training workloads on semantic search and natural language processing.  AWS Inferentia, a custom processor for ML inference.\\\\n\\\\nPlus Google Cloud TPUs.  Cloud TPU is designed to run cutting-edge machine learning models with AI services on Google Cloud.\\\\n\\\\nExecute some Python code in ML on the cluster and train a simple ML model with PySpark on Databricks ML.\\\\n\\\\nCode with MLflow and experience first insights on the Community edition on ML (options for PyTorch and TensorFlow).\\\\n\\\\nApache SPARK 3.2 with Delta Lake architecture, Pandas-on-Spark API and databricks runtime 10 ML on free community edition of databricks.\\\\n\\\\n00:00 Apache SPARK MLlib\\\\n02:32 ML Data-Frame based API\\\\n03:39 Databricks CE\\\\n07:20 Create Cluster\\\\n08:44 Azure-AWS-Google Cloud\\\\n10:55 Databricks ML Quickstart\\\\n14:44 Data problem solved\\\\n17:12 MLflow \\\\u0026 train model\\\\n\\\\n#pyspark \\\\n#datascience \\\\n#machinelearning \\\\n#deeplearning \\\\n#databricks \\\\n#apachespark \\\\n#ml \\\\n#tpu \\\\n#computerscience\"",
    "lengthSeconds": "1293",
    "uploadDate": "2021-11-04",
    "thumbnail_url": "https://i.ytimg.com/vi/1o5qD3hvThg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=dkXjr8NLn_I",
    "title": "Learn SBERT Sentence Embedding:  SBERT  TSDAE - Transformer based Denoising AutoEncoder (SBERT 22)",
    "tags": "film, udost",
    "scraped_at": 1685113839.6229606,
    "genre": "Science",
    "views": "1845",
    "desc": "SBERT TSDAE (Transformer based Denoising Auto Encoder): You want to code Sentence Transformers (based on BERT models) to extract semantic information on millions of documents? Here is your python code - with October 2021 updates.\\\\n\\\\nNew pre-trained models of BERT transformer models and Sentence Transformer models on HuggingFace accelerate Sentence embedding for semantic content visualization of huge text documents.\\\\n\\\\nLearn to apply Sentence transformer models (BERT based) in real time coding. Free python coding sequences for Sentence Embedding in high dimensional topological spaces. \\\\n\\\\n\\\\\"",
    "lengthSeconds": "1169",
    "uploadDate": "2021-10-27",
    "thumbnail_url": "https://i.ytimg.com/vi/dkXjr8NLn_I/maxresdefault.jpg"
  },
  {
    "link": "watch?v=VVocMHWqoHE",
    "title": "New Pandas API in Spark 3.2 for single node and multi node",
    "tags": "film, udost",
    "scraped_at": 1685113837.4996,
    "genre": "Science",
    "views": "2607",
    "desc": "With SPARK 3.2 a new PANDAS API on SPARK is available. \\\\nInstall and first tests of this new Pandas API locally on a single node machine. Learn new PANDAS on SPARK ( = KOALAS 2.0), also on multi-node SPARK cluster (upcoming vid).\\\\n\\\\nExperience the benefits from the NEW PANDAS API on SPARK 3.2!\\\\nUnify small data API and big data API with new visualizations, a faster single node performance on PANDAS and huge speed improvements from PANDAS on SPARK (on different SPARK cluster).\\\\n\\\\nStep by Step video to install SPARK 3.2 on your local machine (Laptop, PC), start up a SPARK session and experience speed improvements on PANDAS on SPARK 3.2. \\\\n\\\\n00:00 Install SPARK 3.2\\\\n01:45 Databricks RUNTIME 10\\\\n03:15 Scalability \\\\n04:20 Visualization for PANDAS \\\\n06:10 Pandas API on SPARK\\\\n09:30 Transform \\\\u0026 apply a function\\\\n12:17 Pandas to PySPARK DataFrames\\\\n\\\\n\\\\nMy personal JupyterLab of this YouTube vid is available for demonstration purposes only:\\\\nhttps://gist.github.com/qcdquark/47d07d942068e7ae82a0bb688b7db033\\\\n\\\\nNew PANDAS API on SPARK 3.2\\\\nSpeed up Pandas performance\\\\nHuge Pandas files in Spark\\\\nSwitch from Koalas on Spark to PANDAS on Spark.\\\\n\\\\n#pandasdataframe \\\\n#pandas \\\\n#pyspark \\\\n#databricks \\\\n#datascience \\\\n#dataanalysis \\\\n#dataframe \\\\n#jupyterlab \\\\n#koalas \\\\n#api \\\\n#apachespark\"",
    "lengthSeconds": "773",
    "uploadDate": "2021-10-22",
    "thumbnail_url": "https://i.ytimg.com/vi/VVocMHWqoHE/maxresdefault.jpg"
  },
  {
    "link": "watch?v=lQJfz3LnqWQ",
    "title": "Download pre-trained BERT models - at HuggingFace - incl. Sentence Transformers Models (SBERT 21)",
    "tags": "film, udost",
    "scraped_at": 1685113837.5635765,
    "genre": "Science",
    "views": "2782",
    "desc": "New to coding artificial intelligence? Bidirectional Encoder Representations from Transformers (or BERT) is a transformer-based machine learning technique for natural language processing - NLP.\\\\n\\\\nUnfamiliar with the benefits of HuggingFace ? Learn to apply its pretrained models and accelerate your ML training!\\\\n\\\\nNo problem. This is a short intro to choose a pretrained transformer model for NLP. Thousands of pretrained models are available on HuggingFace. An open source AI platform  in natural language processing.\\\\n\\\\nIf you are a beginner, there is a simple way to explore the different models. Pretrained Transformer models are searchable for their properties and intended applications. \\\\n\\\\nPretrained BERT models with its characteristics are available to choose from, select if you use TensorFlow or PyTorch and your specific language. \\\\n\\\\nPretrained BERT models  like BERT_base_uncased or Roberta_large are available, plus GPT2 or XLM, XLNet ...\\\\n\\\\nYou will find out on what datasets these models have been trained on and their specific Transformer architecture.\\\\n\\\\n00:00 Welcome \\\\n02:10 HuggingFace BERT models\\\\n06:05 Sentence Transformer models\\\\n\\\\n\\\\nfree pre-trained Transformer models of HuggingFace.\\\\nApply knowledge encoding of already pre-trained BERT models.\\\\nLimitations of pre-trained transformer models. \\\\nopen source in natural language processing.\\\\n\\\\n#Open_Source\\\\n#HuggingFace\\\\n#BERT\"",
    "lengthSeconds": "573",
    "uploadDate": "2021-10-22",
    "thumbnail_url": "https://i.ytimg.com/vi/lQJfz3LnqWQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=FvgAEFJT1kQ",
    "title": "From PARQUET to DELTA LAKE - Data Lake 2.0 !",
    "tags": "film, udost",
    "scraped_at": 1685113837.6305737,
    "genre": "People",
    "views": "117",
    "desc": "Intro-video to Databricks DELTA LAKE functionality with Databricks runtime version 8.3!\\\\n\\\\nDELTA LAKE vs PARQUET (from a coding perspective).\\\\nCompare speed, functionality, complexity and ACID transformations. \\\\n\\\\nExplore structured streaming to a PARQUET file - to add data.\\\\nAnd compare this to streaming data to a DELTA TABLE. \\\\n\\\\n00:00 Explore Parquet w/ streaming\\\\n03:52 Create DELTA from data\\\\n05:25 Fix streaming w/ DELTA\\\\n\\\\n\\\\nCheck out this advantages:\\\\nBatch and stream processing and schema enforcement with DELTA LAKE.\\\\n\\\\nLinks:\\\\nhttps://databricks.com/notebooks/gallery/IntroductionDeltaLake.html\\\\nhttps://databricks.com/notebooks/gallery/DivingIntoDeltaLake.html\\\\n\\\\n#datalake \\\\n#deltalake\\\\n#lakehouse \\\\n#pyspark \\\\n#databricks \\\\n#datascience  \\\\n#datascientist \\\\n#sql  \\\\n#dataengineering \\\\n#artificialintelligence \\\\n#businessanalytics\\\\n\\\\nFree photo credits:\\\\nhref=\\'https://www.freepik.com/photos/people\\'\\\\nPeople photo created by jcomp - www.freepik.com\"",
    "lengthSeconds": "484",
    "uploadDate": "2021-10-14",
    "thumbnail_url": "https://i.ytimg.com/vi/FvgAEFJT1kQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Oz8GV27jw4Q",
    "title": "DELTA LAKE for Data Streams - PySpark code",
    "tags": "film, udost",
    "scraped_at": 1685113837.6986003,
    "genre": "People",
    "views": "53",
    "desc": "Intro video addendum to code an example of enforcing \\\\\"",
    "lengthSeconds": "387",
    "uploadDate": "2021-10-13",
    "thumbnail_url": "https://i.ytimg.com/vi/Oz8GV27jw4Q/maxresdefault.jpg"
  },
  {
    "link": "watch?v=arJ0THD57Xg",
    "title": "DELTA LAKE w/ PySpark on Databricks CE",
    "tags": "film, udost",
    "scraped_at": 1685113836.3164525,
    "genre": "People",
    "views": "54",
    "desc": "Build Lakehouses with Delta Lake:\\\\nDelta lake is an open-source project that enables building a Lakehouse Architecture on top of existing storage systems such as S3, ADLS, GCS, and HDFS.\\\\n\\\\nData lakes typically have multiple data pipelines reading and writing data concurrently, and data engineers have to go through a tedious process to ensure data integrity, due to the lack of transactions. Delta Lake brings ACID transactions to your data lakes. \\\\n\\\\nIn big data, even the metadata itself can be \\\\\"",
    "lengthSeconds": "649",
    "uploadDate": "2021-10-12",
    "thumbnail_url": "https://i.ytimg.com/vi/arJ0THD57Xg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=Bx16-U1sGtk",
    "title": "DELTA LAKE w/ Delta Spark code for Streaming /Business Analytics/BI",
    "tags": "film, udost",
    "scraped_at": 1685113839.225954,
    "genre": "Science",
    "views": "127",
    "desc": "Delta lake is an open-source project that enables building a Lakehouse architecture on top of these storage systems: S3, ADLS, GCS, HDFS. \\\\nAll documentation used: from Databricks.com\\\\n\\\\nhttps://docs.delta.io/latest/api/python/index.html\\\\n\\\\nDelta Lake supports some statements to facilitate deleting data from and updating data in Delta tables. Delete, Update, insert and Merge are presented in this video, plus a code example of Streaming to a Delta Table. \\\\n\\\\nDelta Lakes are kind of a metadata layer on top of your data lakes and support finally ACID transactions, a fast SQL engine and direct data access for your ML, AI or Business Intelligence / Analytics task. \\\\n\\\\nDelta Lakes are kind of Data Lakes 2.0. Standard Apache Parquet file format with a transaction log, as presented in this video, with time travel to recover older versions and history.\\\\n\\\\nReal-time coding. SQL and Python APIs. PySpark and Delta Lake. New SQL Engine. \\\\nACID transactions and time travel for your data management.\\\\n\\\\nArtificial intelligence, machine learning and Business Intelligence with advanced analytics.\\\\nData Engineering. Data Scientist. Data Science. Jupyter. JupyterLab.\\\\n\\\\n00:00 Install Delta Spark \\\\n01:54 Delta Table\\\\n03:57 Parquet file format\\\\n06:24 Operations on a Delta Lake /table\\\\n08:05 Update and merge Delta Tables\\\\n10:10 Delta Table Streaming / reads and writes\\\\n11:13 Automatic Schema Evolution\\\\n11:25 Code Examples\\\\n\\\\n#datalake \\\\n#deltalake\\\\n#lakehouse \\\\n#pyspark \\\\n#databricks \\\\n#datascience  \\\\n#datascientist \\\\n#sql  \\\\n#dataengineering \\\\n#artificialintelligence \\\\n#businessanalytics\"",
    "lengthSeconds": "1056",
    "uploadDate": "2021-10-05",
    "thumbnail_url": "https://i.ytimg.com/vi/Bx16"
  },
  {
    "link": "watch?v=c55Diu3M-lw",
    "title": "Data Warehouse (1980s) to Data Lake (2010s) to LAKEHOUSE (2020s) w/ Delta Lake by Databricks",
    "tags": "film, udost",
    "scraped_at": 1685113836.3804567,
    "genre": "Science",
    "views": "101",
    "desc": "A Lakehouse is a new open data management architecture that combines the best elements of data lakes (scalable, flexible) and data warehouses for machine learning and artificial intelligence, including business analytics and intelligence.\\\\n \\\\nACID transactions, Schema enforcement, BI support, unstructured to structured data, for data science, machine learning, SQL and analytics. \\\\nOptimized for end-to-end streaming. \\\\nUtilizing cheap data storage options. \\\\n\\\\nDoes your company still run a data lake, multiple dedicated data warehouses and image databases? Not cost effective at all? Check out the latest evolution in open source and open standard development: Lake House by Databricks.\\\\n\\\\n00:00 Explain DELTA Lake\\\\n03:10 Lake House Architecture\\\\n05:55 ACID table storage layer\\\\n09:50 Code DELTA SPARK\\\\n\\\\nBased on the Apache Parquet file format the latest step in the evolution of data management for BI, BA and Machine Learning (ML) is: the Lakehouse. \\\\n\\\\nDatabricks developed in 2020 the Lakehouse architecture and delta lake implementation. \\\\nTo understand LAKEHOUSE, a short summary and intro is presented in this video, including links to its original technical papers.\\\\n\\\\nReal-time coding: PySpark, Spark and DELTA-SPARK w/ Jupyter Lab.\\\\nInstallation of Delta-Spark on a laptop.\\\\nWrite \\\\u0026 read operations of a Spark dataframe to a Delta table (ACID transactions).\\\\n\\\\nSource:\\\\nhttps://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html\\\\n\\\\n\\\\nReal-time coding in PySpark.\\\\nReal-time coding in DELTA Spark.\\\\nDelta Lake explained.\\\\nLake House explained.\\\\nData Warehouses and Data Lakes lead to Delta Lake and Lake House by Databricks.\\\\nACID Transaction.\\\\nDelta tables and Spark dataframes.\\\\n\\\\n#datalake \\\\n#deltalake\\\\n#lakehouse \\\\n#pyspark \\\\n#databricks \\\\n#datascience  \\\\n#datascientist \\\\n#sql  \\\\n#dataengineering \\\\n#artificialintelligence \\\\n#businessanalytics\"",
    "lengthSeconds": "847",
    "uploadDate": "2021-09-29",
    "thumbnail_url": "https://i.ytimg.com/vi/c55Diu3M"
  },
  {
    "link": "watch?v=m5y579SnvzI",
    "title": "Self-learning Course Data Scientist, Data Engineer w/ PySpark and Delta Lake",
    "tags": "film, udost",
    "scraped_at": 1685113836.445452,
    "genre": "Science",
    "views": "73",
    "desc": "You want to become a Data Scientist, a Data engineer or a Platform Administrator for Cloud Computing with PySpark utilizing DELTA lakes and brand new Lakehouse architecture? \\\\n\\\\nStart right here: https://docs.databricks.com/onboarding/paths/index.html\\\\n\\\\nNew learning videos for your data specialization, free of charge on Databricks.\\\\n\\\\nLearn how to implement your code with PySpark on Jupyter, running DELTA lake and utilizing the new LAKEHOUSE architecture.\\\\n\\\\n\\\\n#datalake \\\\n#deltalake\\\\n#lakehouse \\\\n#pyspark \\\\n#databricks \\\\n#datascience  \\\\n#datascientist \\\\n#sql  \\\\n#dataengineering \\\\n#artificialintelligence \\\\n#businessanalytics\"",
    "lengthSeconds": "347",
    "uploadDate": "2021-09-24",
    "thumbnail_url": "https://i.ytimg.com/vi/m5y579SnvzI/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vaBpO8deYo8",
    "title": "Real world industrial-service application: AI recruitment - a simple example",
    "tags": "film, udost",
    "scraped_at": 1685113837.759574,
    "genre": "Education",
    "views": "87",
    "desc": "A real-world example is given regarding AI in a specific industrial-service-sector: recruitment for large corporations with insight generated by AI systems (neuro-linguistic plus Graph Neural Networks).\\\\n\\\\nHiring the right individual for an established team structure in an highly competitive environment or choosing the leader for an emerging service-sector can make or break a new revenue stream within a corporation.\\\\n\\\\nThis video is not focused on pure code sequences, but market growth opportunities within established sectors, given new applications of AI algorithms.\\\\n\\\\nSimplest possible example is given: comparing paragraphs of text and computing similarity measures. BERT or Sentence BERT models can be applied on pre-trained models from HuggingFace.\\\\n\\\\n#artificialintelligence  \\\\n#intelligence \\\\n#recruitment \\\\n#hiring \\\\n#insights \\\\n#bert \\\\n#ai \\\\n#algorithm \\\\n#industrial  \\\\n#servicesector \\\\n#market \\\\n#growth \\\\n#sbert \\\\n#vocabulary \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "491",
    "uploadDate": "2021-09-14",
    "thumbnail_url": "https://i.ytimg.com/vi/vaBpO8deYo8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=iLOSunZ3muo",
    "title": "Turbo charge PySpark df with PyArrow for pandas DataFrame and  Parquet files - the code",
    "tags": "film, udost",
    "scraped_at": 1685113837.8205988,
    "genre": "Education",
    "views": "840",
    "desc": "On a single node machine (eg laptop w/ multiple CPU cores):\\\\nActivate all CPU cores/threads with PySpark and apply PyArrow to read pandas df and parquet files.\\\\n \\\\nCaution:\\\\nLAZY Evaluation w/ Spark: execution will not start until an action is triggered (concerning all my speed tests - smile).\\\\n\\\\nSpark and data lakes.\\\\nParquet files and Spark.\\\\nLoad Spark dataframe from parquet file.\\\\nApply PyArrow acceleration to transform Pandas df.\\\\nSpeed test of Spark (8 CPU threads) compared to Pandas operations. \\\\nFile format for data lakes: parquet. \\\\nSQL tables: beautiful interface.\\\\n\\\\nAccelerate your pandas df performance multiple times. \\\\n\\\\n#code_your_own_AI\\\\n#code_in_real_time \\\\n#datascience \\\\n#computerscience \\\\n#spark \\\\n#pandasdataframe \\\\n#dataframe \\\\n#pyspark \\\\n#cpu \\\\n#databricks \\\\n#apachespark\"",
    "lengthSeconds": "704",
    "uploadDate": "2021-08-16",
    "thumbnail_url": "https://i.ytimg.com/vi/iLOSunZ3muo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=MvXy8UvegY0",
    "title": "Pandas DataFrame: turbo charge with PySpark on 12 CPU threads on single node",
    "tags": "film, udost",
    "scraped_at": 1685113837.888575,
    "genre": "People",
    "views": "114",
    "desc": "Speed challenge: \\\\ninput 1.7GB and 5.5GB of data for data science, on a single node machine. Achieve maximum performance on your laptop. How?\\\\n\\\\nOperational performance of working with these Pandas df, compared to a Spark single node, stand-alone implementation with 12 CPU threads. \\\\n\\\\nBeware: \\\\nLazy Evaluation w/ Spark: execution will not start until an action is triggered. \\\\nRemember this when executing speed test: trigger an action!\\\\n\\\\nCode to install Spark and transform Pandas df to Spark df. \\\\n\\\\n#code_your_own_AI\\\\n#code_in_real_time \\\\n#datascience \\\\n#computerscience \\\\n#spark \\\\n#pandasdataframe \\\\n#dataframe \\\\n#pyspark \\\\n#cpu \\\\n#databricks \\\\n#speedtest \\\\n#multicore\"",
    "lengthSeconds": "1094",
    "uploadDate": "2021-08-09",
    "thumbnail_url": "https://i.ytimg.com/vi/MvXy8UvegY0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=C1yt25NszNA",
    "title": "Operational Data Science w/ Python in 8 minutes (max performance for laptop)",
    "tags": "film, udost",
    "scraped_at": 1685113837.955601,
    "genre": "Science",
    "views": "39",
    "desc": "Accelerate and optimize Data Science on single node machines (eg laptops, PC at home). Experiences and personal recommendations for beginners regarding which tech platform / processing frameworks to choose. \\\\n\\\\nBeware: \\\\nLazy Evaluation w/ Spark: execution will not start until an action is triggered.\\\\n\\\\nAccelerate your workflow as a data scientist or a data analyst with data processing and data storage cloud platforms (EMR on EC2, S3-datalake, etc.) with identical coding platforms on single-node and multi-node clusters in the cloud (azure, google cloud, amazon EMR).\\\\n\\\\nClean BI interface to tableau, amazon Athena. \\\\nAccess to data warehouses like Redshift. \\\\nFuture proof file formats: like parquet.\\\\nAcceleration by PyArrow. \\\\nFamiliar pandas API. \\\\n\\\\nNext video on installing and coding w/ these frameworks. \\\\n\\\\n#code_your_own_AI\\\\n#code_in_real_time \\\\n#datascience \\\\n#computerscience \\\\n#spark \\\\n#pandasdataframe \\\\n#dataframe \\\\n#pyspark \\\\n#cpu \\\\n#databricks\"",
    "lengthSeconds": "526",
    "uploadDate": "2021-08-02",
    "thumbnail_url": "https://i.ytimg.com/vi/C1yt25NszNA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=1RKobQLAk7E",
    "title": "Accelerate pandas df: DASK  2021 = superfast Python!",
    "tags": "film, udost",
    "scraped_at": 1685113838.019574,
    "genre": "Science",
    "views": "101",
    "desc": "DASK scales numpy arrays and pandas dataframes efficiently. \\\\nUtilizes all CPU cores / threads.\\\\n\\\\nVideo shows you that four lines of code set up a local DASK cluster, automatically on my Win10 PC, to supercharge python - even on a single CPU.\\\\n\\\\nOptimize your data input pipeline to your transformer models (AI) with a local cluster configuration, utilizing your system resources.\\\\n\\\\nSpeed tests on numpy arrays w/ DASK.\\\\nSpeed tests on pandas dataframes w/ DASK.\\\\n\\\\n#code_in_real_time\\\\n#real_time_coding\\\\n#DASK\\\\n#parallelize_python\\\\n#cluster\\\\n#JupyterLab\\\\n#python\\\\n#pandas\\\\n#numpy\"",
    "lengthSeconds": "659",
    "uploadDate": "2021-07-27",
    "thumbnail_url": "https://i.ytimg.com/vi/1RKobQLAk7E/maxresdefault.jpg"
  },
  {
    "link": "watch?v=fvNDI1qdzqw",
    "title": "Koalas dataframe on SPARK = Pandas API supercharged!",
    "tags": "film, udost",
    "scraped_at": 1685113836.5174525,
    "genre": "Science",
    "views": "190",
    "desc": "Ever wondered why your CPU computes Python not on all cores/threads?\\\\nSolve this problem for your data engineering and data science tasks, including AI.\\\\n\\\\nAchieve python code execution at 100% on all your CPU threads - with Koalas. \\\\n\\\\nKoalas: a pandas equivalent API that works on Apache Spark. \\\\nTransfer your \\\\\"",
    "lengthSeconds": "672",
    "uploadDate": "2021-07-20",
    "thumbnail_url": "https://i.ytimg.com/vi/fvNDI1qdzqw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=h9ytiaxNJC0",
    "title": "How to apply AI systems for real Insights - lessons learned (SBERT 20)",
    "tags": "film, udost",
    "scraped_at": 1685113836.582438,
    "genre": "Science",
    "views": "177",
    "desc": "Summary of my 3 short video series on three different algorithms to deep-dive on semantic complexities. Aim to augment knowledge on interconnected \\\\u0026 multidisciplinary topics. \\\\n\\\\nBE careful: \\\\n1. for WordDistance (Python) my datasets were all sentences, which included my set of target words.\\\\n2. for word2vec (TensorFlow) my datasets were all available sentences (0.4 mio). Since my set of target-word-sentences was below 5000 sentences, not an adequate cardinal number for training a shallow neural network!\\\\n3. for Sentence Transformer (PyTorch) again my set of target-word-sentences only, since free COLAB RAM restrictions (12GB) did not allow for extended unsupervised learning. But SBERT works well for my 4000 sentences. Even on my local CPU, without cuda core acceleration. \\\\n\\\\nStrength and weaknesses of a 2 dimensional word distance graph representation, which serves as input to further investigate complexities applying word2vec algorithms (TensorFlow 2), resulting in a transformer based AI representation of embedded sentences in a topological space. \\\\n\\\\nFinal insights in thematic clustering of 34000 R\\\\u0026D projects in Europe and hint of missing links between scientific topics and technology in industrial sectors. \\\\n\\\\nThis answers my initial question:\\\\nWhat can a simple AI system do for our understanding of innovation systems at continental scale. \\\\n\\\\n\\\\n#code_in_real_time \\\\n#word2vec \\\\n#SentenceTransformer\\\\n#tensorflow2 \\\\n#pytorch \\\\n#3d \\\\n#graphs \\\\n#graph_theory \\\\n#complexities\\\\n#unsupervised \\\\n#sbert \\\\n#vocabulary \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "801",
    "uploadDate": "2021-07-16",
    "thumbnail_url": "https://i.ytimg.com/vi/h9ytiaxNJC0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=cDir_PifUHQ",
    "title": "34000 EU projects: SBERT Sentence Embedding for Insights, compare to word2vec & 3d graphs (SBERT 19)",
    "tags": "film, udost",
    "scraped_at": 1685113836.650453,
    "genre": "Science",
    "views": "181",
    "desc": "SBERT Sentence Transformers to achieve critical insights to multidisciplinary content of 34000 R\\\\u0026D projects in Europe. SBERT compared to word2vec. And classical graph theory!\\\\n\\\\nThis is video 3 of this series: 3 codes on the Future of Europe. \\\\nHow can AI augment our knowledge on the City of Tomorrow\\\\n\\\\nOur structural approach: \\\\n1.  After classical graph theory applied to word distance (2 dim) and \\\\n2.  word2vec word embedding (128 dim),\\\\n3.  now we apply on a level of sentences contextual embedding (384 dim).\\\\n\\\\nWe use a pre-trained transformer model from HuggingFace, apply some unsupervised learning and explore the resulting thematic clusters. After some dimension reduction (UMAP) and clustering (HDBSCAN).  \\\\n\\\\n#sbert \\\\n#eu \\\\n#research \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "1244",
    "uploadDate": "2021-07-13",
    "thumbnail_url": "https://i.ytimg.com/vi/cDir_PifUHQ/maxresdefault.jpg"
  },
  {
    "link": "watch?v=DAXtzfZCu2w",
    "title": "Visualize missing links given a 3D interactive network of word embedding (word2vec) - Addendum1",
    "tags": "film, udost",
    "scraped_at": 1685113836.7104273,
    "genre": "People",
    "views": "62",
    "desc": "This is an addendum to my video on word2vec word embedding (video 2 of 3).\\\\nGiven a 128 word embedding of our vocabulary with a resulting cosine similarity calculation of topological \\\\\"",
    "lengthSeconds": "608",
    "uploadDate": "2021-07-09",
    "thumbnail_url": "https://i.ytimg.com/vi/DAXtzfZCu2w/maxresdefault.jpg"
  },
  {
    "link": "watch?v=hjbLac8t0Hs",
    "title": "Semantic similarity w/ word2vec: interactive 3D visualizations w/ TensorFlow2",
    "tags": "film, udost",
    "scraped_at": 1685113836.7754521,
    "genre": "Science",
    "views": "151",
    "desc": "Second part shows my code sequences in JupyterLab (Tensorflow2, Python3) for a shallow neural network where to extracts the weights from a hidden layer for a 128 dimensional word embedding in an artificial topological space. \\\\n\\\\n\\\\nCalculate cosine similarity in all dimensions (no dimension reduction with UMAP) and visualize for a set of words - and their close semantic words in their epsilon vicinity - an interactive visualization of their semantic interlinks, given my dataset of 34000 R\\\\u0026D projects of Europe. \\\\n\\\\n\\\\nExample chosen is the city of tomorrow: new urban spaces. \\\\n\\\\n\\\\n#code_in_real_time\\\\n#real_time_coding\\\\n#Tensorflow2\\\\n#JupyterLab\\\\n#python\\\\n#word2vec\\\\n#cosine_similarity\\\\n#3D\\\\n#interactive\\\\n#city_of_tomorrow\"",
    "lengthSeconds": "1293",
    "uploadDate": "2021-07-06",
    "thumbnail_url": "https://i.ytimg.com/vi/hjbLac8t0Hs/maxresdefault.jpg"
  },
  {
    "link": "watch?v=vAMNdhSuhHw",
    "title": "Apply graph theory, shallow neural networks & transformer models (AI) to solve a real world problem",
    "tags": "film, udost",
    "scraped_at": 1685113838.0866013,
    "genre": "Education",
    "views": "124",
    "desc": "A 3 step approach for a 2 dimensional word distance (with a 3D interactive folding to a Graph) to a 128 dimensional word2vec TensorFlow 2 embedding to finally a sentence embedding with BERT-based Transformers in 768 dim.\\\\n\\\\n\\\\nApply theses tools wisely in a structured approach to gain insights to a complexity, like the content of thousands of R\\\\u0026D projects and the common exploration patterns. \\\\n\\\\n\\\\nWith the aim to write a contribution to the Conference on the future of Europe, given the (AI-based) knowledge of all European R\\\\u0026D projects. The visualization of these AI-based insights simply augment my thoughts for the future of Europe, act as a base. \\\\n\\\\n\\\\nAnd make sure, I understand the complexity and interwovenness of the current problem.  My dataset is 34000 R\\\\u0026D projects signed and executed in Europa.\\\\n\\\\n\\\\n\\\\nMy task is to apply AI to define the city of the future. Let\\'s start. \\\\n\\\\n\\\\n#code_in_real_time \\\\n#graphtheory  \\\\n#3D_Graphs\\\\n#jupyterlab \\\\n#pythonprogramming \\\\n#artificialintelligence intelligence\\\\n#real_world \\\\n#sbert \\\\n#vocabulary \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "1607",
    "uploadDate": "2021-06-29",
    "thumbnail_url": "https://i.ytimg.com/vi/vAMNdhSuhHw/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6nMceLIVwBo",
    "title": "Dimension reduction: UMAP to densMAP JupyterLab w/ PyTorch SBERT visualization (SBERT 18)",
    "tags": "film, udost",
    "scraped_at": 1685113838.1556005,
    "genre": "Science",
    "views": "966",
    "desc": "UMAP is a general purpose manifold learning and dimension reduction algorithm, which includes densMAP to preserve local density of your data.\\\\n\\\\nExperience the implications of applying densMAP to sentence embedding with SBERT, given real time coding of embedding 4000 sentences with PyTorch in JupyterLab.\\\\n\\\\nUnsupervised learning for sentence embedding has been performed with SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. \\\\n\\\\nFrom HuggingFace the DistilRoBERTa-base model is used. The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). On average DistilRoBERTa is twice as fast as Roberta-base.\\\\n\\\\nApply UMAP and densMAP in a real world code example. \\\\n\\\\n#sbert \\\\n#vocabulary \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\\\\n\\\\n#dimensional \\\\n#clustering \\\\n#algorithm\"",
    "lengthSeconds": "1320",
    "uploadDate": "2021-06-21",
    "thumbnail_url": "https://i.ytimg.com/vi/6nMceLIVwBo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ySTox2rdguM",
    "title": "Speed up your Cosine Similarity for SBERT sentence embeddings via Sentence Transformers (SBERT 17)",
    "tags": "film, udost",
    "scraped_at": 1685113836.8404262,
    "genre": "Science",
    "views": "1026",
    "desc": "Code shows speed improvements in COLAB for cosine similarity for SBERT:\\\\n(a) new \\\\u0026 improved pre-trained SentenceTransformer models (HuggingFace) and \\\\n(b) utilizing normalized tensors with dot product instead of cosine similarity operators for SBERT sentence embeddings.\\\\n\\\\nBeware of model specific fluctuations. Results may vary based on different use cases and will depend on variable GPU loads at different times. \\\\n\\\\n\\\\n#sbert \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "771",
    "uploadDate": "2021-06-14",
    "thumbnail_url": "https://i.ytimg.com/vi/ySTox2rdguM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=oG297Pvr5RA",
    "title": "SBERT: Semantic Textual Similarity, Clustering, Paraphrase mining, Asymmetric search (SBERT 16)",
    "tags": "film, udost",
    "scraped_at": 1685113839.2959607,
    "genre": "Science",
    "views": "1972",
    "desc": "Run code SBERT to understand (\\\\u0026 choose pre-trained models for):\\\\nA) Semantic Textual Similarity\\\\nB) Clustering \\\\nC) Paraphrase mining\\\\nD) Question-Answer Retrieval \\\\nE) Symmetric vs. Asymmetric Semantic Search\\\\nF) Retrieve \\\\u0026 Re-Rank\\\\n\\\\nFor beginners: \\\\nWhat is the difference between all those methods for semantic search?\\\\nAnd when to apply which pre-trained sentence embedding model?\\\\n- based on SentenceTransformers with BERT (HuggingFace). \\\\n- from hundred of sentences to million of sentences.\\\\n\\\\n00:00 Why Sentence Embedding\\\\n01:40 Choose pretrained model\\\\n03:22 Semantic Textual Similarity\\\\n04:00 Clustering\\\\n05:12 Paraphrase mining \\\\n08:48 Answer Question\\\\n11:10 Asymmetric Semantic Search \\\\n12:02 Complex Semantic Search \\\\n\\\\nAll credits to:\\\\nsbert.net \\\\n\\\\n#sbert \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#ml \\\\n#ai \\\\n#sbert \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "809",
    "uploadDate": "2021-06-07",
    "thumbnail_url": "https://i.ytimg.com/vi/oG297Pvr5RA/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6yPWtdgs5Sg",
    "title": "Learn SBERT Sentence Transformers: TSDAE, SimCSE and CT  #sbert #deeplearning (SBERT 15)",
    "tags": "film, udost",
    "scraped_at": 1685113839.6899862,
    "genre": "Science",
    "views": "4137",
    "desc": "Real time code for SBERT Sentence Embedding in a vector space with SBERT Transformer models, Bi-encoder Transformer models! Learn SBERT Sentence Embedding: TSDAE, SimCSE and CT.\\\\n\\\\nWith NEW pre-trained models best suited for your application. \\\\n\\\\nA) Add \\\\\"",
    "lengthSeconds": "1119",
    "uploadDate": "2021-06-01",
    "thumbnail_url": "https://i.ytimg.com/vi/6yPWtdgs5Sg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=-gAJRD6qT9k",
    "title": "What new technologies will be available in 2021 - COLAB w/ SBERT TSDAE on ERC EU  (SBERT 14)",
    "tags": "film, udost",
    "scraped_at": 1685113838.223583,
    "genre": "People",
    "views": "75",
    "desc": "Apply fine-tuning to your sentence transformers model with domain-specific learning (SBERT v1.2, TSDAE) on ERC projects delivering new tech/code in 2021.\\\\n\\\\n\\\\nAll European Research Council (ERC) projects, primarily research from top universities, which end in 2021, are downloaded (title and abstract) and examined for new complex, intertwined technology/code/insights. \\\\n\\\\nSource:\\\\nhttps://github.com/UKPLab/sentence-transformers\\\\n\\\\n#deeplearning \\\\n#sbert \\\\n#machinelearningwithpython \\\\n#foresight_or_decoded_knowledge\\\\n#code_in_real_time\\\\n#colab\\\\n#design_your_own_AI\"",
    "lengthSeconds": "942",
    "uploadDate": "2021-05-27",
    "thumbnail_url": "https://i.ytimg.com/vi/"
  },
  {
    "link": "watch?v=91mYLNmkPTg",
    "title": "SBERT Sentence Embeddings - the new Google Search Engine?  #sbert (SBERT 13)",
    "tags": "film, udost",
    "scraped_at": 1685113838.2885988,
    "genre": "Science",
    "views": "336",
    "desc": "Why SBERT sentence embedding? \\\\n\\\\nWhy should you apply a pre-trained BERT model to your documents,\\\\nstart a fine-tuning with TSDAE (which is domain-specific learning)\\\\nand even visualize complex, intertwined content?\\\\n\\\\nBecause the result are amazing. \\\\nAnd maybe a building block for the new google search engine.\\\\n\\\\n#sbert \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "562",
    "uploadDate": "2021-05-23",
    "thumbnail_url": "https://i.ytimg.com/vi/91mYLNmkPTg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=kuT11gGCF6A",
    "title": "Complexity of European R&D: Asymmetric Queries w/ SBERT Sentence Transformers  (SBERT 12)",
    "tags": "BERT, SBERT, Sentence Transformer, NLP, Complex patterns",
    "scraped_at": 1685113838.3515744,
    "genre": "Education",
    "views": "51",
    "desc": "Apply asymmetric vs symmetric queries on SBERT pre-trained models for SentenceTransformers (in this video without fine-tuning!). \\\\n\\\\n\\\\nData: 330 industrial R\\\\u0026D projects, starting 2021 in Europe. Investment: 1 bn Euro.\\\\nTask: Discover complex pattern in multidisciplinary R\\\\u0026D - in all of Europe - with sentence embedding (BERT-base).\\\\n\\\\n\\\\nMeta: Extract deep knowledge from 330 project descriptions. \\\\nChallenge: Define intelligent human queries.\\\\n\\\\n#sbert \\\\n#tsdae\\\\n#nlproc \\\\n#datascience \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n\\\\nParallel semantic search algorithm\\\\npre-trained BERT models for asymmetric search algorithm\"",
    "lengthSeconds": "833",
    "uploadDate": "2021-05-19",
    "thumbnail_url": "https://i.ytimg.com/vi/kuT11gGCF6A/maxresdefault.jpg"
  },
  {
    "link": "watch?v=ZP5J0jA0ST4",
    "title": "Apply BERT vocabulary optimisation for semantic content search (SBERT 11)",
    "tags": "BERT, SBERT, Sentence Transformer, Embedding",
    "scraped_at": 1685113836.9026074,
    "genre": "Education",
    "views": "139",
    "desc": "Compare three different pre-trained BERT model from HuggingFace regarding their vocabulary. \\\\n\\\\n\\\\nValidate your vocabulary to find suitable BERT models for SentenceTransformer. \\\\n\\\\n#sbert \\\\n#vocabulary \\\\n#nlproc \\\\n#datascience \\\\n#dataanalytics   \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\\\\n#code \\\\n#code_in_real_time\"",
    "lengthSeconds": "366",
    "uploadDate": "2021-05-14",
    "thumbnail_url": "https://i.ytimg.com/vi/ZP5J0jA0ST4/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XqdPKTzduB0",
    "title": "SBERT: Apply Asymmetric Semantic Search w/ Sentence Transformers  #sbert (SBERT 10)",
    "tags": "SBERT, BERT, BERT Transformer models, HuggingFace",
    "scraped_at": 1685113838.4176,
    "genre": "Science",
    "views": "632",
    "desc": "JupyterLab w/ Python and HuggingFace pre-trained Transformer models .\\\\nFind semantic close-by sentences with SentenceTransformers.\\\\n\\\\nDesign your own AI. \\\\nWatch real time coding.\\\\n\\\\n#sbert\\\\n#nlproc \\\\n#datascience \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\"",
    "lengthSeconds": "368",
    "uploadDate": "2021-05-10",
    "thumbnail_url": "https://i.ytimg.com/vi/XqdPKTzduB0/maxresdefault.jpg"
  },
  {
    "link": "watch?v=XW9UeHXB03s",
    "title": "Apply SBERT Sentence Transformers TSDAE: Semantic Content of 300 R&D projects    (SBERT 9)",
    "tags": "film, udost",
    "scraped_at": 1685113836.9646072,
    "genre": "People",
    "views": "390",
    "desc": "Compare performance of BERT, SBERT und TSDAE domain-specific sentence embedding training for cosine similarity of high-dimensional sentence embeddings in a topological space. For 300 EU R\\\\u0026D projects.\\\\n\\\\nSemantic content :: AI enhancements :: domain-specific SentenceTransformer training \\\\n\\\\n#sbert \\\\n#tsdae\\\\n#nlproc \\\\n#datascience \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\\\\n#code_your_own_AI\\\\n#SentenceTransformers\"",
    "lengthSeconds": "642",
    "uploadDate": "2021-05-07",
    "thumbnail_url": "https://i.ytimg.com/vi/XW9UeHXB03s/maxresdefault.jpg"
  },
  {
    "link": "watch?v=O9w7zSc3-Ao",
    "title": "Apply SBERT TSDAE domain-specific training for Sentence Transformers #sbert (SBERT 8)",
    "tags": "SBERT, BERT, Sentence Embedding, NLP",
    "scraped_at": 1685113838.4846005,
    "genre": "Science",
    "views": "562",
    "desc": "New SBERT TSDAE Sentence Transformer code for domain-specific training, on COLAB. Start w/ pre-trained transformer models: Transformer-based Sequential Denoising Auto-Encoder (SBERT TSDAE).\\\\n\\\\n@article{wang-2021-TSDAE,\\\\n    title = \\\\\"",
    "lengthSeconds": "728",
    "uploadDate": "2021-05-03",
    "thumbnail_url": "https://i.ytimg.com/vi/O9w7zSc3"
  },
  {
    "link": "watch?v=nGoi8lV1vew",
    "title": "125 arxiv pre-prints (full text): EXTRACT CONTENT clusters in 3D visualization",
    "tags": "film, udost",
    "scraped_at": 1685113838.5475767,
    "genre": "People",
    "views": "135",
    "desc": "You have pytorch running on your JupyterLab, downloaded a BERT model from Huggingface and designed a sentence transformer. \\\\n\\\\nTo improve your content extraction of eg. 125 arxiv pre-prints in FULL TEXT:\\\\n take the extra effort to delete all references in this full text. \\\\n\\\\nSince references are standardized, thousands of them will \\\\\"",
    "lengthSeconds": "74",
    "uploadDate": "2021-04-30",
    "thumbnail_url": "https://i.ytimg.com/vi/nGoi8lV1vew/maxresdefault.jpg"
  },
  {
    "link": "watch?v=YtI5YJjd4Ns",
    "title": "Arxiv Cluster of 125 pre-prints w/ Topic \"Graph Neural Networks\" visualized in 3D AI PyTorch Jupyter",
    "tags": "AI, PyTorch, JupyterLab, SBERT, Sentence Transformer, 3D, Visualization",
    "scraped_at": 1685113837.0306084,
    "genre": "Science",
    "views": "158",
    "desc": "Last video focussed on 23 thematic clusters we get, when embedding all 45.500 sentences of the most current 125 arxiv pre-prints on GNN, with a HuggingFace BERT model and design a sentence embedding. \\\\n\\\\nNow we take one of those clusters, the one on \\'GNN models\\', and subcluster it further.\\\\n\\\\nWe get 4 subclusters on machine translation, on knowledge graphs and on convolutional graph networks and gated NN. \\\\n\\\\nEach subcluster incorporates between 600-700 sentences, so a rough summary can be deducted. \\\\n\\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#insight \\\\n#umap \\\\n#algebraic_topology\"",
    "lengthSeconds": "242",
    "uploadDate": "2021-04-27",
    "thumbnail_url": "https://i.ytimg.com/vi/YtI5YJjd4Ns/maxresdefault.jpg"
  },
  {
    "link": "watch?v=WmTtGE2uFuc",
    "title": "New content search algorithm for multi-documents    w/ pytorch JupyterLab HuggingFace UMAP",
    "tags": "film, udost",
    "scraped_at": 1685113837.091608,
    "genre": "People",
    "views": "78",
    "desc": "Deep learning example w/ pytorch on JupyterLab for multi-document content analysis. \\\\n\\\\n\\\\n1. My multi-document: 125 pre-print from arxiv CS.CL on graph neural networks (full text)\\\\n2. My manifold of code: HuggingFace BERT models, design a sentence embeddings model (based on SBERT :: https://www.sbert.net/), high-dim clustering and 3D visualization\\\\n3. My approach: apply all 45.500 sentences to my manifold  of code, visualize emerging clusters and select specific clusters for further subclustering (a highly selective mode of NEW complex content search algorithm)\\\\n4. Benefits: topologically comparable to word2vec on a higher complexity manifold with semantic encoding on sentence level.\\\\n\\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#algebraic_topology\"",
    "lengthSeconds": "189",
    "uploadDate": "2021-04-23",
    "thumbnail_url": "https://i.ytimg.com/vi/WmTtGE2uFuc/maxresdefault.jpg"
  },
  {
    "link": "watch?v=9EqZ0Rxn50o",
    "title": "125 arxiv PDFs in 3D Graph w/ Semantic Segmentation w/ #pytorch  #bert  (SBERT 7)",
    "tags": "film, udost",
    "scraped_at": 1685113837.1525812,
    "genre": "Science",
    "views": "126",
    "desc": "Deep Learning in pytorch on Graph networks. JupyterLab w/ python example. Machine Learning example. \\\\n\\\\nImagine to visualize the content of the latest 125 publications (pre-prints) of arxiv pre-print server CS.CL on GNNs. \\\\n\\\\nEpisode 14. \\\\n\\\\n1. Download full text of 125 publications (about 45.500 sentences)\\\\n2. Download a HuggingFace BERT model and design your own sentence embedding model (w/ SBERT).\\\\n3. Reduce dimensionality of sentence embeddings to 70 dim.\\\\n4. Cluster all 45.500 sentences with HDBSCAN (see github rep).\\\\n5. Apply networkX and plotly code sequences to visualize in 3D.\\\\n6. 23 thematic cluster emerge from your AI \\\\n7. Analyze the emerging  clusters and its dependabilities on different boundary conditions. \\\\n8. The thematic clustering is happening in a certain way (discover yourself).\\\\n\\\\n#machinelearningwithpython \\\\n#deeplearning \\\\n#python \\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#umap \\\\n#insight \\\\n#umap \\\\n#algebraic_topology\\\\n\\\\nDeep learning on pytorch in JupyterLab.\\\\nSentence embedding with HuggingFace models (eg BERT_large).\\\\nCluster algorithm with noise.\"",
    "lengthSeconds": "261",
    "uploadDate": "2021-04-18",
    "thumbnail_url": "https://i.ytimg.com/vi/9EqZ0Rxn50o/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qM2I--HizWk",
    "title": "Graph Theory 3D visualize content of a document in 3D Python example",
    "tags": "Graph Theory, AI, NLP, NLProc, SBERT, Sentnece Transformers, 3D, visualization, Graph",
    "scraped_at": 1685113837.2106078,
    "genre": "Science",
    "views": "380",
    "desc": "Classical GRAPH theory applied to visualize content of a document with 1700 sentences.  3D Graph visualization with networkX and plotly. \\\\nA python example in JupyterLab on Win10. \\\\n\\\\n#Python\\\\n#AI\\\\n#GraphTheory\\\\n#NetworkX\\\\n#Plotly\"",
    "lengthSeconds": "297",
    "uploadDate": "2021-04-14",
    "thumbnail_url": "https://i.ytimg.com/vi/qM2I"
  },
  {
    "link": "watch?v=MAnROXO_bnU",
    "title": "Limitation of cosine similarity on SBERT  #deeplearning  #python  #pytorch  (SBERT 6)",
    "tags": "BERT, NLP, Sentence Embedding, Topology",
    "scraped_at": 1685113838.6156008,
    "genre": "Education",
    "views": "2114",
    "desc": "Can you rely on cosine-similarity in the embedded space for sentence embeddings? Done with sentence transformers based on BERT or other pre-trained HuggingFace models?\\\\n\\\\nWe code a cell comparing all sentences to a reference sentence (from within a set of 1700 sentences and a free defined question/sentence). \\\\n\\\\nEmbedding space: Given the high dimensional sentence embedding of current BERT models, we do not reduce the complexity of this artificial topological space, but calculate the cosine_similarity in all embedding space dimension between all sentences of the document.\\\\n\\\\nWe examine two pre-trained models: a paraphrase model and RoBERTa. \\\\n\\\\nThe results will surprise you.\\\\n\\\\n#pytorch \\\\n#cosinesimilarity \\\\n#sbert \\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight\"",
    "lengthSeconds": "1266",
    "uploadDate": "2021-04-11",
    "thumbnail_url": "https://i.ytimg.com/vi/MAnROXO_bnU/maxresdefault.jpg"
  },
  {
    "link": "watch?v=NxnIpFmcW1I",
    "title": "AI visualizes insight from Accenture's TechVision 2021 (SBERT 5)",
    "tags": "BERT, Sentence Embedding, Neural Networks, SBERT",
    "scraped_at": 1685113838.679597,
    "genre": "Education",
    "views": "76",
    "desc": "An AI explores a tech vision document. AI induced insights. \\\\nAugment your human tech vision with AI.\\\\n\\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#computerscience \\\\n#SentenceTransformer\\\\n#ai \\\\n#networkx \\\\n#plotly \\\\n#3dvisualization \\\\n#visualization \\\\n#3danimation\"",
    "lengthSeconds": "474",
    "uploadDate": "2021-04-08",
    "thumbnail_url": "https://i.ytimg.com/vi/NxnIpFmcW1I/maxresdefault.jpg"
  },
  {
    "link": "watch?v=6vliwxz6j5k",
    "title": "Design your own SciBERT sentence embedding model and explore Deloitte's TechTrends2021 (SciBERT)",
    "tags": "BERT, SBERT, Sentence Transformer",
    "scraped_at": 1685113837.2825837,
    "genre": "Education",
    "views": "1297",
    "desc": "Code your AI with multiple HuggingFace models and different architectures of SentenceTransformers, e.g. SciBERT (BERT pre-trained on scientific text). \\\\n\\\\n\\\\nhttps://github.com/allenai/scibert\\\\n\\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#ipcc \\\\n#umap \\\\n#insight \\\\n#code_your_own_AI\\\\n#code_in_real_time\\\\n#SentenceTransformers\\\\n#AI_reads_a_document\"",
    "lengthSeconds": "1179",
    "uploadDate": "2021-04-04",
    "thumbnail_url": "https://i.ytimg.com/vi/6vliwxz6j5k/maxresdefault.jpg"
  },
  {
    "link": "watch?v=qzluv1L5wEg",
    "title": "Optimize pooling layer options of BERT transformer based sentence embedding models (SBERT 4)",
    "tags": "BERT, Sentence transformer, SBERT, Sentence Embedding, Transformer Models, NLP",
    "scraped_at": 1685113839.3619606,
    "genre": "Education",
    "views": "1154",
    "desc": "Given that BERT is based on wordpieces, aggregating different transformer layers for a word embedding gains complexity when aggregating multiple, semantic different encoded, words for a sentence embedding. The pooling layers design will alter your results for sentene embedding significantly. Check out my python code in JupyterLab!\\\\n\\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#ipcc \\\\n#umap \\\\n#insight \\\\n#pooling_layer_architecture\\\\n#sentence_transformer\\\\n#sentencetransformer_networks\\\\n#umap \\\\n#algebraic_topology\"",
    "lengthSeconds": "954",
    "uploadDate": "2021-04-01",
    "thumbnail_url": "https://i.ytimg.com/vi/qzluv1L5wEg/maxresdefault.jpg"
  },
  {
    "link": "watch?v=wEKKFZA-GqU",
    "title": "Design your own sentence transformer with SBERT (SBERT 3)",
    "tags": "AI, Python, Sentence Transformer, SBERT, BERT, JupyterLab",
    "scraped_at": 1685113839.4269872,
    "genre": "Education",
    "views": "2983",
    "desc": "Download any of the models from HuggingFace and build your own sentence transformer with that particular model. Maybe BERT_large or RoBERTa, like in this video. \\\\n\\\\nCheck out this code:\\\\nhttps://www.sbert.net/\\\\nhttps://www.sbert.net/docs/package_reference/SentenceTransformer.html\\\\nhttps://huggingface.co/models\\\\n\\\\n#python\\\\n#jupyterlab \\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#pytorch \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#insight \\\\n#layers \\\\n\\\\n#sentence_transformer\\\\n#sentence_embedding\\\\n#design_your_own_AI\\\\n#code_your_own_AI\\\\n#code_in_real_time\"",
    "lengthSeconds": "1410",
    "uploadDate": "2021-03-29",
    "thumbnail_url": "https://i.ytimg.com/vi/wEKKFZA"
  },
  {
    "link": "watch?v=hqw3D41a9FM",
    "title": "Code your AI to analyze a Tech report w/ multiple  pre-trained transformer models (SBERT 2)",
    "tags": "AI, BERT, Transformer models, NLP, NLProc, Python, JupyterLab",
    "scraped_at": 1685113838.7486012,
    "genre": "Science",
    "views": "140",
    "desc": "Apply different pretrained transformer models for sentence embeddings of a given document. Empirical data from experimental set-ups of coding your own AI system, on a home PC with JupyterLab /Python. \\\\n\\\\n\\\\nBERT (https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\\\\nand \\\\nRoBERTa  (https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)\\\\nin our example.\\\\n\\\\nUnderstand the different perspectives an AI will discover, given differently trained transformer models from sentence-transformers (https://www.sbert.net/).\\\\n\\\\n#pythonai \\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#ipcc \\\\n#umap \\\\n#insight \\\\n#pooling_layer_architecture\\\\n#sentence_transformer\\\\n#sentencetransformer_networks\\\\n#umap \\\\n#algebraic_topology\\\\n#SentenceTransformer\\\\n#bert \\\\n#sentence_embedding\\\\n#umap \\\\n#networkx \\\\n\\\\n\\\\nby the way: sentence-transformer now available in v1.0 (https://github.com/UKPLab/sentence-transformers/releases/tag/v1.0.2)\"",
    "lengthSeconds": "810",
    "uploadDate": "2021-03-25",
    "thumbnail_url": "https://i.ytimg.com/vi/hqw3D41a9FM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=tYnuE4_RALo",
    "title": "AI code to analyze a Tech Report:Deloitte Tech Trends 2021 (SBERT 1)",
    "tags": "AI, NLProc, Python, JupyterLab, BERT, Transformers",
    "scraped_at": 1685113839.4919605,
    "genre": "Science",
    "views": "182",
    "desc": "Augment your coded artificial intelligence for semantic analysis. With the aim to decipher interlinked semantic topics within a set of documents. \\\\n\\\\nEmpirical reflections from a real world application of self-coded AI: let your AI present the content of a tech report (100 pages) to you. Will it be able to deconstruct crucial topics? \\\\n\\\\nWithout you training the ML system? \\\\nOn Win10 Home edition, without CUDA cores acceleration of Nvidia GPU for deep learning? \\\\nEnjoy. \\\\n\\\\nMy code segments: \\\\nA   sentence-transformers - sentence embedding w/ BERT (https://github.com/UKPLab/sentence-transformers)\\\\nB   BERT (https://github.com/google-research/bert)\\\\nC   pretrained transformer models - HuggingFace (https://huggingface.co/)\\\\nD   pytorch (https://pytorch.org/)\\\\nE   umap - model manifold with a fuzzy topological structure (https://github.com/lmcinnes/umap)\\\\nF   HDBSCAN clustering algorithm (https://github.com/scikit-learn-contrib/hdbscan)\\\\nG   networkX - network analysis in python (https://networkx.org/)\\\\n\\\\n\\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#complex \\\\n#umap \\\\n#insight \\\\n#pooling_layer_architecture\\\\n#sentence_transformer\\\\n#sentencetransformer_networks\\\\n#umap \\\\n#algebraic_topology \\\\n\\\\n#code_in_real_time \\\\n#sentencetransformers\\\\n#sentenceembedding\\\\n#code_your_own_AI\"",
    "lengthSeconds": "812",
    "uploadDate": "2021-03-21",
    "thumbnail_url": "https://i.ytimg.com/vi/tYnuE4_RALo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BGclv3PF0WM",
    "title": "COMPARE Deloitte to Accenture on TechTrends2021 -  3D visualization (UMAP and HDBSCAN)",
    "tags": "AI, NLProc, BERT, 3D, Visualization",
    "scraped_at": 1685113838.8165998,
    "genre": "Science",
    "views": "90",
    "desc": "Code our own AI and analyse 3K sentences of Accenture\\'s and Deloitte\\'s Tech21 for wealth creating companies. \\\\n\\\\nStart with BERT (Transformer based) pre-trained models, apply sentence embedding, utilize mathematical topology insights (UMAP) and cluster sentences with a noise-aware cluster algorithm (HDBSCAN). \\\\n\\\\nVisualize each sentence in 3D to:\\\\n1) see different topic configurations between the documents (Deloitte / Acccenture)\\\\n2) analyse the thematic cluster.\\\\n\\\\nDo Accenture and Deloitte see the future developments in technology democratisation and zero-trust on equal paths? How to integrate technological capabilities, available to different team members, within an interconnected ecosystem? \\\\n\\\\nA self-coded AI (JupyterLab) provides first insights. Episode 5. \\\\n\\\\n\\\\n#AI\\\\n#3D\\\\n#Python\\\\n#Deloitte\\\\n#UMAP\\\\n#Accenture\"",
    "lengthSeconds": "1596",
    "uploadDate": "2021-03-17",
    "thumbnail_url": "https://i.ytimg.com/vi/BGclv3PF0WM/maxresdefault.jpg"
  },
  {
    "link": "watch?v=BjRaU68vWUo",
    "title": "12 CPUs + 1.2GB ML + 3bn words + Algebraic Topology = Tech Insights (SparkNLP w/ UMAP)",
    "tags": "AI, BERT, Python, NLProc",
    "scraped_at": 1685113838.8806007,
    "genre": "Science",
    "views": "55",
    "desc": "No CUDA cores but 12 CPU cores and you want to experience the power of your personal AI on your PC? Here is the code you need. \\\\n\\\\n12 CPU threads at 98%, the latest code in algebraic topology, 1.24 GB of pre-trained pattern recognition from 3.3 B words, applied to Accenture\\'s TechVision 2021 in less then 10 minutes, with an interactive 3D visualisation in JupyterLab?  On your gaming PC,  Win10 Home edition? Enjoy!\\\\n\\\\n#code_in_real_time\\\\n#pythonprogramming \\\\n#ai \\\\n#sbert\\\\n#nlproc \\\\n#nlptechniques \\\\n#clustering \\\\n#semantic \\\\n#bert \\\\n#climatechange \\\\n#3danimation \\\\n#3dvisualization \\\\n#topologicalspace \\\\n#deeplearning \\\\n#machinelearningwithpython \\\\n#pytorch \\\\n#sentence \\\\n#embedding \\\\n#umap \\\\n#insight \\\\n#umap \\\\n#algebraic_topology\"",
    "lengthSeconds": "1419",
    "uploadDate": "2021-03-14",
    "thumbnail_url": "https://i.ytimg.com/vi/BjRaU68vWUo/maxresdefault.jpg"
  },
  {
    "link": "watch?v=a9Un8dqZRV8",
    "title": "Accenture's TechVision202 explored by my AI system (SparkNLP and BERT transformer for NLProc)",
    "tags": "AI, Python, BERT, Transformers, Artificial intelligence, Example AI",
    "scraped_at": 1685113838.944575,
    "genre": "Science",
    "views": "62",
    "desc": "Imagine you code your personal AI, let it read Accenture\\'s TechVision2021 and present it to you. When you have never read the document yourself. \\\\n\\\\nWhat are the obstacles you encounter? You are good at cross words? Buckle up for your next challenge: deduct the content of a document, challenge your topic specific knowledge and achieve free-association-complexity at next level. \\\\n\\\\nA document seen through the eyes of an artificial intelligence.\\\\n\\\\n\\\\n#code_in_real_time\\\\n#Accenture2021\\\\n#AI\\\\n#Python\"",
    "lengthSeconds": "1641",
    "uploadDate": "2021-03-10",
    "thumbnail_url": "https://i.ytimg.com/vi/a9Un8dqZRV8/maxresdefault.jpg"
  },
  {
    "link": "watch?v=GtNKWxqPLqc",
    "title": "Deloitte's TechTrends2021 analyzed by artificial intelligence AI (PySpark, SparkNLP)",
    "tags": "Artificial intelligence, AI, Deloitte, 3D, BERT",
    "scraped_at": 1685113839.552987,
    "genre": "Science",
    "views": "85",
    "desc": "How does an AI structures a human document about AI? A thematic segmentation of the content of the document in 3D. \\\\n\\\\nWe take a document about AI on industrial challenges and code our own AI in order to gain further insights.\\\\n\\\\nApplying  \\'Pre-training of Deep Bidirectional Transformers for Language Understanding\\'  (BERT-large: 24-layer, 1024-hidden, 16-heads, 340M parameters) for sentence embedding, running sparkNLP on 12 CPU-threads, construct a multitude of topological spaces and end up with a thematic segmentation of the content of the document. \\\\n\\\\nAll on a home PC. Your personal private AI. \\\\n\\\\n#code_in_real_time\\\\n#Deloitte\\\\n#AI\"",
    "lengthSeconds": "752",
    "uploadDate": "2021-03-07",
    "thumbnail_url": "https://i.ytimg.com/vi/GtNKWxqPLqc/maxresdefault.jpg"
  }
]